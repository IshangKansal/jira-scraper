{"issue_id": "NUTCH-580", "project": "NUTCH", "title": "Remove deprecated hadoop api calls (FS)", "status": "Closed", "priority": "Minor", "reporter": "Sami Siren", "assignee": "Sami Siren", "created": "2007-11-21T16:47:15.731+0000", "updated": "2009-04-10T12:29:05.430+0000", "description": "There are quite a lot of calls to deprecated hadoop api functionality. Following patch will take care of fs related ones.", "comments": ["I've been using your patch for a while now and it looks to work fine.\n\nDo you think you will commit it ?", "Committed.", "Integrated in Nutch-trunk #333 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/333/])", "Integrated in Nutch-Nightly #334 (See [http://lucene.zones.apache.org:8080/hudson/job/Nutch-Nightly/334/])", "closing issues for released version"], "tasks": {"summary": "Remove deprecated hadoop api calls (FS)", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove deprecated hadoop api calls (FS)"}, {"question": "What is the main context?", "answer": "There are quite a lot of calls to deprecated hadoop api functionality. Following patch will take care of fs related ones."}]}}
{"issue_id": "NUTCH-581", "project": "NUTCH", "title": "DistributedSearch does not update search servers added to search-servers.txt on the fly", "status": "Closed", "priority": "Minor", "reporter": "Rohan Mehta", "assignee": null, "created": "2007-11-21T16:57:15.820+0000", "updated": "2011-06-08T21:34:19.466+0000", "description": "DistributedSearch client updates the search servers added to the search-servers.txt file on the fly. \nThis patch will updates the search servers on the fly and the client does not need a restart.", "comments": ["This is something that we are currently using at Visvo.  It is a simple patch but it allow adding and removing of search servers on the fly without having to shut down the search website.  I am +1 for this patch as it has been in production for us for a few months now.  If nobody objects I would like to commit this in the next day or so.", "+1 as the first step ...\n\nI think we should extend this support to include on-the-fly update in other scenarios (e.g. detecting updated indexes on search servers, requesting reload of backends from the frontend, etc).\n\nSolr implements a good model for searcher reload. When an IndexSearcher needs to be reloaded, Solr prepares a new searcher in the background and warms it up by running the topN queries, and only when the new searcher is \"warm\" then it atomically switches the searchers, and closes the old one. IMHO we should eventually implement a model like this.", "I was talking with Docagan about this very thing.  He has some ideas for a framework that would support monitoring and maintenance of search servers.  Similar to a hadoop or hbase model of master and slaves where you could add and remove search servers, change out indexes, etc.  Maybe we can all start designing this out together fairly soon.  I think it would be a very useful addition, especially for those of us managing large numbers of search servers.", "Update patch.  Passes all unit tests, including those on linux.  Renames DistributedSearchTest to TestDistributedSearch to be consistent with our current build file for unit tests.", "This patch conflicts with my patch in NUTCH-442 (which I really want to commit sometime) but that's my problem :). So +1 from me.\n\n", "Patch committed.  This patch check modified time on search-servers.txt file and automatically reloads if changed.  This allows added and removing search servers on the fly.Thanks Rohan.", "Integrated in Nutch-Nightly #285 (See [http://lucene.zones.apache.org:8080/hudson/job/Nutch-Nightly/285/])"], "tasks": {"summary": "DistributedSearch does not update search servers added to search-servers.txt on the fly", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "DistributedSearch does not update search servers added to search-servers.txt on the fly"}, {"question": "What is the main context?", "answer": "DistributedSearch client updates the search servers added to the search-servers.txt file on the fly. \nThis patch will updates the search servers on the fly and the client does not need a restart."}]}}
{"issue_id": "NUTCH-582", "project": "NUTCH", "title": "Add missing type parameters", "status": "Closed", "priority": "Minor", "reporter": "Sami Siren", "assignee": "Sami Siren", "created": "2007-11-21T18:47:34.578+0000", "updated": "2009-04-10T12:29:00.854+0000", "description": "Hadoop 0.15 added possibility to use type parameters with several interfaces and makes it easier to use correct types in Mappers, Reducers et al. in addition to improved readability. Following patch will add type parameters to Mappers, Reducers, OutputCollectors, MapRunnables, InputFormats and OutputFormats.", "comments": ["I believe this has been addressed as a part of patches related to Hadoop upgrades. If that's the case, I'm going to close this issue.", "yep, all of this has been committed"], "tasks": {"summary": "Add missing type parameters", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add missing type parameters"}, {"question": "What is the main context?", "answer": "Hadoop 0.15 added possibility to use type parameters with several interfaces and makes it easier to use correct types in Mappers, Reducers et al. in addition to improved readability. Following patch w"}]}}
{"issue_id": "NUTCH-583", "project": "NUTCH", "title": "FeedParser empty links for items", "status": "Closed", "priority": "Major", "reporter": "Enis Soztutar", "assignee": "Enis Soztutar", "created": "2007-11-27T14:59:43.329+0000", "updated": "2019-10-13T22:35:26.856+0000", "description": "FeedParser in feed plugin just discards the item if it does not have <link> element. However Rss 2.0 does not necessitate the <link> element for each <item>. \nMoreover sometimes the link is given in the <guid> element which is a globally unique identifier for the item. I think we can search the url for an item first, then if it is still not found, we can use the feed's url, but with merging all the parse texts into one Parse object. ", "comments": ["pushing this to 1.1", "- pushing this out per http://bit.ly/c7tBv9", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "FeedParser empty links for items", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "FeedParser empty links for items"}, {"question": "What is the main context?", "answer": "FeedParser in feed plugin just discards the item if it does not have <link> element. However Rss 2.0 does not necessitate the <link> element for each <item>. \nMoreover sometimes the link is given in t"}]}}
{"issue_id": "NUTCH-584", "project": "NUTCH", "title": "urls missing from fetchlist", "status": "Closed", "priority": "Major", "reporter": "Ruslan Ermilov", "assignee": "Andrzej Bialecki", "created": "2007-11-28T15:57:01.880+0000", "updated": "2009-04-10T12:29:04.712+0000", "description": "When generating an initial set of ~100k URLs for fetching, I've noticed that some URLs are missing from the fetchlist.\nThe test case below has only 2 URLs, and I've used the FreeGenerator tool instead of the standard inject/generate\nthat saves me time when experimenting. It doesn't matter if I run it in clustered or local mode.\n\nSomehow only one of two URLs ends up in the fetchlist:\n\n$ rm -rf segments\n$ cat urls/x\nhttp://tkd.ru/\nhttp://t-f.ru/\n$ nutch org.apache.nutch.tools.FreeGenerator urls segments\n$ nutch readseg -dump segments/* xxx -nocontent -noparse -noparsedata -noparsetext -nofetch\nSegmentReader: dump segment: segments/20071128195720\nSegmentReader: done\n$ cat xxx/dump\n\nRecno:: 0\nURL:: http://tkd.ru/\n\nCrawlDatum::\nVersion: 5\nStatus: 0 (unknown)\nFetch time: Wed Nov 28 19:57:20 GMT 2007\nModified time: Thu Jan 01 00:00:00 GMT 1970\nRetries since fetch: 0\nRetry interval: 0.0 days\nScore: 1.0\nSignature: null\nMetadata: null\n\n$ \n", "comments": ["Thank you for the simple test case! I believe I found the problem - it was quite tricky.\n\nDuring the final step of generation we partition the urls by host, and then sort them by a simple hash(url). At least that was the intention - HashComparator class produces many collisions in hash values, which is ok - we only need to roughly randomize the urls within partition. However, since this comparator is used during the sorting of data submitted to reduce() these collisions caused several urls to become \"equal\". Consequently Hadoop did what it was meant to do - collected all values that matched \"equal\" keys under a single iterator, and then invoked Reducer.reduce() using only a single key picked up from all \"equal\" keys ... so all other \"equal\" urls were dropped. At this point also we were getting multiple records in the output fetchlist (because the default IdentityReducer produced as many output records as many there were input values to reduce - all with the same key!), with the final result being that several CrawlDatum-s coming originally from different urls were stored using the same url ...\n\nThis is a serious bug, which may have caused numerous problems in fetchlist generation, such as missing urls, multiple fetches of the same url, or CrawlDatum-s paired with wrong urls.", "Patch to address this problem - your test case executes fine with this patch. Please test.", "Andrzej,\n\nNice analysis :)\n\n+1 for the patch (it seems you forgot a System.out.println there, though)", "Andrzej,\n\nI've tested your patch both with a simple test case mentioned above, and on real data (~100k urls). It now works as expected, thank you!", "Patch applied (sans System.out.println ;) ) in rev. 612505. Thanks for the review and testing!"], "tasks": {"summary": "urls missing from fetchlist", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "urls missing from fetchlist"}, {"question": "What is the main context?", "answer": "When generating an initial set of ~100k URLs for fetching, I've noticed that some URLs are missing from the fetchlist.\nThe test case below has only 2 URLs, and I've used the FreeGenerator tool instead"}]}}
{"issue_id": "NUTCH-585", "project": "NUTCH", "title": "[PARSE-HTML plugin] Block certain parts of HTML code from being indexed", "status": "Open", "priority": "Major", "reporter": "Andrea Spinelli", "assignee": "Sebastian Nagel", "created": "2007-11-29T11:13:19.135+0000", "updated": "2025-07-09T20:25:54.706+0000", "description": "We are using nutch to index our own web sites; we would like not to index certain parts of our pages, because we know they are not relevant (for instance, there are several links to change the background color) and generate spurious matches.\n\nWe have modified the plugin so that it ignores HTML code between certain HTML comments, like\n<!-- START-IGNORE -->\n... ignored part ...\n<!-- STOP-IGNORE -->\n\nWe feel this might be useful to someone else, maybe factorizing the comment strings as constants in the configuration files (say parser.html.ignore.start and parser.html.ignore.stop in nutch-site.xml).\n\nWe are almost ready to contribute our code snippet.  Looking forward for any expression of  interest - or for an explanation why waht we are doing is plain wrong!\n\n", "comments": ["A more general solution is needed.  This solution should not rely on apriori marked-up content as in your example, but should automatically recognize things like footers, sidebars, repeating navigation and other elements, etc.\n\nI am sure there are PhD thesis out there on this topic...\n", "I absolutely agree that a more general solution is needed; however, I think that some of the Nutch current users might benefit from a quick fix.\n\nIf there is no opposition, I could submit a patch (less than 20 lines)\n\nOn the other hand,anybody thinks that blocking selected portions of text could pose serious architectural or stability risks?\n\nAbout the more general solution, do you think there is a viable path from here to there?\n\n-- andrea\n", "Simplest path forward... that I can think of:\n\n1) Add a new indexing plugin extension-point for filtering page content.\n2) Put your \"apriori marked-up content\" exclusion logic into a plugin.\n3) Someone else figures out a more general-purpose solution later, and swaps out your plugin at that time.\n\nErgo, you generalize the interface, and lazy-load the more general implementation. :-)\n", "Hello All\n\nWe are implementing a search engine based on Lucene/Nutch and we are also facing problems with caching. Would it be possible for you to help me out on this issue and provide me a code snippet?", "Hi,\nIs it possible for you to share the code with me??\nI seem to have found a use of the facility you wish to add to Nutch.\nI'm using a content management system called Infoglue to create my website.\nThe pages I create for my site have a fixed template containing header, footer and a menu system.\nI wish that Nutch should index the template content only for the home page and I want it to index just the relevant (non-template) content on the inner pages.\n\nSo please share your idea and/or code.\nDetails of the implementation are appreciated. \nSo far I have just been a naive Nutch user. \n\nThanks a lot.\nWinz\n\nQuoted from: \nhttp://www.nabble.com/-jira--Created%3A-%28NUTCH-585%29--PARSE-HTML-plugin--Block-certain-parts-of-HTML-code-from-being-indexed-tp14023775p14023775.html\n\n", "Yes, I'd be glad about that.\n\nThere are some caveats, though:\n\n1. I worked on a very old version of nutch (0.7.2)\n2. I have to dig in my sources to find our patch, because it happened a \nlot of time ago\n\nWe have a week long of demos, I will write back at the end of the \nworking week\n\nHi\n  Andrea\n\n-- \nAndrea Spinelli - team QUALITY\nemail: andrea.spinelli@imteam.it\nphone: +39-035-636029\nfax: +39-035-638129\nsurface-mail: Via Sigismondi 40, 24018 Villa d'Alme', BG\n--\nQuesto messaggio è confidenziale; ai sensi del D.P.R. 44/314159/2718 \n01/04/2009 non puoi pubblicarlo o inoltrarlo e non potrai mai\npiù utilizzare nessuna delle parole italiane in esso presenti.\nSe lo ricevi per errore, spruzzalo con spray al peperoncino,\ncancellalo, formatta il tuo hard disk e poi scrivi una cartolina\nall'indirizzo sopra indicato avvisandoci.\n\n\n\n", "Hi Andrea,\n\nI hope your week of demo's went well. I to would be interested in this code as I would like to look at extending to it be slightly more generic allowing for regular expression matches or an xpath like model (the plan is still formulating). From the web crawler view it would be a hard one to get right but we have about 26 sites that are will know to us that we wish to crawl and have common blocks that we wish to remove which a configurable version of your code may achieve.\n\nLook forward to see your patch\n\n\nRegards,\n\nDavid Stuart", "Hi Andrea,\n\nI would also be interested in the code.\n\nThank you.\n\nRich", "We use Solr/Nutch on our corporate web site and are very happy with the results.  Thank you.  We have struggled with something similar to NUTCH-585 for a few months now.\n\nAlthough it is different from the original intent, here's a quick/short patch that might help get this feature going again.\n\n\nh4.Intended use:\n- Let's assume you're crawling a set of internal web sites and would like to exclude certain HTML fragments (from indexing) like the navigation and other common content.\n- If these fragments are contained in DIVs with IDs like \"menuNav\", \"footerNav\", etc., then you can now add a new property to nutch-site.xml to exclude these DIVs.\n- If you don't set this property, the normal behavior remains (backward compatible)\n{code:xml}\n<property>\n  <name>parser.html.divIDsToExclude</name\n  <value>account_menu_container,footer_menu_container,legal,main_menu_container</value>\n  <description>\n  A comma-delimited list of DIV IDs whose content will not be indexed.  Use this to tell\n  the HTML parser to ignore, for example, site navigation text.\n  Note that DIVs with these IDs, and their children, will be silently ignored by the parser\n  so verify the indexed content with Luke to confirm results.\n  </description>\n</property>\n{code}\n\n\nh4.Inclusion/growth:\n- This code was written against nutch 1.2 and is backward compatible in that the new behavior is only present if configured.\n- In future, it might be good to have different \"strategy patterns\" for how exclusions are determined; some might need algorithmic detection (whole web crawls), others might prefer jquery-selectors for HTML fragments, etc.\n\n\n\nBest regards,\n\n-h\n\nHira, N.R.  (Jostens, Inc.)", "The patch provided by N. Hira works as advertised on Nutch 1.2.", "Thanks for mentioning Wim. This patch can be useful for a quick solution. Perhaps it can be incorporated in a Nutch release.", "I can also confirm that the patch works on Nutch 1.3.\n\nHowever, it didn't work for my use-case as I need to filter a diverse set of tag\nbased on different attributes. Besides I needed the links from the filtered area \nwhich did not happen. \n\nSo I altered Hira's patch and I am publishing my work here.\n\nThis is the new changed property.\n{code:xml} \n<property>\n  <name>parser.html.NodesToExclude</name>\n  <value>table;summary;header|div;id;navigation</value>\n  <description>\n  A list of nodes whose content will not be indexed separated by \"|\".  Use this to tell\n  the HTML parser to ignore, for example, site navigation text.\n  Each node has three elements: the first one is the tag name, the second one the\n  attribute name, the third one the value of the attribute.\n  Note that nodes with these attributes, and their children, will be silently ignored by the parser\n  so verify the indexed content with Luke to confirm results.\n  </description>\n</property>\n{code} \n\nI really think this should be present in Nutch. I am available to improve the patch until it is ready for inclusion. Also I am looking for comments on how I implemented my improvements.\n\nThanks,\nRui", "Exclude Nodes Patch.", "Marked for 1.4. Thanks!", "Cool! :)\n\nAnyway, as I said before my patch extracts the link from the filtered area while Hira's patch will filter before any extraction is done.\n\nDo you think that this behavior should be configurable?", "Based on the suggestions/code above, I have created a plugin to blacklist or whitelist html elements (blacklist_whitelist_plugin.patch). This was based on the need for not indexing header/footer/navigation, so the user gets really only relevant results, e.g. even if the term shows up in the navigation.\n\nThe elements to be parsed (or not) can be defined by using CSS-like selectors. A new field called \"strippedContent\" is available in the index which can be used for searching. Links are still crawled and parsed from the \"content\" field, allowing all pages to be parsed. The full documentation is in the README.txt in the patch.", "Marking for 1.5. Needs reviewing and won't make it into 1.4", "I been using this plugin for sometime and recently i wanted to use it to extract text based on url pattern\nExample:\nfor http://x.y.z/?id=12 white-list will only look for div id=12\nfor http://x.y.z/?id=13 white-list will only look for div id=13\n\nref: http://lucene.472066.n3.nabble.com/index-blacklist-whitelist-pluign-for-multiple-set-of-urls-td3711697.html", "I like this contribution Elisabeth. Is there any way it could be updated to trunk with the following suggestions\n1) Please rename the package names to org.apache.nutch.blah.blah\n2) In your ivy.xml please change the ivy-configuration.xml to\n{code}\n  <configurations>\n      <include file=\"../../..//ivy/ivy-configurations.xml\"/>\n  </configurations>\n{code}\nThis is eclipse specific.\n3) Would it be possible to change the CHANGES.txt to package.html and store it in the lowest most folder within the java directory\n4) It would really put the cherry on top if we could get a test case scenario, this would be a big +1.\n5) I think the name is maybe a bit large... but I am fine keeping it if you think it is appropriate as it is your patch afterall.\n\nThank you for the contribution.", "20120304-push-1.6", "Hello all,\n\nI've stumbled upon this ticket in my research to achieve the stated situation: block certain html parts from being indexed.\nI understand that this plugin/patch is achieves the desired situation, only i cannot seem to understand the following:\n\n- Will this feature be implemented in nutch 1.5 (according to Julien Nioche - 28/Sep/11 11:24) or will this be implemented in 1.6 (if this is what Markus Jelsma means with his comment on 03/Apr/12 12:08)?\n\n- I found out that yesterday there was a vote concerning nutch 1.5 rc1: http://lucene.472066.n3.nabble.com/VOTE-Apache-Nutch-1-5-release-rc-1-td3913604.html. Is this a reliable source ? If so, what are the prospects upon releasing this version?\n\n- Reason that I want to know is because I want to use the giving plugin but I can also wait of the nutch 1.5 release date isnt that far away.\n\nIt would be great if someone could advice me.\nMany thanks in advance.\n\nWith kind regards,\nRoberto Gardenier", "This issue is not going to be part of Nutch 1.5 which is likely to be released very soon. However, you can download the patch and see if it works for you, the plugin builds fine for 1.4, 1.5 and the to-be 1.6-SNAPSHOT.", "Thank you for your quick reply! Much appreciated!\n\nWe are using nutch 1.4 so I will use the nutch-585-excludeNodes.patch for blocking certain html blocks. I assume that using the start en stop tags provided in the description is all we need to get things working? So we dont have to edit any config files ?\n\nKind regards,\nRoberto Gardenier", "You should take the latest patch: blacklist_whitelist_plugin.patch. It contains example config etc. Please let us know if you get it to work. Also check Rui's comment. It does not work with start/stop-tags anymore.\n\nhttps://issues.apache.org/jira/browse/NUTCH-585?focusedCommentId=13107294&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13107294", "I can confirm the plugin provided in blacklist_whitelist_plugin.patch also works for Nutch 1.5.1 without extra configuration.", "Will this patch be implemented in Nutch at all? I've seen this patch / feature request being marked from 1.4 up till 1.7 now. \nEven though the patch works with Nutch 1.5 up till 1.5.1 I wonder if this will become part of Nutch at any time, [~markus17]?", "I adapted Elisabeth Adler's plugin for use with Nutch 2.1 and added two small features:\n\n* the ability to protect certain URLs from filtering\n* the ability to configure the field where the filtered content is stored (overwriting the _text_ field by default)\n\nI didn't immediately realize the common practice is creating a patch, so I put my stuff on GitHub: https://github.com/veggen/nutch-element-selector\nbut if anyone cares about including this, I will gladly make a patch as well (and change package names, rename the plugin to it's original name etc).", "Hi Tomic,\n\nIf you are using SVN, please see here for instructions (https://wiki.apache.org/nutch/Becoming_A_Nutch_Developer#Step_Three:_Using_the_JIRA_and_Developing).\n\nIf not, this will be useful (http://docs.moodle.org/dev/How_to_create_a_patch) for general purposes.\n\nThanks for your contribution.", "Hi [~veggen],\n\nDid you succeed making a patch for fixing this issue for Nutch 2.*, it would be nice if this could be included as so in that version as well?", "Hi Bojan Tomic,\n\nThanks for your contribution. I have tested your plugin and it worked well. But, the creation of a new field (filtered_content) in MySQL haven't worked, the field became NULL on MySQL. I have tried to change the \"gora-mysql-mapping.xml\" and add the new field, but when i used the inject command, i have got \"org.apache.gora.util.GoraException: java.io.IOException: java.lang.NullPointerException\". Do I have to do an extra configuration? How can I make it work out?\n\nThank you.\n", "@[~lara_cardozo]\nIt's been a while since I last used that plugin, but from what I remember, there was no need to modify gora-mysql-mapping.xml. Just create a new column in the DB and configure its name in the plugin configs. I now see the README on Github is out of date as I've added a lot better CSS selector support in the meantime. It could be also that I introduced a new bug :(\nIf you could tell me exactly what you did (maybe paste your configs), I will try to replicate. What version of Nutch are you using? As far as I know, relational DB support got dropped from Gora (and thus Nutch) soon after 2.1, which was the version I used.\nPlease put future question directly on my Github issue tracker: https://github.com/kaqqao/nutch-element-selector/issues It makes it easier for me to spot it (it took me over a month to see it here... sorry for that).\nI gave up on making a patch as I don't know how to test that it works with other storages and the only storage I cared about at that time (relational DB) got dropped... Also, I figured a feature like this is maybe better kept in a plugin anyway.", "Has anyone tried the patch on Nutch 1,15?   Would like to know if it will work before installing it.", "[~dbeckstrom] I'm not sure which patch you were asking about. I used the source for the new 1.20 release and applied the patch that [~ad-eli@gmx.at] posted after an edit to the line numbers for the update to src/plugin/build.xml. It built cleanly and seems to work exactly as advertised in my tests with indexchecker."], "tasks": {"summary": "[PARSE-HTML plugin] Block certain parts of HTML code from being indexed", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "[PARSE-HTML plugin] Block certain parts of HTML code from being indexed"}, {"question": "What is the main context?", "answer": "We are using nutch to index our own web sites; we would like not to index certain parts of our pages, because we know they are not relevant (for instance, there are several links to change the backgro"}]}}
{"issue_id": "NUTCH-586", "project": "NUTCH", "title": "Add option to run compiled classes w/o job file", "status": "Closed", "priority": "Major", "reporter": "Enis Soztutar", "assignee": "Andrzej Bialecki", "created": "2007-11-30T10:35:40.675+0000", "updated": "2009-04-10T12:29:02.503+0000", "description": "bin/nutch adds nutch-*.job files under build and base directory to the classpath. However building the job file takes a long time. We have a target compile-core which builds only the core classes w/o plugins, but we need a way to run the compiled core class files. An option to bin/nutch to run the classes compiled with ant compile-core seems enough. ", "comments": ["Attached file adds -core option to bin/nutch. ", "Can someone review this ?", "+1. I think you also need to put a comment, which clarifies that this works only in the \"local\" Hadoop mode.", "bq. I think you also need to put a comment, which clarifies that this works only in the \"local\" Hadoop mode.\nagreed. This patch addresses that.  ", "Fixed in trunk, rev. 604956, with minor changes to the explanation note. Thank you!", "Integrated in Nutch-Nightly #298 (See [http://lucene.zones.apache.org:8080/hudson/job/Nutch-Nightly/298/])", "closing issues for released version"], "tasks": {"summary": "Add option to run compiled classes w/o job file", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add option to run compiled classes w/o job file"}, {"question": "What is the main context?", "answer": "bin/nutch adds nutch-*.job files under build and base directory to the classpath. However building the job file takes a long time. We have a target compile-core which builds only the core classes w/o "}]}}
{"issue_id": "NUTCH-587", "project": "NUTCH", "title": "Upgrade Nutch to use Hadoop 0.15.3 release", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2007-12-03T23:34:44.847+0000", "updated": "2009-04-10T12:29:01.869+0000", "description": "Upgrade Nutch to use the recently released hadoop 0.15.3.  There were some critical changes in hadoop between 0.15.0 and 0.15.1.  The biggest as applies to Nutch is the removal of deprecated method for default and final resources.", "comments": ["This patch updates Nutch to use Hadoop 0.15.1 from 0.15.0.  This includes removing methods that added default and final resources, now simply resources, and changing out the hadoop jar file and native libraries.", "Anybody have any issues with me committing this.  I think it would be good to add something to the changes.txt to show the removal and default and final resources.  Other than that we have this working in a fairly large cluster with no problems.", "+1 except for the docs. Please document this change in CHANGES.txt. I think we should write a short explanation of this change, especially since mapred-default.xml is also going away, and this may be confusing (suddenly properties defined there will stop working). IMHO the ful explanation is too much to put in CHANGES.txt, perhaps you should put just a warning there and refer people to a Wiki page (if such a page exists)?", "Now moving from 0.15.1 to 0.15.3.", "All unit tests passed successfully.  Ran through 3 fetch cycles.  Everything looks good.  If nobody has any objections I will commit this in the next 24 hours.", "Tested on both cygwin and linux.  All unit tests passed.  Ran through multiple search cycles without issue.  This patch adds the hadoop native library for AMD systems.", "closing issues for released version"], "tasks": {"summary": "Upgrade Nutch to use Hadoop 0.15.3 release", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade Nutch to use Hadoop 0.15.3 release"}, {"question": "What is the main context?", "answer": "Upgrade Nutch to use the recently released hadoop 0.15.3.  There were some critical changes in hadoop between 0.15.0 and 0.15.1.  The biggest as applies to Nutch is the removal of deprecated method fo"}]}}
{"issue_id": "NUTCH-588", "project": "NUTCH", "title": "Help Need", "status": "Closed", "priority": "Major", "reporter": "Teccon Ingenieros", "assignee": null, "created": "2007-12-04T16:39:54.137+0000", "updated": "2009-01-25T11:40:50.827+0000", "description": "Hello,\n\nWe are trying to index a word file, if we put the static url like (/servlet/jsp/documento.doc) it works ok, put if we try to do the same with an dinamic url that generates that file (/servlet/jsp/leerFichero.jsp&id=112) it does´t work, it does´t index our url.\nWhat can we do?\n\nRegards,\n", "comments": ["Jira is not for asking questions. You should ask your questions on nutch-user mailing list. See http://lucene.apache.org/nutch/mailing_lists.html\nClosing this issue as invalid. "], "tasks": {"summary": "Help Need", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Help Need"}, {"question": "What is the main context?", "answer": "Hello,\n\nWe are trying to index a word file, if we put the static url like (/servlet/jsp/documento.doc) it works ok, put if we try to do the same with an dinamic url that generates that file (/servlet/"}]}}
{"issue_id": "NUTCH-589", "project": "NUTCH", "title": "Hierarchical Classloaders", "status": "Open", "priority": "Minor", "reporter": "Ryan Levering", "assignee": null, "created": "2007-12-05T00:04:42.510+0000", "updated": "2025-07-09T20:25:49.411+0000", "description": "Currently the Nutch plugin classloader flattens all the jars from a plugins' dependencies and instantiates a new classloader for each plugin.  I think it would be better to create a hierarchical classloader chain.  Currently plugins can't pass objects from a common plugin to one another because the objects are created using different classloaders.  Nutch currently avoids this by only using interfaces from a common classloader to pass objects between plugins, but I can't see the harm in improving the plugin classloader.  It would require a change to PluginDescription and PluginClassLoader in order to override ClassLoader to maintain the export filter functionality that currently exists.", "comments": [], "tasks": {"summary": "Hierarchical Classloaders", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Hierarchical Classloaders"}, {"question": "What is the main context?", "answer": "Currently the Nutch plugin classloader flattens all the jars from a plugins' dependencies and instantiates a new classloader for each plugin.  I think it would be better to create a hierarchical class"}]}}
{"issue_id": "NUTCH-59", "project": "NUTCH", "title": "meta data support in webdb", "status": "Closed", "priority": "Minor", "reporter": "Stefan Groschupf", "assignee": null, "created": "2005-05-23T01:56:43.000+0000", "updated": "2008-01-22T14:18:26.860+0000", "description": "Meta data support in web db would very usefully for a new set of nutch feature that needs long life meta data. \n\nActually page meta data need to be regenerated or lookup every 30 days a page is re-fetched, in a long context web db meta data would bring a dramatically performance improvement for such tasks.\nFurthermore Storage of meta data in webdb would make a new generation of linklist generation filters possible.  \n", "comments": ["Add meta data support to webdb.", "I would like to offer my vote for Nutch-59  (+1)\n\nI do have some comments with regards to the metadata infrastructure in Nutch. Here are some of my thoughts.\n\nStoring metadata in WebDB does offer the potential for a long list of potential new uses for Nutch.\n\n- Location based Queries\n  - The topic of the page relates to what city, state, country, geospatial coordinate\n  - This page has multiple locations (list of WalMart Stores)\n  - The server is located in this country (legal domicile)\n  - The content is targeted to this geographic group (middle east, east chicago)\n  - this particular location has this list of websites associated with it (garage.com has invested in X companies located in this area)\n  - Directions to the store on the website (mapquest)\n  - List of other website/store in the area (google local)\n  ...\n\n- People and Organizations\n - whois info\n - webmaster\n - editor(s)\n - company that owns the website\n - group within the company that owns the website.\n\nThere are several other metadata classes that can be associated with a page.\n - Dublin Core (as mentioned in other Nutch requirement docs)\n - CWM - Common Warehouse Metadat - provide links for datawarehouse (datamart) information to a web page.\n - Products (Froogle, Business.com...)\n\nAs well as new forms of popular website technologies, each which contain a set of unique metadata.\n - wiki (license, topic...)\n - blog (topic, person, group...)\n - personal profiles (dating, facebook.com)\n - ontologies (dmoz, jena - owl, wordnet)\n - ...\n\nUnstructured data (the web) contains a long list of course grained classes of metadata that can be associated with each Page (artifact).\n\n  A CONCEPTUAL META-MODEL FOR UNSTRUCTURED DATA\n    http://www.tdan.com/i024fe01.htm\n\n\nThe models that persist metadata can become very complex.\n\n A UNIVERSAL PERSON AND ORGANIZATION DATA MODEL:\n THE PARTY/PARTY-RELATIONSHIP PATTERN\n    http://www.tdan.com/i021ht04.htm\n\nAs well as the repositories that persist this type of data:\n\n  Advanced Meta Data Architecture\n   http://www.tdan.com/i013fe01.htm\n\nSummary:\n- large number of types of metadata\n- metadata models can be complex\n- number of different archtectures for storing of metadata \n- persisting metadata can be costly (query time, updates...)\n\nSome Options\n(1) WebDb Metadata Storage (changes to index,queryfilter..)\n  - Nutch-59\n  - Nutch-139\n  ...\n  with tools and plugins\n  - Ontologies\n  - Geospacial\n  ...\n(2) Internal Metadata Store - Create a MetaDB store that provides local storage of denomalized metadata in Lucene. This could use an optimized subset of a Metadata API.\n\n(3) Metadata API - Formal API from Nutch into other external Metadata Repositories (lucene, mysql, DB2, Jena (OWL), GIS ...)\n\nIssues to consider:\n- persisting metadata in WebDb/Index offers faster queries\n- as metadata becomes large and more complex and the number of pages increases (50mm - 6 billion) updates and searches will suffer\n- use of external stores will impact any processes that require a call to that store\n- external metadata stores can persist more complex forms of metadata\n- Lucene, which is optimized for unstructured data may not be the best persistent mechanism for complex metadata\n\n\nFeedback:\nPlease tell me if I'm close with regards to articulating the some of the issues that may need to be considered in defining a metadata architecture for Nutch. Suggesting solutions (Metadata API and MetaDB) at this stage is only to enhance discussion. A few more iterations on a requirement for a broader metadata architecture is necessary before we start laying down concreate solutions.\n\nThanks,\nJames\n", "This patch is to the 0.7 release and will not work in the current trunk.\n\nPlease see:\n\nhttp://www.mail-archive.com/nutch-dev@lucene.apache.org/msg02140.html\n\nand \n\nhttp://issues.apache.org/jira/browse/NUTCH-61\n\nSo extensible metadata should be added to CrawlDatum when a fix for NUTCH-61 is committed to trunk.\n", "Nutch 0.8 is very different to 0.7 in the way it stores page data and linkgraph. Therefore a reimplementation of meta data support for nutch 0.8 is on my todo list. It will be simple HashMap style api to store and retrieve key value tupples. Data will be stored in a extra file.\n\n ", "Stefan,\n\nSpot on.\n\nUse of HashMaps - very fast\n\nUse of separate file instead of extending WebDB - good\n\nBackground\nInitially this will help limit the size of the MetaDB (the separate file). For example, association of DMOZ topics to Pages would only be one-to-one on the first fetch. On the supsequent fetches other websites outside of the DMOZ list would then contain a blank topic for that field, thus filling up needless space on WebDB. (some databases are more efficient with regards to managing this type of dead space. Lucene may be one of these). The next senario is adding a new metadata association (simple location - city,state(province),country). Here the MetaDB (temporary name for the convenience of discussion) would only related to the Region section of the DMOZ list, but some of the non-DMOZ pages would have such a Location association. This leads to the question of potentially splitting the file into a multiple file for each metadata artifact (topic, location). As the list of metadata artifacts grows, so does the number of files. This dancing between denormalized data (single big files) versus normalized data (many smaller files - complex relationships) will over time impact the speed of the queries. This type of performance penalty associated with metadata can be even more exaserbated when you move into metadata repositories, where they persist both the metadata and the model of the metadata (customer now roles back his eyes and passes out as you continue speaking of meta-meta models).\n\nThat being said, for simplicities sake, I would not get to far ahead of the game. Your decision of  using of a single separate file gets the job done. Changes to the other components (index, QueryFilter) to handle Extensible Metadata seems like the higher priority. I just wanted to give you a flavor for how metadata stores grow from simple to complex and that some planning is often helpful in order to avoid some small hickups in the users migration from one set of simple metadata stores into more complex structures. Normally applications go through a series of learning experiences as they move up the complexity slope for metadata. (sometimes these applications (companies) actually survive - several don't)\n\nQuick HOW TO for building a metadata store:\n- Write down a list of metadata that you think you may wish to store\n- Map this list to Use Cases that create specific value to the user\n- For each metadata artifact assign it the standard (must have, should have, could have, won't have)  (or a,b,c - red, white blue - whatever) based on your use cases.\n- Define the API containing only a link to metadata that seems the most useful (must haves)\n- Define a simple metadata model to contain that short list of metadata exposed in your API\n- Define and implement the physical model to support that API. The semantics of the model will normally be greater than what is exposed\n- Keep the API stable, grow the underlying physical model. Do Not Expose the physical model.\n- Carefully expand the scope of the API based on what creates real value to the user\n\nWhat happens is the underlying model will change radically over time and will often becomes the limiting factor in your persistence of more complex metadata artifacts. ( think of a person inside a hierarchical organization with matrixed relationships with associations to both titles and roles - yuk - it can get fun very quickly ) Most applications bind thier software tightly to the physical metamodel (its easy - just expose it). The result is unsatisfied customers as the metamodel has to change over time. Cometition usually swoops in since they can green field thier metamodels while you are stuck supporting the semantics of your pervious application. \n\n2-cents worth of comments\n\nPS\nI'm very interested in testing our your DMOZ Topic Metadata Extention on .8. I have a couple websites that might find a use for it.\n\nThanks,\nJames", "I deployed this patch into a Nutch 7.1 sandbox and performed a test run. The 'topic' metadata has been captured. Congrats!\n\nHow do I display this information inside the 'more' section of my query result page?\nHow do I use this metadata to filter a standard query?\n\nThanks,\nJames", "Please let's move this discuss into the user mailing list, since this is no 'real' issue comment.\nAlso please note that meta data support for nutch 0.8 is under development and is comming hopefully soon into sources. So may a better idea is to wait for nutch 0.8 meta data support.", "Thanks,\n\nI have been tracking Nutch-139 and Nutch-192 and look forward to these patches being committed into the .8 trunk.\n\nJames", "This functionality has been committed as a part of NUTCH-192 and NUTCH-139"], "tasks": {"summary": "meta data support in webdb", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "meta data support in webdb"}, {"question": "What is the main context?", "answer": "Meta data support in web db would very usefully for a new set of nutch feature that needs long life meta data. \n\nActually page meta data need to be regenerated or lookup every 30 days a page is re-fet"}]}}
{"issue_id": "NUTCH-590", "project": "NUTCH", "title": "Index multiple docs per call using IndexingFilter extension point", "status": "Closed", "priority": "Major", "reporter": "Nathaniel Powell", "assignee": "Andrzej Bialecki", "created": "2007-12-06T00:59:37.764+0000", "updated": "2009-04-10T12:29:04.916+0000", "description": "There are many applications where extracting and indexing multiple documents from a single HTML web file or other object would be useful. Therefore, it would help a lot if the IndexingFilter extension point were modified to pass in a list of documents as an argument and return a list (or collection) of documents.", "comments": ["Nutch has a provision to return multiple documents from parsers, however you are correct that no such provision exists during the indexing step. The challenge here is that once a ParseResult is stored in a segment the individual pieces are stored separately, and Nutch doesn't maintain the information about their relationships. So based on the information managed in Nutch there is no way to submit a group of documents to Indexer (assuming we rework the API to accommodate for multiple in/out documents). I'm pretty sure that if you created a patch that implements this the changes would be substantial across several Nutch subsystems.\n\nI'm inclined to mark this as \"Won't Fix\".", "No further comments or patches provided."], "tasks": {"summary": "Index multiple docs per call using IndexingFilter extension point", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Index multiple docs per call using IndexingFilter extension point"}, {"question": "What is the main context?", "answer": "There are many applications where extracting and indexing multiple documents from a single HTML web file or other object would be useful. Therefore, it would help a lot if the IndexingFilter extension"}]}}
{"issue_id": "NUTCH-591", "project": "NUTCH", "title": "StringIndexOutOfBoundsException when extracting text from a Word document.", "status": "Closed", "priority": "Major", "reporter": "frank ling", "assignee": null, "created": "2007-12-14T00:47:04.690+0000", "updated": "2011-04-01T15:07:21.164+0000", "description": "see \nhttp://issues.apache.org/bugzilla/show_bug.cgi?id=41076+", "comments": ["can be resolved via NUTCH-691", "duplicate of NUTCH-691", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "StringIndexOutOfBoundsException when extracting text from a Word document.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "StringIndexOutOfBoundsException when extracting text from a Word document."}, {"question": "What is the main context?", "answer": "see \nhttp://issues.apache.org/bugzilla/show_bug.cgi?id=41076+"}]}}
{"issue_id": "NUTCH-592", "project": "NUTCH", "title": "Fetcher2 : NPE for page with status ProtocolStatus.TEMP_MOVED", "status": "Closed", "priority": "Major", "reporter": "Emmanuel Joke", "assignee": "Andrzej Bialecki", "created": "2007-12-16T15:22:56.786+0000", "updated": "2009-04-10T12:29:02.835+0000", "description": "I have a NPE for page when ProtocolStatus.TEMP_MOVED. It seems handleRedirect function can return NULL for few case and it has not been managed in the function as it has been done for the case ProtocolStatus.SUCCESS.", "comments": ["Patch provided.", "This seems to be a duplicate of NUTCH-597. If you have no objections I will close this issue.", "Duplicate of NUTCH-597 and NUTCH-615."], "tasks": {"summary": "Fetcher2 : NPE for page with status ProtocolStatus.TEMP_MOVED", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fetcher2 : NPE for page with status ProtocolStatus.TEMP_MOVED"}, {"question": "What is the main context?", "answer": "I have a NPE for page when ProtocolStatus.TEMP_MOVED. It seems handleRedirect function can return NULL for few case and it has not been managed in the function as it has been done for the case Protoco"}]}}
{"issue_id": "NUTCH-593", "project": "NUTCH", "title": "Nutch crawl problem", "status": "Closed", "priority": "Major", "reporter": "sudarat", "assignee": null, "created": "2007-12-19T02:47:59.816+0000", "updated": "2009-04-10T12:29:01.547+0000", "description": "i use nutch-0.9, hadoop-0.12.2 and i use this command \"bin/nutch crawl \nurls -dir crawled -depth 3\" have error : \n\n- crawl started in: crawled \n- rootUrlDir = input \n- threads = 10 \n- depth = 3 \n- Injector: starting \n- Injector: crawlDb: crawled/crawldb \n- Injector: urlDir: input \n- Injector: Converting injected urls to crawl db entries. \n- Total input paths to process : 1 \n- Running job: job_0001 \n- map 0% reduce 0% \n- map 100% reduce 0% \n- map 100% reduce 100% \n- Job complete: job_0001 \n- Counters: 6 \n- Map-Reduce Framework \n- Map input records=3 \n- Map output records=1 \n- Map input bytes=22 \n- Map output bytes=52 \n- Reduce input records=1 \n- Reduce output records=1 \n- Injector: Merging injected urls into crawl db. \n- Total input paths to process : 2 \n- Running job: job_0002 \n- map 0% reduce 0% \n- map 100% reduce 0% \n- map 100% reduce 58% \n- map 100% reduce 100% \n- Job complete: job_0002 \n- Counters: 6 \n- Map-Reduce Framework \n- Map input records=3 \n- Map output records=1 \n- Map input bytes=60 \n- Map output bytes=52 \n- Reduce input records=1 \n- Reduce output records=1 \n- Injector: done \n- Generator: Selecting best-scoring urls due for fetch. \n- Generator: starting \n- Generator: segment: crawled/segments/25501213164325 \n- Generator: filtering: false \n- Generator: topN: 2147483647 \n- Total input paths to process : 2 \n- Running job: job_0003 \n- map 0% reduce 0% \n- map 100% reduce 0% \n- map 100% reduce 100% \n- Job complete: job_0003 \n- Counters: 6 \n- Map-Reduce Framework \n- Map input records=3 \n- Map output records=1 \n- Map input bytes=59 \n- Map output bytes=77 \n- Reduce input records=1 \n- Reduce output records=1 \n- Generator: 0 records selected for fetching, exiting ... \n- Stopping at depth=0 - no more URLs to fetch. \n- No URLs to fetch - check your seed list and URL filters. \n- crawl finished: crawled \n\nbut sometime i crawl some url it has error indexes time that \n\n- Indexer: done \n- Dedup: starting \n- Dedup: adding indexes in: crawled/indexes \n- Total input paths to process : 2 \n- Running job: job_0025 \n- map 0% reduce 0% \n- Task Id : task_0025_m_000001_0, Status : FAILED \ntask_0025_m_000001_0: - Error running child \ntask_0025_m_000001_0: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000001_0: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000001_0: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000001_0: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000001_0: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000001_0: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000001_0: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \n- Task Id : task_0025_m_000000_0, Status : FAILED \ntask_0025_m_000000_0: - Error running child \ntask_0025_m_000000_0: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000000_0: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000000_0: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000000_0: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000000_0: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000000_0: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000000_0: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \n- Task Id : task_0025_m_000000_1, Status : FAILED \ntask_0025_m_000000_1: - Error running child \ntask_0025_m_000000_1: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000000_1: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000000_1: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000000_1: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000000_1: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000000_1: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000000_1: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \n- Task Id : task_0025_m_000001_1, Status : FAILED \ntask_0025_m_000001_1: - Error running child \ntask_0025_m_000001_1: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000001_1: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000001_1: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000001_1: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000001_1: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000001_1: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000001_1: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \n- Task Id : task_0025_m_000001_2, Status : FAILED \ntask_0025_m_000001_2: - Error running child \ntask_0025_m_000001_2: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000001_2: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000001_2: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000001_2: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000001_2: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000001_2: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000001_2: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \n- Task Id : task_0025_m_000000_2, Status : FAILED \ntask_0025_m_000000_2: - Error running child \ntask_0025_m_000000_2: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000000_2: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000000_2: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000000_2: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000000_2: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000000_2: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000000_2: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \n- map 100% reduce 100% \n- Task Id : task_0025_m_000001_3, Status : FAILED \ntask_0025_m_000001_3: - Error running child \ntask_0025_m_000001_3: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000001_3: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000001_3: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000001_3: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000001_3: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000001_3: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000001_3: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \n- Task Id : task_0025_m_000000_3, Status : FAILED \ntask_0025_m_000000_3: - Error running child \ntask_0025_m_000000_3: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000000_3: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000000_3: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000000_3: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000000_3: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000000_3: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000000_3: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \nException in thread \"main\" java.io.IOException: Job failed! \nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:604) \nat org.apache.nutch.indexer.DeleteDuplicates.dedup \n(DeleteDuplicates.java:439) \nat org.apache.nutch.crawl.Crawl.main(Crawl.java:135) \n\nhow i solve it? \n", "comments": ["This is fixed in the current code. This bug was caused by not detecting empty indexes."], "tasks": {"summary": "Nutch crawl problem", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Nutch crawl problem"}, {"question": "What is the main context?", "answer": "i use nutch-0.9, hadoop-0.12.2 and i use this command \"bin/nutch crawl \nurls -dir crawled -depth 3\" have error : \n\n- crawl started in: crawled \n- rootUrlDir = input \n- threads = 10 \n- depth = 3 \n- Inj"}]}}
{"issue_id": "NUTCH-594", "project": "NUTCH", "title": "Serve Nutch search results in multiple formats including XML and JSON", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2007-12-21T17:10:33.791+0000", "updated": "2009-01-13T04:16:56.670+0000", "description": "Allow search results to be served in XML, JSON, and other configurable formats.  Right now there is an OpenSearch servlet that returns returns in RSS. I would like something that has more flexibility in terms of the XML being served and also supports other formats such as JSON or plain text.", "comments": ["Here is a basic patch (no docs or unit tests) which serve the results in both XML and JSON formats.  All fields are returned along with encoded summaries.  This patch supplies an interface for creating new formats and configuring through the web.xml file.", "I like the concept of this patch that there could be multiple (configurable) formatters, and the implementation looks ok to me. One minor issue: in some scenarios it's not required to return summaries, so perhaps the formatters should allow for summaries to be null?\n\nRegarding the configuration through web.xml - on one hand this allows you to run several instances of the servlet with different formatters, on the other hand Nutch users expect the configuration to be managed through nutch-site.xml ...", "Could you please, write some short instruction how to configure Nutch to serve results in XML or JSON using this path?\nIt would be very useful for newbies like me ;)\n\nthanks in advance", "An example of this in action can be found at: http://search.isc.swlabs.org///nutchsearch?query=jav&hitsPerSite=2&lang=en&hitsPerPage=10&type=json\n\nThe available parameters are query for typed in query, hitsPerSite for deduping content, lang for the users language, hitsPerPage for the number of hits to return and type as either xml or json.  The type defaults to xml.  As you can see from above this is very similar to the opensearch servlet currently in nutch.  The servlet url is configured in the web.xml as normal for servlets.  In this case it defaults to nutchsearch.", "A completely reworked framework with extension point for serving search results in different format.  Included are plugins for serving results in XML and JSON format.  XML is the default.  Uses JSON-Lib to convert the results into JSON format.", "ezmorph jar required for framework", "commons beanutils", "commons collections", "json lib jar", "Fixed some things.  Added the ability to set mime output type using the plugin.xml file.  That way people can have application/json or text.json or text/plain, however they want for their application.", "For third-party jars please include their LICENSE statements. Hmm, isn't the JSON library under LGPL?\n\nOther than that, the patch looks great, +1.", "JSON-LIb and EZMorph are both under Apache.  There is an optional Xom library dependency for JSON-Lib which is not included, that is under LGPL, but everything else is Apache.\n\nhttp://json-lib.sourceforge.net/license.html\nhttp://ezmorph.sourceforge.net/license.html\n\nI put comments about these in the plugin.xml file for response-json.  Is there anything else I need to do?", "Excellent. Re: the text of license - IMHO if both libs are ASL we can skip the license.", "I know this patch has only been out for a day or so, but if nobody has any problems with this one, I am going to commit it later tonight or tomorrow morning.", "Final patch.  Adds the ability to stop summaries from being returned and to only return a given set of fields by name.", "Issues to be addressed before committing:\n\n* many public classes lack javadoc - this needs to be added, especially since you are adding a new extension point. Please add  the class-level, public and protected methods javadoc.\n* SimpleDateFormat is not thread-safe (I wish it were!), so this makes *ResponseWriter-s also not thread-safe. I believe this should be fixed, because servlets can be invoked from multiple threads.\n* new configuration properties should be documented in nutch-default.xml and they should be given sensible defaults.\n* the patch adds SearchServlet to web.xml, so it should ensure also that at least one ResponseWriter plugin is turned on in nutch-default.xml.\n* minor nits: I'm not sure that we really need pretty-printing in XMLResponseWriter - indenting takes space (-> bandwidth). There are 2 spurious spaces in plugin/build.xml diffs.\n\n", "New patch addresses current issues.\n\n>> Issues to be addressed before committing:\n\n>>     * many public classes lack javadoc - this needs to be added, especially since you are adding a new extension point. Please add the class-level, public and protected methods javadoc.\n\nAdded javadoc.\n\n>>     * SimpleDateFormat is not thread-safe (I wish it were!), so this makes *ResponseWriter-s also not thread-safe. I believe this should be fixed, because servlets can be invoked from multiple threads.\n\nWow.  I didn't know that.  I changed it to be local scoped variable.\n\n>>     * new configuration properties should be documented in nutch-default.xml and they should be given sensible defaults.\n\nDone.\n\n>>     * the patch adds SearchServlet to web.xml, so it should ensure also that at least one ResponseWriter plugin is turned on in nutch-default.xml.\n\nDone.  I turned on both xml and json response writers by default.\n\n >>    * minor nits: I'm not sure that we really need pretty-printing in XMLResponseWriter - indenting takes space (-> bandwidth). There are 2 spurious spaces in plugin/build.xml diffs.\n\nI went half way with you on this.  I think it is good for development, debugging, but yes better without it for production.  I put in a configuration variable to allow either pretty printing or not.\n\n", "http://www.joelonsoftware.com/uibook/chapters/fog0000000059.html\n\n:P\n\nBut seriously, +1 .", "I thought you would like that.  Especially since it was just a minor nit :)", "committed with revision 730845", "Integrated in Nutch-trunk #691 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/691/])\n    "], "tasks": {"summary": "Serve Nutch search results in multiple formats including XML and JSON", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Serve Nutch search results in multiple formats including XML and JSON"}, {"question": "What is the main context?", "answer": "Allow search results to be served in XML, JSON, and other configurable formats.  Right now there is an OpenSearch servlet that returns returns in RSS. I would like something that has more flexibility "}]}}
{"issue_id": "NUTCH-595", "project": "NUTCH", "title": "\"Target file:/.... already exists\"", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": null, "created": "2007-12-27T13:08:36.639+0000", "updated": "2011-04-01T15:26:38.358+0000", "description": "This is related to the upgrade to Hadoop 0.15.0. I'm unable to run any Hadoop jobs in local mode under Cygwin:\n\n{noformat}\n2007-12-27 13:54:24,468 WARN  mapred.LocalJobRunner - job_local_1\njava.io.IOException: Target file:/c:/tmp/hadoop-abial/mapred/temp/inject-temp-19350068/_reduce_kmsua5/part-00000 already exists\n        at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:246)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:125)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:116)\n        at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:180)\n        at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:394)\n        at org.apache.hadoop.mapred.Task.moveTaskOutputs(Task.java:452)\n        at org.apache.hadoop.mapred.Task.moveTaskOutputs(Task.java:469)\n        at org.apache.hadoop.mapred.Task.saveTaskOutput(Task.java:426)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:165)\n2007-12-27 13:54:24,843 FATAL crawl.Injector - Injector: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:831)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:162)\n        at org.apache.nutch.crawl.Injector.run(Injector.java:192)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:54)\n        at org.apache.nutch.crawl.Injector.main(Injector.java:182)\n{noformat}\n\nAFAIK this should be fixed in HADOOP-2228, which is a part of 0.15.2.", "comments": ["I had a similar issue and i follow the instruction done by Dennis and it solved my pb.\nhttp://www.nabble.com/File-Paths-2C-Hadoop--3E-3D-0.15-and-Local-Jobs-to13184356.html\n\nIts just a workaround but at least you can run your crawler.", "You say this is only happening on Cygwin/Win XP but this is also a major problem in Linux too (Ubuntu) at least using the nightly build (around 15th Jan 2008 build)", "Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "\"Target file:/.... already exists\"", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "\"Target file:/.... already exists\""}, {"question": "What is the main context?", "answer": "This is related to the upgrade to Hadoop 0.15.0. I'm unable to run any Hadoop jobs in local mode under Cygwin:\n\n{noformat}\n2007-12-27 13:54:24,468 WARN  mapred.LocalJobRunner - job_local_1\njava.io.IOE"}]}}
{"issue_id": "NUTCH-596", "project": "NUTCH", "title": "ParseSegments parse content even if its not CrawlDatum.STATUS_FETCH_SUCCESS", "status": "Closed", "priority": "Minor", "reporter": "Emmanuel Joke", "assignee": "Dogacan Guney", "created": "2007-12-30T09:51:40.168+0000", "updated": "2009-04-10T12:29:07.191+0000", "description": "We have 2 choices to parse the content either within the Fetcher class or with the ParseSegment class\nFetcher(1 or 2) will check first if the CrawlDatum == STATUS_FETCH_SUCCESS nad if its true it will parse the content.\n\nHowever we don't have this check in ParseSegment, thus we parse every content store on the disk without checking the Status.\n\nSo i think we should implement this check, i can see only 3 solutions:\n- read the status code in the Metadata of the Content object\n- don't store content for fetch with a crawldatun <>  STATUS_FETCH_SUCCESS\n- load the crawldatum object in ParseSegement\n\nWhat are your thoughts ?", "comments": ["I agree that ParseSegment's end result should be identical to parsing during fetching. \n\n>read the status code in the Metadata of the Content object\n\nThis is possible but feels like a hack.... Still, if we can't come up with anything better we can use this one.\n\n> don't store content for fetch with a crawldatun <> STATUS_FETCH_SUCCESS\n\nWe _do_ store content during fetch if status is not FETCH_SUCCESS, as long as there is something useful to store.\n\n> load the crawldatum object in ParseSegement\n\nWe can't read CrawlDatum object during map() (*), so during map operations, we would still parse content (even for non-FETCH_SUCCESS), and output CrawlDatum-s, then during reduce, we can read CrawlDatum and store/discard parse object accordingly.\n\nI like this approach, but it brings extra overhead (reading crawl_fetch) and we still unnecessarily try to parse Content-s, only to discard it later.\n\nSo, I think 3rd approach sounds better, but 1st approach is simpler and lightweight.\n\n(*) We may try to create a reader that reads from content and crawl_fetch at the same time, but I don't think that crawl_fetch and content are necessarily in sync, so this will probably not work.", "I agree with you the proper solution will be the third one. However i don't like the idea to have time and processor consuming for record that we are not inetrested in.\n\nWhat do you think ?", "I'm voting for simplicity ;) i.e. the option 1 above - this should solve the issue with minimal changes to the code and virtually no impact on performance.", "I didn't find any usefull information in the Content Object to know if the Crawling has been sucessfull.\n\nSo, i guess this suggestion can be eliminated.\n\nI thought of another way to that, we can create a simplae Map/Reduce task to load CrawlDatum + Content and filter the content that has a DbStatus == Success. The output of this task will be then used by the existing ParseSegment task. This solution avoid to Parse any content would could caused any errors in the parsing.\n\nAny thoughts ?", "bq. I didn't find any usefull information in the Content Object to know if the Crawling has been sucessfull.\n\n\nWell, the idea was to add ProtocolStatus code to Content.metadata, and then retrieve it in ParseSegment. It is a hack, but it carries a minimal impact, both on the code and on the amount of processed data.\n\nbq. I thought of another way to that, we can create a simplae Map/Reduce task to load CrawlDatum + Content and filter the content that has a DbStatus == Success. The output of this task will be then used by the existing ParseSegment task. This solution avoid to Parse any content would could caused any errors in the parsing.\n\nThis costs even more than the option #3 (add crawl_fetch as one of the inputs), because you still need to process the same data, but now you need to run 2 separate jobs. If we go this road, it's better just to stick with option #3.", "A simple patch for option 1, puts fetch status in content metadata and retrieves in during parse, skipping over records if status is not FETCH_SUCCESS....", "Reducing priority to minor.", "This looks beautifully simply to me! +1 for committing it.\n", "+1", "Fixed in rev. 649652.\n\nThanks for the reviews.", "Oops... Forgot to make \"Fix Version/s\" 1.0.0.", "Integrated in Nutch-trunk #425 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/425/])"], "tasks": {"summary": "ParseSegments parse content even if its not CrawlDatum.STATUS_FETCH_SUCCESS", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "ParseSegments parse content even if its not CrawlDatum.STATUS_FETCH_SUCCESS"}, {"question": "What is the main context?", "answer": "We have 2 choices to parse the content either within the Fetcher class or with the ParseSegment class\nFetcher(1 or 2) will check first if the CrawlDatum == STATUS_FETCH_SUCCESS nad if its true it will"}]}}
{"issue_id": "NUTCH-597", "project": "NUTCH", "title": "Fetcher2 - java.lang.NullPointerException when host does not exist and fetcher.threads.per.host.by.ip is set to true causes threads to finish.", "status": "Closed", "priority": "Major", "reporter": "Remco Verhoef", "assignee": "Andrzej Bialecki", "created": "2007-12-30T16:29:37.882+0000", "updated": "2009-04-10T12:29:00.297+0000", "description": "When fetcher.threads.per.host.by.ip is set to true the following exception is thrown when the host does not exist. FetchItem.create returns null when it is not able to resolve the host address when it is redirecting.\n\n2007-12-30 15:34:42,720 WARN  fetcher.Fetcher2 - Unable to resolve: {url}  , skipping.\n2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - java.lang.NullPointerException\n2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - at org.apache.nutch.fetcher.Fetcher2$FetchItemQueues.finishFetchItem(Fetcher2.java:327)\n2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - at org.apache.nutch.fetcher.Fetcher2$FetchItemQueues.finishFetchItem(Fetcher2.java:323)\n2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - at org.apache.nutch.fetcher.Fetcher2$FetcherThread.run(Fetcher2.java:632)\n2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - fetcher caught:java.lang..NullPointerException\n2007-12-30 15:34:42,721 INFO  fetcher.Fetcher2 - -finishing thread FetcherThread, activeThreads=49", "comments": ["Contains the patch code for Fetcher2.java.", "Patch applied in rev. 612264. Thank you!", "Integrated in Nutch-Nightly #330 (See [http://lucene.zones.apache.org:8080/hudson/job/Nutch-Nightly/330/])"], "tasks": {"summary": "Fetcher2 - java.lang.NullPointerException when host does not exist and fetcher.threads.per.host.by.ip is set to true causes threads to finish.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fetcher2 - java.lang.NullPointerException when host does not exist and fetcher.threads.per.host.by.ip is set to true causes threads to finish."}, {"question": "What is the main context?", "answer": "When fetcher.threads.per.host.by.ip is set to true the following exception is thrown when the host does not exist. FetchItem.create returns null when it is not able to resolve the host address when it"}]}}
{"issue_id": "NUTCH-598", "project": "NUTCH", "title": "Remove deprecated use of ToolBase, Migration to the new implementation", "status": "Closed", "priority": "Major", "reporter": "Emmanuel Joke", "assignee": "Andrzej Bialecki", "created": "2008-01-02T08:53:12.211+0000", "updated": "2009-04-10T12:29:04.995+0000", "description": "All major nutch command are using the deprecated ToolBase class. We need to upgrade the code to extend Configured, implements the interface Tool and use ToolRunner.run().", "comments": ["Patch provided\n\nIt includes:\n- remove ToolBase call and move to the new implementation\n- improve the code to remove some minor warning (seen in Eclipse) in the implementation of those classes \n- I noticed that ParseSegment was the only class which wre not extending ToolBase (i don't know if there was really a reason for that...) so i even migrate this class to the new implementation.\n\n", "Migration to Tool looks good, but some of the templates that you have added are too generic (such as, OutputCollector<WritableComparable, Writable>). Those templates should either be more specific or we can just remove them and commit ToolBase migration code.", "Thanks Dogacan for your update.\n\nNew patch provided. Most of the template has been changed to be more specific when its possible.", "I am currently testing latest patch, if I don't run into any problems I am going to commit it in a few days.", "Hi Dogacan,did you finish to review my patch.Is it possible to commit it ?", "Patch synced to the current trunk. If there are no objections I'd like to commit it shortly.", "Patch v3 applied in trunk, rev. 638779 . Thank you!", "Integrated in Nutch-trunk #395 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/395/])"], "tasks": {"summary": "Remove deprecated use of ToolBase, Migration to the new implementation", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove deprecated use of ToolBase, Migration to the new implementation"}, {"question": "What is the main context?", "answer": "All major nutch command are using the deprecated ToolBase class. We need to upgrade the code to extend Configured, implements the interface Tool and use ToolRunner.run()."}]}}
{"issue_id": "NUTCH-599", "project": "NUTCH", "title": "nutch crawl and index problem", "status": "Closed", "priority": "Major", "reporter": "sudarat", "assignee": "Dogacan Guney", "created": "2008-01-08T01:46:06.826+0000", "updated": "2009-04-10T12:29:03.602+0000", "description": "first i set \n# skip file:, ftp:, & mailto: urls\n-^(file|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n#-\\.(png|PNG|ico|ICO|css|sit|eps|wmf|zip|mpg|gz|rpm|tgz|mov|MOV|exe|bmp|BMP)$\n\n# skip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n\n# skip URLs with slash-delimited segment that repeats 3+ times, to break loops\n-.*(/.+?)/.*?\\1/.*?\\1/\n\n# skip everything else\n+.\n\n in conf/crawl-urlfilter.txt and use this command \"bin/nutch crawl urls -dir crawled -depth 3\"  i can crawl http://guide.kanook.com but i can't crawl http://www.kapook.com , some webpage can't crawl all why? and index file after crawl don't have segments file for nutch search it have only\n\n-rw-r--r-- 1 nutch users   365 ม.ค.  7 16:47 _0.fdt\n-rw-r--r-- 1 nutch users     8 ม.ค.  7 16:47 _0.fdx\n-rw-r--r-- 1 nutch users    66 ม.ค.  7 16:47 _0.fnm\n-rw-r--r-- 1 nutch users   370 ม.ค.  7 16:47 _0.frq\n-rw-r--r-- 1 nutch users     9 ม.ค.  7 16:47 _0.nrm\n-rw-r--r-- 1 nutch users   611 ม.ค.  7 16:47 _0.prx\n-rw-r--r-- 1 nutch users   135 ม.ค.  7 16:47 _0.tii\n-rw-r--r-- 1 nutch users 10553 ม.ค.  7 16:47 _0.tis\n-rw-r--r-- 1 nutch users     0 ม.ค.  7 16:47 index.done\n-rw-r--r-- 1 nutch users    41 ม.ค.  7 16:47 segments_2\n-rw-r--r-- 1 nutch users    20 ม.ค.  7 16:47 segments.gen\n\nhow to solve it?\n", "comments": ["Please use nutch-user for asking questions."], "tasks": {"summary": "nutch crawl and index problem", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "nutch crawl and index problem"}, {"question": "What is the main context?", "answer": "first i set \n# skip file:, ftp:, & mailto: urls\n-^(file|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n#-\\.(png|PNG|ico|ICO|css|sit|eps|wmf|zip|mpg|gz|rpm|tgz|mov|MOV|exe|bmp|BMP)$\n\n"}]}}
{"issue_id": "NUTCH-6", "project": "NUTCH", "title": "nutch illustration", "status": "Closed", "priority": "Trivial", "reporter": "Stefan Groschupf", "assignee": null, "created": "2005-03-11T07:14:35.000+0000", "updated": "2005-03-29T03:41:54.000+0000", "description": "just the graphic", "comments": ["the graphic", "new Icons as requested by John X. :-)", "This was never a bug but just a container for some graphic files. "], "tasks": {"summary": "nutch illustration", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "nutch illustration"}, {"question": "What is the main context?", "answer": "just the graphic"}]}}
{"issue_id": "NUTCH-60", "project": "NUTCH", "title": "Bad language identifier plugin performances", "status": "Closed", "priority": "Minor", "reporter": "Jerome Charron", "assignee": null, "created": "2005-05-26T07:05:24.000+0000", "updated": "2005-07-03T04:30:41.000+0000", "description": "As reported by Stefan Groschupf (http://www.mail-archive.com/nutch-developers@lists.sourceforge.net/msg04090.html) the language identifier plugin consumes a lot of processing time.\nSome optimizations and/or configuration options are required.", "comments": ["Patch with some minor performances improvements, but with some configurations parameters that enable to improve performances.\nSee http://wiki.apache.org/nutch/LanguageIdentifierBenchs and http://wiki.apache.org/nutch/NewLanguageIdentifier (coming soon) for more details.\n\nShortly, it adds the following configuration parameters:\n\n * lang.ngram.min.length : The minimum size of ngrams to uses to identify language (must be between 1 and lang.ngram.max.length). The larger is the range between lang.ngram.min.length and lang.ngram.max.length, the better is the identification, but the slowest it is.\n\n * lang.ngram.max.length: The maximum size of ngrams to uses to identify language (must be between lang.ngram.min.length and 4). The larger is the range between lang.ngram.min.length and lang.ngram.max.length, the better is the identification, but the slowest it is.\n\n * lang.analyze.max.length: The maximum bytes of data to uses to indentify the language (0 means full content analysis). The larger is this value, the better is the analyzis, but the slowest it is.\n\nSome new ngram profiles have been generated for en, es, fr, nl, it, pt, da, sv, de, fi, el cause the new implementation need more ngrams in the profile, but it is backward compatible with old ones.\n\nSome unitary tests added.\n\n", "This patch, keeps the improvements of the previous one (configuration), and provides some optimizations that reduce the processing time from 70% to 20%, depending on the configuration (size of data to process), with an average gain of 25%.\nI will provides more detailled results of my benchs on the Wiki as soon as possible (http://wiki.apache.org/nutch/LanguageIdentifierBenchs).\n\n", "Committers, don't apply these patches, there is a loss of precision on identification.\nI have identified the problem and have just quantified it.\nI'm currently working on a new patch version solving this issue.", "Here it is: the final (?) patch. It provides around +25% performance and increase the identification precision.  More details are availale on http://wiki.apache.org/nutch/LanguageIdentifierBenchs", "Do you have some ready made scripts you used to measure the performance (quality and speed) that I could use to see if my additional optimization have any impact.", "Sami, \n\n* for the performance speed, I simply uncomment some lines commented as \"used for benchs\" in the main method of LanguageIdentifier. Then, I launch the TestIdentifier on a big test of file using the fileset command line argument.\n\n* for the performance quality, I just configure the language identifier plugin with the desired size of data to analyze, I comment the line of code uncommented for performance speed, and simply launch the command line with the fileset command line argument on a big set of documents of the same language with grep and wc commands piped in order to get the number of failed identifications:\njava org.apache.nutch.analysis.lang.LanguageIdentifier -identifyfileset /somewhere/fr/*.txt | grep -v \"identified as fr\" | wc -l\n\n Hope this can help. But you are true, a set of scripts could be a good idea.", "In the previous patch there were no more public default constructor in the LanguageIdentifier IndexingFilter, because it was a singleton (cause a RuntimeException at runtime). This new patch split the code of the LanguageIdentifier class into a LanguageIdentifier singleton and a LanguageIndexingFilter so that all is ok...", "Patches have been applied. Thanks!"], "tasks": {"summary": "Bad language identifier plugin performances", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Bad language identifier plugin performances"}, {"question": "What is the main context?", "answer": "As reported by Stefan Groschupf (http://www.mail-archive.com/nutch-developers@lists.sourceforge.net/msg04090.html) the language identifier plugin consumes a lot of processing time.\nSome optimizations "}]}}
{"issue_id": "NUTCH-600", "project": "NUTCH", "title": "Nutch index problem", "status": "Closed", "priority": "Major", "reporter": "sudarat", "assignee": "Dogacan Guney", "created": "2008-01-09T04:53:43.055+0000", "updated": "2008-01-11T18:03:17.947+0000", "description": "after crawl wtth this command \"bin/nutch crawl urls -dir crawled -depth 3\" index file not complete it's not have segments file \nit have only \n/user/nutch/crawld/indexes/part-00000/index.done        <r 1>   0\n/user/nutch/crawld/indexes/part-00000/segments.gen      <r 1>   20\n/user/nutch/crawld/indexes/part-00000/segments_1        <r 1>   20\n/user/nutch/crawld/indexes/part-00001/index.done        <r 1>   0\n/user/nutch/crawld/indexes/part-00001/segments.gen      <r 1>   20\n/user/nutch/crawld/indexes/part-00001/segments_2        <r 1>   20\n\nwhich nutch search cannot work it try to find segments file.  can i solve it?", "comments": ["Please ask around nutch-user mailing list for your problems before creating an issue in JIRA."], "tasks": {"summary": "Nutch index problem", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Nutch index problem"}, {"question": "What is the main context?", "answer": "after crawl wtth this command \"bin/nutch crawl urls -dir crawled -depth 3\" index file not complete it's not have segments file \nit have only \n/user/nutch/crawld/indexes/part-00000/index.done        <r"}]}}
{"issue_id": "NUTCH-601", "project": "NUTCH", "title": "Recrawling on existing crawl directory using force option", "status": "Closed", "priority": "Minor", "reporter": "Susam Pal", "assignee": "Andrzej Bialecki", "created": "2008-02-04T18:08:28.671+0000", "updated": "2009-04-10T12:29:02.439+0000", "description": "Added a '-force' option to the 'bin/nutch crawl' command line. With this option, one can crawl and recrawl in the following manner:\n\n{code}\nbin/nutch crawl urls -dir crawl -depth 2 -topN 10 -threads 5\nbin/nutch crawl urls -dir crawl -depth 2 -topN 10 -threads 5 -force\n{code}\n\nThis option can be used for the first crawl too:\n\n{code}\nbin/nutch crawl urls -dir crawl -depth 2 -topN 10 -threads 5 -force\nbin/nutch crawl urls -dir crawl -depth 2 -topN 10 -threads 5 -force\n{code}\n\nIf one tries to crawl without the -force option when the crawl directory already exists, he/she finds a small warning along with the error message:\n\n{code}\n# bin/nutch crawl urls -dir crawl -depth 2 -topN 10 -threads 5\nException in thread \"main\" java.lang.RuntimeException: crawl already\nexists. Add -force option to recrawl.\n       at org.apache.nutch.crawl.Crawl.main(Crawl.java:89)\n{code}", "comments": ["Patch attached.", "Thank you for creating this issue. I think that perhaps the old behavior should be removed altogether - it was based on an assumption that users don't want by default to recrawl the same pages. Our experience as a community shows that this is usually not the case, and the old behavior is confusing - so why not remove this artificial limitation altogether, and if users do want to keep each cycle in a separate directory they can do this by specifying different output directories.", "Attached a revised patch (NUTCH-601v0.2.patch), which removes the old behaviour completely as per Andrzej's comment. Since, now we have the new behaviour only, we do not need the -force option to switch the behaviour. So, this option has been removed.", "I think the section that handles the presence of an old merged index is not needed - we want to re-create it anyway, and if something bad happens it's better not to leave the old index with the new dbs/segments. So I think it's best to remove the merged index the same way as you remove the partial indexes.", "The 'if (newIndex != index)' condition is just a check whether this is a new crawl directory being constructed, or it is a recrawl on a previous crawl directory.\n\nIf it is a new crawl directory being constructed, a few lines above this check, there is another 'if (!fs.exists(index))' condition which will set the newIndex = index = '/index'. So, if both newIndex and index are same, we know that it is a new crawl directory and we need not delete old 'index' because it would not be present.\n\nIf it is a recrawl over a previous crawl directory, newIndex = '/new_index' and index = '/index'. Since they are different, it indicates that this is a recrawl and thus after '/new_index' is created, it'll quickly replace the old '/index' with the '/new_index'.\n\nThis seems fine to me.\n\nIf you want to avoid the possibility of having new segments with old index if something bad happens, then I should delete both 'index' and 'indexes' even before the generate call. But I didn't want to delete old 'index' so early. I was trying to minimize the time for which 'index' directory is unavailable. This would be helpful in case someone is running a recrawl on the same 'crawl' directory which the web-gui is using to serve search results.\n\nPlease let me know what you feel about this.", "Attached a revised patch (NUTCH-601v0.3.patch) that makes the code simpler and easier to read.", "Attached another patch (NUTCH-601v1.0.patch) that always deletes the old mergex index as per the suggestion of Andrzej.\n\nThe v0.4 patch would leave the old merged index with the new segments in case something goes wrong during the generation of new index. Whether the index merger fails or succeeds, we will always have an 'index' directory. So, after the completion of a recrawl, a user may want to verify whether the 'index' directory is the new merged index or the old merged index. This may be confusing.\n\nHowever, one advantage is that one can run a recrawl on the same crawl directory which the web-gui is using to serve the users. This patch minimizes the duration for which the index directory would be unavailable.\n\nThe v1.0 patch always deletes the old indexes as well as old merged index. Therefore, the old index would never remain once the index generation has begun. If the index merger fails, we won't have an 'index' directory which would be a clear indication of index generation failure. This prevents the confusion discussed above.\n\nPlease review both the patches and accept whichever the community feels is better.", "Hello,\n\nI tested this patch and for now it works. I had few problems, but Susam help me :D \n\nI have only one question, request. As I'm checking right now, it looks that it checks existing crawl folder and recrawl all the sites again, but is it possible to filter them out? So to only crawl sites that we set?\n\notherwise, I think it very useful patch..", "It continues the recrawl using the existing crawl directory. It generates new segments using the already existing crawl/crawldb directory. You can assume a recrawl to be a crawl resumed after a break (taken for generating and merging indexes). In other words, if you did two crawls with \"depth\" set as 5, effectively you have done a crawl of depth 10.\n\nI am not clear about what you mean by filtering. Isn't conf/crawl-urlfilter.txt enough for what you want to filter in the second crawl?", "Patch v. 1.0 applied to trunk in rev. 637122. Thank you!", "Integrated in Nutch-trunk #390 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/390/])"], "tasks": {"summary": "Recrawling on existing crawl directory using force option", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Recrawling on existing crawl directory using force option"}, {"question": "What is the main context?", "answer": "Added a '-force' option to the 'bin/nutch crawl' command line. With this option, one can crawl and recrawl in the following manner:\n\n{code}\nbin/nutch crawl urls -dir crawl -depth 2 -topN 10 -threads 5"}]}}
{"issue_id": "NUTCH-602", "project": "NUTCH", "title": "Allow configurable number of handlers for search servers", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-02-05T16:49:27.437+0000", "updated": "2011-06-08T21:34:19.206+0000", "description": "This improvement changes the distributed search server to allow a configurable number of RPC handlers.  Before the number was hardcoded at 10 handlers.  For high volume environments that limit will be quickly reached and the overall search will slowdown.  The patch changes nutch-default.xml with the configuration parameter searchers.num.handlers and changes DistributedSearch to pull the number of handlers from the configuration.", "comments": ["Changes nutch-default.xml and DistributedSearch.", "+1", "If nobody has any objections I will commit this soon.", "+1", "Committed.", "Integrated in Nutch-trunk #355 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/355/])", "closing issues for released version"], "tasks": {"summary": "Allow configurable number of handlers for search servers", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Allow configurable number of handlers for search servers"}, {"question": "What is the main context?", "answer": "This improvement changes the distributed search server to allow a configurable number of RPC handlers.  Before the number was hardcoded at 10 handlers.  For high volume environments that limit will be"}]}}
{"issue_id": "NUTCH-603", "project": "NUTCH", "title": "Add more default url normalizations", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-02-05T16:58:04.263+0000", "updated": "2009-04-10T12:29:05.231+0000", "description": "By default the regex-urlnormalizers only remove PHPSESSID strings.  I propose adding in more default url normalizers including expressions for removing different types of session ids, removing default pages, remvoing interpage links, and cleaning up url strings.  The point of these expressions is to decrease the number of duplicate urls that are being stored and scored in the crawl database and being fetched.", "comments": ["Added normalizations for removing different session ids, for changing default pages such as index.html to /, for removing #something interpage anchors, and for cleaning up urls such as multiple ampersands, ending ?, ., or & characters.  Unit tests were added to show results of expressions.  All current expressions were tuned for performance.", "If nobody has any objections I will go ahead and commit this tonight.", "I'm of a split mind towards one of these rules: the one that strips /index.html and similar. I know of a few sites where /index.html != /index.php, I even remember creating one like that :) Some sites redirect / not to /index.html but somewhere down in the hierarchy, and they don't have any proper /index.html at all. In other words, I vote for removing this rule, or at least commenting it out.\n\nOther than that, +1.", "I am ok with commenting it out.  As long as it is there for people to use (instead of having to create) I think it will be ok.  I will comment that out and if no objections will commit that version.", "This patch comments out the default page removal (i.e. index.html) and adds the _ character to be removed if attached to session ids (i.e. _sessionid)", "Committed.", "Integrated in Nutch-trunk #362 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/362/])", "closing issues for released version"], "tasks": {"summary": "Add more default url normalizations", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add more default url normalizations"}, {"question": "What is the main context?", "answer": "By default the regex-urlnormalizers only remove PHPSESSID strings.  I propose adding in more default url normalizers including expressions for removing different types of session ids, removing default"}]}}
{"issue_id": "NUTCH-604", "project": "NUTCH", "title": "Upgrade Nutch to Lucene 2.3.0", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2008-02-05T22:32:56.729+0000", "updated": "2011-06-08T21:34:19.717+0000", "description": "Upgrade Nutch to Lucene 2.3.0 release jars.", "comments": ["Patch applied in rev. 618975.", "Integrated in Nutch-trunk #354 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/354/])"], "tasks": {"summary": "Upgrade Nutch to Lucene 2.3.0", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade Nutch to Lucene 2.3.0"}, {"question": "What is the main context?", "answer": "Upgrade Nutch to Lucene 2.3.0 release jars."}]}}
{"issue_id": "NUTCH-605", "project": "NUTCH", "title": "Change deprecated configuration methods for Hadoop", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-02-08T01:07:46.656+0000", "updated": "2009-04-10T12:29:04.014+0000", "description": "Changes use of the now deprecated addFinalResource and addDefaultResource methods to just use addResouce", "comments": ["Changes deprecated resource methods for hadoop configuration.", "If nobody has any objections I will go ahead and commit this tonight.", "+1", "Committed.", "Integrated in Nutch-trunk #360 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/360/])", "closing issues for released version"], "tasks": {"summary": "Change deprecated configuration methods for Hadoop", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Change deprecated configuration methods for Hadoop"}, {"question": "What is the main context?", "answer": "Changes use of the now deprecated addFinalResource and addDefaultResource methods to just use addResouce"}]}}
{"issue_id": "NUTCH-606", "project": "NUTCH", "title": "Refactoring of Generator, run all urls through checks", "status": "Closed", "priority": "Minor", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-02-08T22:10:00.895+0000", "updated": "2009-04-10T12:29:02.805+0000", "description": "Refactor the generator to make sure all host run through checks such as host and protocol checks, ip checks if necessary.  Currently the generator only does this for urls if generate.max.per.host > 0 which by default is -1.  So by default all urls will get collected without checks.", "comments": ["Refactors the generator and ensures the checks are run on all urls the could be collected and not just if generate.max.per.host is > 0 (i.e. not default)", "Adds some refactoring to close file readers before exiting if no urls have been fetched.", "+1. A minor issue: I don't think URL.getHost() can return a null value - even for URLs with unspecified host name it returns an empty non-null String.", "Added an empty check for hostnames", "I'm sorry, I should have been clearer ... My point was that it's not necessary to check for null host names, because AFAIK URL.getHost() never returns null. On the other hand, there are legitimate situations when it can return an empty string, so this check that you added in patch v. 3 is in fact harmful. E.g. it would filter out all \"file:///\" urls.", "Yup, did some simple tests and any null or empty urls will be filtered out on creating the url and removing empty hosts will filter out root paths.  Removed the checks for null and empty hosts.", "+1, looks great now.", "If nobody has any objections I will go ahead and commit this tonight.", "Committed.", "Integrated in Nutch-trunk #360 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/360/])", "closing issues for released version"], "tasks": {"summary": "Refactoring of Generator, run all urls through checks", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Refactoring of Generator, run all urls through checks"}, {"question": "What is the main context?", "answer": "Refactor the generator to make sure all host run through checks such as host and protocol checks, ip checks if necessary.  Currently the generator only does this for urls if generate.max.per.host > 0 "}]}}
{"issue_id": "NUTCH-607", "project": "NUTCH", "title": "Update build.xml to include tika jar in war file", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-02-08T22:23:17.153+0000", "updated": "2009-04-10T12:29:00.616+0000", "description": "Update the build.xml to include the tika jar in the war file.  Currently the jar is not included and the cached.jsp page errors out.", "comments": ["Updates build to include the tika.jar for the war.  Correct errors on cached.jsp.", "+1, looks good!", "Committed.", "Integrated in Nutch-trunk #357 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/357/])", "closing issues for released version"], "tasks": {"summary": "Update build.xml to include tika jar in war file", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Update build.xml to include tika jar in war file"}, {"question": "What is the main context?", "answer": "Update the build.xml to include the tika jar in the war file.  Currently the jar is not included and the cached.jsp page errors out."}]}}
{"issue_id": "NUTCH-608", "project": "NUTCH", "title": "Upgrade nutch to use released apache-tika-0.1-incubating", "status": "Closed", "priority": "Major", "reporter": "Chris A. Mattmann", "assignee": "Chris A. Mattmann", "created": "2008-02-09T01:47:27.201+0000", "updated": "2011-06-08T21:35:38.378+0000", "description": "This patch will upgrade Nutch to use the released tika-0.1-incubating jar containing stable APIs and code, as opposed to the -dev version of the jar file that's currently in place in SVN.", "comments": ["If there are no objections, I'd like to commit this patch within the next 24 hrs.\n\nThanks,\n Chris\n", "Sorry folks, the patch didn't go through the first time, just noticed this. Will attach now.\n\nAlso, I'll extend the open time for patch review from 24 hrs to let's say a week. While upgrading to the released version of Tika, I noticed that I had to update the APIs in a few key places. It seems to be passing basic crawls, and unit tests right now, but I'd be much more happy if someone with a great test bed like Dennis (or others) tries the patch out and provides feedback.\n\nThanks, \n Chris\n", "Initial patch, horrendously late :)", "apache tika 0.1-incubating", "This patch includes many whitespace-only changes. It's better to submit such changes separately, because they cause the patch to be much larger than necessary and difficult to review.", "- updated patch, removes unintentional white space changes.\n\nThanks for the review, Andrzej!", "One additional comment, now that the important changes are visible ;) Since you add a util-type class anyway (MimeUtils), why not encapsulate all interactions with Tika inside this class? This way we can protect the rest of Nutch code from future changes in Tika API, and we can avoid adding Tika imports to various classes ...\n\nThe class could be patterned after many other similar classes, having a constructor like MimeUtils(Configuration conf). Then it can wrap all the initialization code, string splitting and the fallback strategies without exposing any Tika classes.", "Hi Andrzej:\n\nGood idea. The facade interface is attached, along with all the appropriate hooks in the latest patch. Comments?\n\nCheers, \n Chris\n", "Looks great. +1\n\nHowever, this patch uncovered two minor bugs - the use of \"static\" keyword in MoreIndexingFilter, ZipExtractor and FileResponse. IMO this should never be static, because MIME attribute may be initialized differently depending on the current Configuration. The bugs are minor because we reuse the same JVM only in the \"local\" mode, where jobs are started usually with the same Configuration. We can track these bugs in a separate issue if you prefer.", "Hi Andrzej,\n\nThanks for your comments. I've removed the static keywords, and attached an updated patch. Does this address all of your concerns? \n\nFurthermore, if it does, are there any other objections from any of the committers, and can I go ahead and commit this patch?\n\nThanks,\n Chris\n", "You missed one in Content ... other than that, +1.", "For completeness sake, an attached patch with the missed Content static ref taken out.\n\nIf there are no further objections, I'd like to commit this sometime in the next 24 hrs.\n\nThanks for the thorough review Andrzej! :)\n\nIt feels good to do some Nutch development again ;)", "+1 looks good.", "- added MimeUtil facade class to insulate Nutch from underlying mime type detector, Tika\n- cleaned up static refs to MimeUtil/MimeType in Content/index-more/parse-zip/protocol-file\n- upgrade to tika-0.1-incubating\n", "- Patch applied to trunk:\n\nhttp://svn.apache.org/viewvc?rev=620811&view=rev\n\nThanks for the reviews, everyone!", "Integrated in Nutch-trunk #360 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/360/])"], "tasks": {"summary": "Upgrade nutch to use released apache-tika-0.1-incubating", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade nutch to use released apache-tika-0.1-incubating"}, {"question": "What is the main context?", "answer": "This patch will upgrade Nutch to use the released tika-0.1-incubating jar containing stable APIs and code, as opposed to the -dev version of the jar file that's currently in place in SVN."}]}}
{"issue_id": "NUTCH-609", "project": "NUTCH", "title": "Allow Plugins to be Loaded from Jar File(s)", "status": "Closed", "priority": "Minor", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-02-09T18:55:10.072+0000", "updated": "2019-10-13T22:35:44.684+0000", "description": "Currently plugins cannot be loaded from a jar file.  Plugins must be unzipped in one or more directories specified by the plugin.folders config.  I have been thinking about an extension to PluginRepository or PluginManifestParser (or both) that would allow plugins to packaged into multiple independent jar files and placed on the classpath.  The system would search the classpath for resources with the correct folder name and would load any plugins in those jars.\n\nThis functionality would be very useful in making the nutch core more flexible in terms of packaging.  It would also help with web applications where we don't want to have a plugins directory included in the webapp.\n\nThoughts so far are unzipping those plugin jars into a common temp directory before loading.  Another option is using something like commons vfs to interact with the jar files.  VFS essential uses a disk based temporary cache for jar files, so it is pretty much the same solution.   What are everyone else's thoughts on this?", "comments": ["What is the reason that the plugin classloader can't load resources directly from a jar file, without unzipping it first?", "It may be able to, I will look into it.  I am still in the early stages of this but have gotten it to unzip jars and load the plugins.  One thing I am seeing is that the actual resources need to be around after the initial parse because the resources are lazily loaded. :)", "My point was that perhaps we can avoid unzipping - e.g URLClassLoader is able to load resources directly from jars, using the bang-path notation, it can also list resources at a specific path inside a jar.", "Sorry, I should have been more clear.  I know that it is possible to load the resources directly from the jar files, I just don't how much work that is going to take.  I agree that avoiding the unzipping of jar files into temp directories and having to manage those directories for deletion is the preferred solution.\n\nAnother thing I was thinking of and would like to get thoughts on is a convention versus configuration solution.  Instead of browsing jar files for named resources, and then having to deal with the contention issues between directories and resources in jars being named the same, what if we were to have plugin jar files named a given way, something like name-plugin.jar.  For example the prefix urlfilter plugin would be named urlfilter-prefix-plugin.jar.  There would be a single plugin per jar and each jar would be the root directory for its plugin.  Then to find plugin jars we are just scanning the classpath for certain named jars.  The downside to this is we could end up with a lot of jars, but currently we are ending up with a lot of folders so I don't know if that is a big difference.  Thoughts?", "bq. the downside to this is we could end up with a lot of jars, but currently we are ending up with a lot of folders so I don't know if that is a big difference. Thoughts?\n\n+1 for this strategy, even if it manes having more jar files to manage. Adopting this strategy would suggest to a more component-based build management system, e.g., a Maven-type. I've been a proponent of using Maven in Nutch for a while, and I think to move the plugins to a .jar file format would ease their adoption as say, remotely downloable Maven style plugins, that then Nutch would rely upon. Then, we could get out of the business of having to CM jar files, which I've never been a fan of.\n\n\n", "Well, as it turns out I haven't found a way to put a jar file inside of another jar file on the classpath.  So something like jar:file:/path/to/jar.jar!/containing.jar won't work.  I will research a little more but it looks to me like even if we have each plugin in its own jar file, which I prefer, we would still need to unzip to a temporary directory to put them on the classpath.  Does anybody know a different way to include a jar inside of another jar on the classpath?", "Rough first draft of patch.  After research I determined that to load classes via  a jar within a jar that a custom classloader would need to be written.  I figured that wasn't the right path to go down right now so created a utility to manage the deletion of resources (files and folders) during shutdown and methods to allow plugins to be unzipped into a temporary folder (the system temp dir by default although this is configurable).  This patch will take any jar file on the classpath that ends in plugin.jar or plugins.jar and will unzip its contents into the plugins temp directory.  This is then added to the plugin folders and parsed as normal.  The plugins temp dir will be kept until the JVM shuts down at which point it and all the resources it contains will be deleted by a shutdown hook.\n\nPlease let me know thoughts on this approach.  I would still need to add unit tests and documentation for these classes and methods.  :)", "bq.  I figured that wasn't the right path to go down right now\n\n\nWhy? It seems like a much cleaner solution.", "Changed to minor status.  Other issues are more important currently.", "I think the direction in general is good, I know that we are the only ones feeling bad with the current solution. We should perhaps put some more effort in thinking about a longer term solution for the next generation Nutch.", "pushing this to 1.1, feel free to put back if there is traction", "- pushing this out per http://bit.ly/c7tBv9", "Not been activity on this one for sometime. Does anyone have any comments re: relevance of this with regards to the way Nutch is moving? I think it is out with the scope of the 1.4 release. ", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "Allow Plugins to be Loaded from Jar File(s)", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Allow Plugins to be Loaded from Jar File(s)"}, {"question": "What is the main context?", "answer": "Currently plugins cannot be loaded from a jar file.  Plugins must be unzipped in one or more directories specified by the plugin.folders config.  I have been thinking about an extension to PluginRepos"}]}}
{"issue_id": "NUTCH-61", "project": "NUTCH", "title": "Adaptive re-fetch interval. Detecting umodified content", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2005-06-06T05:38:45.000+0000", "updated": "2009-04-10T12:29:03.823+0000", "description": "Currently Nutch doesn't adjust automatically its re-fetch period, no matter if individual pages change seldom or frequently. The goal of these changes is to extend the current codebase to support various possible adjustments to re-fetch times and intervals, and specifically a re-fetch schedule which tries to adapt the period between consecutive fetches to the period of content changes.\n\nAlso, these patches implement checking if the content has changed since last fetching; protocol plugins are also changed to make use of this information, so that if content is unmodified it doesn't have to be fetched and processed.", "comments": ["The first round:\n\n* change Page to use a 1-byte float, representing fetchInterval in seconds.\n\n* implement a pluggable FetchSchedule, which adjusts fetchInterval and nextFetchTime\n\n* change FetchListTool and UpdateDatabaseTool to use them. NOTE: it appears there was a bug in FetchListTool, where the fetchlist entries recorded in segments would have their fetchTime increased by 1 week. This is not needed, only pages in WebDB need this.\n\n* improve status reporting throughout all plugins.\n\n* change plugins to detect if the content is unchanged. If possible, plugins will not fetch such content, but in any case they will set their status accordingly.", "Will the same thing work for a filesystem\n\nFor a file system , We can directly get the modified date store it in the db\n\nThe plugins will have a look at the content date and if it is different they will index it \n\nOtherwise they will not fetch it \n\nThis can be a solution for file based content \n\n(The thing is it does away entirely with fetch interval and takes decision only based upon file modification date)", "This patch already supports this. Anyway, it needs to be significantly re-worked to fit into the current development version.", "Is there a patch modified for the current branch or should i take a stab at this?", "I'm working on this, the patch will be available in a couple of days. I could use then your help with review and testing... ;-)", "Most definately! I'll be happy to give it a whirl!", "Updated version for the latest mapred branch.", "This patch is updated to the current trunk/ . The default configuration works as before, and uses DefaultFetchSchedule.\n\nIf there are no major objections I will commit it shortly.", "Not an objection, but a simple comment.\nWhy not making FetchSchedule a new ExtensionPoint and then DefaultFetchSchedule and AdaptiveFetchSchedule some fetch schedule plugins?\n", "I contemplated this for a while, and then decided against it.\n\nThe main reason was that currently most of the \"pluggable\" extensions that result in running a single selected plugin are handled using a simple Factory pattern; as opposed to ChainedFilter pattern, where we use extension points. \n\nI guess the original reason was that implementations would almost always consist of a single class, so it didn't make sense to complicate it and require the whole plugin infrastructure ... It would be the same in this case (just a single class), so I followed the same pattern.\n\nIt's easy to change this to use an extension point, if people prefer it this way.", "This patch, besides bringing it up-to-date with the trunk/, also adds a maximum cap on fetch interval and a better strategy for merging records in Injector.", "Has anyone been using the code with this patch applied?  Just wondering if/how well it works.", "Has this patch by any chance been included in the newer release of nucth or is any one using as Otis asked. The reason is I am about to build a similar patch but if this patch is already working, I can just adapt it to my context. Or will nutch in the future planning to provide this feature out of the box? ", "Unfortunately, this patch hasn't been applied yet, due to its complexity and lack of testing.\n\nBut it will be, sooner or later, because this functionality is required for any serious use.\n\nI'm planning to bring this patch to the latest trunk, and then apply it piece-wise over the next couple of weeks.", "In the fetcher source code : src\\java\\org\\apache\\nutch\\fetcher.java there is this condition which checks to see status of file or url (not to sure) to see if it has been modified. (line ~ 212: case ProtocolStatus.NOTMODIFIED: ). The implementation checks to see if the file is not modified, if TRUE then do nothing, right? It seems that the system already checks the modification data and behaves accordingly ( correct me if I'm wrong). Therefore, the patch here will not be useful in the context of checking the file and providing the appropriate data. One of the use for this patch is when the user requires the system to give lower priorities to unmodified files by increasing the re-fetch time. So it seems that the fetcher can already identify unmodified content. I'll run a few test and post the results here.", "I was able to apply the patch to Nutch 0.8.1 and have it successfully running. I think this patch should be part of the core code. When crawling a terrabyte of data, it is important that only changed data be fetched and parsed. Prior to apply this patch, we run Nutch in our lab and were confronted with SYSTEM OUT MEMORY messages when trying to crawl files as small as 10Gb of data. Now with this patch, it's true the performance will be slower because of checking for the unmodified data but overall it's worth it.\n\n+5 for this patch.\n\n", "Havent looked the patch (tm)\n\nHow would one manage segments after something linke this gets included, i mean now it's more or less safe to delete segments older than configured refetch interval + some marginal,  but after the lifetime of page can vary there's no more such a simple way to manage fetched data.\n", "Actually, there is a way to do this, and this patch implements it.\n\nWe define a maximum \"time to live\" for _any_ page, no matter when it was last fetched or what is its re-fetch interval. This is a system-wide setting. If re-fetch interval is longer than this value, or somehow the page wasn't re-fetched at least that long for other reasons (e.g. because it was unmodified, and we don't fetch unmodified content) - such pages will be forcefully included in fetchlist candidates as if they had DB_UNFETCHED status.\n\nThis means we can be sure that any pages still present in segments older than this maximum TTL will have been refetched, and we can safely discard all segments older than TTL.", "ok, so in my usual use case where there are far more urls than I can fetch this shouldn't have any \"effect\" at all negative or positive.", "I have attached a new patch as the old one need updating before using with Nutch 0.8.1. It will be great if more people can test the feature as I have encounter some issues with plugins such the parse-xml when used with this patch. Over http protocol the patch works well when indexing text/xml/html. When used with a plugins such parse-xml, the fetcher throws a java IllegalStateException. If anybody has this error and knows how to fix, please share it with the rest of us. As of now, i'm working on trying to fix this issue and hoperfully adapt the feature the 0.9.0 version. ", "Applied with some modifications in rev. 542903.", "closing issues for released version"], "tasks": {"summary": "Adaptive re-fetch interval. Detecting umodified content", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Adaptive re-fetch interval. Detecting umodified content"}, {"question": "What is the main context?", "answer": "Currently Nutch doesn't adjust automatically its re-fetch period, no matter if individual pages change seldom or frequently. The goal of these changes is to extend the current codebase to support vari"}]}}
{"issue_id": "NUTCH-610", "project": "NUTCH", "title": "Can't Update or modify an index while web gui is running", "status": "Closed", "priority": "Major", "reporter": "Ciminera Frederic", "assignee": null, "created": "2008-02-11T18:11:46.998+0000", "updated": "2011-06-08T21:34:20.068+0000", "description": "When the search web application is started a NutchBean is created and initializes its searcher on the index files (and also a FetchedSegment on segments).\n\nThis index searcher (and also FetchedSegment) is holding a lock on the files on disk that prevent the index to be updated or modified.\n\nIt would be nice to be able to update an index without having to restart the web server.\n\n", "comments": ["This early patch contains an implementation of a NutchBean that creates a new Searcher for each query and release it once finished to avoid keeping a lock on the index files.\n\nThis patch is not yet compatible with distributed search servers\n", "Patch updated", "Based on your description (the patch is hard to read due to unrelated whitespace changes), this is not the right solution. Creating a Searcher is a heavy operation, and should be done as rarely as possible, otherwise this will cause a dramatic performance hit.\n\nFor DistributedSearch there is already a solution (see NUTCH-581), a similar solution could be implemented for the local searcher (or individual search servers locally), i.e. define a \"magic\" file name that causes the server to reload its indexes. Or implement an additional RPC call to do the same.", "If there are no objections I would like to close this issue as Invalid."], "tasks": {"summary": "Can't Update or modify an index while web gui is running", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Can't Update or modify an index while web gui is running"}, {"question": "What is the main context?", "answer": "When the search web application is started a NutchBean is created and initializes its searcher on the index files (and also a FetchedSegment on segments).\n\nThis index searcher (and also FetchedSegment"}]}}
{"issue_id": "NUTCH-611", "project": "NUTCH", "title": "Upgrade Nutch to use Hadoop 0.16", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-02-13T02:01:24.544+0000", "updated": "2009-04-10T12:29:04.849+0000", "description": "Upgrade Nutch to use the recently released Hadoop 0.16 libraries.  This change removes the deprecated methods addDefaultResource and addFinalResource for configuration.  All configuration is now done through addResource.", "comments": ["This patch upgrades the jar and native libraries, also fixes a stragler unit test in plugins that still used addDefaultResource.", "+1", "Committed.", "Integrated in Nutch-trunk #362 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/362/])"], "tasks": {"summary": "Upgrade Nutch to use Hadoop 0.16", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade Nutch to use Hadoop 0.16"}, {"question": "What is the main context?", "answer": "Upgrade Nutch to use the recently released Hadoop 0.16 libraries.  This change removes the deprecated methods addDefaultResource and addFinalResource for configuration.  All configuration is now done "}]}}
{"issue_id": "NUTCH-612", "project": "NUTCH", "title": "URL filtering is always disabled in Generator when invoked by Crawl", "status": "Closed", "priority": "Major", "reporter": "Susam Pal", "assignee": "Andrzej Bialecki", "created": "2008-02-15T19:49:59.744+0000", "updated": "2009-04-10T12:29:07.519+0000", "description": "When a crawl is done using the 'bin/nutch crawl' command, no filtering is done in Generator even if 'crawl.generate.filter' is set to true in the configuration file.\n\nThe problem is that in the Generator's generate method, the following code unconditionally sets the filter value of the job to whatever is passed to it:-\n\n{code}job.setBoolean(CRAWL_GENERATE_FILTER, filter);{code}\n\nThe code in Crawl.java always passes this as false. \n\nThis has been fixed by exposing an overloaded generate method which takes only the 5 arguments that Crawl needs to set. This overloaded method reads the configuration and sets the filter value appropriately.", "comments": ["Attached patch to fix the bug. This modifies Crawl.java and Generator.java.", "Patch committed to trunk rev. 637114. Thank you!", "Integrated in Nutch-trunk #390 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/390/])"], "tasks": {"summary": "URL filtering is always disabled in Generator when invoked by Crawl", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "URL filtering is always disabled in Generator when invoked by Crawl"}, {"question": "What is the main context?", "answer": "When a crawl is done using the 'bin/nutch crawl' command, no filtering is done in Generator even if 'crawl.generate.filter' is set to true in the configuration file.\n\nThe problem is that in the Genera"}]}}
{"issue_id": "NUTCH-613", "project": "NUTCH", "title": "Empty Summaries and Cached Pages", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Andrzej Bialecki", "created": "2008-02-19T06:27:48.086+0000", "updated": "2011-06-08T21:34:20.329+0000", "description": "There is a bug where some search results do not have summaries and viewing their cached pages causes a NullPointer.  This bug is due to redirects getting stored under the new url and the getURL method of FetchedSegments getting the wrong (old) url which is stored in crawldb but has no content or parse objects.", "comments": ["This patch checks the hit details for an orig field and uses that as the url field if it exists.  This allows the system to correctly find the summary and cached contents.  I don't know if this solves the entire problem of redirects and how they are stored but it does solve the symptom of summaries not showing up and cached pages erroring.", "It seems to me that this code inside of the basic indexing filter is wrong and is what is causing the problem:\n\n    // url is both stored and indexed, so it's both searchable and returned\n    doc.add(new Field(\"url\",\n                      reprUrlString == null ? urlString : reprUrlString,\n                      Field.Store.YES, Field.Index.TOKENIZED));\n    \n    if (reprUrlString != null) {\n      // also store original url as both stored and indexes\n      doc.add(new Field(\"orig\", urlString,\n                        Field.Store.YES, Field.Index.TOKENIZED));\n    }\n\nOk some background.  Fetcher goes to get page A called sourceA and gets redirected to targetA.  Both sourceA and targetA are stored in segments and crawldb.  But sourceA doesn't have parseText, parseData, or Content, only crawl fetch.  TargetA has everything.  TargetA in its metadata has a reprURL possibly pointing to itself, possibly to a different version of itself due to normalization, but more likely pointing to its source, in this case sourceA.  \n\nNow we come to indexer.  Here we add the reprURL, sourceA as the url and the targetA as the orig field.  Then when getting summary (before patch) it got the url field, sourceA, which had no parse objects and hence no summaries and no content so null pointer trying to get cached page.  IMO, url should point to targetA and orig should point to sourceA.  Essentially flipped from what it is here.  ", "The null pointer in opensearch, NUTCH-575, though fixed in a recent patch, was caused by NUTCH-613", "I have the same analysis.  I just change my local code to store the repUrl in the \"orig\" and the urlString in url and now the pb is solved. ", "Patch committed to trunk. Thank you!", "Integrated in Nutch-trunk #390 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/390/])"], "tasks": {"summary": "Empty Summaries and Cached Pages", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Empty Summaries and Cached Pages"}, {"question": "What is the main context?", "answer": "There is a bug where some search results do not have summaries and viewing their cached pages causes a NullPointer.  This bug is due to redirects getting stored under the new url and the getURL method"}]}}
{"issue_id": "NUTCH-614", "project": "NUTCH", "title": "Order Inlinks by OPIC score of parent page", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-02-20T04:37:18.470+0000", "updated": "2009-04-10T12:29:00.359+0000", "description": "Currently when saving inlinks there is a max number of inlinks (configurable) which get saved and very little logic goes into deciding which inlinks get saved.  This patch uses the OPIC score of the encompassing page to set a score for each inlink.  Inlinks are then reverse sorted according to score and the best inlinks are saved first.  The logic behind this is that pages with higher OPIC scores should have better links which they are pointing to.", "comments": ["Orders inlinks by parents OPIC score.", "Very, very messy patch.  This is a first cut at both allowing urls to be ordered by their OPIC score and to allow those inlinks to be indexed according to those scores, meaning field boosts for anchor text.  This patch removes the default document boost for nutch and changes the index filter api to pass in the document or another relevant boost, which must now be handled by the indexing filters.  The base indexing filter has been changed.  Because of this many different plugins have been touched and not all of them have been correctly updated for current behavior.  Inlinks inherit their score from their \"coming from\" parent's OPIC score.  In the current cut of the anchor filter, anchors with no text or zero score are not indexed.", "Not a good solution currently."], "tasks": {"summary": "Order Inlinks by OPIC score of parent page", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Order Inlinks by OPIC score of parent page"}, {"question": "What is the main context?", "answer": "Currently when saving inlinks there is a max number of inlinks (configurable) which get saved and very little logic goes into deciding which inlinks get saved.  This patch uses the OPIC score of the e"}]}}
{"issue_id": "NUTCH-615", "project": "NUTCH", "title": "Redirected URL are fetched wihtout setting any FetchInterval", "status": "Closed", "priority": "Major", "reporter": "Emmanuel Joke", "assignee": "Andrzej Bialecki", "created": "2008-02-26T14:29:00.286+0000", "updated": "2009-04-10T12:29:02.876+0000", "description": "An url which is redirected result to a new URL. We create a new CrawlDatum for the new URL within the Fetcher but the FetchInterval was not initialized.\nThe new url was recorded in the DB with a FetchInterval = 0 and the FetchTime is never correctly updated to be fetch later in the future. Thus we keep crawling those URL at each generation.\n\nThis patch fix this issue.", "comments": ["Another class was impacted by this issue. \n\nNew patch provided\n", "I think the code in ParseOutputFormat doesn't matter that much. Any CrawlDatum-s created with LINKED status will be used only as a source of metadata in CrawlDbReducer, and if it defines a truly new URL then the FetchSchedule will be initialized in CrawlDbReducer anyway.\n\nSo, I think we could apply the parts of the patch in Fetcher-s, and skip the ParseOutputFormat part.", "After a second thought, i agree with you.\nShould i provide another patch or you will handle that with the current patch ?", "I'll apply the parts of the current patch.", "I applied the relevant parts of the patch, plus assorted fixes to check that reprUrl != null, rev. 637858 . Thank you!", "Integrated in Nutch-trunk #393 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/393/])"], "tasks": {"summary": "Redirected URL are fetched wihtout setting any FetchInterval", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Redirected URL are fetched wihtout setting any FetchInterval"}, {"question": "What is the main context?", "answer": "An url which is redirected result to a new URL. We create a new CrawlDatum for the new URL within the Fetcher but the FetchInterval was not initialized.\nThe new url was recorded in the DB with a Fetch"}]}}
{"issue_id": "NUTCH-616", "project": "NUTCH", "title": "Reset Fetch Retry counter when fetch is successful", "status": "Closed", "priority": "Major", "reporter": "Emmanuel Joke", "assignee": "Andrzej Bialecki", "created": "2008-02-26T17:25:07.672+0000", "updated": "2009-04-10T12:29:03.394+0000", "description": "We manage a counter to check how many time the URL has been consecutively in state Retry following some trouble to get the page.\nHere is a sample of the code:\n\ncase ProtocolStatus.RETRY:          // retry\n                 fit.datum.setRetriesSinceFetch(fit.datum.getRetriesSinceFetch()+1);\n \n However i notice that we don't reinitialize this counter at 0 in the case of successful fetch.", "comments": ["Patch provided", "I'm considering a different approach to this patch. There are already 2 Fetcher implementations, and in the future we may want to go even more modular, so patching this issue in every fetching tool doesn't seem appropriate. IMHO this should be handled in the CrawlDb maintenance tools (i.e. CrawlDbReducer). Patch is forthcoming.", "This patch uses FetchSchedule to maintain the counter.", "I applied the latest patch with minor changes, in rev. 637861 . Thank you!", "Integrated in Nutch-trunk #393 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/393/])"], "tasks": {"summary": "Reset Fetch Retry counter when fetch is successful", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Reset Fetch Retry counter when fetch is successful"}, {"question": "What is the main context?", "answer": "We manage a counter to check how many time the URL has been consecutively in state Retry following some trouble to get the page.\nHere is a sample of the code:\n\ncase ProtocolStatus.RETRY:          // r"}]}}
{"issue_id": "NUTCH-617", "project": "NUTCH", "title": "Cached Text Only", "status": "Closed", "priority": "Critical", "reporter": "Siddharth Jha", "assignee": null, "created": "2008-03-04T08:45:44.284+0000", "updated": "2008-03-04T19:22:00.408+0000", "description": "Hello All\n\nI would like to know if it is possible to do Cached Text implementation of webpages in Nutch. By Cached Text , I mean that this should store only the text part of the webpage without any images??\n\nThanks\nSiddharth", "comments": ["This type of question belongs to the nutch-user mailing list - please ask there."], "tasks": {"summary": "Cached Text Only", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Cached Text Only"}, {"question": "What is the main context?", "answer": "Hello All\n\nI would like to know if it is possible to do Cached Text implementation of webpages in Nutch. By Cached Text , I mean that this should store only the text part of the webpage without any im"}]}}
{"issue_id": "NUTCH-618", "project": "NUTCH", "title": "Tika error \"Media type alias already exists\"", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Chris A. Mattmann", "created": "2008-03-06T07:16:51.918+0000", "updated": "2011-06-08T21:35:38.846+0000", "description": "After the upgrade to the latest Tika jar we see a lot of errors like this:\n\n2008-03-06 08:07:20,659 WARN org.apache.tika.mime.MimeTypesReader: Invalid media type alias: text/xml\norg.apache.tika.mime.MimeTypeException: Media type alias already exists: text/xml\n\tat org.apache.tika.mime.MimeTypes.addAlias(MimeTypes.java:312)\n\tat org.apache.tika.mime.MimeType.addAlias(MimeType.java:238)\n\tat org.apache.tika.mime.MimeTypesReader.readMimeType(MimeTypesReader.java:168)\n\tat org.apache.tika.mime.MimeTypesReader.read(MimeTypesReader.java:138)\n\tat org.apache.tika.mime.MimeTypesReader.read(MimeTypesReader.java:121)\n\tat org.apache.tika.mime.MimeTypesFactory.create(MimeTypesFactory.java:56)\n\tat org.apache.nutch.util.MimeUtil.(MimeUtil.java:58)\n\tat org.apache.nutch.protocol.Content.(Content.java:85)\n\tat org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:226)\n\tat org.apache.nutch.fetcher.Fetcher2$FetcherThread.run(Fetcher2.java:523)\n\nThis is caused most likely by the duplicate tika-mimetypes.xml file - one copy is embedded inside the Tika jar, the other is found in Nutch conf/ directory. The one inside the jar seems to be more recent, so I propose to simply remove the one we have in conf.\n", "comments": ["I noticed also another problem: o.a.n.u.MimeUtil doesn't use ObjectCache, so it instantiates MimeTypes over and over again. It should do this once for a given Configuration, and then use ObjectCache to store this object.", "Hey Andrzej:\n\nbq. I noticed also another problem: o.a.n.u.MimeUtil doesn't use ObjectCache, so it instantiates MimeTypes over and over again. It should do this once for a given Configuration, and then use ObjectCache to store this object.\n\nYikes :/ Okay, I will get working on this right away. In addition, I will investigate the cause of the doubly loaded media types -- I'm not positive that it's due to the mime xml file being present inside the tika jar file too -- that's a default one, that we should have the capability to override (like we're doing in Nutch), if we need to.\n\nThanks!\n\nCheers,\n Chris\n", "Any progress on this issue?", "Hey Andrzej:\n\nSorry, I haven't made much progress on the issue. My time has dwindled a bit in the past few months. If someone else has time and wants to reassign the issue, please feel free. Otherwise, I just returned from vacation and will have some free time this weekend, so if there is time until then I can at least prepare a draft patch and submit it for review.\n\nCheers,\n Chris\n", "Hey Guys:\n\nOkey dok: here's a candidate patch. Could someone who has an environment set up already in which these types of errors were manifesting please trying this patch out and see if it makes them go away? I'm thinking that the root of the issue is that the MimeTypes object was not necessarily being re instantiated many many times as much as it wasn't being cached in the ObjectCache. We'll see.\n\nThis attached patch passes all unit tests. So, please let me know what you think.\n\nThanks!\n\nCheers,\n Chris\n", "Dennis Kubes tested this patch for me. According to Dennis, there were 2 lingering log warnings that still came up:\n\n1. For alias:\n\n<alias type=\"application/x-dosexec;exe\" />\n\nremoving the ;exe removed one of the errors\n\n2. removing the subclass from:\n<mime-type type=\"application/xhtml+xml\">\n<sub-class-of type=\"text/xml\" />\n<glob pattern=\"*.xhtml\" />\n<root-XML namespaceURI='http://www.w3.org/1999/xhtml'\nlocalName='html' />\n</mime-type>\n\nremoves the second of the errors.\n\nI am going to attach an updated patch that address these issues.\n\nThanks,\n Chris\n", "Updated patch that includes the updates to tika-mimetypes.xml identified by Dennis Kubes. Thanks, Dennis!\n\nDennis tested this on his testbed environment and it ran through great. So, I'd like to call for 24-48 hr review on the patch, and then if no objections, I'd like to commit it.\n\nThanks!\n\nCheers,\n Chris\n", "- patch applied to trunk:\n\nhttp://svn.apache.org/viewvc?rev=663092&view=rev", "Integrated in Nutch-trunk #471 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/471/])", "closing issues for released version"], "tasks": {"summary": "Tika error \"Media type alias already exists\"", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Tika error \"Media type alias already exists\""}, {"question": "What is the main context?", "answer": "After the upgrade to the latest Tika jar we see a lot of errors like this:\n\n2008-03-06 08:07:20,659 WARN org.apache.tika.mime.MimeTypesReader: Invalid media type alias: text/xml\norg.apache.tika.mime.M"}]}}
{"issue_id": "NUTCH-619", "project": "NUTCH", "title": "Another Language Identifier Plugin using Unicode code point range", "status": "Closed", "priority": "Major", "reporter": "Vinci", "assignee": null, "created": "2008-03-15T15:40:06.254+0000", "updated": "2011-11-28T13:11:00.224+0000", "description": "After I checked the language-identifier plugin, I found the internal implementation is inefficient for language that can be clear identify based on their unicode codepoint  (e.g. CJK Language)\n\nIf Nutch work under unicode, can anybody write a language identifier based on unicode  code point range? The map is here:\nhttp://en.wikipedia.org/wiki/Basic_Multilingual_Plane\n\nalso you can refer to NutchAnalysis.jj for some of language code range \n\n* Some late developed language or rare character - include some CJK character, are moved to SIP\n* May be a special property should be set if multiple language character detected (languages that are other than English alphabet) - my suggestion here is, let CJK locale be the default case as they need bi-gram or other analyzer for better indexing\n** CJK character is very difficult to further divide as they are share han characters - if you really want to identify the specific  member of CJK, you need to use the language identifier plugin", "comments": ["If language identification is delegated to Apache Tika, will all of the above point be considered and addressed?\n\nUnderstandably Apache Tika is still evolving (and this issue is quite clearly not), however I suppose the points made above referring to linguistic properties should be considered within any language identification process.\n\nIf on the other hand we can confirm that the above points will be addressed then I suggest we close this issue and make reference to the fact that it has been superseded by NUTCH-1075.    ", "Language identification is now delegated to Tika."], "tasks": {"summary": "Another Language Identifier Plugin using Unicode code point range", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Another Language Identifier Plugin using Unicode code point range"}, {"question": "What is the main context?", "answer": "After I checked the language-identifier plugin, I found the internal implementation is inefficient for language that can be clear identify based on their unicode codepoint  (e.g. CJK Language)\n\nIf Nut"}]}}
{"issue_id": "NUTCH-62", "project": "NUTCH", "title": "Add html META tag information into metaData in index-more plugin", "status": "Closed", "priority": "Trivial", "reporter": "Jack Tang", "assignee": null, "created": "2005-06-07T10:50:25.000+0000", "updated": "2013-05-22T03:54:46.666+0000", "description": "Now(version dev-0.7), only some metaData  in http response such as type, date, content-length are available int the index-more plugin. And we cannot index/sotre the meta data in html header (<META> exactly)", "comments": ["The attachment contains MetaDataParser and config file. It looks up html META tag, and stored the name-value pairs into metaData map, then you can index the info. in index-more plugin.", "The latest SVN version already contains similar code (see parse-html/..../HTMLMetaProcessor.java). The only thing that is missing is to put the content meta tags into ParseData.metadata.\n\nAs you know, we actually have two places to put metadata into: one is Protocol.metadata, where all protocol-related metadata should be stored, and the other is ParseData.metadata, where parse-related metadata should be stored, which is the case here.\n\nHowever... potentially this may overwrite other properties coming from protocol handlers, or discovered by other plugins or other parts of the code. E.g. the \"lang\" tag is such example, \"content-encoding\" and \"charset\" are other examples. The language identifier plugin works around this by using an \"X-meta-lang\" property name. (BTW: it could be cleaned up to avoid traversing the node tree once again, but instead make use of the already discovered meta tags, which are now passed as an argument to HtmlParseFilters).\n\nI suggest to rework this to use a consistent schema in both cases (i.e. Content.metadata and ParseData.metadata): let's put them  under \"X-nutch-<name>-\" (where <name> is e.g. the value of the key retrieved from HtmlMetaTags.getGeneralTags()), or \"X-nutch-http-equiv-<name>\" prefix (where name is the value of the key retrieved from HtmlMetaTags.getHtpEquivTags)), and so on. So, this would be e.g. \"X-nutch-robots\", \"X-nutch-base\", \"X-nutch-http-equiv-pragma\", \"X-nutch-http-equiv-refresh\").\n\nThis way we can store all <meta> information, without any danger of overwriting the original values.", "There are various comments above which create slight confusion about what to do to resolve this issue... or infact what exactly the issue is that needs to be resolved!\n\nIs there a requirement to rework the htmlMetaProcessor class to incorporate the suggestions above e.g. \"consistent schema in both cases...\"\n\nProtocol.metadata aside, what we are essentially talking about is picking up all Parsedata.metadata included within meta tags which I assume we would wish to index at a later stage. Focussing on the HTMLMetaProcessor class we already acquire name, http-equiv and content attributes from meta tags. WOuld an improvement be to configure the class to pick up other attributes not already mentioned?", "This can be done in a more flexible way using index-metadata\nhttps://issues.apache.org/jira/browse/NUTCH-1264"], "tasks": {"summary": "Add html META tag information into metaData in index-more plugin", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add html META tag information into metaData in index-more plugin"}, {"question": "What is the main context?", "answer": "Now(version dev-0.7), only some metaData  in http response such as type, date, content-length are available int the index-more plugin. And we cannot index/sotre the meta data in html header (<META> ex"}]}}
{"issue_id": "NUTCH-620", "project": "NUTCH", "title": "BasicURLNormalizer should collapse runs of slashes with a single slash", "status": "Closed", "priority": "Minor", "reporter": "Mark DeSpain", "assignee": "Andrzej Bialecki", "created": "2008-03-16T07:22:06.531+0000", "updated": "2009-04-10T12:29:02.994+0000", "description": "The BasicURLNormalizer should collapse runs of slash characters '/' with a single slash.  \n\nFor example,  the following URLs should be normalized to http://lucene.apache.org/nutch/about.html\n\n* http://lucene.apache.org/nutch//about.html \n\n* http://lucene.apache.org//nutch/about.html \n\n* http://lucene.apache.org/////nutch////about.html (an exaggerated example)\n", "comments": ["Is this an often occurring problem? I'm wary of extending the rules in basic normalizer, because they will affect a lot of users, and running each new rule adds certain cost in terms of performance and possible lock-ups of the regex engine. If this is a problem experienced by many users, we should add this rule to the default rule set, but if it's specific just to our environment I think we shouldn't add it.", "Hi Andrzej,\n\nThough I'm very interested in using and learning more about Nutch, I'm still very much new to it.  Please let me know if I have a flawed understand of the behavior I describe below.\n\nYesterday when I had Nutch perform a crawl a site within our intranet, it appeared that Nutch re-visited pages multiple times.  Also, log entries of the \"fetched\" URLs for the repeatedly visited pages would have more and more slashes in them.   Using the URL I posted earlier as an example, I would first see the \"clean\" URL http://lucene.apache.org/nutch/about.html logged.\n\nThen later I would see a progression similar to the following\n\nhttp://lucene.apache.org//nutch/about.html\nhttp://lucene.apache.org///nutch/about.html\nhttp://lucene.apache.org////nutch/about.html\nhttp://lucene.apache.org/////nutch/about.html\n\nI have not debugged this to be sure, but my guess is that there is a web page with a relative URL back to a \"parent\" page which has an extra slash in it, something like ..//../index.html.  Aside from the fact that the the web developer really should clean up the URL, one would hope that a dirty URL like the one describled would not cause the crawl to re-visit a graph of pages.\n\nIf this issue is actually the cause of the behavior I observed, avoiding the re-visitation of a graph of pages is probably worth this extra step in the normalization process.\n\nAgain, please let me know if I have a misunderstanding of how Nutch is supposed to perform its crawl.  I would also be happy to provide more debugging information if needed.\n", "It would be interesting to see the source HTML, which causes these links to appear ... I think your point is valid, Nutch should collapse such adjacent slashes. Could you provide a patch to BasicURLNormalizer that implements this rule?", "Sure :)  I'm a bit swamped at the moment, but I'll try to get a patch attached this coming weekend.  I'll see if I can drum up some relevant HTML source, too.\n", "I did a quick grep of the HTML that was being crawled, and the site did indeed have two anchor tags of the form\n\n<a href=\"../..//javadoc/myPackage/MyClass.html\">\n\nA grep of hadoop.log shows that those \"parent\" pages and everything reachable from them get visited quite a few times.  I'm guessing is because, in my case, a cycle existed and because of the need to collapse adjacent slashes.  \n\nI'm not sure what eventually stops the crawl, but I would assume it is capped by the maximum crawl depth.\n\n ", "Here is a patch with updated BasicURLNormalizer such that it will collapse adjacent slashes.  It also updates the corresponding unit test.\n", "Patch applied with minor whitespace changes (tabs -> spaces). Thank you for the patch and for the unit test!", "Integrated in Nutch-trunk #395 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/395/])"], "tasks": {"summary": "BasicURLNormalizer should collapse runs of slashes with a single slash", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "BasicURLNormalizer should collapse runs of slashes with a single slash"}, {"question": "What is the main context?", "answer": "The BasicURLNormalizer should collapse runs of slash characters '/' with a single slash.  \n\nFor example,  the following URLs should be normalized to http://lucene.apache.org/nutch/about.html\n\n* http:/"}]}}
{"issue_id": "NUTCH-621", "project": "NUTCH", "title": "Nutch needs to declare it's crypto usage", "status": "Closed", "priority": "Blocker", "reporter": "Grant Ingersoll", "assignee": "Chris A. Mattmann", "created": "2008-03-18T13:00:55.223+0000", "updated": "2009-04-10T12:29:07.378+0000", "description": "Per the ASF board direction outlined at http://www.apache.org/dev/crypto.html, Nutch needs to declare it's use of crypto libraries (i.e. BouncyCastle, via PDFBox/Tika).\n\nSee TIKA-118.", "comments": ["Can someone in Nutch-land look into this?  This is a requirement of the ASF and can impact pending releases.\n\nThanks,\nGrant", "Grant,\n\nWill do. Thanks.\n\nCheers, \n Chris\n", "Hi Chris,\n\nI'm sure you have lots on your plate, but I would like to have this wrapped up for our next board report, which is due on the 16th of June.  Is that feasible given your load or does it make sense to try to get some help from other Nutch members?\n\nThanks,\nGrant", "Hi Grant:\n\nThanks for the poke on this. I was speaking with Jukka Zitting about this. Tika requires the crypto declaration because of its transitive Maven dependencies in its Parsing framework on the Bountycastle libraries. Nutch, on the other hand, is using Tika at this point for mime detection only, and Nutch achieves its usage of Tika (0.1-incubating) by CM'ing only the Apache Tika 0.1 jar, and not making use of any of its transitive dependencies (which are inherently Parsing specific, and not Mime Detection specific). In addition, there was a similar thread discussed here:\n\nhttp://markmail.org/message/u7sjfzt7naknsv34\n\nwhere the consensus was you don't need crypto notifications if you don't include any crypto libraries or use the related functionality in an included other library that has an optional dependency on a crypto library. So, I think that Nutch falls within that category. Would you agree?\n\nThanks for your help and guidance.\n\nCheers,\n Chris\n", "I agree, seem to me that we're in same situation as jackrabbit ? I think we do not provide bc libraries with nutch, only pdfbox.", "My understanding is (and I haven't looked at the code) that Nutch has/had the following lines of code somewhere in it:\n\nif (pdf.isEncrypted()) {\n       DocumentEncryption decryptor = new DocumentEncryption(pdf);\n       //Just try using the default password and move on\n       decryptor.decryptDocument(\"\");\n}\n\nWe discussed this at the PMC level a while back and felt that this, unfortunately, was enough to qualify Nutch as having crypto capabilities at some point in time since it explicitly refers to PDFBox's API for decrypting.  Note, also, that it doesn't matter whether it is removed going forward, the code is \"out there\" already, as I understand it.\n\nI can't speak to Jackrabbit's assessment.", "Hi Grant:\n\nThanks. The code does exist in nutch, in the parse-pdf plugin. It seems to be using PDFBox's decrypt functionality:\n\nhttp://svn.apache.org/viewvc/lucene/nutch/trunk/src/plugin/parse-pdf/src/java/org/apache/nutch/parse/pdf/PdfParser.java?view=markup\n\nJudging by your comment, it sounds like this makes Nutch have to declare its crypto usage. I will work to move Nutch towards this. Thanks for the clarification.\n\nCheers,\n Chris\n", "Hmmm, it's PMC board report time again and still no progress on this one.  Can we get this wrapped up by the 17th of September?", "Hi:\n\nOkey dok, could someone with site-dev karma commit the attached patch to:\n\nhttps://svn.apache.org/repos/asf/infrastructure/site/trunk/xdocs/licenses/exports/index.xml\n\nas specified in step 2 of http://www.apache.org/dev/crypto.html ?\n\nThis will get us started. Once that's complete, I'll begin step 3, notifying the U.S. govt.\n\nThanks,\nChris", "Done.  May take an hour or two for the main website to reflect the changes.", "Thanks, Grant!\n\nI will begin step 3 in a few hours...", "Hey Grant:\n\nSorry about this, but I put Nutch in the wrong place on the original patch you committed (I put it under the Incubator project -- which is incorrect).\n\nThis new patch:\n\n1. creates an entry for Apache Lucene as a top-level project with crypto Products\n2. lists Nutch as one of those products, with 2 versions (dev, and releases 0.7 and later)\n\nYour commit mojo is appreciated! ^_^\n\nCheers,\n Chris\n", "Updated.", "Hey Doug:\n\nI've attached a text file containing an email template that we need you to send as the PMC Chair for Lucene, regarding Nutch's crypto status. Could you send ASAP to the TO: addresses in the attached txt file, using the attached text email body, and then let me know when this has been complete?\n\nAt that point, I'll move onto step 4.\n\nThanks!\n\nCheers,\n Chris\n", "I'll take care of it.  Doug actually resigned his chair duties a while ago.  :-)  http://www.apache.org/foundation/  ", "Done.", "Grant:\n\nGreat, thanks. Okay, once you get back the email from the govt (which hopefully we will since perhaps they will CC nutch-dev@ on the reply), I will proceed with step 4:\n\nhttp://www.apache.org/dev/crypto.html#inform\n\nAnd update the appropriate Nutch README file here:\n\nhttp://svn.apache.org/repos/asf/lucene/nutch/trunk/README.txt\n\nwith the crypto notice and then I think we're done!\n\nCheers, \nChris\n", "> [...] get back the email from the govt [...]\n\nIt's a one-way notification, AFAIK the government never responds to the crypto notifications. I guess it's just for archival purposes.\n\nSo it's better not to wait for a response. :-)", "Folks,\n\nBased on Jukka's comments, I've ahead and updated Nutch's README file and completed step 4/4 of the crypto usage for Nutch:\n\nhttp://svn.apache.org/viewvc?rev=699866&view=rev\n\nNutch is now fully compliant with Apache crypto reqts!\n\nGrant, if this is satisfactory, and you are +1, I will go ahead and close this issue. Thanks for everyone's help!\n\nCheers, \nChris\n", "Looks good!\n\nThanks, Chris!", "- resolved in r699866", "Integrated in Nutch-trunk #585 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/585/])\n    - \n", "closing issues for released version"], "tasks": {"summary": "Nutch needs to declare it's crypto usage", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Nutch needs to declare it's crypto usage"}, {"question": "What is the main context?", "answer": "Per the ASF board direction outlined at http://www.apache.org/dev/crypto.html, Nutch needs to declare it's use of crypto libraries (i.e. BouncyCastle, via PDFBox/Tika).\n\nSee TIKA-118."}]}}
{"issue_id": "NUTCH-622", "project": "NUTCH", "title": "Support for application/x-suggestions+json", "status": "Closed", "priority": "Major", "reporter": "Bobby Hubbard", "assignee": null, "created": "2008-03-26T15:09:42.505+0000", "updated": "2011-11-28T13:12:06.061+0000", "description": "Would be really cool to expose an application/x-suggestions+json search reply to support OpenSearch [1] suggestions.\n\n[1] http://developer.mozilla.org/en/docs/Supporting_search_suggestions_in_search_plugins\n", "comments": ["The search is now delegated to SOLR "], "tasks": {"summary": "Support for application/x-suggestions+json", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Support for application/x-suggestions+json"}, {"question": "What is the main context?", "answer": "Would be really cool to expose an application/x-suggestions+json search reply to support OpenSearch [1] suggestions.\n\n[1] http://developer.mozilla.org/en/docs/Supporting_search_suggestions_in_search_p"}]}}
{"issue_id": "NUTCH-623", "project": "NUTCH", "title": "Change plugin source directory \"languageidentifier\" to \"language-identifier\"", "status": "Closed", "priority": "Trivial", "reporter": "Ignacio J. Ortega", "assignee": "Lewis John McGibbney", "created": "2008-03-29T18:33:55.871+0000", "updated": "2011-10-14T09:06:55.949+0000", "description": "When trying to develop and debug Nutch  in eclipse, following the instructions at http://wiki.apache.org/nutch/RunNutchInEclipse0%2e9, you cant run with languageidentifier is rename to language-identifier, when later issue an svn update, you end having two languageidentifier src dirs, one with the dash and another without it, it's an annoyance only, i know, but it stucks me for 2 weeks..so if can be corrected... ", "comments": ["Having checked branch-1.3 in ~/runtime/local/plugins this exists as language-identifier, whereas in ~/src/plugin it exists without the hyphen. Can we make this consistent or is there a reason for difference?\n", "On second thoughts, and taking into consideration other tasks at hand, I am prepared to mark this issue as won't fix due to the fact that we have agreed that language identification is being delegated to Tika as per NUTCH-1075\n", "The functionality being delegated to Tika does mean that it is not happening within the plugin languageidentifier.\nI am not sure whether the problem comes specifically from the use of Eclipse but Nutch is not meant to be compiled or ran from Eclipse : if there are side issues related to that then there are self inflicted :-) ", "If we wished to fix this, then it would be a case of editing both \n/src/plugin/languageidentifier/plugin.xml and build.xml as these both include the following\n\nplugin.xml\n{{{\n<plugin\n   id=\"language-identifier\"\n   name=\"Language Identification Parser/Filter\"\n   version=\"1.0.0\"\n   provider-name=\"nutch.org\">\n\n    <runtime>\n      <library name=\"language-identifier.jar\">\n         <export name=\"*\"/>\n      </library>\n   </runtime>\n}}}\n\nbuild.xml\n{{{\n<project name=\"language-identifier\" default=\"jar-core\">\n\n  <import file=\"../build-plugin.xml\"/>\n}}}\n\nThis surely results in the compiled languageidentifier plugin being built as language-identifier\n\nAs I mentioned above, I am happy to close this one, however it depends whether we have delegation of language identification to Tika on the road map to be included within branch-1.4 release and trunk 2.0 release? ", "Lewis, \n\nAgain this is a separate issue from the delegation of the implementation to Tika : this is still done within this plugin regardless of how you name it. Note that the delegation is already implemented in trunk + I've filed an issue with a patch recently for 1.4\n\n", "yes your right Julien... sorry my mind was wandering.\n\nOK then I will submit a patch once I have tested it myself and hopefully we can maintain a consistent plugin named languageidentifier (as per source code prior to compiling), unless of course we would rather it was changed to language-identifier (as per /runtime/local/plugin/language-identifier after compiling).\n\nAny preference?\n\n", "This patch for branch-1.4 simply resolves some discrepancies between the src name of the language identifier plugin and the name of plugin within /runtime directories after it was compiled.\n\nThe consistent name is now \"languageidentifier\" this means that the plugin jar file is also now consistently named languageidentifier.jar  ", "patch for trunk.\n\nBoth of the above patches can be tested as follows\n{code}\ndownload patch into $NUTCH_HOME\n$ant clean\n$patch -p0 -i $NUTCH_HOME/name-of-patch.patch\n$ant runtime\n{code}\n\nThis will successfully compile and will resolve the the naming discrepancy between language identifier plugin.", "Sorry maybe this time I will attach the correct patch.\n\nThank you", "Committed @ revision 1156683 in branch-1.4\nCommitted @ revision 1156692 in trunk 2.0", "Lewis, \n\nIf you haven't opened the issue yourself, then don't close it unless the person who reported it does not respond after a while.\n\nMore importantly, your modifications break the tests. In the future, please check by running 'ant test' BEFORE committing.\n\nCan you please either revert your patch or fix it so that the tests work again?\n\nThanks\n\nPS I don't really understand the point in changing the name of the plugin (language-identifier was a better name IMHO) if you keep generating a jar called language-identifier.jar\n\n", "Thanks for pointing this out Julien and for your comments, I have reverted the changes. This obviously needs a bit more work before it is fixed.\n\nApologies for any inconvenience caused.", "Integrated in Nutch-trunk-ant #5 (See [https://builds.apache.org/job/Nutch-trunk-ant/5/])\n    commit to revert changes by NUTCH-623 which broke tests.\ncommit to address NUTCH-623 and changes.txt\n\nlewismc : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156712\nFiles : \n* /nutch/trunk/src/plugin/languageidentifier/plugin.xml\n* /nutch/trunk/src/plugin/languageidentifier/build.xml\n* /nutch/trunk/CHANGES.txt\n\nlewismc : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156692\nFiles : \n* /nutch/trunk/src/plugin/languageidentifier/plugin.xml\n* /nutch/trunk/src/plugin/languageidentifier/build.xml\n* /nutch/trunk/CHANGES.txt\n", "Integrated in Nutch-trunk #1575 (See [https://builds.apache.org/job/Nutch-trunk/1575/])\n    commit to revert changes by NUTCH-623 which broke tests.\ncommit to address NUTCH-623 and changes.txt\n\nlewismc : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156712\nFiles : \n* /nutch/trunk/src/plugin/languageidentifier/plugin.xml\n* /nutch/trunk/src/plugin/languageidentifier/build.xml\n* /nutch/trunk/CHANGES.txt\n\nlewismc : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156692\nFiles : \n* /nutch/trunk/src/plugin/languageidentifier/plugin.xml\n* /nutch/trunk/src/plugin/languageidentifier/build.xml\n* /nutch/trunk/CHANGES.txt\n", "Lewis - have you reverted the changes in 1.4 as well??\nSee https://issues.apache.org/jira/browse/NUTCH-1075", "yes he did in 1.4 branch:\n\n{code}\nr1156711 | lewismc | 2011-08-11 20:16:31 +0200 (Thu, 11 Aug 2011) | 1 line\n\nreverting changes made by commit of NUTCH-623 as the patch breaks tests\n{code}", "Hi Julien yes this was changed and then reverted as per your comments and that the tests failed. I've reverted the minor changes, which for clarity changed only the plugin folder name, this was why the tests failed. ", "Patch attachment changes src/plugin/languageidentifier to language-identifier as although the plugin compiles as the latter it is a discrepancy in the source code.\n\nAlthough the patch doesn't indicate that the old languageidentifier source folder will be replaced with the new language-identifier, the following indicates that it will.\n\n{code}\nlewis@lewis-01:~/ASF/branch-1.4/src/plugin$ svn status\nD       languageidentifier\nD       languageidentifier/ivy.xml\nD       languageidentifier/src\nD       languageidentifier/src/test\nD       languageidentifier/src/test/org\nD       languageidentifier/src/test/org/apache\nD       languageidentifier/src/test/org/apache/nutch\nD       languageidentifier/src/test/org/apache/nutch/analysis\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang/en.test\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang/pt.test\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang/da.test\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang/TestHTMLLanguageParser.java\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang/es.test\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang/fr.test\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang/de.test\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang/sv.test\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang/nl.test\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang/it.test\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang/test-referencial.txt\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang/fi.test\nD       languageidentifier/src/test/org/apache/nutch/analysis/lang/el.test\nD       languageidentifier/src/java\nD       languageidentifier/src/java/org\nD       languageidentifier/src/java/org/apache\nD       languageidentifier/src/java/org/apache/nutch\nD       languageidentifier/src/java/org/apache/nutch/analysis\nD       languageidentifier/src/java/org/apache/nutch/analysis/lang\nD       languageidentifier/src/java/org/apache/nutch/analysis/lang/LanguageIndexingFilter.java\nD       languageidentifier/src/java/org/apache/nutch/analysis/lang/HTMLLanguageParser.java\nD       languageidentifier/src/java/org/apache/nutch/analysis/lang/langmappings.properties\nD       languageidentifier/src/java/org/apache/nutch/analysis/lang/package.html\nD       languageidentifier/plugin.xml\nD       languageidentifier/build.xml\nM       build.xml\nA       language-identifier\nA  +    language-identifier/ivy.xml\nA  +    language-identifier/src\nA  +    language-identifier/plugin.xml\nA  +    language-identifier/build.xml\n{code} \n\nI've been learning somewhat as I've been going along here so can people please scrutinise this completely.\nThank you", "It took a little bit to see this in, but now it's done, many thanks all for the interest and the work..", "any comments on testing or the source plugin name change. Julien, this reflects you (and my) preference to keep the language-identifier naming rather than languageidentifier concatenated  and blurry.\n\nIf I get the go-head to commit, I'll patch up trunk as well and submit this in due course. ", "final patch for new trunk 1.4", "Committed @ revision 1175188.\n", "Integrated in Nutch-trunk #1611 (See [https://builds.apache.org/job/Nutch-trunk/1611/])\n    commit to address NUTCH-623 and update to changes.txt\n\nlewismc : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1175188\nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/src/plugin/build.xml\n* /nutch/trunk/src/plugin/language-identifier\n* /nutch/trunk/src/plugin/language-identifier/build.xml\n* /nutch/trunk/src/plugin/language-identifier/ivy.xml\n* /nutch/trunk/src/plugin/language-identifier/plugin.xml\n* /nutch/trunk/src/plugin/language-identifier/src\n* /nutch/trunk/src/plugin/languageidentifier\n", "Integrated in Nutch-trunk #1613 (See [https://builds.apache.org/job/Nutch-trunk/1613/])\n    NUTCH-623 fix source directory\n\nsiren : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1175739\nFiles : \n* /nutch/trunk/build.xml\n", "reopening and applying to nutchgora branch as this is a fairly trivial mapping", "patch attachment for nutchgora branch.", "Committed @ revision 1182114 in nutchgora branch\nfollow up Committed @ revision 1182119 in nutchgora branch", "Integrated in Nutch-nutchgora #32 (See [https://builds.apache.org/job/Nutch-nutchgora/32/])\n    commit to address NUTCH-623 and update to changes.txt\n\nlewismc : http://svn.apache.org/viewvc/nutch/branches/nutchgora/viewvc/?view=rev&root=&revision=1182114\nFiles : \n* /nutch/branches/nutchgora/CHANGES.txt\n* /nutch/branches/nutchgora/src/plugin/build.xml\n* /nutch/branches/nutchgora/src/plugin/language-identifier\n* /nutch/branches/nutchgora/src/plugin/language-identifier/build.xml\n* /nutch/branches/nutchgora/src/plugin/language-identifier/ivy.xml\n* /nutch/branches/nutchgora/src/plugin/language-identifier/plugin.xml\n* /nutch/branches/nutchgora/src/plugin/language-identifier/src\n* /nutch/branches/nutchgora/src/plugin/languageidentifier/build.xml\n* /nutch/branches/nutchgora/src/plugin/languageidentifier/ivy.xml\n* /nutch/branches/nutchgora/src/plugin/languageidentifier/plugin.xml\n* /nutch/branches/nutchgora/src/plugin/languageidentifier/src\n"], "tasks": {"summary": "Change plugin source directory \"languageidentifier\" to \"language-identifier\"", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Change plugin source directory \"languageidentifier\" to \"language-identifier\""}, {"question": "What is the main context?", "answer": "When trying to develop and debug Nutch  in eclipse, following the instructions at http://wiki.apache.org/nutch/RunNutchInEclipse0%2e9, you cant run with languageidentifier is rename to language-identi"}]}}
{"issue_id": "NUTCH-624", "project": "NUTCH", "title": "Better parsed text by default parser", "status": "Closed", "priority": "Major", "reporter": "Vinci", "assignee": "Andrzej Bialecki", "created": "2008-03-30T11:55:11.650+0000", "updated": "2009-01-09T18:58:39.741+0000", "description": "I found the parsed text by default parser, Neko in 1.0 nightly is not easy to process - it just add a space to the end of the tag. \nFor easier analysis,  neko (or other parser) should change the behaviour to \n1.adding tab for inline element\n2.add a tab+newline for block level element end\ninstead of  space\n\nThat will help another application to use the parsed text.", "comments": ["[Add more information on raising this issue]", "Closing due to lack of details and lack of progress."], "tasks": {"summary": "Better parsed text by default parser", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Better parsed text by default parser"}, {"question": "What is the main context?", "answer": "I found the parsed text by default parser, Neko in 1.0 nightly is not easy to process - it just add a space to the end of the tag. \nFor easier analysis,  neko (or other parser) should change the behav"}]}}
{"issue_id": "NUTCH-625", "project": "NUTCH", "title": "Non-ascii character broken in dumped content for mixed encoding (utf-8 and multi-byte)", "status": "Closed", "priority": "Minor", "reporter": "Vinci", "assignee": null, "created": "2008-03-30T12:05:01.418+0000", "updated": "2013-05-22T03:54:51.810+0000", "description": "If the crawl db contains both utf-8 non-ascii character and non-utf-8 non-ascii character(i.e. multi-byte character), the dumped contents by readseg utility will have garbled character appear in all of the non-utf8 non-ascii text, and those texts are unable to repair by encoding reload.\n\nAt the same time, the utf-8 text is normal, only the non-utf8 text broken.\n\nAny possible solution available for repairing the broken text?", "comments": ["I am demoting this to minor as it is not a big deal.\n\nAlso I don't think that there is a solution. So maybe we should just close it as WONTFIX?", "as per Dogacan's comments"], "tasks": {"summary": "Non-ascii character broken in dumped content for mixed encoding (utf-8 and multi-byte)", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Non-ascii character broken in dumped content for mixed encoding (utf-8 and multi-byte)"}, {"question": "What is the main context?", "answer": "If the crawl db contains both utf-8 non-ascii character and non-utf-8 non-ascii character(i.e. multi-byte character), the dumped contents by readseg utility will have garbled character appear in all o"}]}}
{"issue_id": "NUTCH-626", "project": "NUTCH", "title": "fetcher2 breaks out the domain with db.ignore.external.links set at cross domain redirects", "status": "Closed", "priority": "Major", "reporter": "Remco Verhoef", "assignee": "Sami Siren", "created": "2008-04-06T21:16:34.179+0000", "updated": "2009-04-10T12:29:00.759+0000", "description": "Fetcher2 breaks out of the db.ignore.external.links directive when encounterin a cross domain redirect. The redirected url is followed without checking for db.ignore.external.links and cross domain. ", "comments": ["this patch also fixes an other issue with redirects.", "I updated your patch to apply and compile in latest trunk.\n\nI am not committing this patch since I don't want to mess with Todd's\nFetcher work. For now :D", "committed", "Integrated in Nutch-trunk #735 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/735/])\n     - Fetcher2 breaks out the domain with db.ignore.external.links set at cross domain redirects, contributed by Remco Verhoef, dogacan\n", "closing issues for released version"], "tasks": {"summary": "fetcher2 breaks out the domain with db.ignore.external.links set at cross domain redirects", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "fetcher2 breaks out the domain with db.ignore.external.links set at cross domain redirects"}, {"question": "What is the main context?", "answer": "Fetcher2 breaks out of the db.ignore.external.links directive when encounterin a cross domain redirect. The redirected url is followed without checking for db.ignore.external.links and cross domain. "}]}}
{"issue_id": "NUTCH-627", "project": "NUTCH", "title": "Minimize host address lookup", "status": "Closed", "priority": "Major", "reporter": "Otis Gospodnetic", "assignee": "Otis Gospodnetic", "created": "2008-04-10T04:10:14.285+0000", "updated": "2009-01-25T11:40:18.354+0000", "description": "The simple patch that I'm about to attach keeps track of hosts whose \"max URLs per host\" limit we already reached, as well as hosts whose hostname->IP lookup already failed.  For such hosts, further DNS lookups are skipped:\n- there is no point in looking up a hostname yet again if we already have the max number of URLs for that host\n- there is little point in attempting to look up a hostname yet again if the previous lookup already failed\n\nIn a simple test, this saved a few hundred thousand lookups for the first case and a few hundred lookups for the second case.\n\nIf nobody complains, I'll commit by the end of the week.\n", "comments": ["Otis, is the patch already applied? If not, +1 from me.", "Thanks Otis.\nSending        CHANGES.txt\nSending        src/java/org/apache/nutch/crawl/Generator.java\nTransmitting file data ..\nCommitted revision 734257.\n", "Integrated in Nutch-trunk #692 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/692/])\n     - Minimize host address lookup while running generate\n"], "tasks": {"summary": "Minimize host address lookup", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Minimize host address lookup"}, {"question": "What is the main context?", "answer": "The simple patch that I'm about to attach keeps track of hosts whose \"max URLs per host\" limit we already reached, as well as hosts whose hostname->IP lookup already failed.  For such hosts, further D"}]}}
{"issue_id": "NUTCH-628", "project": "NUTCH", "title": "Host database to keep track of host-level information", "status": "Closed", "priority": "Major", "reporter": "Otis Gospodnetic", "assignee": null, "created": "2008-04-12T04:19:56.422+0000", "updated": "2012-07-09T13:54:05.416+0000", "description": "Nutch would benefit from having a DB with per-host/domain/TLD information.  For instance, Nutch could detect hosts that are timing out, store information about that in this DB.  Segment/fetchlist Generator could then skip such hosts, so they don't slow down the fetch job.  Another good use for such a DB is keeping track of various host scores, e.g. spam score.\n\nFrom the recent thread on nutch-user@lucene:\n\nOtis asked:\n> While we are at it, how would one go about implementing this DB, as far as its structures go?\n\nAndrzej said:\nThe easiest I can imagine is to use something like <Text, MapWritable>.\nThis way you could store arbitrary information under arbitrary keys.\nI.e. a single database then could keep track of aggregate statistics at\ndifferent levels, e.g. TLD, domain, host, ip range, etc. The basic set\nof statistics could consist of a few predefined gauges, totals and averages.", "comments": ["Enis' DomainStatistics tool from NUTCH-439.\n(not a solution to this issue, just something that may go well with it)\n\nHere is example usage, for anyone who wants to try DomainStatistics (works nicely):\n\n$ bin/nutch org.apache.nutch.util.domain.DomainStatistics\nhdfs://nn:9000/user/otis/crawl/crawldb/current\nhdfs://nn:9000/user/otis/ds-host host 8\n\nYou can then -cat ds-host file from DFS and pipe it to sort -nrk1 for sorting by count, higher count first.\n", "HostDatum.java\n  - really just a holds MapWritable\n\nHostDb.java\n  - can read an existing HostDb (MapReduce job)\n  - can merge host info from segments into the main HostDb (MapReduce job)\n\n\nThe above classes are in the patch.  Their descriptions are what the plan is and where the patch is headed.  While I have not run/tested this code yet, I would *very* much appreciate if others could have a look and comment on the approach, and have a look at the 2 inner Mapper and 2 inner Reducer classes.\n\nAs for where the host data will come from, I intend to modify Fetcher2 to dump host stats (number of requests, successes, failures, exceptions, timeouts, etc.)  to, say, fetch_hosts file in the current segment.  At this point I don't know what the best file format would be for that, so please .... show me the way.", "IMHO a better option would be to put this data into CrawlDb, and then maintain HostDB data using CrawlDb as the source. The reason is that segments may contain duplicate urls, they may be missing,may be unparsed,  etc - in short, they are transient and not unique. Whereas a CrawlDb is a persistent store of our knowledge about all known urls, and contains only unique urls.\n\nSo, I think that Fetcher-s should put this information in crawl_fetch, the updatedb should stick this information into CrawlDb-s CrawlDatum (this should happen automatically), and the HostDb would simply perform an aggregation of this info from CrawlDb, using hostname / domain name / tld as the keys.", "+1 for extracting hostdb from crawldb...\n\n(also, do we really want to make hostdb just a map file of <Text,MapWritable>? IMHO, it would be better to design a proper HostDatum class with some statistics built-in, and then maybe a Metadata element [I guess it's just me but I hate MapWritable, I prefer Metadata:D])", "Not everything looks like a String ;) MapWritable is useful in situations where you need to (de)serialize non-String types. And most of the information in HostDb is numeric, so if we decided to use simple Metadata it would cause constant pointless conversion from/to Strings.\n\nHaving said that, I'm for a specialized class (which can contain MapWritable as a placeholder for anything else than the specific built-in types of info).", "After seeing NUTCH-650 I have a feeling this issue should be closed with \"Won't Fix\".  Thoughts?\n\nDoes it make sense to save and commit the Domain Statistics patch, though?\n(to be ported to Hbase approach later, once Hbase stuff from NUTCH-650 is in)\n", "I don't know if this issue should be closed or not, but I am moving it to 1.1.\n\n(Should Domain Statistics tool be in 1.0?);", "I'm +1 on getting Domain Stats into 1.0.  The patch will need a small update, I think.", "I don't know much about the patch here. Otis, do you have time to update and commit Domain Stats? If not, I will take a look.", "Could you take it if you have time, please?", "Here is an update to DomainStatistics patch so that it compiles with latest trunk.\n\nOtis, it seems the tool works when given crawldb/current and not just crawldb. Is this\nintended or did I mess something up :) ?", "DomainStatistics is committed as of rev. 738175 .\n\nI am leaving this issue open. We can deal with it after 1.0.", "Integrated in Nutch-trunk #707 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/707/])\n     - DomainStatistics tool\n", "Thanks for the update.  Sorry, I don't recall the details around crawldb/current... is referring to \"current\" bad?\n", "When someone thinks of crawldb, he would probably think of \"crawldb\" directory and not crawldb/current since\ncurrent is pretty much an implementation detail (so that jobs that change crawldb can write their results to a temp directory under crawldb first then this dir can move to crawldb/current). \n\nSo, it is not exactly bad to refer to current, it is just that it may be counter-intuitive for people, who may try to pass crawldb directory to DomainStatistics. Maybe we can add some documentation to command line?\n\nWhat do you think?", "I agree that the crawldb/current/ subdir is an implementation detail that should be hidden from users. All other tools take the name of the parent directory (crawldb/), so I see no reason why this tool should do it differently.", "This tool can also read crawl_fetch and other directories as well. And that is the problem. If you are reading crawl_fetch\nMapFile parts are right under there but for crawldb, MapFile parts are under crawldb/current. I guess we can add a special case for any path that ends in \"crawldb\" but this is not a complete fix either as someone else may rename his crawl database something else.", "- pushing this out per http://bit.ly/c7tBv9", "As part of this fix can someone check that the documentation is up to date too.\n\nI've added a page to our wiki based upon the example above (and yes - it seems to work with a very recent 1.1RC)\n\nhttp://wiki.apache.org/nutch/DomainStatistics\n\nI'm not happy I understand the parameters though eg\n\nnutch@reynolds:/nutch/search$ bin/nutch org.apache.nutch.util.domain.DomainStatistics crawl/crawldb ds-host host 2\nException in thread \"main\" java.io.FileNotFoundException: File does not exist: hdfs://rio46:9000/user/nutch/crawl/crawldb/current/data\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:457)\n\tat org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:51)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:201)\n\tat org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:810)\n\tat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:781)\n\tat org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1249)\n\tat org.apache.nutch.util.domain.DomainStatistics.run(DomainStatistics.java:113)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.util.domain.DomainStatistics.main(DomainStatistics.java:204)\nnutch@reynolds:/nutch/search$ \n\n\nBut this worked.\n\nnutch@reynolds:/nutch/search$ bin/nutch org.apache.nutch.util.domain.DomainStatistics hdfs://rio46:9000/user/nutch/crawl/crawldb/current/ ds-host host 2\n", "From previous discussion on this ticket I think there is evidence that this class has some useful credentials. The problem is that the issue is still open and that there is no entry for this in current Nutch 1.3 /bin/nutch script. Is it worth while providing a patch for this?\n", "Yes, i think this one can be resolved. Command should be added to bin/nutch:\nhttps://issues.apache.org/jira/browse/NUTCH-1049", "Hi Markus, can you confirm if this has been completely integrated and that we have all the functionality from this issue? Thanks", "This one should be closed as it is already implemented by various related issues. Please re-open if you do not agree."], "tasks": {"summary": "Host database to keep track of host-level information", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Host database to keep track of host-level information"}, {"question": "What is the main context?", "answer": "Nutch would benefit from having a DB with per-host/domain/TLD information.  For instance, Nutch could detect hosts that are timing out, store information about that in this DB.  Segment/fetchlist Gene"}]}}
{"issue_id": "NUTCH-629", "project": "NUTCH", "title": "Detect slow and timeout servers and drop their URLs", "status": "Closed", "priority": "Major", "reporter": "Otis Gospodnetic", "assignee": "Otis Gospodnetic", "created": "2008-04-12T07:06:48.617+0000", "updated": "2013-05-22T03:53:30.653+0000", "description": "Fetch jobs will finish faster if we find a way to prevent servers that are either slow or time out from slowing down the whole process.\n\nI'll attach a patch that counts per-server exceptions and timeouts and tracks download speed per server.\nQueues/sservers that exceed timeout or download thresholds are marked as \"tooManyErrors\" or \"tooSlow\".  Once they get marked as such, all of their subsequent URLs get dropped (i.e. they do not fetched) and marked GONE.\n\nAt the end of the fetch task, stats for each server processed are printed.\n\nAlso, I believe the per-host/domain/TLD/etc. DB from NUTCH-628 would be the right place to add server data collected by this patch.\n", "comments": ["While the patch improves fetch speed when there are lots of timeouts, the \"slow but not slow enough\" servers are still a problem.  By that, I mean servers whose download speed is over the threshold, which stops them from just getting dropped from the fetch job, but slow enough that, if they have a lot of URLs in the fetchlist, they still take longer to fetch from, and thus draaaaaaag the fetch run out.\n\nI *think* this download speed information has to be stored in the host DB (NUTCH-628).  Generator could then use this information when generating the fetchlist.  For hosts that are slower, it would generate fewer URLs, and for hosts that are faster, it could generate more URLs.  In the ideal scenario, I think, this would result in URLs from all hosts getting fetched around the same time.\n\nDoes this make sense?  Is my thinking OK or is it flawed?\n", "Hi,\nI'd like to know why this wasn't included in nutch 1.0\nFor memory, this patch is about slow and timeout servers.\nDoes something similar to the patch 629 has ben applied to nutch 1.1 ? I've\ndon't seen something about this problem in the 1.1 version ...\n", "The 2 features below have been added to 1.1 and provide something comparable \n\nhttps://issues.apache.org/jira/browse/NUTCH-769 : Fetcher to skip queues for URLS getting repeated exceptions\nhttps://issues.apache.org/jira/browse/NUTCH-770 : Timebomb for Fetcher\n", "What is the situation with this issue? There is no explanation why it was not included in previous Nutch releases, however as Julien suggests, the issue of the fetcher being slowed/bogged down by slow server responses has been somewhat (fully?) addressed by subsequent Jira issues which have now been resolved, fixed and closed. ", "I think this can be can be marked as won't fix. NUTCH-1067 also offers a similar feature.", "As Otis is no longer with us, as as per Markus' comments I think it best to close this one out. As time has moved on, the functionality suggested has been shared amongst other patches, therefore this is seems to be a duplication."], "tasks": {"summary": "Detect slow and timeout servers and drop their URLs", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Detect slow and timeout servers and drop their URLs"}, {"question": "What is the main context?", "answer": "Fetch jobs will finish faster if we find a way to prevent servers that are either slow or time out from slowing down the whole process.\n\nI'll attach a patch that counts per-server exceptions and timeo"}]}}
{"issue_id": "NUTCH-63", "project": "NUTCH", "title": "the distributed search client generate too much logging statements", "status": "Closed", "priority": "Critical", "reporter": "Stefan Groschupf", "assignee": null, "created": "2005-06-18T05:58:36.000+0000", "updated": "2011-06-08T21:34:03.468+0000", "description": "For each query (depending on the number of segments) tooo many logging statements are generated.\nAfter a short timethis generates gigs of log files. \n\nThis logging  should change to debug:  \nLOG.info(\"Client: segment \"+segments[j]+\" at \"+addr);", "comments": ["Changed to LOG.finest()."], "tasks": {"summary": "the distributed search client generate too much logging statements", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "the distributed search client generate too much logging statements"}, {"question": "What is the main context?", "answer": "For each query (depending on the number of segments) tooo many logging statements are generated.\nAfter a short timethis generates gigs of log files. \n\nThis logging  should change to debug:  \nLOG.info("}]}}
{"issue_id": "NUTCH-630", "project": "NUTCH", "title": "Error caused by index-more plugin  in the latest svn revision - 652259 ", "status": "Closed", "priority": "Major", "reporter": "taknev ivrok", "assignee": "Dogacan Guney", "created": "2008-04-30T15:21:15.405+0000", "updated": "2009-04-10T12:29:05.677+0000", "description": "This problem is reported in the user mailng list: http://www.nabble.com/index-more-problem--td16757538.html\nUpon running bin/nutch  crawl urls -dir crawl  in the latest svn version the following error occurs. \n\nNote: This error does not happen after I remove index-more plugin from plugin.includes in the conf/nutch-site.xml file. \n\nFetcher: done\nCrawlDb update: starting\nCrawlDb update: db: crawlfs/crawldb\nCrawlDb update: segments: [crawlfs/segments/20080430051112]\nCrawlDb update: additions allowed: true\nCrawlDb update: URL normalizing: true\nCrawlDb update: URL filtering: true\nCrawlDb update: Merging segment data into db.\nCrawlDb update: done\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: starting\nGenerator: segment: crawlfs/segments/20080430051126\nGenerator: filtering: true\nGenerator: topN: 100000\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=2 - no more URLs to fetch.\nLinkDb: starting\nLinkDb: linkdb: crawlfs/linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: true\nLinkDb: adding segment:\nfile:/home/admin/nutch-2008-04-30_04-01-32/crawlfs/segments/20080430051112\nLinkDb: adding segment:\nfile:/home/admin/nutch-2008-04-30_04-01-32/crawlfs/segments/20080430051053\nLinkDb: done\nIndexer: starting\nIndexer: linkdb: crawlfs/linkdb\nIndexer: adding segment:\nfile:/home/admin/nutch-2008-04-30_04-01-32/crawlfs/segments/20080430051112\nIndexer: adding segment:\nfile:/home/admin/nutch-2008-04-30_04-01-32/crawlfs/segments/20080430051053\nIFD [Thread-102]: setInfoStream\ndeletionPolicy=org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy@1cfd3b2\nIW 0 [Thread-102]: setInfoStream:\ndir=org.apache.lucene.store.FSDirectory@/tmp/hadoop-admin/mapred/local/index/_1406110510\nautoCommit=true\nmergePolicy=org.apache.lucene.index.LogByteSizeMergePolicy@1536eec\nmergeScheduler=org.apache.lucene.index.ConcurrentMergeScheduler@9770a3\nramBufferSizeMB=16.0 maxBuffereDocs=50 maxBuffereDeleteTerms=-1\nmaxFieldLength=10000 index=\nException in thread \"main\" java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:894)\n        at org.apache.nutch.indexer.Indexer.index(Indexer.java:311)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:144) ", "comments": ["Duplicate of NUTCH-631"], "tasks": {"summary": "Error caused by index-more plugin  in the latest svn revision - 652259 ", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Error caused by index-more plugin  in the latest svn revision - 652259 "}, {"question": "What is the main context?", "answer": "This problem is reported in the user mailng list: http://www.nabble.com/index-more-problem--td16757538.html\nUpon running bin/nutch  crawl urls -dir crawl  in the latest svn version the following error"}]}}
{"issue_id": "NUTCH-631", "project": "NUTCH", "title": "MoreIndexingFilter fails with NoSuchElementException", "status": "Closed", "priority": "Blocker", "reporter": "Stefan Will", "assignee": "Sami Siren", "created": "2008-05-09T20:55:38.033+0000", "updated": "2009-04-10T12:29:03.064+0000", "description": "I did a simple crawl and started the indexer with the index-more plugin activated. The index job fails with the following stack trace in the task log:\n\njava.util.NoSuchElementException\n        at java.util.TreeMap.key(TreeMap.java:433)\n        at java.util.TreeMap.firstKey(TreeMap.java:287)\n        at java.util.TreeSet.first(TreeSet.java:407)\n        at java.util.Collections$UnmodifiableSortedSet.first(Collections.java:1114)\n        at org.apache.nutch.indexer.more.MoreIndexingFilter.addType(MoreIndexingFilter.java:207)\n        at org.apache.nutch.indexer.more.MoreIndexingFilter.filter(MoreIndexingFilter.java:90)\n        at org.apache.nutch.indexer.IndexingFilters.filter(IndexingFilters.java:111)\n        at org.apache.nutch.indexer.Indexer.reduce(Indexer.java:249)\n        at org.apache.nutch.indexer.Indexer.reduce(Indexer.java:52)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:333)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:164)\n\nI traced this down to the part in MoreIndexingFilter where the mime type is split into primary type and subtype for indexing:\n\n    contentType = mimeType.getName();\n    String primaryType = mimeType.getSuperType().getName();\n    String subType = mimeType.getSubTypes().first().getName();\n\nApparently Tika does not have a subtype for text/html. Furthermore, the supertype for text/html is set as application/octet-stream, which I doubt is what we want indexed. Don't we want primaryType to be \"text\" and subType to be \"html\" ?\n\nSo I changed the code to:\n\n    contentType = mimeType.getName();\n    String[] split = contentType.split(\"/\");\n    String primaryType = split[0];\n    String subType = (split.length>1)?split[1]:null;\n    \nThis does what I think it should do, but perhaps I'm missing something ? ", "comments": ["I ran into the same problem, and this seems to fix it. But I don't know enough about Tika so I can't say if this is the right approach.\n\nChris, can you enlighten us here?", "Even after applying the fix above I still ran into problems.\n\n$ search/bin/nutch index crawled/newindexes crawled/crawldb crawled/linkdb crawled/segments/*\n..\n..\n Indexing [http://cap-wiki.somedomain.com/twiki/bin/view/WCE/WikiHintsAndTips] with analyzer org.apache.nutch.analysis.NutchDocumentAnalyzer@402af3 (null)\n29-Sep-2008 10:44:17 org.apache.nutch.util.MimeUtil forName\nWARNING: Exception getting mime type by name: [text/html; charset=iso-8859-15]: Message: Invalid media type name: text/html; charset=iso-8859-15\n Indexing [http://cap-wiki.somedomain.com/twiki/bin/view/WCE/WorldCargo] with analyzer org.apache.nutch.analysis.NutchDocumentAnalyzer@402af3 (null)\n29-Sep-2008 10:44:17 org.apache.nutch.util.MimeUtil forName\nWARNING: Exception getting mime type by name: [text/html; charset=iso-8859-15]: Message: Invalid media type name: text/html; charset=iso-8859-15\n Indexing [http://cap-wiki.somedomain.com/twiki/bin/view/WCE/WrongJVMArgsPickedUp] with analyzer org.apache.nutch.analysis.NutchDocumentAnalyzer@402af3 (null)\n29-Sep-2008 10:44:17 org.apache.nutch.util.MimeUtil forName\nWARNING: Exception getting mime type by name: [text/html; charset=iso-8859-15]: Message: Invalid media type name: text/html; charset=iso-8859-15\n Indexing [http://cap-wiki.somedomain.com/twiki/bin/view/WCE/XPHintsandTips] with analyzer org.apache.nutch.analysis.NutchDocumentAnalyzer@402af3 (null)\nIndexer: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1062)\n        at org.apache.nutch.indexer.Indexer.index(Indexer.java:311)\n        at org.apache.nutch.indexer.Indexer.run(Indexer.java:333)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.indexer.Indexer.main(Indexer.java:316)\n\n\nIn hadoop.log:\n\n2008-09-29 10:44:17,686 INFO  indexer.Indexer -  Indexing [http://cap-wiki.somedomain.com/twiki/bin/view/WCE/WrongJVMArgsPickedUp] with analyzer org.apache.nutch.analysis.NutchDocumentAnalyzer@402af3 (null)\n2008-09-29 10:44:17,688 INFO  indexer.Indexer -  Indexing [http://cap-wiki.somedomain.com/twiki/bin/view/WCE/XPHintsandTips] with analyzer org.apache.nutch.analysis.NutchDocumentAnalyzer@402af3 (null)\n2008-09-29 10:44:17,699 WARN  mapred.LocalJobRunner - job_local_1\njava.util.NoSuchElementException\n        at java.util.TreeMap.key(TreeMap.java:433)\n        at java.util.TreeMap.firstKey(TreeMap.java:287)\n        at java.util.TreeSet.first(TreeSet.java:407)\n        at java.util.Collections$UnmodifiableSortedSet.first(Collections.java:1114)\n        at org.apache.nutch.indexer.more.MoreIndexingFilter.addType(MoreIndexingFilter.java:207)\n        at org.apache.nutch.indexer.more.MoreIndexingFilter.filter(MoreIndexingFilter.java:90)\n        at org.apache.nutch.indexer.IndexingFilters.filter(IndexingFilters.java:111)\n        at org.apache.nutch.indexer.Indexer.reduce(Indexer.java:249)\n        at org.apache.nutch.indexer.Indexer.reduce(Indexer.java:52)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:391)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:201)\n2008-09-29 10:44:18,247 FATAL indexer.Indexer - Indexer: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1062)\n        at org.apache.nutch.indexer.Indexer.index(Indexer.java:311)\n        at org.apache.nutch.indexer.Indexer.run(Indexer.java:333)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.indexer.Indexer.main(Indexer.java:316)", "Edward, you ran into this problem even after you applied the patch? Because, this patch removes the call to first(), but your stack trace still shows a call to first().", "I am promoting this to Blocker, because as of now, index-more is almost guaranteed to fail with an exception on any crawl.", "Attaching a patch that fixes the problem as proposed, If there are no objections I will commit this soon.", "Sami, +1. Sorry I didn't have time to get to this. \n\nThanks for whipping it up.", "committed, thanks", "I had this problem after I install NUTCH-631.patch, the problem has been solved. Thanks", "closing issues for released version"], "tasks": {"summary": "MoreIndexingFilter fails with NoSuchElementException", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "MoreIndexingFilter fails with NoSuchElementException"}, {"question": "What is the main context?", "answer": "I did a simple crawl and started the indexer with the index-more plugin activated. The index job fails with the following stack trace in the task log:\n\njava.util.NoSuchElementException\n        at java"}]}}
{"issue_id": "NUTCH-632", "project": "NUTCH", "title": "Bug in TextParser with encoding", "status": "Closed", "priority": "Major", "reporter": "Antony Bowesman", "assignee": null, "created": "2008-05-20T02:58:17.937+0000", "updated": "2009-04-10T12:29:03.723+0000", "description": "If a Content object is created with the following Content-Type: text/plain; charset=\"windows-1251\"\n\nthe Content object discards the charset parameter.  As a result, when the TextParser calls\n\nString encoding = StringUtil.parseCharacterEncoding(content.getContentType());\n\nit always gets null because the contentType stored in the Content object no longer contains the charset string.  The code has changed a lot from 0.9, so I am not sure if this is still a problem, but I made a fix that simply saves charset in Content with\n\n    if (this.contentType.startsWith(\"text/\"))\n        this.charset = StringUtil.parseCharacterEncoding(contentType);\n\nand TextParser just calls\n\n    String encoding = content.getCharset();\n\n", "comments": ["I am closing this issue as \"Won't Fix\" because TextParser now uses the new EncodingDetector class."], "tasks": {"summary": "Bug in TextParser with encoding", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Bug in TextParser with encoding"}, {"question": "What is the main context?", "answer": "If a Content object is created with the following Content-Type: text/plain; charset=\"windows-1251\"\n\nthe Content object discards the charset parameter.  As a result, when the TextParser calls\n\nString e"}]}}
{"issue_id": "NUTCH-633", "project": "NUTCH", "title": "ParseSegment no longer allow reparsing", "status": "Closed", "priority": "Minor", "reporter": "Xue Yong Zhi", "assignee": "Dogacan Guney", "created": "2008-05-26T07:27:26.573+0000", "updated": "2009-04-10T12:29:03.278+0000", "description": "ParseSegment used to allow reparsing even if parsing has been enabled in Fetcher. But now it throws a NumberFormatException as 'content.getMetadata().get(Nutch.FETCH_STATUS_KEY)' is null.\n\nThis patch will fix the problem:\n\n--- a/src/java/org/apache/nutch/parse/ParseSegment.java\n+++ b/src/java/org/apache/nutch/parse/ParseSegment.java\n@@ -70,8 +70,10 @@ public class ParseSegment extends Configured implements Tool, Mapper<WritableCom\n       key = newKey;\n     }\n     \n+    //status_key is only available when parsing is not done in fetcher\n+    String status_key = content.getMetadata().get(Nutch.FETCH_STATUS_KEY);\n     int status =\n-      Integer.parseInt(content.getMetadata().get(Nutch.FETCH_STATUS_KEY));\n+      (null == status_key) ? CrawlDatum.STATUS_FETCH_SUCCESS : Integer.parseInt(status_key);\n     if (status != CrawlDatum.STATUS_FETCH_SUCCESS) {\n       // content not fetched successfully, skip document\n       LOG.debug(\"Skipping \" + key + \" as content is not fetched successfully\");\n", "comments": ["OK, I shouldn't have missed this one :)\n\nAnyway, I think it is better to modify the fetchers so that they always store FETCH_STATUS_KEY instead of modifying parser.\n\nAnd, here is a patch which does exactly that :D", "Patch committed as of rev. 697896.", "Integrated in Nutch-trunk #580 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/580/])"], "tasks": {"summary": "ParseSegment no longer allow reparsing", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "ParseSegment no longer allow reparsing"}, {"question": "What is the main context?", "answer": "ParseSegment used to allow reparsing even if parsing has been enabled in Fetcher. But now it throws a NumberFormatException as 'content.getMetadata().get(Nutch.FETCH_STATUS_KEY)' is null.\n\nThis patch "}]}}
{"issue_id": "NUTCH-634", "project": "NUTCH", "title": "Patch - Nutch - Hadoop 0.17.1", "status": "Closed", "priority": "Major", "reporter": "Michael Gottesman", "assignee": "Andrzej Bialecki", "created": "2008-06-10T04:41:26.196+0000", "updated": "2009-04-10T12:29:05.592+0000", "description": "This is a patch so that Nutch can be used with Hadoop 0.17.0. The patch is located at http://pastie.org/212001\n\nThe patch compiles and passes all current Nutch unit tests.\n\nI have tested that the crawler side of Nutch (i.e. inject, generate, fetch, parse, merge w/crawldb) definetly works, but have not tested the lucene indexing part. It might work, but it might not. \n\n*NOTE* - the two main bugs that had to be overcome were not noticed by any of the unit tests. The bugs only came up during actual testing. The bugs were:\n\n1. Changes to the Hadoop Iterator\n2. Addition of Serialization to MapReduce Framework\n", "comments": ["I apologize, when I was written this up, I made a mistake, the second bug was definitely caught because the code would not have compiled otherwise... Lack of Sleep => Stupid things...", "Please attach the correct patch to this issue - remember to mark the checkbox that grants ASL license.", "There's a bug in Michael's patch where segments are being passed to the index merger rather than indexes.  I've attached an updated patch.", "Fixes a small problem in the previous patch where segments get passed to the index merger instead of indexes from within Craw.main ", "The attached diff is not a valid patch created with 'svn diff'. Please create a patch using 'svn diff', from the top of the source tree of Nutch trunk/.\n\nI'm not sure whether the FileOnlySequenceFileOutputFormat is the right answer to the problem of _logs directories ... I think the existence of these directories is caused by a setting in Hadoop contiguration, hadoop.job.history.user.location, which defaults to the output directory (which sounds awfully strange to me to use this as a default!). Further investigation is needed before we mess up things on our side. ;)\n\nThe code formatting on these two new files and in some other places doesn't conform to the Nutch formatting, which is basically the Sun style with 2 space indents. Please note also that you use different curly brace placement than the Sun style advises.\n\nGenerics on the CrawlDbReducer are too general, instead of\n\nbq. implements Reducer<WritableComparable,Writable,WritableComparable,Writable>\n\nit should be\n\nbq. implements Reducer<Text, CrawlDatum, Text, CrawlDatum>\n\nSimilar tightening should be done in other places where you added generics.\n\nThe CrawlDatum.shallowCopy() method is dangerous IMHO - newly created copies still contain references to the same metaData instance, which may be modified any time by the framework as you iterate through the input items. We should do a deep clone using WritableUtils.clone().\n\nIndexDoc.copyConstructor() should be replaced by a deep clone().\n\n\n\n", "So actually, I remembered to make it have an ASF, but forgot to redo the diff =p. Sorry. But it looks like Lincoln's patch suffices. Also here is a quick rundown on your comments.\n\n1. I just put in FileOnlySequenceFileOutputFormat because it was the last bug I was getting. I was a little annoyed at the time so I just stuck it in. There is actually a native hadoop way of doing this via a static class. I have seen it before in the code, I just dont remember exactly where.\n\n2. About the code indenting. I was screwing with my emacs trying to get it to do that. But I figured you were more interested in the code and I could deal with that latter =p.\n\n3. Generics easy fix =).\n\n4. The reason that I did the shallowcopy thing even with the metadata, it was not clear to me at the time (I remember being distinctly very tired) since it is of type byte[] if it would be considered a native type or an object. Now of course, I realize that I was really smoking something there... but thats besides the point =p.\n\n5. The IndexDoc.copyConstructor() was just put in because I was not sure if a deep clone would be needed or not.\n\nSo in sum all of what you suggest should be easy changes. =). I will redownload the trunk and do the svn from the trunk, and correct those points.", "My previous patch left out FileSequenceOnlyOutputFileFormat.java and FileOnlyPathFilter.java.  This patch includes them.\n\nHowever, I don't think these are long-term solutions.  Michael can clarify, but it seems that this code is meant to deal with thinks like '_logs' directories in Hadoop.  What we really need is some way to ask Hadoop whether the a directory or file is somehow 'special'.  I'm not sure what the definition of 'special' is here though...\n\nMichael suggested there was a way to do this and I will be looking into it as well, but it would be nice if someone more familiar with Hadoop than I am could chime in to suggest a course of action here.", "This issue will likely be fixed in Hadoop 0.19, until then we can work around this in Nutch by overriding Hadoop property hadoop.job.history.user.location and set it to e.g. ${hadoop.log.dir}/history/user . IMHO using special OutputFormat introduces more confusion and complicates the future upgrades ... Either way, this would have to be documented in the release notes.\n\nI'd like to move forward on this issue in the next few days, if the solution I propose above seems acceptable - that is, to remove the use of special OutputFormats and add an override for that Hadoop property in nutch-default.xml", "There is actually a special thing in hadoop called the HiddenFileFilter in FileInputFormat (or filter I dont remember which). I recently emailed the hadoop dev-list and asked if that could be at the public vs private scope (it resolves the issue by filtering all files that being with _ i.e. _logs). The list said to submit a patch and it would be integrated into hadoop 0.19.\n\nI am going to submit the hadoop patch in a few minutes. In the meantime your idea seems absolutely lovely.\n\nSo yes, your suggestion is prefect =).\n", "I ran a test crawl using Hadoop 0.17.1 release, after applying the portions of this patch without the OutputFormat and setting the property as above. The crawl succeeded with no problems.\n\nIf there are no further objections, I'd like to commit this patch with these changes within a day or two.", "Is there any blocking/pending code on this ? Cannot see it on trunk... I think that (hadoop 0.16 related bug):\n\nhttp://issues.apache.org/jira/browse/HADOOP-3007\n\nhttp://article.gmane.org/gmane.comp.jakarta.lucene.hadoop.user/6947/match=dfs+connection+reset\n\nThis issue is slowing down/blocking my DFS operations with nutch... I'm offering to betatest 0.17 hadoop (this patch) + nutch trunk on my modest 3 node cluster :)", "Sorry, I was away ... I don't think there are any pending issues, I'm going to commit this in a few days.", "In the meantime Hadoop has released version 0.17.1, so it makes sense to upgrade to this version instead of 0.17.0.", "Patch to upgrade to Hadoop 0.17.1. This builds upon the previous patches, but it also replaces many deprecated API uses. It also uses the workaround discussed previously, instead of using specialized InputFormat-s.", "Integrated in Nutch-trunk #516 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/516/])", "As promised, I've tested this patch in production (7-node cluster)... the crawl gets halted after these exceptions:\n\njava.lang.AbstractMethodError: org.apache.nutch.crawl.PartitionUrlByHost.getPartition(Ljava/lang/Object;Ljava/lang/Object;I)I\n\tat org.apache.nutch.crawl.Generator$Selector.getPartition(Generator.java:171)\n\tat org.apache.nutch.crawl.Generator$Selector.getPartition(Generator.java:83)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:464)\n\tat org.apache.nutch.crawl.Generator$Selector.map(Generator.java:165)\n\tat org.apache.nutch.crawl.Generator$Selector.map(Generator.java:83)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)\n\tat org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)\n\njava.lang.AbstractMethodError: org.apache.nutch.crawl.PartitionUrlByHost.getPartition(Ljava/lang/Object;Ljava/lang/Object;I)I\n\tat org.apache.nutch.crawl.Generator$Selector.getPartition(Generator.java:171)\n\tat org.apache.nutch.crawl.Generator$Selector.getPartition(Generator.java:83)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:464)\n\tat org.apache.nutch.crawl.Generator$Selector.map(Generator.java:165)\n\tat org.apache.nutch.crawl.Generator$Selector.map(Generator.java:83)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)\n\tat org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)\n\njava.lang.AbstractMethodError: org.apache.nutch.crawl.PartitionUrlByHost.getPartition(Ljava/lang/Object;Ljava/lang/Object;I)I\n\tat org.apache.nutch.crawl.Generator$Selector.getPartition(Generator.java:171)\n\tat org.apache.nutch.crawl.Generator$Selector.getPartition(Generator.java:83)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:464)\n\tat org.apache.nutch.crawl.Generator$Selector.map(Generator.java:165)\n\tat org.apache.nutch.crawl.Generator$Selector.map(Generator.java:83)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)\n\tat org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2124)\n\nException in thread \"main\" java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1062)\n\tat org.apache.nutch.crawl.Generator.generate(Generator.java:457)\n\tat org.apache.nutch.crawl.Generator.generate(Generator.java:394)\n\tat org.apache.nutch.crawl.Crawl.main(Crawl.java:116)\n", "Please make sure your environment is clean - i.e. there are no leftover classes from previous versions of Hadoop or Nutch. I tested Generator again, and I can't reproduce this error.", "Sure, it was my fault :/\n\nant clean && ant solved the problem, now the crawl is progressing as it should.\n\nThanks !\n\nPS: I've also ran the test suite and there are errors after cleaning the environment:\n\nhadoop@braintop:~/nutch$ ant test | grep -i failed\n    [junit] Test org.apache.nutch.crawl.TestCrawlDbMerger FAILED\n    [junit] Test org.apache.nutch.crawl.TestGenerator FAILED\n    [junit] Test org.apache.nutch.crawl.TestInjector FAILED\n    [junit] Test org.apache.nutch.crawl.TestLinkDbMerger FAILED\n    [junit] Test org.apache.nutch.crawl.TestMapWritable FAILED\n    [junit] Test org.apache.nutch.fetcher.TestFetcher FAILED\n    [junit] Test org.apache.nutch.indexer.TestDeleteDuplicates FAILED\n    [junit] Test org.apache.nutch.searcher.TestDistributedSearch FAILED\n", "Let's handle these in a separate issue - could you please create it? I'm closing this one."], "tasks": {"summary": "Patch - Nutch - Hadoop 0.17.1", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Patch - Nutch - Hadoop 0.17.1"}, {"question": "What is the main context?", "answer": "This is a patch so that Nutch can be used with Hadoop 0.17.0. The patch is located at http://pastie.org/212001\n\nThe patch compiles and passes all current Nutch unit tests.\n\nI have tested that the craw"}]}}
{"issue_id": "NUTCH-635", "project": "NUTCH", "title": "LinkAnalysis Tool for Nutch", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-06-12T18:11:44.259+0000", "updated": "2013-05-02T02:29:17.343+0000", "description": "This is a basic pagerank type link analysis tool for nutch which simulates a sparse matrix using inlinks and outlinks and converges after a given number of iterations.  This tool is mean to replace the current scoring system in nutch with a system that converges instead of exponentially increasing scores.  Also includes a tool to create an outlinkdb.", "comments": ["Basic patch, doesn't include unit tests but it has been tested.  Includes the LinkAnalysis tool and the Outlink tool.  Still needs to handle cases such at telelportation and rank sinks.  But here it is as a first pass for people to see.", "Updated patch.  Contains a score updater for crawl db.  A scoring filter to work with the link analysis tool.  Updated the LinkAnalysis tool to handle reciprocal links, links from the same domain/subdomains, rank sinks, and link loops.  Also included a display tool to view inlinks/outlinks and scores for a given url.  Should be ready for large scale testing.  Tested on a dataset of 25K pages and the results were promising.", "This patch looks great! A few comments:\n\n* in OutlinksDb.reduce() you use a simple assignment mostRecent = next. This doesn't work as expected, because Hadoop iterator reuses the same single instance of Outlinks under the hood, so if you keep a reference to it its value will mysteriously change under your feet as you call values.next(). This should be replaced with a deep copy (or clone) of the instance, either through a dedicated method of Outlinks or WritableUtils.copy().\n\n* you should avoid spurious whitespace changes to existing classes, this makes the reading more difficult ... (e.g. Outlink.java)\n\n* in Outlinks.write() I think there's a bug - you write out System.currentTimeMillis() instead of this.timestamp, is this intentional?\n\n* in LinkAnalysis.Counter.map() , since you output static values, you should avoid creating new instances and use a pair of static instances.\n\n* by the way, in an implementation of similar algo I used Hadoop Counters to count the totals, this way you avoid storing magic numbers in the db itself (although you still need to preserve them somewhere, so I'd create an additional file with this value ... well, perhaps not so elegant either after all ;) ).\n\n* LinkAnalysis.Analyzer.reduce() - you should retrieve config parameters in configure(Job), otherwise you pay the price of getting floats from Configuration (which involves repeated creation of Float via Float.parseFloat()). Also, HashPartitioner should be created once. Well, this is a general comment to this patch - it creates a lot of objects unnecessarily. We can optimize it now or later, whatever you prefer.\n\nI didn't go into the algorithm itself yet to give any useful comments ... But I have a dataset of ~4mln pages I can test it on.\n\n", "Andrzej Bialecki Wrote:\n\n    *  in OutlinksDb.reduce() you use a simple assignment mostRecent = next. This doesn't work as expected, because Hadoop iterator reuses the same single instance of Outlinks under the hood, so if you keep a reference to it its value will mysteriously change under your feet as you call values.next(). This should be replaced with a deep copy (or clone) of the instance, either through a dedicated method of Outlinks or WritableUtils.copy().\n\nFixed this.  Thanks.  I knew it happened for writables but wasn't aware that it was implemented the same way in the iterators.\n\n    * you should avoid spurious whitespace changes to existing classes, this makes the reading more difficult ... (e.g. Outlink.java)\n\nThat was a mistake, fixed it.\n\n    * in Outlinks.write() I think there's a bug - you write out System.currentTimeMillis() instead of this.timestamp, is this intentional?\n\nNope, that was a bug from an earlier version of it.  Fixed.\n\n    * in LinkAnalysis.Counter.map() , since you output static values, you should avoid creating new instances and use a pair of static instances.\n\n    * by the way, in an implementation of similar algo I used Hadoop Counters to count the totals, this way you avoid storing magic numbers in the db itself (although you still need to preserve them somewhere, so I'd create an additional file with this value ... well, perhaps not so elegant either after all ).\n\nThis is really just a temp file.  I count the urls put it into a file using a single reduce task and then read it back in the update method of LinkAnalysis and pass it into the jobs through conf.  Once it is read I delete the file.\n\n    * LinkAnalysis.Analyzer.reduce() - you should retrieve config parameters in configure(Job), otherwise you pay the price of getting floats from Configuration (which involves repeated creation of Float via Float.parseFloat()). Also, HashPartitioner should be created once. Well, this is a general comment to this patch - it creates a lot of objects unnecessarily. We can optimize it now or later, whatever you prefer.\n\nI think a bit of both.  I fixed the HashPartitioner one.  My intention with this first version is to get a workable tool that converges the score and to provide workarounds for the common types of link spam such at reciprocal links and link farms / tightly knit communities.  Once it is working we can always optimize the speed later.  That being said the current version is faster than I thought it would be.  The current patch does converge and it handled reciprocal links and some cases of link farms but it is currently being overinflued by link loops of three or more sights.  Once I have that taken care of I will post a new path.\n", "Stable patch that fixes some of the issues commented on and mentioned previously.  This patch converges well on a dataset of over 100K pages and handles reciprocal linking.  As of yet link farms don't seem to be a problem but we shall see.", "One more question: you said the algorithm converges, but do you have a reference set of values from this dataset, calculated using some other pagerank impl? It would be worthwhile to make sure that the values are indeed the PageRank, as described, and not yet another subtle variation such as our OPIC ;)\n\nThere are a few Java packages for computing PageRank, we could adapt one of those to serve as a baseline:\n\nhttp://law.dsi.unimi.it/\nhttp://webla.sourceforge.net/javadocs/pt/tumba/links/PageRank.html\n", "Adds normalization for many links from a single domain and a penalty threshold for very few inlinks.  Also adds the ability to alter the boost into the index to compensate for front end query boosts.", "Andrzej Bialecki wrote:\n\n> One more question: you said the algorithm converges, but do you have a reference set of values from this dataset, calculated using some other pagerank impl? It would be worthwhile to make sure that the > > values are indeed the PageRank, as described, and not yet another subtle variation such as our OPIC\n\nI was doing it low tech.  By turning on the debug logging, warning it is a large output, and using grep you can see the score converge after a few iterations ;)\n\n> There are a few Java packages for computing PageRank, we could adapt one of those to serve as a baseline:\n> \n> http://law.dsi.unimi.it/\n> http://webla.sourceforge.net/javadocs/pt/tumba/links/PageRank.html\n\nI agree it would be a good comparison.  Strictly speaking though it is not just pagerank.  There are optimizations for multiple links from a given domain, penalties for very few inlinks, and a minimum score value.  All of which are able to be changed through the configuration.  Besides that it does follow the original pagerank algorithm closely.", "Refactored patch that removes network calls using MapFile.Readers and simulates better a row matrix though inverting and merging inlink scores.  This patch works in the general sort-merge-process structure of MapReduce and as such should be significantly faster.  The previous jobs were taking far to long to process on a large dataset.  This patch includes the link anlaysis tool, a tool for updating the crawl db with a new score and clearing scores of urls with no score, an outlink database tool, a new inlink database tool that will keep inlinks consistent with outlinks, and a new scoring plugin which replaces the opic plugin.\n\nThe order of tool runs should now be: Inject, Generate, Fetch, UpdateDb, OutlinkDb, InlinkDb, LinkAnalysis, ScoreUpdater, Indexer", "Finished link analysis and indexer framework along with tools.", "A few comments to the latest patch:\n\n* some crucial javadoc is missing, such as the comments on class level (at least), especially if they are cmd-line utilities or classes that support a major functionality.\n* perhaps we don't need a separate Node db, this information can be added directly to the CrawlDb, which could save us the trouble with running the ScoreUpdater.\n* minor thing, but in many classes you use a repeating pattern of creating instances of List, HashSet, ObjWritable, etc, etc inside the map()/reduce() methods, while they should be created once and reused.\n* LinkDatum:\n** linkType should be byte, not int - this saves 3 bytes on each entry.\n* LinkRank:\n** I wonder if we couldn't skip the Counter job, and instead collect the total number of links via Hadoop job counters. I.e. define counters in Mapper/Reducer of the analysis job, and then after the job is done you can retrieve them from a RunningJob instance. We could then maintain this value on each update of the db in a well-known location, as you do this already, except we could skip this additional runCounter(..) job ...\n* Loops:\n** Loops.Route.readFields(): I think it's better to use Text.readString() instead of DataInput.readUTF(). Or for that matter, replace the plain Strings with Text, since many times in other places in Loops you need to create a Text object anyway, out of one of Route's fields.\n* LinkUpdater:\n** I don't understand why clearScore is set to 0.00001f. What's with the magic number?\n* ReprUrlFixer should go into tools.compat\n* ResolveUrls uses ReprUrlFixer log, it should use its own. Besides, this tool is not relevant to this patch, so I think it should be submitted separately.\n* the new indexing framework: I like the added flexibility, but the cost for that seems high. Previously we only had to run a single map-red job to create an index, now we have to run at least 6 jobs, each with a large dataset. I vote for splitting the patch and creating a separate issue for this framework, so that we can discuss it further.\n", "> # some crucial javadoc is missing, such as the comments on class level (at least), especially if they are cmd-line utilities or classes that support a major functionality.\n\nYup.  Still going through and doing the javadoc.  I will have all that done before any final commit.\n\n> # perhaps we don't need a separate Node db, this information can be added directly to the CrawlDb, which could save us the trouble with running the ScoreUpdater.\n\nThe crawldb can get huge as you know.  It could be updated into the crawldb but then we are stuck using the crawldb everywhere we currently use the nodedb, which is a lot of places both in the analysis and in the indexing.  The way it currently is works much faster and allows us to at a glance see scores and number of links per url using the NodeReader tool.\n\n> linkType should be byte, not int - this saves 3 bytes on each entry.\n\nDone\n\n> Loops.Route.readFields(): I think it's better to use Text.readString() instead of DataInput.readUTF(). Or for that matter, replace the plain Strings with Text, since many times in other places in Loops you need to create a Text object anyway, out of one of Route's fields.\n\nThis has been fixed in a more recent patch\n\n> I don't understand why clearScore is set to 0.00001f. What's with the magic number?\n\nLeftovers.  This has been fixed to 0.0f\n\n> ReprUrlFixer should go into tools.compat\n\nDone\n\n> the new indexing framework: I like the added flexibility, but the cost for that seems high. Previously we only had to run a single map-red job to create an index, now we have to run at least 6 jobs, each with a large dataset. I vote for splitting the patch and creating a separate issue for this framework, so that we can discuss it further.\n\nI agree.  This patch is getting big and the indexing stuff should go into a separate issue.  I will create one.  Also I have reworked the indexer to allow for field filters.  I will post the new patch on the new issue.  \n\nI agree that it is more jobs but I don't see a way around that.  And the new analysis is also more jobs.  I am not afraid of running more jobs on the system as that can be automated.  I am afraid of not having the flexibility that I need and the ability to apply a type of analysis.  The current indexer locks in the databases that can be used and we need more flexibility than that, not just in the what is indexed but also how.  With this approach we can create fields from any MR job and then integrate and index all of those fields.  New fields and analysis scores can be added without changing the indexing code.  The newer patch also creates an extension point for field filters that allow manipulation of the fields and document in the index once the fields are aggregated together.  This allows a great deal of flexibility in indexing fields, aggregates and manipulating document boosts, and in taking other actions such as blacklisting.  Again I will post the new patch soon.\n", "Final patch, includes comments, change suggestions, the new scoring and link analysis tools, and the new indexing framework.", "Final patch.  Includes comment and code change suggestions.  Includes new scoring, link analysis, and indexing frameworks and tools.", "Dennis, please split this patch into the link analysis and indexing parts, and move the part related to the new indexing framework to a separate issue, so that we deal only with the link analysis patch here. Thank you!", "Breaks out the new indexing framework into its own patch NUTCH-646.  Removes the ResolveURLs tool into its own patch.  Makes the patch java 5 compatible.", "I have skimmed through the last patches in this one and NUTCH-646. But I am confused. Are the patches swapped? This one here seems to be about indexing, while NUTCH-646 has loops and link analysis and web graphs :)", "Ooops, yeah crud.  I must have switched them.  I have a little cleanup to do on the indexing one, then I will repost.  Sorry about that.", "Sorry for the late review.... \n\nPatch looks great, and since this is very self contained I see no reason why we do not commit this immediately.\n\nSome notes:\n\n  - Can we also commit a small (5K-6K nodes maybe) test graph, so that future changes can be tested against it?\n  - There are many WritableUtils.clone calls in the code. I don't think that they are necessary.\n  - Instead of ObjectWritable, I would suggest using NutchWritable. NutchWritable is lighter.\n  - There are a couple of new warnings. Mostly with unused JobConf-s and with OptionBuilder. \n  - It may be a good idea to create some plugins for webgraph package to give users some control over which\n    outlinks they want to filter and which to keep (obviously for later)\n  - Can you explain your score formula?\n\n{code}\n\n      // calculate linkRank score formula\n      float linkRankScore = (1 - this.dampingFactor)\n        + (this.dampingFactor * totalInlinkScore);\n\n{code}\n \n      I may be mistaken, but you only seem to have the use case where the random surfer clicks a link on a page and not the he-types-a-new-url-to-start-over use case. Also, why do you add 0.15 (as default value) to every score?", "Updated final patch for new link analysis framework.  I am also going to write up some documentation on the wiki for how this new process works.", "Committed with revision 723441", "Integrated in Nutch-trunk #667 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/667/])\n    "], "tasks": {"summary": "LinkAnalysis Tool for Nutch", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "LinkAnalysis Tool for Nutch"}, {"question": "What is the main context?", "answer": "This is a basic pagerank type link analysis tool for nutch which simulates a sparse matrix using inlinks and outlinks and converges after a given number of iterations.  This tool is mean to replace th"}]}}
{"issue_id": "NUTCH-636", "project": "NUTCH", "title": "Http client plug-in https doesn't work on IBM JRE", "status": "Closed", "priority": "Major", "reporter": "Curtis d'Entremont", "assignee": "Andrzej Bialecki", "created": "2008-06-23T19:10:44.037+0000", "updated": "2009-04-10T12:29:04.965+0000", "description": "I want to crawl my site, which is https, using the protocol-httpclient plug-in. However it throws exceptions each request, something about an unknown algorithm \"SunX509\" for SSL. I don't recall the exact message. I don't have permission to change the JRE on our production server.\n\nI had to modify DummyX509TrustManager to hardcode the string to \"IbmX509\" instead of \"SunX509\" in order to work. It would be better if the plug-in could automatically figure out which one to use. At the very least, try the major ones until you don't hit any exception and take that one.\n", "comments": ["Please check this patch and see if it fixes your issue.", "Actually, under Sun JVM the following line works, too:\n\nTrustManagerFactory factory = TrustManagerFactory.getInstance(\"X509\");\n\nCan you test this with IBM runtime?", "I ran a test with both the default algorithm, which is \"PKIX\" for me, and also with \"X509\" and didn't get the error with either of them.", "Fixed in rev. 741559. Thank you!", "Integrated in Nutch-trunk #717 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/717/])\n     Httpclient plugin https doesn't work on IBM JRE.\n"], "tasks": {"summary": "Http client plug-in https doesn't work on IBM JRE", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Http client plug-in https doesn't work on IBM JRE"}, {"question": "What is the main context?", "answer": "I want to crawl my site, which is https, using the protocol-httpclient plug-in. However it throws exceptions each request, something about an unknown algorithm \"SunX509\" for SSL. I don't recall the ex"}]}}
{"issue_id": "NUTCH-637", "project": "NUTCH", "title": "Add method to nutch and tika system(Code written)", "status": "Closed", "priority": "Major", "reporter": "Michael Bostwick", "assignee": null, "created": "2008-07-16T18:32:38.775+0000", "updated": "2009-04-10T12:29:07.288+0000", "description": "  public MimeType getMimeType(byte[] content){\n\t  return this.mimeTypes.getMimeType(content);\n  }\n paste this code into nutch/src/java/org/apache/nutch/util/MimeUtil.java which will allow byte content to be used to determine file type", "comments": ["Why do you need this method?\n\n(Content-type is already auto-discovered for a content anyway...)", "Unless there are more arguments in favor of this issue, I suggest closing it as Won't Fix."], "tasks": {"summary": "Add method to nutch and tika system(Code written)", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add method to nutch and tika system(Code written)"}, {"question": "What is the main context?", "answer": "  public MimeType getMimeType(byte[] content){\n\t  return this.mimeTypes.getMimeType(content);\n  }\n paste this code into nutch/src/java/org/apache/nutch/util/MimeUtil.java which will allow byte content"}]}}
{"issue_id": "NUTCH-638", "project": "NUTCH", "title": "Launching Distributed Searchers with URI indicating filesystem to use rather than relying on hadoop config files.", "status": "Closed", "priority": "Minor", "reporter": "Aaron Nall", "assignee": null, "created": "2008-07-28T19:32:34.529+0000", "updated": "2011-06-08T21:34:20.650+0000", "description": "I wanted to conduct all index creation operations in hdfs but search from the local file system using the same cluster of machines.  I believe that this is a common use case.  \n\nThis required either a parallel nutch install or edits (scripted or manual) to hadoop-site.xml to change the file system from hdfs to local when starting a distributed searcher service.  This minor patch makes IndexSearcher and NutchBean honor URIs as supported by hadoop 0.17 without altering existing functionality if simple paths are entered.", "comments": ["This is the patch that I used to address the issue.", "I think in NutchBean.java we can also use dir.getFileSystem(conf) instead of FileSystem.get(dir.toUri(), this.conf). Could you please test if this works for you? Other than that the patch looks fine.", "Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Launching Distributed Searchers with URI indicating filesystem to use rather than relying on hadoop config files.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Launching Distributed Searchers with URI indicating filesystem to use rather than relying on hadoop config files."}, {"question": "What is the main context?", "answer": "I wanted to conduct all index creation operations in hdfs but search from the local file system using the same cluster of machines.  I believe that this is a common use case.  \n\nThis required either a"}]}}
{"issue_id": "NUTCH-639", "project": "NUTCH", "title": "Change LuceneDocumentWrapper visibility from private to protected", "status": "Closed", "priority": "Minor", "reporter": "Guillaume Smet", "assignee": "Dogacan Guney", "created": "2008-08-06T08:20:12.718+0000", "updated": "2009-04-10T12:29:01.940+0000", "description": "Hi,\n\nI'm currently working on a nutch-solr integration based on http://wiki.apache.org/nutch/RunningNutchAndSolr .\n\nI changed what is proposed in this page a bit so that I don't have to patch Nutch anymore - I just have to add a jar containing a custom Indexer in nutch/lib.\n\nThe only remaining problem which forced me to patch nutch is the following step:\n9. Edit nutch-trunk/src/java/org/apache/nutch/indexer/Indexer.java changing scope on LuceneDocumentWrapper from private to protected\n\nCould we consider changing the visibility of LuceneDocumentWrapper to protected directly in Nutch trunk so that we could inherit from Indexer to create our own ones without patching nutch?\n\nI attached the (trivial...) patch to do so.\n\nThanks.", "comments": ["This seems to be a frequently requested feature. So I am going to commit this one if no one has objections (actually I am going to make LuceneDocumentWrapper public)", "+1", "Visibility changed to public as of rev 697395.", "Integrated in Nutch-trunk #578 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/578/])"], "tasks": {"summary": "Change LuceneDocumentWrapper visibility from private to protected", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Change LuceneDocumentWrapper visibility from private to protected"}, {"question": "What is the main context?", "answer": "Hi,\n\nI'm currently working on a nutch-solr integration based on http://wiki.apache.org/nutch/RunningNutchAndSolr .\n\nI changed what is proposed in this page a bit so that I don't have to patch Nutch an"}]}}
{"issue_id": "NUTCH-64", "project": "NUTCH", "title": "no results after a restart of a search--server (without tomcat restart)", "status": "Closed", "priority": "Trivial", "reporter": "Michael Nebel", "assignee": null, "created": "2005-06-30T18:31:16.000+0000", "updated": "2011-06-08T21:34:03.224+0000", "description": "After restarting the search-server without restarting the tomcat,  the resultpage stays occasionally white. The reason:\n\n2005-06-25 12:56:16 StandardWrapperValve[jsp]: Servlet.service() for servlet jsp threw exception\njava.lang.NullPointerException\n        at java.util.Hashtable.get(Hashtable.java:333)\n        at net.nutch.ipc.Client.getConnection(Client.java:278)\n        at net.nutch.ipc.Client.call(Client.java:253)\n        at net.nutch.searcher.DistributedSearch$Client.getSummary(DistributedSearch.java:418)\n        at net.nutch.searcher.NutchBean.getSummary(NutchBean.java:248)\n        at org.apache.jsp.meta_jsp._jspService(meta_jsp.java:115)\n        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n        at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:324)\n        at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:292)\n        at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:236)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:237)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:157)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:214)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.StandardContextValve.invokeInternal(StandardContextValve.java:198)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:152)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:137)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:117)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:102)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:929)\n        at org.apache.coyote.tomcat5.CoyoteAdapter.service(CoyoteAdapter.java:160)\n        at org.apache.jk.server.JkCoyoteHandler.invoke(JkCoyoteHandler.java:300)\n        at org.apache.jk.common.HandlerRequest.invoke(HandlerRequest.java:374)\n        at org.apache.jk.common.ChannelSocket.invoke(ChannelSocket.java:743)\n        at org.apache.jk.common.ChannelSocket.processConnection(ChannelSocket.java:675)\n        at org.apache.jk.common.SocketConnection.runIt(ChannelSocket.java:866)\n        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:683)\n        at java.lang.Thread.run(Thread.java:534)\n\n", "comments": ["the following patch seems to solve the problem: \n\ndiff -u  net/nutch/ipc/Client.java* \n--- net/nutch/ipc/Client.java   2005-06-29 14:41:02.000000000 +0200\n+++ net/nutch/ipc/Client.java.~1.10.~   2005-02-21 19:54:08.000000000 +0100\n@@ -275,13 +275,7 @@\n     throws IOException {\n     Connection connection;\n     synchronized (connections) {\n-       // mijo: after a restart of the search-server connections\n-       // might be null\n-       if (connections !=null ) {\n-           connection = (Connection)connections.get(address);\n-       } else {\n-           connection=null;\n-       }\n+      connection = (Connection)connections.get(address);\n       if (connection == null) {\n         connection = new Connection(address);\n         connections.put(address, connection);\n", "duplicate with NUTCH-14"], "tasks": {"summary": "no results after a restart of a search--server (without tomcat restart)", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "no results after a restart of a search--server (without tomcat restart)"}, {"question": "What is the main context?", "answer": "After restarting the search-server without restarting the tomcat,  the resultpage stays occasionally white. The reason:\n\n2005-06-25 12:56:16 StandardWrapperValve[jsp]: Servlet.service() for servlet js"}]}}
{"issue_id": "NUTCH-640", "project": "NUTCH", "title": "confusing description \"set it to Integer.MAX_VALUE\"", "status": "Closed", "priority": "Minor", "reporter": "Stijn Vermeeren", "assignee": "Dogacan Guney", "created": "2008-08-06T12:47:11.810+0000", "updated": "2008-10-03T04:18:14.730+0000", "description": "This property \"indexer.max.tokens\" has the following description in nutch-default.xml :\n\n\" The maximum number of tokens that will be indexed for a single field\n  in a document. This limits the amount of memory required for\n  indexing, so that collections with very large files will not crash\n  the indexing process by running out of memory.\n\n  Note that this effectively truncates large documents, excluding\n  from the index tokens that occur further in the document. If you\n  know your source documents are large, be sure to set this value\n  high enough to accomodate the expected size. If you set it to\n  Integer.MAX_VALUE, then the only limit is your memory, but you\n  should anticipate an OutOfMemoryError.\"\n\nApparently, \"set it to Integer.MAX_VALUE\" here means <<substitute the integer value of Integer.MAX_VALUE>>, and not <<put the text \"Integer.MAX_VALUE\" between the value tags>>. I think this is very confusing and the description should be improved.\n\nI first put <value>Integer.MAX_VALUE</value> in my configuration, and it took a long time to figure out what was wrong, especially since Nutch rolled back on the default value of 10000 instead of giving an error.", "comments": ["You are right that it is confusing but asking users to substitute value of Integer.MAX_VALUE would also be unnecessarily difficult. \n\nAttached patch instead changes conf description to use -1 instead of Integer.MAX_VALUE. Also, Indexer is modified to check for negatives in indexer.max.tokens and make them Integer.MAX_VALUE.", "Committed as of rev. 701052.", "Integrated in Nutch-trunk #588 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/588/])\n     - confusing description \"set it to Integer.MAX_VALUE\"\n"], "tasks": {"summary": "confusing description \"set it to Integer.MAX_VALUE\"", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "confusing description \"set it to Integer.MAX_VALUE\""}, {"question": "What is the main context?", "answer": "This property \"indexer.max.tokens\" has the following description in nutch-default.xml :\n\n\" The maximum number of tokens that will be indexed for a single field\n  in a document. This limits the amount "}]}}
{"issue_id": "NUTCH-641", "project": "NUTCH", "title": "IndexSorter incorrectly copies stored fields", "status": "Closed", "priority": "Critical", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2008-08-06T21:57:02.549+0000", "updated": "2009-04-10T12:29:03.883+0000", "description": "Recent versions of Lucene introduced IndexReader.document(int, FieldSelector) method. When using IndexWriter.addIndexes(IndexReader[]) Lucene now uses that method from IndexReader instead of the old one IndexReader.document(int).\n\nUnfortunately, this new method is not overriden in IndexSorter, which leads to a subtle corruption of sorted indexes - while the indexed fields are sorted properly, the values from stored fields are not sorted and remain in the sorted index in the original order. This means that in a sorted index the values of indexed fields and stored fields are completely out of sync, which later results in incorrect documents being retrieved from segments.", "comments": ["This patch fixes the issue, and adds a unit test.", "Patch applied.", "Integrated in Nutch-trunk #545 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/545/])"], "tasks": {"summary": "IndexSorter incorrectly copies stored fields", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "IndexSorter incorrectly copies stored fields"}, {"question": "What is the main context?", "answer": "Recent versions of Lucene introduced IndexReader.document(int, FieldSelector) method. When using IndexWriter.addIndexes(IndexReader[]) Lucene now uses that method from IndexReader instead of the old o"}]}}
{"issue_id": "NUTCH-642", "project": "NUTCH", "title": "Unit tests fail when run in non-local mode", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2008-08-06T23:07:46.873+0000", "updated": "2009-04-10T12:29:03.177+0000", "description": "Unit tests work correctly only when run in Hadoop \"local\" mode. In distributed mode the classpath that JUnit uses doesn't contain the job jar, so Hadoop doesn't know where to find the implementing classes, and consequently all map-reduce jobs fail.", "comments": ["This patch fixes the issue. If there are no objections I'd like to commit it within a day or two.", "Ok, now I see your point (ant test from NUTCH-634 was from my local laptop, not from the cluster)...\n\nI just did ant test on the cluster and:\n\nBUILD FAILED\n/home/hadoop/nutch/build.xml:284: Could not create task or type of type: junit.\n\nAnt could not find the task or a class this task relies upon.\n\nApplied your patch and I got the same result. Should I \"ant clean\" or put junit.jar somewhere ?", "Patch applied.", "Integrated in Nutch-trunk #545 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/545/])"], "tasks": {"summary": "Unit tests fail when run in non-local mode", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Unit tests fail when run in non-local mode"}, {"question": "What is the main context?", "answer": "Unit tests work correctly only when run in Hadoop \"local\" mode. In distributed mode the classpath that JUnit uses doesn't contain the job jar, so Hadoop doesn't know where to find the implementing cla"}]}}
{"issue_id": "NUTCH-643", "project": "NUTCH", "title": "ClassCastException in PdfParser on encrypted PDF with empty password", "status": "Closed", "priority": "Major", "reporter": "Guillaume Smet", "assignee": "Andrzej Bialecki", "created": "2008-08-08T13:48:29.285+0000", "updated": "2009-04-10T12:29:03.318+0000", "description": "Hi,\n\nIf a PDF document is encrypted with an empty password, the PdfParser should decrypt it using the empty password.\n\nThis behaviour is implemented with the following code:\n      if (pdf.isEncrypted()) {\n        DocumentEncryption decryptor = new DocumentEncryption(pdf);\n        //Just try using the default password and move on\n        decryptor.decryptDocument(\"\");\n      }\nIt uses a deprecated API and moreover it seems there is a bug in PDFBox in this deprecated API (we have a ClassCastException in PDFBox) as we have the following error:\n\n2008-08-07 19:15:56,860 WARN  parse.pdf - General exception in PDF parser: org.pdfbox.pdmodel.encryption.PDEncryptionDictionary cannot be cast to org.pdfbox.pdmodel.encryption.PDStandardEncryption\n2008-08-07 19:15:56,862 WARN  parse.pdf - java.lang.ClassCastException: org.pdfbox.pdmodel.encryption.PDEncryptionDictionary cannot be cast to org.pdfbox.pdmodel.encryption.PDStandardEncryption\n2008-08-07 19:15:56,862 WARN  parse.pdf - at org.pdfbox.encryption.DocumentEncryption.decryptDocument(DocumentEncryption.java:197)\n2008-08-07 19:15:56,862 WARN  parse.pdf - at org.apache.nutch.parse.pdf.PdfParser.getParse(PdfParser.java:98)\n2008-08-07 19:15:56,862 WARN  parse.pdf - at org.apache.nutch.parse.ParseUtil.parse(ParseUtil.java:82)\n2008-08-07 19:15:56,862 WARN  parse.pdf - at org.apache.nutch.fetcher.Fetcher$FetcherThread.output(Fetcher.java:336)\n2008-08-07 19:15:56,862 WARN  parse.pdf - at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:178)\n2008-08-07 19:15:56,874 WARN  fetcher.Fetcher - Error parsing: http://www2.culture.gouv.fr/deps/fr/stateurope071.pdf: failed(2,0): Can't be handled as pdf document. java.lang.ClassCastException: org.pdfbox.pdmodel.encryption.PDEncryptionDictionary cannot be cast to org.pdfbox.pdmodel.encryption.PDStandardEncryption\n\nUsing the new security API, we don't have any error parsing this document and we can get its content:\n\t\t\tif (pdf.isEncrypted()) {\n\t\t\t\t// Just try using the default password and move on\n\t\t\t\tpdf.openProtection(new StandardDecryptionMaterial(\"\"));\n\t\t\t}\n\nI attached the patch fixing this problem: it works perfectly with the above document and get rids of the deprecated API.\n\nRegards,\n\n-- \nGuillaume", "comments": ["In fact, the problem is more complex than an API problem and is solved in current PDFBox trunk (from Apache incubator). I used the revision 683874 .\n\nI made the following changes:\n- upgrade from FontBox-0.1-dev to FontBox-0.2-dev (shipped in PDFBox lib/ directory)\n- upgrade from PDFBox-0.7.3 to PDFBox-0.7.4-dev (rev: 683874)\n- copy bcprov-jdk14-132.jar, bcmail-jdk14-132.jar and their licence to parse-pdf lib/ directory: the license seems to be compatible with Apache license (I took the jars from PDFBox trunk)\n- fix the deprecation issues in PdfParser\n\nI had a lot of errors indexing a bunch of PDF files from several websites. After this upgrade, it's far far better: I don't have any ClassCastException issues in PDFBox anymore (they fixed them in the current trunk, for example see this patch from Feb 2007: http://pdfbox.cvs.sourceforge.net/pdfbox/pdfbox/src/org/pdfbox/filter/FlateFilter.java?r1=1.10&r2=1.11 ).\n\nPatch attached. The patch doesn't contain the jars but they are referenced in the patch for completeness. I can add them if needed.", "AFAIK we can't include libraries from projects undergoing incubation, because their legal status is not fully confirmed by ASF. I think we have to wait until PDFBox comes out from the incubation, or to use the latest non-Apache version (which unfortunately doesn't yet address this problem).", "Hi Andrzej,\n\nThis problem is also fixed in the non-Apache repository of PDFBox (on sf.net - the link I posted is from the sf.net CVS tree). I don't know though if you can build and ship a non released version of PDFBox according to ASF release policy.\n\nEven if we can't solve it in the Nutch tree right now, the problem is now referenced and people can solve it by themselves quite easily.\n\nRegards,\n\n-- \nGuillaume\n\n", "So... Can we commit this patch and pdfbox? It seems pdfbox is released under a BSD license. Is it compatible with ASF license?", "Hi Doğacan,\n\nThe problem isn't the license of PDFBox which is already included in Nutch. It's more than PDFBox is on its way to become an Apache project (it's in the incubator - see http://incubator.apache.org/pdfbox/) and it seems that you can't include a library which is in the incubator.\n\nSo you can either wait for PDFBox to be a real Apache project or build a development version of the latest PDFBox tree which is on sourceforge.net, which is what I did (the problem is fixed in the sf.net tree) but you then have a development version in the Nutch tree and not a stable release: I'm not sure it's acceptable.\n\nIt's more a problem of release policy and release rules than a technical or license problem.\n\n-- \nGuillaume", "+1. Yes, it's compatible.", "(sorry Guillame, missed your comment) - there is an existing precedent in Nutch source tree, namely the Tika library, which is still incubating. This practice is however frowned upon ;) I'm ok with using the latest SF.net version of PDFBox built from sources, provided we include a notice about the SVN revision of the library. This is probably better than using the version from the incubator and make the legal situation worse.", "Right, we should update tika to 0.2 (post-incubation) too before releasing 1.0 :) I actually would do that a while back, but then I know nothing about tika, so worried about breaking stuff.\n", "Fixed in rev. 741558, using CVS HEAD version of PDFBox 0.7.4 from SourceForge. During tests on documents containing images I discovered that it's necessary to add JAI libraries too - this unfortunately increased the size of the plugin.", "Integrated in Nutch-trunk #717 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/717/])\n     ClassCastException in PDF parser, upgrade to unofficial PDFBox 0.7.4\n"], "tasks": {"summary": "ClassCastException in PdfParser on encrypted PDF with empty password", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "ClassCastException in PdfParser on encrypted PDF with empty password"}, {"question": "What is the main context?", "answer": "Hi,\n\nIf a PDF document is encrypted with an empty password, the PdfParser should decrypt it using the empty password.\n\nThis behaviour is implemented with the following code:\n      if (pdf.isEncrypted("}]}}
{"issue_id": "NUTCH-644", "project": "NUTCH", "title": "RTF parser doesn't compile anymore", "status": "Closed", "priority": "Major", "reporter": "Guillaume Smet", "assignee": null, "created": "2008-08-08T14:26:23.103+0000", "updated": "2011-04-01T15:07:22.570+0000", "description": "Due to API changes, the RTF parser (which is not compiled by default due to licensing problem) doesn't compile anymore.\n\nThe build.xml script doesn't work anymore too as http://www.cobase.cs.ucla.edu/pub/javacc/rtf_parser_src.jar doesn't exist anymore (404). I didn't fix the build.xml as I don't know from where we want to get the jar file but only the compilations issues.\n\nRegards,\n\n-- \nGuillaume", "comments": ["I am going to commit this but \n\n1) parse-rtf unit test is not updated\n2) Does anyone know where the rtf_parser_src.jar is?", "I found sources of RTFParser.jj (ASF) and RTFParserDelegate.java (LGPL) at https://atleap.dev.java.net/source/browse/atleap/application/src/common/com/blandware/atleap/common/parsers/rtf/.\n\n", "this parser incorrectly handles non-ascii input (when system encoding UTF-8).\nso I create new issue (NUTCH-705) whith new parser, and I think it can be released with nutch-1.0", "RTF parsing is now handled by the TikaPlugin (NUTCH-766) which solves the issue of licensing.", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "RTF parser doesn't compile anymore", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "RTF parser doesn't compile anymore"}, {"question": "What is the main context?", "answer": "Due to API changes, the RTF parser (which is not compiled by default due to licensing problem) doesn't compile anymore.\n\nThe build.xml script doesn't work anymore too as http://www.cobase.cs.ucla.edu/"}]}}
{"issue_id": "NUTCH-645", "project": "NUTCH", "title": "Parse-swf unit test failing", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2008-08-19T00:29:18.818+0000", "updated": "2009-04-10T12:29:06.758+0000", "description": "Parse-swf unit tests fail under Java 1.6, but run successfuly under Java 1.5", "comments": ["This patch fixes the problem. The failure came from a different iteration order of HashSet.iterator() in Java 1.5 and Java 1.6. Action strings are collected without preserving their order, so this patch simply sorts the strings before adding them to the output. Test responses have been updated accordingly. Tests pass now.", "I took the liberty to apply the patch and close this issue, as the fix was simple enough.", "Integrated in Nutch-trunk #545 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/545/])"], "tasks": {"summary": "Parse-swf unit test failing", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Parse-swf unit test failing"}, {"question": "What is the main context?", "answer": "Parse-swf unit tests fail under Java 1.6, but run successfuly under Java 1.5"}]}}
{"issue_id": "NUTCH-646", "project": "NUTCH", "title": "New Indexing Framework for Nutch", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-08-19T04:03:23.399+0000", "updated": "2013-05-02T02:29:16.146+0000", "description": "New indexing framework for Nutch that provides a more generic field abstraction consistent with Lucene index semantics.  Allows multiple MR jobs to be created for different fields and those fields to be aggregated and indexed in the end.  Overcomes limitations of the current indexer that limits what databases are passed into the indexer.  Creates a new extension point as well for field-filters for manipulation of fields during the indexing process.", "comments": ["The new indexing framework including new field indexer, field extension point, and field filter plugins.  This patch relys on the arity java arithmetic parser and on the new scoring framework defined in NUTCH-635.", "Arity jar licensed under the apache license.  Needed for the field-boost plugin.  Arity is an arithmetic parser.  This is used in the field-boost plugin to do simple arithmetic manipulations during indexing.  Arity can be found at http://code.google.com/p/arity/", "The new indexing framework depends on the new scoring framework for nutch.", "Updated indexing patch.", "For the final version of this I have removed the arity dependencies and computation functionality.  I still think that type of functionality is needed but it didn't feel like the right place for it at this time.", "Committed with revision 723447", "I should probably not be so lazy and just check it myself, but do we have a tutorial in wiki about how to use the new indexing system (or the scoring system for that regard) ?", "Not yet.  I need to write up some serious documentation about how to use both the new scoring and indexing systems.  I will try to get to that soon.", "Integrated in Nutch-trunk #667 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/667/])\n    "], "tasks": {"summary": "New Indexing Framework for Nutch", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "New Indexing Framework for Nutch"}, {"question": "What is the main context?", "answer": "New indexing framework for Nutch that provides a more generic field abstraction consistent with Lucene index semantics.  Allows multiple MR jobs to be created for different fields and those fields to "}]}}
{"issue_id": "NUTCH-647", "project": "NUTCH", "title": "Resolve URLs tool", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-08-19T04:05:03.058+0000", "updated": "2013-05-02T02:29:17.338+0000", "description": "A tool that takes a listing of urls and attempts to resolve their IP addresses.  Useful for running after the fetcher has run to determine if DNS problems exist.", "comments": ["ResolveURLs tool.", "The ResolveURL tool depends on the new scoring framework for nutch., requires URLUtil classes.", "Updated patch.", "Committed with revision 722478", "Integrated in Nutch-trunk #667 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/667/])\n    "], "tasks": {"summary": "Resolve URLs tool", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Resolve URLs tool"}, {"question": "What is the main context?", "answer": "A tool that takes a listing of urls and attempts to resolve their IP addresses.  Useful for running after the fetcher has run to determine if DNS problems exist."}]}}
{"issue_id": "NUTCH-648", "project": "NUTCH", "title": "debian style autocomplete", "status": "Closed", "priority": "Minor", "reporter": "Jim", "assignee": null, "created": "2008-08-28T00:24:51.983+0000", "updated": "2013-05-22T03:54:47.209+0000", "description": "\n        Here is a suggested improvement:  At the end of this file is a debian style bash autocomplete script, just place into /etc/bash_complete.d/ with filename nutch, and you can tab complete at the command prompt, ie\n\nbash> nutch [tab][tab]\n\n   crawl readdb convdb mergedb readlinkdb inject generate freegen fetch fetch2 parse\n   readseg mergesegs updatedb invertlinks mergelinkdb index merge dedup plugin server\n\nbash> nutch c[tab][tab]\n\n   crawl convdb\n\netc.\n\n   This also includes optional parameters, and filename completion where it can be used.  I really like having this when typing in long nutch commands, and think it would be a great addition to the project.\n\n   The file is heavily taken from the corresponding svn file that does the same thing.\n\nFile begins here:\n\n\n\nshopt -s extglob\n\n_nutch()\n{\n       local cur cmds cmdOpts optsParam opt\n       local i\n\n       COMPREPLY=()\n       cur=${COMP_WORDS[COMP_CWORD]}\n\n       # Possible expansions\n       cmds='crawl readdb convdb mergedb readlinkdb inject generate freegen fetch fetch2 parse readseg mergesegs updatedb invertlinks \\\nmergelinkdb index merge dedup plugin server'\n\n       if [[ $COMP_CWORD -eq 1 ]] ; then\n               COMPREPLY=( $( compgen -W \"$cmds\" -- $cur ) )\n               return 0\n       fi\n\n       # options that require a parameter\n       # This needs to be filled in better\n       optsParam=\"-topN|-depth\"\n\n\n       # if not typing an option, or if the previous option required a\n       # parameter, then fallback on ordinary filename expansion\n       if [[ \"$cur\" != -* ]] || \\\n          [[ ${COMP_WORDS[COMP_CWORD-1]} == @($optsParam) ]] ; then\n               return 0\n       fi\n\n       # possible options for the command\n       cmdOpts=\n       case ${COMP_WORDS[1]} in\n       crawl)\n               cmdOpts=\"-dir -threads -depth -topN\"\n               ;;\n       readdb)\n               cmdOpts=\"-stats -dump -topN -url\"\n               ;;\n       convdb)\n               cmdOpts=\"-withMetadata\"\n               ;;\n       mergedb)\n               cmdOpts=\"-normalize -filter\"\n               ;;\n       readlinkdb)\n               cmdOpts=\"-dump -url\"\n               ;;\n       inject)\n               cmdOpts=\"\"\n               ;;\n       generate)\n               cmdOpts=\"-force -topN -numFetchers -adddays -noFilter\"\n               ;;\n       freegen)\n               cmdOpts=\"-filter -normalize\"\n               ;;\n       fetch)\n               cmdOpts=\"-threads -noParsing\"\n               ;;\n       fetch2)\n               cmdOpts=\"-threads -noParsing\"\n               ;;\n       parse)\n               cmdOpts=\"\"\n               ;;\n       readseg)\n               cmdOpts=\"-dump -list -get -nocontent -nofetch -nogenerate -noparse -noparsedata -noparsetext -dir\"\n               ;;\n       mergesegs)\n               cmdOpts=\"-dir -filter -slice\"\n               ;;\n       updatedb)\n               cmdOpts=\"-dir -force -normalize -filter -noAdditions\"\n               ;;\n       invertlinks)\n               cmdOpts=\"-dir -force -noNormalize -noFilter\"\n               ;;\n       mergelinkdb)\n               cmdOpts=\"-normalize -filter\"\n               ;;\n       index)\n               cmdOpts=\"\"\n               ;;\n       merge)\n               cmdOpts=\"-workingdir\"\n               ;;\n       dedup)\n               cmdOpts=\"\"\n               ;;\n       plugin)\n               cmdOpts=\"\"\n               ;;\n       server)\n               cmdOpts=\"\"\n               ;;\n       *)\n               ;;\n       esac\n\n       # take out options already given\n       for (( i=2; i<=$COMP_CWORD-1; ++i )) ; do\n               opt=${COMP_WORDS[$i]}\n\n               cmdOpts=\" $cmdOpts \"\n               cmdOpts=${cmdOpts/ ${opt} / }\n\n               # skip next option if this one requires a parameter\n               if [[ $opt == @($optsParam) ]] ; then\n                       ((++i))\n               fi\n       done\n\n       COMPREPLY=( $( compgen -W \"$cmdOpts\" -- $cur ) )\n\n       return 0\n}\ncomplete -F _nutch -o default nutch", "comments": ["This looks great but can you attach your changes as a patch?", "I'm not convinced about the usefulness of this change. It adds complexity to the bash script, and requires that we keep two lists of options in sync.", "Although i would be +1, the problem is that in 2.0 the batchID nor crawlID can be auto-completed; it's not a command nor a file. It would be very useful in 1.x though.\n\n+1 for closing this issue unless someone has an idea for 2.0!", "Same Markus...\n\nI think given the open issues for both 1.X and 2.0 as well as the additional complexity you highlight for 2.0 we should close this issue and mark as won't fix. Should it be deemed necessary (or indeed wished for) then I'm sure we could come back to it, utilise the code posted above and patch it up!  ", "see comments above"], "tasks": {"summary": "debian style autocomplete", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "debian style autocomplete"}, {"question": "What is the main context?", "answer": "\n        Here is a suggested improvement:  At the end of this file is a debian style bash autocomplete script, just place into /etc/bash_complete.d/ with filename nutch, and you can tab complete at th"}]}}
{"issue_id": "NUTCH-649", "project": "NUTCH", "title": "Log list of files found but not crawled.", "status": "Closed", "priority": "Major", "reporter": "Jim", "assignee": null, "created": "2008-08-28T00:33:47.319+0000", "updated": "2019-10-13T22:36:26.467+0000", "description": "\n\n        I use Nutch to find the location of executables on the web, but we do not download the executables with Nutch.  In order to get nutch to give the location of files without downloading the files, I had to make a very small patch to the code, but I think this change might be useful to others also.  The patch just logs files that are being filtered at the info level, although perhaps it should be at the debug level.\n\n   I have included a svn diff with this change.  Use cases would be to both use as a diagnostic tool (let's see what we are skipping) as well as a way to find content and links pointed to by a page or site without having to actually download that content.\n\n\n\nIndex: ParseOutputFormat.java\n===================================================================\n--- ParseOutputFormat.java      (revision 593619)\n+++ ParseOutputFormat.java      (working copy)\n@@ -193,17 +193,20 @@\n                toHost = null;\n              }\n              if (toHost == null || !toHost.equals(fromHost)) { // external links\n+               LOG.info(\"filtering externalLink \" + toUrl + \" linked to by \" + fromUrl);\n+\n                continue; // skip it\n              }\n            }\n            try {\n              toUrl = normalizers.normalize(toUrl,\n                          URLNormalizers.SCOPE_OUTLINK); // normalize the url\n-              toUrl = filters.filter(toUrl);   // filter the url\n-              if (toUrl == null) {\n-                continue;\n-              }\n-            } catch (Exception e) {\n+\n+             if (filters.filter(toUrl) == null) {   // filter the url\n+                     LOG.info(\"filtering content \" + toUrl + \" linked to by \" + fromUrl);\n+                     continue;\n+                 }\n+           } catch (Exception e) {\n              continue;\n            }\n            CrawlDatum target = new CrawlDatum(CrawlDatum.STATUS_LINKED, interval);", "comments": ["patches for trunk and 2.x", "What is the overhead of implementing such URLs as Hadoop counters? Would this be a more efficient method for analyzing what you are skipping or is it better for the user to trawl through his/her DEBIG logging? ", "Hi [~lewismc], Now thats an awesome idea... certainly better than lame debug statements. I would get that done soon.", "Tejas, you can take most of what we implemented over on NUTCH-1370 with this one. It will give you a real head start with implementing some more useful metrics than the common log. Thanks Tejas", "Hi [~lewismc],\nThe method where I need to introduce the counters is not a map / reduce method but its something called while writing records (as its a ParseOutputFormat class). AFAIK, we can use hadoop counters from within a map/reduce method. A simple google search gave me [this |http://stackoverflow.com/questions/12645652/how-to-increment-a-hadoop-counter-from-outside-a-mapper-or-reducer] which indicates the same. Do you know how to do that ? If not, should we go ahead with the current patch ?", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "Log list of files found but not crawled.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Log list of files found but not crawled."}, {"question": "What is the main context?", "answer": "\n\n        I use Nutch to find the location of executables on the web, but we do not download the executables with Nutch.  In order to get nutch to give the location of files without downloading the fi"}]}}
{"issue_id": "NUTCH-65", "project": "NUTCH", "title": "index-more plugin can't parse large set of  modification-date", "status": "Closed", "priority": "Major", "reporter": "YourSoft", "assignee": null, "created": "2005-07-01T18:55:29.000+0000", "updated": "2005-09-02T07:24:24.000+0000", "description": "I found a problem in MoreIndexingFilter.java.\nWhen I indexing segments, I get large list of error messages:\ncan't parse errorenous date: Wed, 10 Sep 2003 11:59:14 or\ncan't parse errorenous date: Wed, 10 Sep 2003 11:59:14GMT\n\nI modifiing source code (I don't make a 'patch'):\nOriginal (lines 137-138):\nDateFormat df = new SimpleDateFormat(\"EEE MMM dd HH:mm:ss yyyy zzz\");\nDate d = df.parse(date);\nNew:\nDateFormat df = new SimpleDateFormat(\"EEE, MMM dd HH:mm:ss yyyy\", Locale.US);\nDate d = df.parse(date.substring(0,25));\n\nThe modified code works fine.", "comments": ["It could be a good idea to declare the DataFormat as a final static constant ... no?", "I indexed 1.5 million pages with modified source, and found the following:\n\n- Need to check the length of date (date.length()>25) before substring, from it.\n- Some other date formats (10-20 / 1.5 million):\nEEE MMM dd HH:mm:ss yyyy zzz\nEEE MMM dd HH:mm:ss yyyy", "Re:  It could be a good idea to declare the DataFormat as a final static constant ... no?\n\nNO! SimpleDateFormat is not thread safe. Using it as a final static field is a bug. See (for instance): http://www-106.ibm.com/developerworks/java/library/j-jtp09263.html", "Thta's a VERY good reason. Thanks for info.", "Dear Developers,\n\nI have a finally solution (I have a firewall, I can't make patch with svn), I suggested please commit it:\n\nIn the import section:\nimport java.util.Locale; \n\nReplace getTime function with:\n  private long getTime(String date, String url) {\n    long time = -1;\n    try {\n      time = HttpDateFormat.toLong(date);\n    } catch  (ParseException e) {\n      // try to parse it as date in alternative format\n      String date2 = date;\n      try {\n        if (date.length() > 25 ) date2 = date.substring(0, 25);\n        DateFormat df = new SimpleDateFormat(\"EEE, dd MMM yyyy HH:mm:ss\", Locale.US);\n        time = df.parse(date2).getTime();\n      } catch (Exception e1) {\n        try {\n          if (date.length() > 24 ) date2 = date.substring(0, 24);\n          DateFormat df = new SimpleDateFormat(\"EEE MMM dd HH:mm:ss yyyy\", Locale.US);\n          time = df.parse(date2).getTime();\n        } catch (Exception e2) {\n          LOG.warning(url+\": can't parse erroneous date: \"+date);\n        }\n      }\n    }\n    return time;\n  }", "Patches applied. Thanks!", "I checked out the trunc at 24/Aug/05 and still get some errors:\n\n... can't parse erroneous date: Fri, 29 Okt 2004 11:08:24 GMT\n... can't parse erroneous date: Thu, 28 Okt 2004 08:59:16 GMT\n... can't parse erroneous date: Thu, 14 Okt 2004 07:17:15 GMT\n... can't parse erroneous date: Fri, 08 Okt 2004 12:42:00 GMT\n... can't parse erroneous date: Tue, 26 Okt 2004 09:31:48 GMT\n... can't parse erroneous date: Tue, 19 Okt 2004 06:03:00 GMT\n\nAttention: it's the german \"Okt\" for \"Oktober\" not  \"Oct\" for the english \"October\". I think, the Local.US is confusing the object. \n", "Michael Nebel reports some other date parsing problems on the nutch-dev\n(http://www.mail-archive.com/nutch-dev%40lucene.apache.org/msg00663.html)\n\n*****\n    ...can't parse erroneous date: 12.06.2005 22:02:54 GMT\n    ...can't parse erroneous date: 14.07.2005 GMT\n    ...can't parse erroneous date: 15.10.2003 04:58:08\n    ...can't parse erroneous date: 16 6 2005 00:00:00 GMT\n    ...can't parse erroneous date: 16.06.2005 10:10:57 GMT\n    ...can't parse erroneous date: 2005/06/21 20:51:40.618 GMT+2\n    ...can't parse erroneous date: 29.06.2005 GMT\n    ...can't parse erroneous date: 31.5.2005; 10:14:49\n    ...can't parse erroneous date: 968776128\n*****\n\nAn so on....\n\nI don't thing using a fixed local (Local.US) is the solution since the format of the date can takes various forms (as Michael's logs show it). \nInstead, the solution is perhaps to use Jakarta Commons DateUtils.parseDate method:\nhttp://jakarta.apache.org/commons/lang/api/org/apache/commons/lang/time/DateUtils.html#parseDate(java.lang.String,%20java.lang.String[])\n\nIt will gives something like:\n\nDate parsedDate = DateUtils.parseDate(date,\n        new String [] {\"yyyy/MM/dd\",\n                       \"yyyy.MM.dd HH:mm:ss\",\n                       \"yyyy-MM-dd HH:mm\",\n                       ...\n                       and so on\n                       ...\n                       });\n\n", "See previous comments for some reasons of reopening this issue.", "As Jerome suggested, I changed the function getTime() to use the DateUtils from commons-lang . ", "Patch committed (http://svn.apache.org/viewcvs.cgi?rev=265794&view=rev)"], "tasks": {"summary": "index-more plugin can't parse large set of  modification-date", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "index-more plugin can't parse large set of  modification-date"}, {"question": "What is the main context?", "answer": "I found a problem in MoreIndexingFilter.java.\nWhen I indexing segments, I get large list of error messages:\ncan't parse errorenous date: Wed, 10 Sep 2003 11:59:14 or\ncan't parse errorenous date: Wed, "}]}}
{"issue_id": "NUTCH-650", "project": "NUTCH", "title": "Hbase Integration", "status": "Closed", "priority": "Major", "reporter": "Dogacan Guney", "assignee": "Dogacan Guney", "created": "2008-09-18T12:03:25.953+0000", "updated": "2013-05-02T02:29:20.109+0000", "description": "This issue will track nutch/hbase integration", "comments": ["This patch is what I have done so far. Right now, hbase integration is functional enough that you can inject/generate/fetch from http pages/parse html pages /create a basic index ( Only parse-html, protocol-http and index-basic are updated for hbase )\n\nBefore I go into design, first, a note: Don't worry about the size of the patch :D I know that it is huge but for simplicity I created a new package (org.apache.nutchbase) and moved code there instead of modifying it directly. So, bulk of the patch is just old code really. In general, if you are interested in reviewing this patch (and I hope you are:), interesting parts are: InjectorHbase, GeneratorHbase, FetcherHbase, ParseTable, UpdateTable, IndexerHbase and anything under util.hbase.\n\nA) Why integrate with hbase?\n  - All your data in a central location\n  - No more segment/crawldb/linkdb merges.\n  - No more \"missing\" data in a job. There are a lot of places where we copy data from one structure to another just so that it is available in a later job. For example, during parsing we don't have access to a URL's fetch status. So we copy fetch status into content metadata. This will no longer be necessary with hbase integration.\n  - A much simpler data model. If you want to update a small part in a single record, now you have to write a MR job that reads the relevant directory, change the single record, remove old directory and rename new directory. With hbase, you can just update that record. Also, hbase gives us access to Yahoo! Pig, which I think, with its SQL-ish language may be easier for people to understand and use.\n  \nB) Design\nDesign is actually rather straightforward. \n\n  - We store everything (fetch time, status, content, parsed text, outlinks, inlinks, etc.) in hbase. I have written a small utility class that creates \"webtable\" with necessary columns.\n  - So now most jobs just take the name of the table as input. \n  - There are two main classes for interfacing with hbase. ImmutableRowPart wraps around a RowResult and has helper getters (getStatus(), getContent(), etc.). RowPart is similar to ImmutableRowPart but also has setters. The idea is that RowPart also wraps RowResult but also keeps a list of updates done to that row. So when getSomething is called, it first checks if Something is already updated (if so then returns the updated version) or returns from RowResult. RowPart can also create a BatchUpdate from its list of updates.\n   - URLs are stores in reversed host order. For example, http://bar.foo.com:8983/to/index.html?a=b becomes com.foo.bar:http:8983/to/index.html?a=b. This way, URLs from the same tld/host/domain are stored closer to each other. TableUtil has methods for reversing and unreversing URLs.\n   - CrawlDatum Status-es are simplifed. Since everything is in central location now, no point in having a DB and FETCH status. \n\nJobs:\n  - Each job marks rows so that the next job knows which rows to read. For example, if GeneratorHbase decides that a URL should be generated it marks the URL with a TMP_FETCH_MARK (Marking a url is simply creating a special metadata field.) When FetcherHbase runs, it skips over anything without this special mark.\n  - InjectorHbase: First, a job runs where injected urls are marked. Then in the next job, if a row has the mark but nothing else (here, I assumed that if a row has \"status:\" column, that it already exists), InjectorHbase initializes the row.\n   - GeneratorHbase: Supports max-per-host configuration and topN. Marks generated urls with a marker.\n   - FetcherHbase: Very similar to original Fetcher. Marks urls successfully fetched. Skips over URLs not marked by GeneratorHbase\n   - ParseTable: Similar to original Parser. Outlinks are stored \"outlinks:<fromUrl>\" -> \"anchor\".\n   - UpdateTable: Does updatedb's and invertlink's job. Also clears any markers.\n   - IndexerHbase: Indexes the _entire_ table. Skips over URLs not parsed successfully.\n\nPlugins:\n  - Plugins now have a\n  \n  Set<String> getColumnSet();\n  \n  method. Before starting a job, we ask relevant plugins what exactly they want to read from hbase and read those columns. For example, FetcherHbase reads some columns but doesn't read \"modifiedTime:\". However, protocol-httphbase needs this column. So the plugin adds this column to its set and FetcherHbase reads \"modifiedTime:\" when protocol-httphbase is active. This way, plugins read exactly what they want, whenever they want it. For example, during parse normally CrawlDatum's fields are not available. However, with this patch, a parse plugin can ask for any of those fields and they will get it.\n  \n  - Also, plugin API is simpler now. Most plugins will look like a variation of this:\n  \n  public void doStuff(String url, RowPart row);\n  \n  So now a plugin can also choose to update any column it wants.\n   \nC) What's missing\n  - A LOT of plugins.\n  - No ScoringFilters at all.\n  - Converters from old data into hbase\n  - GeneratorHbase: no byIP stuff. does not shuffle URLs for fetching. no -adddays\n  - FetcherHbase: no byIP stuff. no parsing during fetch. Shuffling is important for performace, but can be fixed. (One solution that comes to mind is to randomly partition URLs into reducers during map, and perform the actual fetching during reduce). Supports following redirects, but not immediately. Http headers are not stored. Since no parsing in fetcher, fetcher always stores content.\n  - ParseTable: No multi-parse (i.e ParseResult). \n  - IndexerHbase: No way to choose a subset of urls to index. (There is a marker in UpdateTable but I haven't put it in yet)\n  - FetchSchedule: prevModifiedTime, prev... stuff are missing as I haven't yet figured a way to read older versions of the same column.\n  \n  Most of what's missing is stuff I didn't have time to code. Should be easy to add later on.\n\nAs always, suggestions/reviews are welcome.", "+1 from an HBase perspective. Very nice!", "This sounds great, Doğacan! Simplification is good, so +1 for the approach.\nAre you running this code in some Dev/QA/Prod system?  Any observations about any kind of performance or scaling differences?\n", "Thanks Otis ! \n\n> Are you running this code in some Dev/QA/Prod system? Any observations about any kind of performance or scaling differences? \n\nNo unfortunately it is just my laptop for now :D", "Take a look at this, might be a useful inspiration to move this patch forward ... :)\n\nhttp://code.google.com/p/hbase-writer/\n\n", "New patch. Contains some fixes and:\n\n- Support page modification detection in nutchbase (store previous signature and fetch time as distinct columns until we get support for scanning multiple versions)\n- A new PluggableHbase interface for nutchbase plugins\n- Converted HtmlParseFilters for nutchbase\n- Index cache-policies in index-basichbase.\n- Added no-caching support to parse-htmlhbase.\n- Added support for content encoding auto detection to nutchbase\n- Do not instantiate a new MimeUtil for _every_ content\n- Added support for (Http-)headers\n", "This is an important issue, and it would be good if more people could collaborate on it. I suggest that you make a branch, e.g. branches/nutch_hbase, apply this patch there, and continue developing it until it reaches a usable state. Those interested in this direction can contribute patches to the branch, and once we feel it's functionally good enough to replace the current custom DBs we can merge the code to trunk/ .", "I am moving this issue to 1.1.\n\nI agree with Andrzej's comments with creating a branch to make collaboration easier. But I will wait after 1.0 to do so.", "OK, I didn't want until after 1.0 :)\n\nI created a git repository in \n\nhttp://github.com/dogacan/nutch.dogacan/tree/master\n\nI will probably not update this tree until after 1.0, but feel free to play with it.", "Hi Doğacan, \n\nI've been running this on a pseudo distributed hadoop/hbase install I setup for the purpose and for testing and development on my own. To get it to run out of the box I needed to make a couple of changes - I've attached a patch with them in it.  \n\nPatch is against the git repository, btw. \n\n1) I changed the nutch-default.xml to use the hbase classes. \n2) I changed the parse-plugins.xml file to use the hbase classes\n3) I tweaked the IndexerHbase so that the ouput is wrapped in a hadoop.io.Text otherwise it wouldn't work for me. \n4) Altered the WebTableCreator so that it's a command that can be executed from the command line like any other, also made the table name an option like the others. \n\nThis should now checkout, compile and allow the following comands to be run:\n\n./bin/nutch org.apache.nutchbase.util.hbase.WebTableCreator webtable\n\n./bin/nutch org.apache.nutchbase.crawl.InjectorHbase webtable file:///path/to/urls_dir\n\n./bin/nutch org.apache.nutchbase.crawl.GeneratorHbase webtable\n\n./bin/nutch org.apache.nutchbase.fetcher.FetcherHbase webtable\n\n./bin/nutch org.apache.nutchbase.parse.ParseTable webtable\n\n./bin/nutch org.apache.nutchbase.indexer.IndexerHbase /index webtable\n\n./bin/nutch org.apache.nutchbase.crawl.UpdateTable webtable\n\nI've been running a test crawl using this code and it seems to be working well for me. \n\n", "Patch against the git repo. ", "patch for nutch-hbase that does the same as patch supplied for NUTCH-693", "Ran across a MalformedUrlException when running GeneratorHbase\n\nStacktrace: \n\njava.net.MalformedURLException: no protocol: http?grp_name=MideastWebDialog&grp_spid=1600667023&grp_cat=://answers.yahoo.com/Regional/Regions/Middle_East/Cultures___Community&grp_user=0\n\tat java.net.URL.<init>(URL.java:567)\n\tat java.net.URL.<init>(URL.java:464)\n\tat java.net.URL.<init>(URL.java:413)\n\tat org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer.normalize(BasicURLNormalizer.java:88)\n\tat org.apache.nutch.net.URLNormalizers.normalize(URLNormalizers.java:286)\n\tat org.apache.nutchbase.crawl.GeneratorHbase$GeneratorMapReduce.map(GeneratorHbase.java:135)\n\tat org.apache.nutchbase.crawl.GeneratorHbase$GeneratorMapReduce.map(GeneratorHbase.java:108)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:155)\n\nThe problem lies with URLs in links with no file and no / between the host and the querystring.  \n\ne.g. http://answers.yahoo.com?grp_name=MideastWebDialog&grp_spid=1600667023&grp_cat=/Regional/Regions/Middle_East/Cultures___Community&grp_user=0\n\nThe first / in the grp_cat field gets interpreted as the beginning of the file. \n\nAttached patch solves the problem by ensuring / is added after host:port if it doesn't exist. Also includes tests and updates the build.xml to run the tests in org.apache.nutch*/** instead of just org.apache.nutch/**\n\n\n", "Fixes an issue the above patch creates, if there is a zero length filename without a slash will throw an exception. ", "I've updated the way the TMP_X_MARK metadata is handled to allow multiple fetch cycles to take place at the same time. \n\n* GeneratorHbase adds the TMP_FETCH_MARK as before\n* FetcherHbase \n** crawls any rows with TMP_FETCH_MARK set and sets TMP_PARSE_MARK so the Parser knows to parse the row as before\n** removes the column TMP_FETCH_MARK so that any other later fetch between now and calling UpdateTable won't re-fetch the row. \n* ParseTable \n** parses any rows with TMP_PARSE_MARK set and sets TMP_UPDATE_MARK as before\n** removes the column TMP_PARSE_MARK so that a later parse won't re-parse the row. \n* UpdateTable now only updates rows with TMP_UPDATE_MARK set by default leaving rows that have not been fetched or parsed yet in their current state.\n* calling UpdateTable with the new -all option forces UpdateTable to update all rows in the table and acts as it did before the patch removing any TMP_X_MARK rows.  \n\n", "Slight update to previous patch, calling deleteMeta during the map phase stopped the scanner from getting anymore results. I've moved it into the reduce and it works properly now. \n\nAlso patched a bug in UpdateTable.java where a bad URL could crash the whole update process. I've just wrapped it in a try/catch block and dumped a warning to the logs.", "Added a few bits, so that you can now search. I've added a very basic NutchBeanHbase which does most of what the previous version of NutchBean does. I'm pretty sure it wont work for distributed searching yet. I've also altered the build.xml to include the hbase jar with the war and altered the jsp pages to use the new NutchBean and the ImmutableRow object I expose. \n\nI've been testing this on my local machine through a few crawls and it seems to be working well. ", "Doğacan, I think http://github.com/dogacan/nutch.dogacan/tree/master is gone.\nNutch 1.0 is out.\nIs it time for that branch?\n\nThe work you and Andrew have done so far looks too useful to drop!\n", "I recreated the tree at \n\nhttp://github.com/dogacan/nutchbase/tree/nutchbase \n\nand\n\ngit://github.com/dogacan/nutchbase.git\n\nNote that now, master branch is the vanilla *1.0* branch of nutch. All development is done in nutchbase branch.\n\nI have also applied several of Andrew's patches.", "Many changes.\n\nFirst, for simplicity, I changed master branch to be the main development branch. So to take a look at nutchbase simply do:\n\ngit clone git://github.com/dogacan/nutchbase.git\n\n(sorry Andrew for the random change :)\n\n* Upgraded to hbase trunk and hadoop 0.20.\n\n* FetcherHbase now fetches URLs in reduce(). I added a randomization part so that now reduce does not get URLs from the same host one after another but in a random order. Still politeness rules are followed and one host will always be in one reducer no matter how many URLs it has (at least, that's what I tried to do, testing is welcome :). \n\n* If your fetch is cut short, you almost do not lost any fetched URL as we immediately write the fetched content to the table*. For example, if you are doing a HUGE one day fetch, and at the 20th hour your fetch dies, then 20 hour fetching worth of URLs will already be in hbase. Next execution of FetcherHbase will simply pick up where it left.\n\n* Same thing for ParseTable. If parse crashes in midstream, next execution will continue at the crash point*.\n\n* Added a \"-restart\" option for ParseTable and FetcherHbase. If \"-restart\" is present then these classes start at the beginning instead of continuing from whereever last run finished.\n\n* Added a \"-reindex\" option to IndexerHbase to reindex the entire table (Normally only successfully parsed URLs in that iteration are processed).\n\n* Added a SolrIndexerHbase so you can use solr with hbase (which is awesome :). Also has a \"-reindex\" option.\n\n*= We do not immediately write content as hbase client code uses a write buffer to buffer updates. Still, you will lose very few URLs as opposed to all (and write buffer size can be made smaller for more safety)\n\nThere are still some more stuff to go (such as updating scoring for hbase) but most of the stuff is, IMHO, ready. Can I get some reviews about what people think of the general direction, about API, etc? Because this (and katta integration) are my priorities for next nutch.", "I forgot to add: I also made some API changes so now plugins have\n\nCollection<HbaseColumn> getColumns();\n\nThis will allow us to support versioning (i.e exposing multiple versions of the same column to plugins) in the future (this is easy to do, but not yet implemented).", "You're making good progress! I think it's time to import this to Nutch SVN, on a branch, so that we can go through a regular patch review process.", "Thanks Andrzej!\n\nI am also close to finishing the scoring API as well. Once scoring is finished, most of the major functionality will be in. One benefit is that I think we will be able to do the real OPIC scoring finally*. Though I don't know how relevant that is after Dennis' pagerank work.\n\nHow do you suggest we do the import? Create a new branch (with my choice of name being 'nutchbase' :), close this issue and open a new issue for new features or bug fixes?\n\n* With constant 'cash' distribution to pages.\n\n\n", "\"nutchbase\" is ok for now, although it sounds cryptic. +1 on importing and closing this issue.\n\nI don't believe OPIC scoring can work well, even if we implement it as intended - the dynamic nature of the webgraph is IMHO not properly addressed even in the original paper (authors propose a smoothing schema based on a history of past values). In my opinion we should strive to create a more elegant scoring API than the current one (which owes much to the way Nutch passed bits of data between different data stores), and use PageRank as the default.\n\nRe: use of Katta for distributed indexing - let's discuss this on the list.", "I am getting ready to merge nutchbase into an svn branch.\n\nI have moved the code into org.apache.nutch package (instead of org.apache.nutchbase). This causes many changes to nutch core (that would be needed anyway), so I ended up deleting a lot of old classes. I kept CrawlDatum, Content, etc... around for data conversion but it is possible that I missed something.\n\nSome packages such as arc, webgraph scoring etc... also had to go as they depended on old nutch structures. I will add them as I convert more code for nutchbase.\n\nAny objections to bringing the code into org.apache.nutch?", "We already have some compat stuff in o.a.n.util.compat, mostly related to 0.7 and early 0.8 conversion. I guess we can drop this stuff from the new branch.\n\nThis is a bigger question of back-compat. What data is it worth to convert and preserve? I'd say the following: CrawlDb and perhaps unparsed content. Everything else can be generated from this data.\n\nWith such major changes I'm in favor of a limited back-compat based on converter tools, and not on back-compat shims scattered throughout the code. So feel free to morph the core classes as you see fit according to the requirements of the new design.\n\nAnd answering your question: no objections here.", "I just committed code to branch nutchbase. The scoring API did not turn out as clean as I expected but I decided to put in what I have. Also, I made some changes so that web UI also works.\n\nI am leaving this issue open because I will add documentation tomorrow. Meanwhile,\n\nTo download: \n\n  svn co http://svn.apache.org/repos/asf/lucene/nutch/branches/nutchbase\n\nUsage:\n\nAfter starting hbase 0.20 (checkout rev. 804408 from hbase branch 0.20), create a webtable with\n\n  bin/nutch createtable webtable\n\nAfter that, usage is similar.\n\n  bin/nutch inject webtable url_dir # inject urls\n\nfor as many cycles as you want;\n    bin/nutch generate webtable #-topN N works\n    bin/nutch fetch webtable # -threads N works\n    bin/nutch parse webtable\n    bin/nutch updatetable webtable\n\n  bin/nutch index <index> webtable\nor\n  bin/nutch solrindex <solr url> webtable\n\nTo use solr, use this schema file\nhttp://www.ceng.metu.edu.tr/~e1345172/schema.xml\n\n\nAgain, a note of warning: This is extremely new code. I hope people will test and use it but there is no guarantee that it will work :)\n", "Exception:\n\norg.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family mtdt: does not exist in region crawl,,1264048608430 in table {NAME => 'crawl', FAMILIES => [{NAME => 'bas', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'cnt', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'cnttyp', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'fchi', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'fcht', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'hdrs', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'ilnk', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'modt', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'mtdt', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'olnk', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'prsstt', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'prtstt', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'prvfch', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'prvsig', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'repr', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'rtrs', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'scr', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'sig', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'stt', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'ttl', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'txt', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}\n    at org.apache.hadoop.hbase.regionserver.HRegion.checkFamily(HRegion.java:2381)\n    at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1241)\n    at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1208)\n    at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1834)\n    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:648)\n\n    at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)\n\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n    at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:94)\n    at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getRegionServerWithRetries(HConnectionManager.java:995)\n    at org.apache.hadoop.hbase.client.HConnectionManager$TableServers$2.doCall(HConnectionManager.java:1193)\n    at org.apache.hadoop.hbase.client.HConnectionManager$TableServers$Batch.process(HConnectionManager.java:1115)\n    at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.processBatchOfRows(HConnectionManager.java:1201)\n    at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:605)\n    at org.apache.hadoop.hbase.client.HTable.put(HTable.java:470)\n    at org.apache.nutch.crawl.Injector$UrlMapper.map(Injector.java:92)\n    at org.apache.nutch.crawl.Injector$UrlMapper.map(Injector.java:62)\n    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:583)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)\n    at org.apache.hadoop.mapred.Child.main(Child.java:170)\n\nThis may be caused by invalid column family names:\nI found some names end with colon while some doesn't in package org.apache.nutch.util.hbase.WebTableColumns\nIs this a bug?\npublic interface WebTableColumns {\n  public static final String BASE_URL_STR         = \"bas\";\n  public static final String STATUS_STR           = \"stt\";\n  public static final String FETCH_TIME_STR       = \"fcht\";\n  public static final String RETRIES_STR          = \"rtrs\";\n  public static final String FETCH_INTERVAL_STR   = \"fchi\";\n  public static final String SCORE_STR            = \"scr\";\n  public static final String MODIFIED_TIME_STR    = \"modt\";\n  public static final String SIGNATURE_STR        = \"sig\";\n  public static final String CONTENT_STR          = \"cnt\";\n  public static final String CONTENT_TYPE_STR     = \"cnttyp:\";\n  public static final String TITLE_STR            = \"ttl:\";\n  public static final String OUTLINKS_STR         = \"olnk:\";\n  public static final String INLINKS_STR          = \"ilnk:\";\n  public static final String PARSE_STATUS_STR     = \"prsstt:\";\n  public static final String PROTOCOL_STATUS_STR  = \"prtstt:\";\n  public static final String TEXT_STR             = \"txt:\";\n  public static final String REPR_URL_STR         = \"repr:\";\n  public static final String HEADERS_STR          = \"hdrs:\";\n  public static final String METADATA_STR         = \"mtdt:\";", "Some instructions for NUTCH-650.patch\n1. API in hbase-0.20.0-r804408.jar is different from the final release.\n2. Avoid some NullPointer error\n3. Change invalid Column family name\n4. Add \"id\" field to index to avoid this error:\njava.lang.IllegalArgumentException: it doesn't make sense to have a field that is neither indexed nor stored\n\tat org.apache.lucene.document.Field.(Field.java:279)\n\tat org.apache.nutch.indexer.lucene.LuceneWriter.createLuceneDoc(LuceneWriter.java:136)\n\tat org.apache.nutch.indexer.lucene.LuceneWriter.write(LuceneWriter.java:245)\n\tat org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:46)\n\tat org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:41)\n\tat org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n\tat org.apache.nutch.indexer.IndexerReducer.reduce(IndexerReducer.java:79)\n\tat org.apache.nutch.indexer.IndexerReducer.reduce(IndexerReducer.java:20)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)\n\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:563)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\n\n", "Xiao's patch works for HBase 0.20.3", "- pushing this out per http://bit.ly/c7tBv9", "I encountered the following NULL exception while running nutchbase.\n\n2010-04-24 01:58:47,012 WARN org.apache.hadoop.mapred.TaskTracker: Error running child java.lang.NullPointerException at org.apache.hadoop.hbase.io.ImmutableBytesWritable.<init>(ImmutableBytesWritable.java:59) at org.apache.nutch.fetcher.Fetcher$FetcherMapper.map(Fetcher.java:81) at org.apache.nutch.fetcher.Fetcher$FetcherMapper.map(Fetcher.java:77) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305) at org.apache.hadoop.mapred.Child.main(Child.java:170)\n\nI downloaded nutchbase from svn co http://svn.apache.org/repos/asf/lucene/nutch/branches/nutchbase and applied Xiao's patch. I am running hadoop-0.20.3, hbase-0.20.3 and zookeeper-3.2.2. \n\nIn my application the error occurs after the first iteration of the fetch/generate cycle and is limited to the base url with a generator mark=csh, e.g.:\nkeyvalues={host:http:8080/wikipedia/de/de/index.html/mtdt:_csh_/1272088691273/Put/vlen=4}\n\nBut it works fine for values with generator mark=genmrk, e.g.,:\nkeyvalues={host:http:8080/wikipedia/de/de/images/wikimedia-button.png/mtdt:__genmrk__/1272088714395/Put/vlen=4, host:http:8080/wikipedia/de/de/images/wikimedia-button.png/mtdt:_csh_/1272088691109/Put/vlen=4}\n\nI modified my map function to check for null values in outKeyRaw in  org.apache.nutch.fetcher.Fetcher$FetcherMapper.map. This masks the error but I am not sure if this is the right action to take. Please let me know.\n\nThanks.", "Here are two patches for generated using git vs two different branch points:\n\n1) Patch generated against svn revision 790789. This was the branch point for original nutchbase work. This revision is slightly (but not much) newer than nutch-1.0.\n2) Patch generated against current svn nutchbase.\n\nBoth should apply cleanly.", "So far as one can digest such a giant patch ;) I think this is ok, at least from the legal POV it clarifies the situation and it doesn't bring any dependencies with incompatible licenses. As for the content itself, we'll need to resolve this incrementally, as discussed on the list.\n\nSo, a cautious +1 from me to apply this on branches/nutchbase.", "I have written a short installation guide and a short design guide for nutchbase. Design guide is especially short because nutchbase's current design is mostly still the same. So reading through this issue should give you a good idea on design.\n\nIf anything is unclear, please ask and I will try to clarify as best as I can.", "The patch has been committed with revision # 959259. The content of https://svn.apache.org/repos/asf/nutch/branches/nutchbase is now the same as github.", "NutchBase is now in the trunk + most of the issues listed in this JIRA refer to an older, pre-GORA version of NutchBase. \nClose? ", "+1, this should be wrapped up.", "I am out of office on vacation and will be slower than usual in\nresponding to emails. If this is urgent then please call my cell phone\n(or send an sms), otherwise I will reply to your email when I get\nback.\n\nThanks for your patience,\n\n-- amr\n", "+1 and a YEY! from me.", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Hbase Integration", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Hbase Integration"}, {"question": "What is the main context?", "answer": "This issue will track nutch/hbase integration"}]}}
{"issue_id": "NUTCH-651", "project": "NUTCH", "title": "Remove bin/{start|stop}-balancer.sh from svn tracking", "status": "Closed", "priority": "Minor", "reporter": "Dogacan Guney", "assignee": "Dogacan Guney", "created": "2008-09-19T12:04:00.108+0000", "updated": "2009-04-10T12:29:01.449+0000", "description": "Files bin/{start|stop}-balancer.sh are version controlled. I don't see any reason for why they should be tracked since ant generates them anyway. So, if no one objects I will remove them from version control.", "comments": ["Files removed as of rev. 697781.", "Integrated in Nutch-trunk #580 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/580/])", "Integrated in Nutch-trunk #582 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/582/])"], "tasks": {"summary": "Remove bin/{start|stop}-balancer.sh from svn tracking", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove bin/{start|stop}-balancer.sh from svn tracking"}, {"question": "What is the main context?", "answer": "Files bin/{start|stop}-balancer.sh are version controlled. I don't see any reason for why they should be tracked since ant generates them anyway. So, if no one objects I will remove them from version "}]}}
{"issue_id": "NUTCH-652", "project": "NUTCH", "title": "AdaptiveFetchSchedule#setFetchSchedule doesn't calculate fetch interval correctly", "status": "Closed", "priority": "Major", "reporter": "Dogacan Guney", "assignee": "Dogacan Guney", "created": "2008-09-19T13:01:59.699+0000", "updated": "2009-04-10T12:29:01.380+0000", "description": "Fetch interval is stored in seconds, but in AdaptiveFetchSchedule#setFetchSchedule it is treated as if in milliseconds. Also, fetch interval bound checking is done after interval value is stored in CrawlDatum, thus, has no effect.", "comments": ["Patch for the problem. Also changes SYNC_DELTA_RATE to double. So that Math.round operation returns long In (In java, Math.round(double) -> long, Math.round(float) -> int). I think this may help with over/under flow problems.", "Andrzej, can you give me a hand here? I think I got this one right, but I would feel much better with a +1 from you :D", "I think there is still some confusion about the units ...  The patch calculates delta as delta[s] = (fetchTime[ms] - modifiedTime[ms]) / 1000L;\", which means it's in units [s], but then it sets refTime[ms] = fetchTime[ms] - Math.round(delta[s] * SYNC_DELTA_RATE);", "OK, another try :D\n\nI just added a *1000.0 there, it should be enough right?", "I think this is ok. Let's commit it.", "Fixed in rev. 733747.", "Integrated in Nutch-trunk #691 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/691/])\n     - AdaptiveFetchSchedule#setFetchSchedule doesn't calculate fetch interval correctly\n"], "tasks": {"summary": "AdaptiveFetchSchedule#setFetchSchedule doesn't calculate fetch interval correctly", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "AdaptiveFetchSchedule#setFetchSchedule doesn't calculate fetch interval correctly"}, {"question": "What is the main context?", "answer": "Fetch interval is stored in seconds, but in AdaptiveFetchSchedule#setFetchSchedule it is treated as if in milliseconds. Also, fetch interval bound checking is done after interval value is stored in Cr"}]}}
{"issue_id": "NUTCH-653", "project": "NUTCH", "title": "Upgrade to hadoop 0.18", "status": "Closed", "priority": "Major", "reporter": "Dogacan Guney", "assignee": "Dogacan Guney", "created": "2008-09-19T13:04:28.060+0000", "updated": "2009-04-10T12:29:04.171+0000", "description": "This issue will track nutch's update to hadoop 0.18 version.", "comments": ["Upgrading to hadoop 0.18 only causes a single compiler error. This patch fixes that compiler error.", "Does anyone object to upgrading? I want to go forward with this, but I don't want to mess stuff up (especially, with native libraries) since I have never committed a hadoop upgrade before. ", "OK, I am committing this patch and resolving this issue.\n\nThere is still a chance that I messed things up :), so I will leave the issue open for a while.", "Integrated in Nutch-trunk #582 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/582/])"], "tasks": {"summary": "Upgrade to hadoop 0.18", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade to hadoop 0.18"}, {"question": "What is the main context?", "answer": "This issue will track nutch's update to hadoop 0.18 version."}]}}
{"issue_id": "NUTCH-654", "project": "NUTCH", "title": "urlfilter-regex's main does not work", "status": "Closed", "priority": "Trivial", "reporter": "Dogacan Guney", "assignee": "Dogacan Guney", "created": "2008-10-01T07:47:01.880+0000", "updated": "2009-04-10T12:29:05.521+0000", "description": "As you know, nutch allows you to run plugin with \"bin/nutch plugin ...\". But, RegexURLFilter's main is broken because it is initialized without a conf, so it can't read any of the rules and fails with exception if you try to do something with it.", "comments": ["Oops. Fixing priority.", "I will commit this patch in a few hours.", "+1", "Committed as of rev. 701045. ", "Integrated in Nutch-trunk #588 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/588/])\n     - urlfilter-regex's main does not work\n"], "tasks": {"summary": "urlfilter-regex's main does not work", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "urlfilter-regex's main does not work"}, {"question": "What is the main context?", "answer": "As you know, nutch allows you to run plugin with \"bin/nutch plugin ...\". But, RegexURLFilter's main is broken because it is initialized without a conf, so it can't read any of the rules and fails with"}]}}
{"issue_id": "NUTCH-655", "project": "NUTCH", "title": "Injecting Crawl metadata", "status": "Closed", "priority": "Minor", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2008-10-01T12:46:36.338+0000", "updated": "2010-08-05T23:04:39.368+0000", "description": "the patch attached allows to inject metadata into the crawlDB. The input file has to contain fields separated by tabs, with the URL being on the first column. The metadata names and values are separated by '='. A input line might look like this:\nhttp://www.myurl.com  \\t  categ=value1 \\t categ2=value2\n\nThis functionality can be useful to store external knowledge and index it with a custom plugin", "comments": ["Patch for injecting metadata into a crawlDB", "We may discuss if tab-separation is the best way to go, but +1 for the idea from me.", "I think we need a generic way for keeping meta data about hosts ... I think I started that somewhere in JIRA a while back.... aha: NUTCH-628\n\nI'm mentioning this simply because we can probably use the same or very similar mechanism for keeping meta data about hosts and individual URLs.\n\nBut it looks like NUTCH-650 may be the way of the future.\n", "I agree that https://issues.apache.org/jira/browse/NUTCH-650 would provide a cleaner way of doing this but since it is a substantial change it might take some time before it is committed. \n\nRegarding https://issues.apache.org/jira/browse/NUTCH-628 we could also have a similar injector for hostDBs that could be used to store / update statistics or any other information about hosts without necessarily getting it from the crawlDB. ", "Is everyone OK with moving this issue to target 1.1 release?", "1.1 sounds good to me.\n", "Moved to 1.1.", "Improved version of the patch which allows to specify custom scores for the URLs. A score is specified by simply setting a float value instead of a name=value couple e.g. \nhttp://www.lemonde.fr/    label=newspaper  10.0\nhttp://www.lequipe.fr/    label=sports  2.0", "Any objections to committing this patch?  ", "I'm not sure about the latest addition (the score option). If we go this route, then I suggest doing the last minor step and recognize reserved metadata keys to do also other useful things like setting fetch interval. I.e. define and recognize \"nutch.score\" and \"nutch.fetchInterval\", and document it properly somewhere ...(wiki? javadoc? cmd-line synopsis?).", "good idea. I've made the modification and documented in the javadoc :\n\nThe URL files contain one URL per line, optionally followed by custom metadata separated by tabs with the metadata key separated from the corresponding value by '='. \nNote that some metadata keys are reserved : \n- <i>nutch.score</i> : allows to set a custom score for a specific URL <br>\n- <i>nutch.fetchInterval</i> : allows to set a custom fetch interval for a specific URL <br>\ne.g. http://www.nutch.org/ \\t nutch.score=10 \\t nutch.fetchInterval=2592000 \\t userType=open_source\n ", "Committed revision 896539", "Hi Julien, thanks for this patch...\nis there any way to inherit the metadata or parts of it to suburls while crawling? \nI fiddled around with a scoring filter but with no success.\n\nCheers\nClaus", "Claus, see my patch: NUTCH-855"], "tasks": {"summary": "Injecting Crawl metadata", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Injecting Crawl metadata"}, {"question": "What is the main context?", "answer": "the patch attached allows to inject metadata into the crawlDB. The input file has to contain fields separated by tabs, with the URL being on the first column. The metadata names and values are separat"}]}}
{"issue_id": "NUTCH-656", "project": "NUTCH", "title": "DeleteDuplicates based on crawlDB only ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2008-10-09T08:35:00.500+0000", "updated": "2021-01-28T14:03:45.899+0000", "description": "The existing dedup functionality relies on Lucene indices and can't be used when the indexing is delegated to SOLR.\nI was wondering whether we could use the information from the crawlDB instead to detect URLs to delete then do the deletions in an indexer-neutral way. As far as I understand the content of the crawlDB contains all the elements we need for dedup, namely :\n* URL \n* signature\n* fetch time\n* score\n\nIn map-reduce terms we would have two different jobs : \n* read crawlDB and compare on URLs : keep only most recent element - oldest are stored in a file and will be deleted later\n\n* read crawlDB and have a map function generating signatures as keys and URL + fetch time +score as value\n* reduce function would depend on which parameter is set (i.e. use signature or score) and would output as list of URLs to delete\n\nThis assumes that we can then use the URLs to identify documents in the indices.\n\nAny thoughts on this? Am I missing something?\n\nJulien\n\n\n", "comments": ["Please see the ongoing discussion in SOLR-799 . This solution will likely be implemented soon in Solr, and it will be better handled there in case you use Solr/Nutch integration. The envisioned scenario is that you would implement a DuplicateDeletePolicy in Solr to follow the above algorithm, and then you wouldn't have to run the Nutch dedup tool at all.\n\nRe: CrawlDb-based dedup - CrawlDb only ever contains a single record for a unique URL, while multiple segments may contain multiple records for the same URL, so the URL dedup is not a CrawlDb issue but a segment issue, and it needs to be applied there (or in the corresponding index). So the CrawlDb-based dedup would only address the content-duplicate detection.", "Thanks for the link Andrzej. It makes a lot of sense to have that on the SOLR side ", "I suppose that the SOLR dedup mechanism is valid on a single instance. If the documents are distributed across a number of SOLR shards (by modifying NUTCH-442) there will be no way of detecting that two documents have the same signature if they are sent to different shards. Assuming that the documents are distributed across SOLR shards based on their unique ID (i.e. their URL) the deduplication of documents based on URLs is already done. What the SOLR-dedup could do would be to use the crawlDB as described earlier to find duplicates based on the signature and send deletion orders to the SOLR shards.\n\nnot an urging issue for the moment as NUTCH-442 supports only one SOLR backend though ", "Am reopening this one as we need a generic deduplicator now that we have committed NUTCH-1047. The SOLR dedup did work but was not very efficient as it required pulling all the documents from SOLR and use them as an input to the MapReduce code. The patch attached implements a generic deduplicator that can be used with any indexing backend (e.g. elasticsearch, cloudsearch but also solr). The main difference with the SOLR-specific code is that the score is the one from the crawldb and not from the index and the entries are currently not deleted from the crawldb, just in the index.\nAndrzej's point about having duplicates entries in the segment is still relevant but in practice documents with the same URLs (i.e. unique in the crawldb) override each other when indexed so they are already deduped in a way.\n\n", "Attached patch, please test. We will need to remove the nutch.indexer.solr package from src/java and remove the solrdedup from the nutch script and replace it with a generic command, as we did with the indexing. This would mean that the whole of the SOLR code will live in its plugin and as a consequence we could have different plugins for different versions of SOLR.", "We've did an attempt like this patch before and got into trouble because of not using a scoring plugin, variable fetch time intervals (adaptive scheduling) and indexers skipping not modified documents. I think you will need to be very careful in production.", "Hi Markus\n\nGood points. Re-scoring plugin we could make the code more robust by giving a default score if none can be found. As for not_modified not being in the index, then nothing wrong will happen to it when we ask the indexing backend to delete it, right? I assume your point about the fetch time intervals is that it does not differentiate between when a page was last fetched and when it will be fetched. Is that correct?\n\nDoesn't the current solr-dedup  have the same shortcomings anyway? e.g. tstamp generated by index-basic - what happens if it is not activated?\n\nThanks a lot\n\nJulien", "Hi Julien, hi Markus,\n\nregarding robustness: what happens in a continuous crawl if two duplicate documents switch their order regarding score? Previously, A had a higher score than B, consequently B has been removed from index. Now B gets the higher score, and DeduplicationJob will remove A from index. The current solr-dedup is immune because in the second call only A is retrieved from Solr and there is no need for deduplication. \n\nFor crawlDb-based deduplication deduplicated docs/urls must be flagged in CrawlDb so that the index status is reflected in CrawlDb. Deduplication jobs then can draw decisions dependent on previous deduplications/deletions. Also status changes from \"duplicate\" to \"not modified\" could treated in a save way by forcing a re-index (and re-fetch if required).\n\nAfter duplicates are flagged in CrawlDb, deletion of duplicates than could be done by indexing jobs. Also indexing backends (eg. CSV) which cannot really delete documents (deletion means not to index) will profit. In addition, re-fetch scheduling of duplicate docs could be altered to lower priority in a scoring filter. \n\nFlagging is possible in CrawlDatums meta data. But a new db status DUPLICATE, although a significant change, may be more explicit and efficient. And it would be simpler to combine various dedup jobs, eg, first by canonical links (NUTCH-710), second by signature. It's clear that docs of status DUPLICATE need no second (possibly contradicting) deduplication.\n", "Hi Seb\n\nExcellent points as usual, thanks! I am not sure whether to add a new status which would be quite disruptive or have a reserved key in the metadata. The latter would allow us to store a value with it e.g. the URL which has been kept. The crawldb status is more about the physical status of a URL but at the same time having a new status would probably be cleaner and we can always store any other information in the metadata anyway.\n\nLet's see what others think about adding a new status\n\nJulien", "Attached is a new patch which creates a new db status and a deduplication job which sets the status of a crawldatum to duplicate based on the heuristics described above. The deletion of the document is done with the CleaningJob task.\nThis addresses the comments made by Sebastian and should be produce similar results to the SOLR indexer, except that it would be more efficient on large crawls and usable by other indexing backends.\nCan you please have a look and let me know what you think? Thanks!", "[~wastl-nagel] [~markus17] any chance you could have a look at the latest patch ?", "I have not tried the code, we still run with 1.6 indexing code, but only read what you intend to do, which looks fine!", "About DeduplicationJob:\n* shouldn't this be in package o.a.n.crawl instead of o.a.n.indexer? Only CrawlDb is involved, although it's related to indexing, of course.\n* got a NPE if signature is null (may happen for successfully fetched docs, e.g., if parsing is skipped because of truncated content): we can skip docs without signature, they are not indexed and, consequently, never duplicates. \n\nWith a fix of the NPE DeduplicationJob worked well!\n\nStatus db_duplicate is used only in CleaningJob. Shouldn't it be used also in IndexerMapReduce? If DeduplicationJob is run before IndexingJob duplicates are even not indexed. Also indexer backends which do not allow to remove docs after indexing would profit.\n\nIn a continuous crawl it may happen that a deduplicated doc loses this status because the doc it is a duplicate of disappears. DeduplicationJob does not reset the duplicate status in this case. The doc get indexed not before it is re-fetched. To trigger a re-index is hard because we would need the old segment with fetch content. So we can ignore this problem (for now). Right?", "Thanks for your comments Seb. This new patch addresses some of the issues you pointed out  : \n\n{quote}\nshouldn't this be in package o.a.n.crawl instead of o.a.n.indexer? Only CrawlDb is involved, although it's related to indexing, of course.\n{quote}\n\ndone. it does not call the indexers directly and only modifies the crawldb so this is indeed the right place for it\n\n{quote}\ngot a NPE if signature is null (may happen for successfully fetched docs, e.g., if parsing is skipped because of truncated content): we can skip docs without signature, they are not indexed and, consequently, never duplicates.\n{quote}\n\nfixed  \n\n\n{quote}\nStatus db_duplicate is used only in CleaningJob. Shouldn't it be used also in IndexerMapReduce? If DeduplicationJob is run before IndexingJob duplicates are even not indexed. Also indexer backends which do not allow to remove docs after indexing would profit.\n{quote}\n\nI added some code in IndexerMapReduce so that entries marked as duplicates are now sent for deletion \n\n{quote}\nIn a continuous crawl it may happen that a deduplicated doc loses this status because the doc it is a duplicate of disappears. DeduplicationJob does not reset the duplicate status in this case. The doc get indexed not before it is re-fetched. To trigger a re-index is hard because we would need the old segment with fetch content. So we can ignore this problem (for now). Right?\n{quote}\n\nyes let's keep it simple for now\n\nWill commit this shortly and will remove the indexer.solr subpackage in a separate JIRA\n\nThanks for taking the time to review this\n", "correct attachment", "Committed revision 1541883.\n\nCommitted with a few minor changes compared to the latest patch (e.g. decides which one is the duplicate on the URL length as a last resort)\n\nThanks for the review and comments", "Well done! Run successful test crawl with deduplication.\nOne last remark: the dedup command is missing in command-line help of bin/nutch :)\n\n", "SUCCESS: Integrated in Nutch-trunk #2421 (See [https://builds.apache.org/job/Nutch-trunk/2421/])\nNUTCH-656 Generic Deduplicator (jnioche, snagel) (jnioche: http://svn.apache.org/viewvc/nutch/trunk/?view=rev&rev=1541883)\n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/src/bin/crawl\n* /nutch/trunk/src/bin/nutch\n* /nutch/trunk/src/java/org/apache/nutch/crawl/CrawlDatum.java\n* /nutch/trunk/src/java/org/apache/nutch/crawl/DeduplicationJob.java\n* /nutch/trunk/src/java/org/apache/nutch/indexer/CleaningJob.java\n* /nutch/trunk/src/java/org/apache/nutch/indexer/IndexerMapReduce.java\n", "[~wastl-nagel] yep, I did that as part of NUTCH-1668 which I will commit shortly unless someone opposes", "This patch was for 1.x only.  We've ported it to 2.x.  Should we reopen this issue and add a patch or?\n", "Please open a new issue with your patch for 2.x and link to this issue for some background. Thanks"], "tasks": {"summary": "DeleteDuplicates based on crawlDB only ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "DeleteDuplicates based on crawlDB only "}, {"question": "What is the main context?", "answer": "The existing dedup functionality relies on Lucene indices and can't be used when the indexing is delegated to SOLR.\nI was wondering whether we could use the information from the crawlDB instead to det"}]}}
{"issue_id": "NUTCH-657", "project": "NUTCH", "title": "Estonian N-gram profile has wrong name", "status": "Closed", "priority": "Trivial", "reporter": "Jonathan Young", "assignee": null, "created": "2008-10-14T17:11:31.766+0000", "updated": "2011-11-28T13:20:57.553+0000", "description": "The Nutch language identifier plugin contains an ngram profile, ee.ngp, in src/plugin/languageidentifier/src/java/org/apache/nutch/analysis/lang .  \"ee\" is the ISO-3166-1-alpha-2 code for Estonia (see http://www.iso.org/iso/country_codes/iso_3166_code_lists/english_country_names_and_code_elements.htm), but it is the ISO-639-2 code for Ewe (see \nhttp://www.loc.gov/standards/iso639-2/php/English_list.php).  \"et\" is the ISO-639-2 code for Estonian, and the language profile in ee.ngp is clearly Estonian.\n\nProposed solution: rename ee.ngp to et.ngp .\n\n", "comments": ["We could still fix this easily for 1.3. But why are these files actually missing in trunk?", "I have been unsuccessful in submitting a patch for a file name change as oppose to content changes within the file... any pointers please? I am not familiar with submitting patches for file name changes.\n\nYes Markus, non of these files exist within trunk... strange. From doing some background reading into the classes I can see that two authors are Sami Siren and Jerome Charron. Is there anyone on board that has experience working with the language identifier code? This is really the first time I have looked over it...", "Further to this the file package.html contains the following... however the link is broken.\n\n<html>\n<body>\n<p>Text document language identifier.</p><p>Language profiles are based on material from\n<a href=\"http://www.isi.edu/~koehn/europarl/\">http://www.isi.edu/~koehn/europarl/</a>.</p>\n</body>\n</html>\n\nThe correct link should be \n\nhttp://www.homepages.inf.ed.ac.uk/pkoehn/publications/europarl.ps\n\nI will submit a patch.", "I'd thought that Nutch was now delegating language detection to Tika (which contains a port of what Nutch has).\n\nIn any case, it's et.ngp over in Tika-land.", "Hi Ken\n\nI think this has been done for 2.0 but not in 1.x. Would be a very good thing to do.\n\nJulien\n", "I opened a separate issue for the issue above [1]\n\nYes Julien, this would make sense as to why none of the files are present in branch.\n\nWould it be more suitable to mark this as won't fix and instead open a new issue affecting 1.4 for replicating current functionality as shown in trunk?\n\n[1] https://issues.apache.org/jira/browse/NUTCH-1055", "As per Julien's issue which delegates language identification to Tika. I'm here to ask again if we can mark this as won't fix for the 1.4 current trunk?", "bq.  I'm here to ask again if we can mark this as won't fix for the 1.4 current trunk?\n\nSounds like this is the right thing to do if/when tika lang-id is used. I think the lang-id component in Tika lost some of it's accuracy when it got moved from Nutch to Tika but I think it makes most sense to build on top of that and improve the one in Tika instead of having something special in Nutch.", "OK Sami, I agree. I think we should leave this open for a wee while to give anyone else their say. Thank you", "https://issues.apache.org/jira/browse/TIKA-453"], "tasks": {"summary": "Estonian N-gram profile has wrong name", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Estonian N-gram profile has wrong name"}, {"question": "What is the main context?", "answer": "The Nutch language identifier plugin contains an ngram profile, ee.ngp, in src/plugin/languageidentifier/src/java/org/apache/nutch/analysis/lang .  \"ee\" is the ISO-3166-1-alpha-2 code for Estonia (see"}]}}
{"issue_id": "NUTCH-658", "project": "NUTCH", "title": "Add Counter for # of doc fetched in Reporter", "status": "Closed", "priority": "Trivial", "reporter": "Julien Nioche", "assignee": null, "created": "2008-10-31T16:31:43.723+0000", "updated": "2010-01-05T10:16:47.396+0000", "description": "Having a Counter for the number of documents fetched duplicates the information in the status of each Map but should be summed across all the Map instances and displayed along with the standard Counters. The same mechanism could be used for the other possible status of a URL (redir / gone etc...)", "comments": ["This is a great idea. I think this should go in before 1.0.\n\nBut more counters would be nice. For fetch, gone, error, etc as you mentioned. For parse, number of urls succesfully parsed or failed. .....\n\nCan you send an updated patch? If you don't have the time I can do it too.", "Hi Dogacan, \n\nI am off work for several weeks and won't be able to work on that. If you want to do it yourselves that would be great otherwise I'd be happy to do it in January.\n\n", "Hi, \n\nI eventually managed to make the change. The new patch (ReporterCounter) has been generated against the Revision 724410. Both implementations of the Fetchers now report statistics on all possible status, including the status for the parsing step. Stand alone parsing with ParseSegment does the same.\n\nJ. ", "If no one objects I'll commit this one in the next couple of days. OK for everybody?", "Committed revision 895972"], "tasks": {"summary": "Add Counter for # of doc fetched in Reporter", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add Counter for # of doc fetched in Reporter"}, {"question": "What is the main context?", "answer": "Having a Counter for the number of documents fetched duplicates the information in the status of each Map but should be summed across all the Map instances and displayed along with the standard Counte"}]}}
{"issue_id": "NUTCH-659", "project": "NUTCH", "title": "Help! No urls fetched for internal repository website", "status": "Closed", "priority": "Critical", "reporter": "Bryan", "assignee": null, "created": "2008-11-09T23:33:00.219+0000", "updated": "2011-04-01T15:07:21.849+0000", "description": "I am new to Nutch, and implemented Nutch for my internal company websites search. The version is nutch-2008-11-02_04-01-26.tar.\n\n \n\nMy internal company websites includes several HTTP websites. \n\nAnother one is SVN repository HTTPS websites in XML structure, using <dir> and <file> tag.\n\n \n\nThe search in HTTP websites is good. \n\nThe HTTPS is ok. We have some links in those HTTP websites which point to Word files under SVN website. They can be indexed.\n\n \n\nBut the Nutch does not search my SVN website. If I only search the SVN website, it is always: 0 urls fetched.\n\n \n\nMy nutch-site.xml is as following:\n\n<property>\n\n  <name>plugin.includes</name>\n\n  <value>protocol-httpclient|urlfilter-regex|parse-(text|html|js|msexcel|msword|mspowerpoint|pdf|zip|swf|rss)|index-(basic|anchor)|query-(basic|site|url)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)</value>\n\n \n\n# skip file:, ftp:, & mailto: urls\n\n-^(ftp|mailto):\n\n \n\n# accept hosts in MY.DOMAIN.NAME\n\n+^http://([a-z0-9]*\\.)*smartlabs.com.au/\n\n \n\nAny help would be much appreciated. Thanks in advnce.", "comments": ["Please ask questions on the mailing list.", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Help! No urls fetched for internal repository website", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Help! No urls fetched for internal repository website"}, {"question": "What is the main context?", "answer": "I am new to Nutch, and implemented Nutch for my internal company websites search. The version is nutch-2008-11-02_04-01-26.tar.\n\n \n\nMy internal company websites includes several HTTP websites. \n\nAnoth"}]}}
{"issue_id": "NUTCH-66", "project": "NUTCH", "title": "Cookies are not being read properly", "status": "Closed", "priority": "Minor", "reporter": "CC Chaman", "assignee": null, "created": "2005-07-03T05:31:30.000+0000", "updated": "2005-07-21T06:38:39.000+0000", "description": "Cookies that do not begin with a period are not being accepted. For example \"cnn.com\" instead of the RFC \".cnn.com\". But A LOT of sites seem to not know the standard. It would be nice if the plugin accepted those cookies as well.\n", "comments": ["If you are using protocol-httpclient, add the following lines to Http.java, around line 395:\n\nparams.setParameter(\"http.protocol.cookie-policy\", CookiePolicy.BROWSER_COMPATIBILITY);\nparams.setBooleanParameter(\"http.protocol.single-cookie-header\", true);\n\nPlease report if this helps.", "It appears that this was a correct fix, but applied at the wrong level... :-) These are per-method properties - setting them inside HttpResponse fixes the problem.", "Fixed. The plugin now uses BROWSER_COMPATIBLITY Cookie specification, which should be flexible enough to accomodate most common cookie misformatting. If the problem persists, please reopen this issue - we can continue fixing this by using our own CookiePolicy."], "tasks": {"summary": "Cookies are not being read properly", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Cookies are not being read properly"}, {"question": "What is the main context?", "answer": "Cookies that do not begin with a period are not being accepted. For example \"cnn.com\" instead of the RFC \".cnn.com\". But A LOT of sites seem to not know the standard. It would be nice if the plugin ac"}]}}
{"issue_id": "NUTCH-660", "project": "NUTCH", "title": "Does anybody know how to let nutch crawl this kind of website?", "status": "Closed", "priority": "Critical", "reporter": "Bryan", "assignee": null, "created": "2008-11-11T05:26:57.080+0000", "updated": "2009-01-25T11:40:07.783+0000", "description": "My company intranet website is a svn repository, similar to : http://svn.apache.org/repos/asf/lucene/nutch/ .\nDoes anybody have an idea on how to let nutch do search on it?\n\nThanks.\nBryan\n\n", "comments": ["Sorry the information above is not quite clear.\nI just tried to search http://svn.apache.org/repos/asf/lucene/nutch/, and it did work. \n\nBut I still can not search my own svn repository site.\nAlso I fine other two websites,\nhttp://svn.macosforge.org/repository/macports/\nhttp://svn.collab.net/repos/svn/\n\nWhen I use my nutch to crawl them, I got same results as well:\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=0 - no more URLs to fetch.\n\n\nI am new here. What I was told is that in teh case of my company svn the xml files are just file/folder names, most of the useful stuff in the svn is just referenced by the xml. What the XML Stylesheet does is turn the XML into HTML so the broswers can follow the links.\n\nI guess there must be something difference inbetween NutchSVN and my company SVN, which I do not know yet.\n\n\n\nI have configured the nutch-site.xml and crawl-urlfilter.txt.\nAs I can crawl http://svn.apache.org/repos/asf/lucene/nutch/ , so I assume my configuration is ok. Do u think so?\nJust make sure no more work with my nutch configuration.\n\nThanks & best regards,.\n\n", "I see you already asked on the list.  That's the right place to ask questions, so I'm closing this."], "tasks": {"summary": "Does anybody know how to let nutch crawl this kind of website?", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Does anybody know how to let nutch crawl this kind of website?"}, {"question": "What is the main context?", "answer": "My company intranet website is a svn repository, similar to : http://svn.apache.org/repos/asf/lucene/nutch/ .\nDoes anybody have an idea on how to let nutch do search on it?\n\nThanks.\nBryan\n\n"}]}}
{"issue_id": "NUTCH-661", "project": "NUTCH", "title": "errors when the uri contains space characters ", "status": "Closed", "priority": "Major", "reporter": "Christos LAIOS", "assignee": "Dogacan Guney", "created": "2008-11-12T17:15:33.794+0000", "updated": "2009-04-10T12:29:04.290+0000", "description": "While spidering our intranet, i get the following errors when the uri contains space characters\n\n\nfetch of http://intranet-rtd.rtd.cec.eu.int/services/docs/AAR_2007 - FINAL.doc failed with: java.lang.IllegalArgumentException: Invalid uri 'http://intranet-rtd.rtd.cec.eu.int/services/docs/AAR_2007 - FINAL.doc': escaped absolute path not valid\n", "comments": ["The Uniform Resource Locators (URL) specification (RFC 1738) defines the space character as \"unsafe\" and states that all unsafe characters must always be encoded within a URL. The URL encoding for space is %20 so the document URL should be: \nhttp://intranet-rtd.rtd.cec.eu.int/services/docs/AAR_2007%20-%20FINAL.doc\n \nYour intranet includes links to invalid URLs that do not conform to the spec. While Nutch could handle this error, it shouldn't necessarily allow malformed URLs to be fetched. It is because some clients attempt to automatically encode illegal characters when they are encountered that people don't realise there are problems with their links.", "I think you can write a URL normalizer that (in SCOPES inject and outlink) just replaces spaces with %20 as Kristian B. suggests. \n\nAnd since this is fixable with a plugin, I am going to close this issue if no one has objections.", "Closing this issue as Won't Fix.\n\nThis can be fixed with a urlnormalizer plugin as suggested in comments."], "tasks": {"summary": "errors when the uri contains space characters ", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "errors when the uri contains space characters "}, {"question": "What is the main context?", "answer": "While spidering our intranet, i get the following errors when the uri contains space characters\n\n\nfetch of http://intranet-rtd.rtd.cec.eu.int/services/docs/AAR_2007 - FINAL.doc failed with: java.lang."}]}}
{"issue_id": "NUTCH-662", "project": "NUTCH", "title": "Upgrade Nutch to use Lucene 2.4", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-11-21T10:15:29.329+0000", "updated": "2009-04-10T12:29:03.456+0000", "description": "Upgrade nutch to use Lucene 2.4.  This release changes the lucene file format.  New indexes created by this lucene version will NOT be readable by older versions.  Lucene 2.4 can read and update older index formats although updating an older format will convert it to the new format.  There are also some performance and functionality improvments.", "comments": ["This is a basic patch which upgrades Nutch to use Lucene 2.4.  This is a different file format that the 2.3 branch and while 2.4 can read and update 2.3 format files, new format index files will be created using this version that will not be able to be read using previous versions.", "The upgrade to Lucene 2.4 causes a weird problem that might need some discussion.  The o.a.n.indexer.FsDirectory$DfsIndexOutput class is used to interact with an index stored on DFS.  The 2.4 version of Lucene in the ChecksumIndexOutput.prepareCommit method and finalizeCommit methods do a pseudo two-phase commit.  To do this it writes an intential mismatched checksum (long = checkum - 1) then flushes and seeks back and writes the correct checksum in the same spot.  They say this is to ensure the commit.  Because DFS doesn't have append functionality we can't write to it, seek back to a position, and write again.  DFS is write only.\n\nTo handle this problem in the attached patch, I first write out to a local temporary file that is deleted upon exit, then when close is called on the IndexOutput, that file is written out to DFS all at once.  I don't know if this is the best way to do this or if there is a better way, but it does handle the new write and seek functionality of lucene 2.4.  The previous implementation of DfsIndexOutput simply threw an UnsupportedOperationException when the seek method was called.  This was fine before 2.4 as lucene wasn't calling that method during writing to DFS.  In 2.4 it does and unit tests were failing because of it.  What does everybody think about this implementation?\n\nOther than that I don't see any major issues in upgrading to 2.4.  Some people have said performance we down in 2.4.  My thoughts are, that might be the case but those will be fixed and it would be good to be on the most recent lucene version as we move to a 1.0 release for Nutch.  Also we have been using 2.4 in production for a month now without any issues.", "+1 on moving to 2.4 anyway. Regarding the patch: I think this is a viable solution for now. Performance-wise the impact of local buffering, especially in case of large indexes, could be significant - the indexing may take much longer with this change.\n\nAnyway, this problem should be reported to the Lucene devs, so that they may discuss what's the best strategy to address this in the future.", "> +1 on moving to 2.4 anyway. Regarding the patch: I think this is a viable solution for now. Performance-wise the impact of local buffering, \n> especially in case of large indexes, could be significant - the indexing may take much longer with this change. \n\nI think this is only a problem with updating old indexes to new format. During indexing (in Indexer.OutputFormat) we write index to a local file first anyway so seeking should not be a problem... Or am I missing something here?", "We had been running in production for about a month and never saw any issues with the indexing processes using 2.4.  Then I was doing some work for upgrading the trunk and it popped up in delete duplicates unit testing.  We don't do delete duplicates in our JobStream, we do it query side.  \n\nFirst problem was that the old DfsIndexOutput didn't implement the seek method (probably because DFS can't seek), so when that was changed to allow it to seek, it was throwing Checksum errors on the index when it was trying to open it.  Come to find out as above 2.4 is purposefully writing a bad checksum, then seeking back, then writing a correct checksum in closing the index as a pseudo-two-phase commit.  So I don't think it will affect the indexing process because as you noted it writes to local first then just transfers to DFS.  In changing DfsIndexOutput to allow DeleteDuplicates to work I just took the same approach, local first, then put to DFS.", "Committed with revision 722475", "closed", "Integrated in Nutch-trunk #667 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/667/])\n    "], "tasks": {"summary": "Upgrade Nutch to use Lucene 2.4", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade Nutch to use Lucene 2.4"}, {"question": "What is the main context?", "answer": "Upgrade nutch to use Lucene 2.4.  This release changes the lucene file format.  New indexes created by this lucene version will NOT be readable by older versions.  Lucene 2.4 can read and update older"}]}}
{"issue_id": "NUTCH-663", "project": "NUTCH", "title": "Upgrade Nutch to use Hadoop 0.19", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-11-21T10:17:27.916+0000", "updated": "2009-06-02T05:37:16.155+0000", "description": "Upgrade Nutch to use a newer hadoop, version 0.18.2.  This includes performance improvements, bug fixes, and new functionality.  Changes some current APIs.", "comments": ["I think 0.19 is about to be released. Maybe worthwhile to wait a few more days and directly upgrade to 0.19.", "@buddha1021\nThe 1.0 release for Nutch has some of the features for Nutch 2 but it is not a complete Nutch 2 Architecture.  We felt it was best to do add some needed features into the current version of Nutch and get them deployed to the community quickly.  A lot of people have been asking about the development of Nutch and releasing.  Truth is we have just been busy adding in needed features and patches.  We should have a release out in the next couple of weeks.  That will be a 1.0 release for Nutch but will probably contain a 18.2 or 19 release for Hadoop. We aren't waiting for hadoop to go to 1.0.\n\n@Doğacan Güney\nI am not opposed to waiting for 0.19 as long as it will be released soon.  I was looking and it seemed they tried to release a little while back and didn't finish because of some big errors.", "hadoop 0.19 was release.  I am integrating it in and should have a patch shortly.", "Updates jar and native files", "Native files", "Hadoop core jar", "change to 0.19 instead of 0.18.2", "Updated patch to include API changes in Nutch classes.", "Committed with revision 722477", "Integrated in Nutch-trunk #667 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/667/])\n    "], "tasks": {"summary": "Upgrade Nutch to use Hadoop 0.19", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade Nutch to use Hadoop 0.19"}, {"question": "What is the main context?", "answer": "Upgrade Nutch to use a newer hadoop, version 0.18.2.  This includes performance improvements, bug fixes, and new functionality.  Changes some current APIs."}]}}
{"issue_id": "NUTCH-664", "project": "NUTCH", "title": "Possibility to update already stored documents.", "status": "Closed", "priority": "Minor", "reporter": "Sergey Khilkov", "assignee": null, "created": "2008-11-26T06:30:43.169+0000", "updated": "2019-10-13T22:35:22.630+0000", "description": "We have huge index of stored documents. It is high cost procedure to fetch page, merge indexes any time we update some information about page. The information can be changed 1-3 times per day. At this moment we have to store changed info in database, but in this case we have lots of problems with sorting, search restricions and so on. Lucene itself allows delete single document and add new one into existing index. But there is a problem with hadoop... As I understand hadoop filesystem has no possibility to write in random positions. But it will be great feature if nutch will be able to update created index.", "comments": ["There is no proposed design, so this is a Wish.", "Yes, It will be great to have changeDocument() method of IndexWriter class. Hope it's possible )", "This is possible with a hbase-solr/katta/etc combo. I am working on hbase support but it will wait after 1.0 to go in.", "Good news! So, I'll wait until 1.0 and prepare project for hbase-solr!", "Moving this issue to 1.1.", "- pushing this out per http://bit.ly/c7tBv9", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "Possibility to update already stored documents.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Possibility to update already stored documents."}, {"question": "What is the main context?", "answer": "We have huge index of stored documents. It is high cost procedure to fetch page, merge indexes any time we update some information about page. The information can be changed 1-3 times per day. At this"}]}}
{"issue_id": "NUTCH-665", "project": "NUTCH", "title": "Search Load Testing Tool", "status": "Closed", "priority": "Minor", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-11-26T14:40:48.155+0000", "updated": "2011-06-08T21:34:21.133+0000", "description": "A tool which spawn a number of threads and executes searches against configured search servers.  This is used for light load testing of search servers.", "comments": ["Search load testing tool.", "Committed with revision 722481", "Integrated in Nutch-trunk #667 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/667/])\n    "], "tasks": {"summary": "Search Load Testing Tool", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Search Load Testing Tool"}, {"question": "What is the main context?", "answer": "A tool which spawn a number of threads and executes searches against configured search servers.  This is used for light load testing of search servers."}]}}
{"issue_id": "NUTCH-666", "project": "NUTCH", "title": "Analysis plugins for multiple language and new Language Identifier Tool", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-11-26T14:54:10.486+0000", "updated": "2011-08-09T16:51:50.955+0000", "description": "Add analysis plugins for czech, greek, japanese, chinese, korean, dutch, russian, and thai.  Also includes a new Language Identifier tool that used the new indexing framework in NUTCH-646.", "comments": ["Part one of patch.  This includes the new analyzers for different languages.  Part two will include the new language identifier tool.", "Fixed patch.  Now includes the changes to AnalyzerFactory to allow multiple languages per plugin.", "Dennis, is it OK to move this issue out of 1.0? Or do you want to commit it before?", "It is ok to move to 1.1.  ", "Dennis, could you please describe how this new Lang ID tool is better/different from the previous one?", "Hi,\n\nI am looking forward to use only the language identifier (language-identifier.jar) plugin for identification of chinese, japanese and korean languages.  \n\nCan someone help me in this regard ?\n\nIs this already implemented ? If yes, how can i take the dev version and use it ?\n\nCan i use the language identifier of version 1.0 and train it (create N-Gram profiles) to identify the above 3 languages ??\n\nAny help is highly appreciated.\n\nRegards\nRaja", "Dennis, what's the status of this patch (especially the missing part, the new language identifier)?", "We should also consider switching to Tika for language identification and route the proposed improvements in that area through Tika?", "Here is the patch as I last used it, almost a year ago now.  I am not sure if it is functioning or not with the current codebase.  It uses a hacky version of textcat to create fingerprint files on known language content, this creates a dictionary, that dictionary is configured through the textcat.conf file in the conf directory.  The Language Identifier tool is then used to create a database of url -> langugage code, which before was included using the CustomFields job of the fields indexer.  The other language analysis plugins from the previous patch acted off of locale or chosen language on the query side I think.", "BTW, the reason we did this code, which we worked with an NLP firm to create, versus using the current Langauge identification tool in Nutch was accuracy.  The current tool we were getting around 70% accuracy level while this new tool routinely came in above 99.5% accuracy.  We trained off of wikipedia and most of the errors we saw were english characters in other-language version of the training data.  ", "Do you think it was related to the quality of language models that you built (presumably the ones in the patch?) versus the ones in the Nutch plugin, or due to a different classification algorithm? I'm trying to understand the source of such a big difference, because AFAIK the algorithm in textcat is essentially the same as the one we use.", "I don't remember exactly what the difference was, but I do remember that there was a subtle difference in the algorithms that was only noticed after creating the new tools.  I think it had something to do with how the ngrams were being handled or that it was taking spaces into account.  But try running the identifiers side by side, you will see there is a considerable difference.", "I agree with Sami that this should be contributed to Tika and that we delegate the language identification handling in Nutch to Tika, just as we are doing or planning to for the MimeType and the parsing ", "- pushing this out per http://bit.ly/c7tBv9", "Chris excuse my naivety but I am unfamiliar with you're phrasing above so excuse if you have dealt with this issue to some degree.\n\nIf this is not the case can I suggest that we mark this as won't fix? Julien has instigated the transition to language detection via delegation to Apache Tika as per NUTCH-1075 .", "Hi Lewis,\n\nThat's fine with me. My comment just meant that I was pushing this issue out and wasn't going to solve it in the last release (aka, push it to release N+1). I'm fine with marking it was \"won't fix\" though for the reasons you mentioned.\n\nThanks!\n\n", "I understand that this is not my issue to close, however I have not seen Dennis on the lists for a wee while and in an attempt to clean up dated issues on the JIRA I think it important to progress with closing this issue as there has been no objection otherwise. In addition, it seems only logical that the issue be marked as won't fix and closed as per the comments provided by various committers.\n\nIf this decision is not welcomed then I will happily reopen :|\n", "I am still here.  I still keep track of the lists even though I haven't been as active with Nutch because I have been doing more real time search work, Zoie.  I agree with closing this issue, Tika is a better solution nowadays then we had when this patch was started.", "Thank you Dennis for confirming."], "tasks": {"summary": "Analysis plugins for multiple language and new Language Identifier Tool", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Analysis plugins for multiple language and new Language Identifier Tool"}, {"question": "What is the main context?", "answer": "Add analysis plugins for czech, greek, japanese, chinese, korean, dutch, russian, and thai.  Also includes a new Language Identifier tool that used the new indexing framework in NUTCH-646."}]}}
{"issue_id": "NUTCH-667", "project": "NUTCH", "title": "Input Format for working with Content in Hadoop Streaming", "status": "Closed", "priority": "Minor", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-11-26T15:57:53.811+0000", "updated": "2009-04-10T12:29:06.979+0000", "description": "This is a ContextAsText input format that removes line endings with spaces that allow Nutch content to be used more effectively inside of Hadoop streaming jobs that allow MapReduce jobs to be written in any language that can communicate with stdin and stdout.", "comments": ["Input format for working with hadoop streaming.", "Committed with revision 722483", "Integrated in Nutch-trunk #667 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/667/])\n    "], "tasks": {"summary": "Input Format for working with Content in Hadoop Streaming", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Input Format for working with Content in Hadoop Streaming"}, {"question": "What is the main context?", "answer": "This is a ContextAsText input format that removes line endings with spaces that allow Nutch content to be used more effectively inside of Hadoop streaming jobs that allow MapReduce jobs to be written "}]}}
{"issue_id": "NUTCH-668", "project": "NUTCH", "title": "Domain URL Filter", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2008-12-03T01:41:26.960+0000", "updated": "2009-04-10T12:29:01.651+0000", "description": "A URLFilter that adds the ability to filter out URLs by top level domain or by hostname.  A configuration file with a listing of URLs is used to denote accepted urls.", "comments": ["Includes the DomainURLFilter and test files.  Domains can either be filtered by top level domains ignoring subdomains, or by hostnames through configuration.  There is a configuration file where valid domains are placed one per line.  Those domains are used to create valid domain set against which we validate urls at runtime.  Only urls which match domains in the domain set are considered valid.", "Updated to include URLUtil methods that were missing.  Sorry.", "The test case contains a reference to a path on your local machine ...\n\nAlso, the issue of domain vs. subdomain vs. host matching ... I'd love to be able to specify patterns like this:\n\nedu\nexample.com\nblurfl.foobar.org\n\nmeaning: accept everything from .com TLD, everything from example.com including subdomains and hosts, and anything from blurfl.foobar.org, whether that's a hostname or a subdomain.\n\nWe could do it with a suffix tree, or by matching the increasing number of hostname elements to the HashSet, e.g. for www.blurfl.foobar.org we would check:\n\n org - no match\n foobar.org - no match\n blurfl.foobar.org - match, break and return\n\nFor www.foobar.com we would check:\n\n com - no match\n foobar.com - no match\n www.foobar - no match\n return null\n\nThe price is that we need to make as many probes in the HashSet as there are domain elements, but the advantage is the increased flexibility in configuring allowed domains / hosts.\n\nI'm also fine if you want to commit it as it is, and create an issue to enhance this plugin later.\n\n", "I agree.  Being able to search for tlds like .com would make it much more flexible.  Let me work up the changes and I will post a new patch (without my local path :)).  Although I do want to get this in quickly I think the new functionality is worth the wait.", "New domain filter patch that matches against suffix, domain, and hostname in that order.", "Anybody have a problem if I commit this today or tommorrow?", "+1. Minor cosmetic change I would do: DomainURLFitler.java uses StringUtils twice, each time using the full package name - just add an import statement to make this shorter.", "It uses two different StringUtils classes, one from commons lang, one from org.apache.hadoop.util.StringUtils.  I just chose commons as I thought I would use that one more times.  As it happens I only use it once in this patch.", "Committed with revision 729958.", "Integrated in Nutch-trunk #691 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/691/])\n    ", "at line 173 - shouldn't we return 'url' instead of null? otherwise we are in contradiction with the comment \n// if an error happens, allow the url to pass\nand block URLS which are pure IP adresses.\n"], "tasks": {"summary": "Domain URL Filter", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Domain URL Filter"}, {"question": "What is the main context?", "answer": "A URLFilter that adds the ability to filter out URLs by top level domain or by hostname.  A configuration file with a listing of URLs is used to denote accepted urls."}]}}
{"issue_id": "NUTCH-669", "project": "NUTCH", "title": "Consolidate code for Fetcher and Fetcher2", "status": "Closed", "priority": "Major", "reporter": "Todd Lipcon", "assignee": "Sami Siren", "created": "2008-12-04T21:12:40.698+0000", "updated": "2009-04-10T12:29:04.579+0000", "description": "I'd like to consolidate a lot of the common code between Fetcher and Fetcher2.java.\n\nIt seems to me like there are the following differences:\n  - Fetcher relies on the Protocol to obey robots.txt and crawl delay settings whereas Fetcher2 implements them itself\n  - Fetcher2 uses a different queueing model (queue per crawl host) to accomplish the per-host limiting without making the Protocol do it.\n\nI've begun work on this but want to check with people on the following:\n\n- What reason is there for Fetcher existing at all since Fetcher2 seems to be a superset of functionality?\n\n- Is it on the road map to remove the robots/delay logic from the Http protocol and make Fetcher2's delegation of duties the standard?\n\n- Any other improvements wanted for Fetcher while I am in and around the code?", "comments": [">     *  What reason is there for Fetcher existing at all since Fetcher2 seems to be a superset of functionality?\n\nAgreed. We should just rename Fetcher2 to Fetcher and be done with it :D", "* +1 on keeping Fetcher2 alone, and bringing any enhancements / bugfixes to this code base. The queueing model in Fetcher1 doesn't scale, and it's nearly impossible to implement additional per-host controls there.\n\n* crawl delay handling should be implemented in Fetcher and not in protocol plugins, because Fetcher has better knowledge of the crawl conditions. E.g. a common modification to Fetcher2 is to implement bandwidth controls per host queue, or to skip hosts that respond too slowly - this would have to be re-implemented over and over again per each protocol ... We may consider refactoring the code so that the per-host queueing is not tied so tightly to the Fetcher code, but put it instead in a separate class.\n\nIn light of the above, I vote for removing the robots/delay logic from protocol implementations.\n\n* Robots.txt fetching and caching is still not where it should be, i.e. in a host database (see NUTCH-628). However, that issue is nowhere near completion, so I feel that we need to stick to the current model for now.\n\n* improvements: some people reported performance issues with Fetcher2, i.e. that it doesn't use the available bandwidth. These reports are unconfirmed, and they may have been caused by suboptimal URL / host distribution in a fetchlist - but it would be good to review the synchronization and threading aspects of Fetcher2.", "Agreed on all fronts.\n\nI spent several hours yesterday refactoring/rewriting Fetcher2 to be a little cleaner . One of the changes was to factor out the queueing policies into a new class and replace the Thread-based model with one based on ExecutorServices. I may also try to factor out the actual fetching into a new class as well.\n\nI haven't gotten to testing the new version quite yet but hopefully should have a patch available next week, and perhaps some intermediate commits available on github this afternoon so people can see where I'm headed.\n\nIs there a unit (or functional) testing infrastructure I can use somewhere to test this?\n\n-Todd", "+1 for only have Fetcher2 as Fetcher.\n\nDo we want to try to get this into 1.0?  And we have briefly talked about trying to move to a more generic crawling model for Nutch, something like Droids.  Any thoughts on how this might affect that move in the future?", "I've pushed the initial commit of this rewrite/refactor to github:\n\nhttp://github.com/toddlipcon/nutch/commit/5c9d99a856628c842b50b1d76f62b375f377bf95\n\nMight be worth just reviewing it as if it were a new file rather than a diff:\n\nhttp://github.com/toddlipcon/nutch/tree/5c9d99a856628c842b50b1d76f62b375f377bf95/src/java/org/apache/nutch/fetcher/Fetcher.java\n\nStill have some more cleanup and revisions here, plus I want to test it on a real crawl or two from our cluster. It currently passes the TestFetcher unit test but I don't know what the coverage is on that.\n\nI'll attach a patch here before it's ready to be comitted it so I can check off the license grant checkbox, which I know is important for ASF.", "+1 -- people, vote for it.  This could go in 1.0, right?\n", "For those watching this issue: I pushed a couple more changes to the github repo linked above. I'm about to try it on production with a 100K url segment, 80 threads, limit by IP, 8 crawler nodes. We'll see how it goes.", "Well ... have you tried it? How did it go?\n\nI think it's time to upload the patch to JIRA, so that we can decide what to do using a concrete snapshot of your work.", "I, too, am very anxious to see how the new Fetcher did compared to the old Fetcher/Fetcher2.\n\nSome of Todd's recent (Nutch) work can be seen on the right side of http://github.com/toddlipcon\n", "Hey guys,\n\nI tried it on production, but ran into an Exception of some sort that happened very rarely. Then I went on vacation for 2 weeks and came back to find the logs gone from my hadoop tracker, so I can't figure out what the Exception was ;-) I'll run another segment today hopefully and let you know the results.\n\n-Todd", "Found the exception in a screen log:\n\n{noformat}\njava.lang.NullPointerException\n        at org.apache.nutch.crawl.MapWritable$KeyValueEntry.access$102(MapWritable.java:469)\n        at org.apache.nutch.crawl.MapWritable.readFields(MapWritable.java:362)\n        at org.apache.nutch.crawl.CrawlDatum.readFields(CrawlDatum.java:250)\n        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)\n        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)\n        at org.apache.hadoop.io.SequenceFile$Reader.deserializeValue(SequenceFile.java:1817)\n        at org.apache.hadoop.io.SequenceFile$Reader.getCurrentValue(SequenceFile.java:1790)\n        at org.apache.hadoop.mapred.SequenceFileRecordReader.getCurrentValue(SequenceFileRecordReader.java:103)\n        at org.apache.hadoop.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:78)\n        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:186)\n        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:170)\n        at org.apache.nutch.fetcher.Fetcher$FetchMapper.run(Fetcher.java:399)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)\n        at org.apache.hadoop.mapred.Child.main(Child.java:155)\n{noformat}\n\nI think NUTCH-676 may help this. Trying another run in a minute.", "Here's a further report on my progress:\n\n  - It turns out the change in NUTCH-676 caused things to break - there's some behavior in nutch's MapWritable that differs from Hadoop's, so it was spending all of its time in output.collect - I think the writables were accruing lots of key/value pairs that they weren't sposed to. So, this doesn't depend on NUTCH-676.\n\n  - I implemented adaptive crawl delay (NUTCH-475) in the new fetcher.\n\n  - Also implemented early termination as discussed in this mailing list thread: http://www.nabble.com/proposal:-fetcher-performance-improvements-td20939872.html\n\nResults so far are looking good. I was able to run a 1M url fetch with 5000 urls per host at a sustained rate of 25 pages/second (total around 11 hours). About 60% of the URLs ended up parsed, which isn't significantly worse than I usually see without early termination, but past attempts to run 1M fetches have taken several days because of some slow hosts.\n\nI'm running a 2M+ URL fetch right now and have been sustaining 40-60mbit inbound from 8 fetchers for the last couple hours.\n\n  - I did experience one GC error - I think I need to add some cleanup of empty queues out of the FetchQueue structure when the number of unique hosts is very high.\n\nComplete history is here: http://github.com/toddlipcon/nutch/tree/nutch-669", "Todd, and when you say \"sustained rate of 25 pages/second\" that means the final rate you see on one of the status screens?  In other words, this is not a rate you see being steady while the fetch run is in the full swing (which could be a lot higher), but rather the final rate?\n", "Well, the status output on the fetch screen is a bit different in my new Fetcher, but I got that number by dividing the total number of pages fetched by the amount of time spent. At the beginning of the crawl I'm fetching upwards of 600/sec from 8 fetchers. The fetch rate is highly dependent on the spread of hosts, though - I'm getting better rates with a 2M URL fetch with 5000 max per host - about 60%-70% done after 3.5 hours.\n\n-Todd", "Hi Todd,\n\nCan you upload your work to JIRA now, so that we can review and merge it for 1.0?", "Moving this back to 1.0\n\nAre you close with your patch? As discussed in this thread we should just replace Fetcher With Fetcher2, change Crawl class and check that the tests pass. other issues we can deal within their own tickets.\n\nI can also help with this if you don't have the time.\n\n", "replaced fetcher with fetcher2", "Integrated in Nutch-trunk #742 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/742/])\n    ", "closing issues for released version"], "tasks": {"summary": "Consolidate code for Fetcher and Fetcher2", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Consolidate code for Fetcher and Fetcher2"}, {"question": "What is the main context?", "answer": "I'd like to consolidate a lot of the common code between Fetcher and Fetcher2.java.\n\nIt seems to me like there are the following differences:\n  - Fetcher relies on the Protocol to obey robots.txt and "}]}}
{"issue_id": "NUTCH-67", "project": "NUTCH", "title": "I  want  crawl the websites including news.yahoo.com,game.yahoo.com,blog.yahoo.com,etc!", "status": "Closed", "priority": "Major", "reporter": "zhangjin", "assignee": null, "created": "2005-07-04T12:33:13.000+0000", "updated": "2005-11-27T23:11:30.000+0000", "description": "how do  I  config them in the crawl-urlfilter.txt? I  config  them below,but  it is not successful.\n# The url filter file used by the crawl command.\n\n# Better for intranet crawling.\n# Be sure to change MY.DOMAIN.NAME to your domain name.\n\n# Each non-comment, non-blank line contains a regular expression\n# prefixed by '+' or '-'.  The first matching pattern in the file\n# determines whether a URL is included or ignored.  If no pattern\n# matches, the URL is ignored.\n\n# skip file:, ftp:, & mailto: urls\n-^(file|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n-\\.(gif|GIF|jpg|JPG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe)$\n\n# skip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n\n# accept hosts in MY.DOMAIN.NAME\n#+^http://([a-z0-9]*\\.)*MY.DOMAIN.NAME/\n+^http://([a-z0-9]*\\.)*yahoo.com/\n# skip everything else\n#-.\nbut It can not work, and can not crawl the  domain name (DOMAIN.NAME) inluding  news.yahoo.com,game.yahoo.com,blog.yahoo.com\nwhy?", "comments": ["Please ask such questions in the nutch-user mailing list."], "tasks": {"summary": "I  want  crawl the websites including news.yahoo.com,game.yahoo.com,blog.yahoo.com,etc!", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "I  want  crawl the websites including news.yahoo.com,game.yahoo.com,blog.yahoo.com,etc!"}, {"question": "What is the main context?", "answer": "how do  I  config them in the crawl-urlfilter.txt? I  config  them below,but  it is not successful.\n# The url filter file used by the crawl command.\n\n# Better for intranet crawling.\n# Be sure to chang"}]}}
{"issue_id": "NUTCH-670", "project": "NUTCH", "title": "feed plugin does not parse RSS2 enclosures", "status": "Closed", "priority": "Minor", "reporter": "Todd Lipcon", "assignee": null, "created": "2008-12-10T04:28:33.419+0000", "updated": "2019-10-13T22:35:42.708+0000", "description": "The feed parse in plugins/feed does not get count links found in RSS2 \"enclosure\" tags as Outlinks.\n\nIt's a pretty simple patch - SyndEntry has a getEnclosures call. I'll submit the patch tomorrow.", "comments": ["Turns out this is actually a bit trickier if I'm understanding the code correctly. It looks like the feed plugin outputs parse data for each of the feed URLs rather than for the feed itself. In the reduce phase of parsing, however, multiple parse datas for a single URL get reduced by simply picking the first. Therefore if an RSS feed has an enclosure link, but the HTML version of the post is also in the index *without* that link, then the link may be lost.\n\nI'm not entirely sure how to deal with this... any thoughts?", "That is indeed a problem for feed. But since the problem is unrelated to enclosures, we can commit your patch anyway.\n\nAnd deal with collision problem later on.", "Todd, do you have a patch for this. I know it was ages ago but it would be nice to put this old dog to bed :)", "Wow, blast from the past :) Unfortunately it looks like I never pushed this patch to my public github, and I moved to a new job 3 years ago, so the patch is lost into the ether. Sorry.", "Sure is. Not to worry. Thanks", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "feed plugin does not parse RSS2 enclosures", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "feed plugin does not parse RSS2 enclosures"}, {"question": "What is the main context?", "answer": "The feed parse in plugins/feed does not get count links found in RSS2 \"enclosure\" tags as Outlinks.\n\nIt's a pretty simple patch - SyndEntry has a getEnclosures call. I'll submit the patch tomorrow."}]}}
{"issue_id": "NUTCH-671", "project": "NUTCH", "title": "JSP errors in Nutch searcher webapp running with Tomcat 6", "status": "Closed", "priority": "Major", "reporter": "Edwin Chu", "assignee": null, "created": "2008-12-10T07:13:45.366+0000", "updated": "2009-04-10T12:29:02.168+0000", "description": "Tomcat throws an exception like the following when accessing search.jsp, cached.jsp, explain.jsp and anchors.jsp.\n\n{{org.apache.jasper.JasperException: /anchors.jsp(63,22) Attribute value  language + \"/include/header.html\" is quoted with \" which must be escaped when used within the value}}\n\nIt is caused by the stricter rules about attribute value quoting implemented in Tomcat 6.", "comments": ["+1", "Fixed in rev. 740324. Thank you!", "Integrated in Nutch-trunk #714 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/714/])\n     - JSP errors in Nutch searcher webapp.\n"], "tasks": {"summary": "JSP errors in Nutch searcher webapp running with Tomcat 6", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "JSP errors in Nutch searcher webapp running with Tomcat 6"}, {"question": "What is the main context?", "answer": "Tomcat throws an exception like the following when accessing search.jsp, cached.jsp, explain.jsp and anchors.jsp.\n\n{{org.apache.jasper.JasperException: /anchors.jsp(63,22) Attribute value  language + "}]}}
{"issue_id": "NUTCH-672", "project": "NUTCH", "title": "allow unit tests to be run from bin/nutch", "status": "Closed", "priority": "Minor", "reporter": "Todd Lipcon", "assignee": "Lewis John McGibbney", "created": "2008-12-11T03:54:26.277+0000", "updated": "2011-12-20T11:30:19.039+0000", "description": "In development it's handy to be able to run a single test case easily. You can do it with ant -Dtestcase=foo test, but that's slow since it still checks all the plugins for changes, rebuilds jars, etc.\n\nThis patch adds a command to bin/nutch to run a junit against what's already compiled. It's much faster than using ant. Recommended for use with nutch -core", "comments": ["Well, it's a convenient improvement for developers and shouldn't be too much work. I believe we should incorporate this.", "Updated patch which enables us to run JUint test from the commandline.\n\nThis has NOT been tested, merely updated against branch 1.4. I will test and comment in due course.\n\n", "OK having tried to get this working for sometime I am pretty puzzled, and think that it must be more than a class loading problem, however I cannot prove this.\n\nIt was my intention to run individual test classes by simulating exactly what the patch would do, however setting the classpath manually then running the junit command passing the TestFetcher class as an argument I get the following:\n\n{code}\nlewis@lewis-01:~/ASF/branch-1.4/runtime/local$ export CLASSPATH=$CLASSPATH:~/ASF/branch-1.4/src/test\nlewis@lewis-01:~/ASF/branch-1.4/runtime/local$ echo $CLASSPATH\n:/home/lewis/ASF/branch-1.4/src/test\nlewis@lewis-01:~/ASF/branch-1.4/runtime/local$ bin/nutch junit org.apache.nutch.fetcher.TestFetcher\nClass not found \"org.apache.nutch.fetcher.TestFetcher\"\n{code}\n\nI have been able to run test classes using the method described by Todd above, however I'm wondering if anyone else can  advise where I am going wrong with this issue? It's been a loooong day ;)", "This small patch makes JUnit testing so much easier. It's taken me a while to figure this one out, therefore thank you Julien for the recent comments. Possible to include in 1.4 release?", "One additional bit of commentary. This patch requires ant compile-test-core prior to ant runtime as the genrated test classes need to be copied to the runtime configurations.", "patch attachment for nutchgora. In my opinion, it's even more important that this gets integrated into nutchgora as it will greatly help when I get round to fixing the tests/classes. Thanks again Julien for the guidance.", "You are welcome. Looks fine to me +1 to commit\n\nWe could refine that later e.g. copy the test dir with \n\n{code:xml} \n    <copy todir=\"${runtime.local}/test\">\n      <fileset dir=\"${build.dir}/test\"/>\n    </copy>\n{code}\n\nonly if ${build.dir}/test is not empty. Otherwise we'd always get an empty test dir even for users who won't do any testing at all.\n\n\n\n\n", "Committed @ revision 1177269 in nutchgora\nCommitted @ revision 1177270 in trunk-1.4\n\n", "Integrated in Nutch-nutchgora #21 (See [https://builds.apache.org/job/Nutch-nutchgora/21/])\n    commit to address NUTCH-672 to update to changes.txt\n\nlewismc : http://svn.apache.org/viewvc/nutch/branches/nutchgora/viewvc/?view=rev&root=&revision=1177269\nFiles : \n* /nutch/branches/nutchgora/CHANGES.txt\n* /nutch/branches/nutchgora/build.xml\n* /nutch/branches/nutchgora/src/bin/nutch\n", "Integrated in Nutch-trunk #1619 (See [https://builds.apache.org/job/Nutch-trunk/1619/])\n    commit to address NUTCH-672 and update to changes.txt\n\nlewismc : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177270\nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/build.xml\n* /nutch/trunk/src/bin/nutch\n", "Bulk close of resolved issues of 1.4. bulkclose-1.4-20111220"], "tasks": {"summary": "allow unit tests to be run from bin/nutch", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "allow unit tests to be run from bin/nutch"}, {"question": "What is the main context?", "answer": "In development it's handy to be able to run a single test case easily. You can do it with ant -Dtestcase=foo test, but that's slow since it still checks all the plugins for changes, rebuilds jars, etc"}]}}
{"issue_id": "NUTCH-673", "project": "NUTCH", "title": "Upgrade the Carrot2 plug-in to release 3.0", "status": "Closed", "priority": "Minor", "reporter": "Sean Dean", "assignee": null, "created": "2008-12-15T23:01:23.302+0000", "updated": "2011-04-13T22:50:01.329+0000", "description": "Release 3.0 of the Carrot2 plug-in was released recently.\n\nWe currently have version 2.1 in the source tree and upgrading it to the latest version before 1.0-release might make sence.\n\nDetails on the release can be found here: http://project.carrot2.org/release-3.0-notes.html\n\nOne major change in requirements is for JDK 1.5 to be used, but this is also now required for Hadoop 0.19 so this wouldnt be the only reason for the switch.", "comments": ["Priority has been changed to \"minor\".", "Is any code change necessary for 3.0? ", "It seems that carrot2 API indeed changed. I am getting tons of compile errors. Could you help me figure out the necessary changes?", "Moving to 1.1 - needs more work.", "Hi guys. I'd be willing to proceed with this and upgrade to Carrot2 3.x line. The first issue I have encountered is Lucene incompatibilities between 2.9 (currently in Nutch) and 3.0 (currently in Carrot2). Any plans or reasons not to upgrade to Lucene 3.0? It's been with us for quite a while. If there are no objections, I can prepare a patch replacing Lucene 2.9 with Lucene 3.0 (as a separate issue).", "{quote}\nAny plans or reasons not to upgrade to Lucene 3.0?\n{quote}\n\nI see no reason to stick with 2.9\n\n{quote}\nI can prepare a patch replacing Lucene 2.9 with Lucene 3.0 (as a separate issue).\n{quote}\n\n+1", "+1 on both counts. Upgrade to Lucene 3.0 may involve more work than expected because of deprecated 2.x APIs that are no longer available in 3.0.", "O.K., I'll see into the complexity of upgrading to 3.0 first then. Filing a separate issue.", "- pushing this out per http://bit.ly/c7tBv9", "Folks: if you get time to put together a patch for 1.1 or feel that this should go into 1.1, please see:  http://bit.ly/c7tBv9 and comment in the next 48 hrs...", "Closing a legacy issue:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Upgrade the Carrot2 plug-in to release 3.0", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade the Carrot2 plug-in to release 3.0"}, {"question": "What is the main context?", "answer": "Release 3.0 of the Carrot2 plug-in was released recently.\n\nWe currently have version 2.1 in the source tree and upgrading it to the latest version before 1.0-release might make sence.\n\nDetails on the "}]}}
{"issue_id": "NUTCH-674", "project": "NUTCH", "title": "NutchBean doesn't check for searcher.dir existance.", "status": "Closed", "priority": "Major", "reporter": "Kuba Kończyk", "assignee": null, "created": "2008-12-18T13:12:18.370+0000", "updated": "2011-06-08T21:34:21.390+0000", "description": "If searcher.dir doesn't exists or it's not accessible, searcher will just continue and report that there is 0 hits found.It should throw an exception or log an error instead.As an starting point, there was a patch proposed some time ago on Nuch-dev: http://www.mail-archive.com/nutch-developers@lists.sourceforge.net/msg09422.html to solve this problem.\n", "comments": ["Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "NutchBean doesn't check for searcher.dir existance.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "NutchBean doesn't check for searcher.dir existance."}, {"question": "What is the main context?", "answer": "If searcher.dir doesn't exists or it's not accessible, searcher will just continue and report that there is 0 hits found.It should throw an exception or log an error instead.As an starting point, ther"}]}}
{"issue_id": "NUTCH-675", "project": "NUTCH", "title": "Reduce tasks do not report their status and are killed by jobtracker", "status": "Closed", "priority": "Major", "reporter": "sha feng", "assignee": null, "created": "2008-12-22T03:08:44.985+0000", "updated": "2009-01-25T11:39:48.712+0000", "description": "We choose Fetcher2 as our fetcher. Map tasks of Fetcher2 fetches about 2,000,000 urls, but at reduce stage, all reduce tasks can not report their status and be killed by jobtracker. Although we change mapred.task.timeout from 60,000 to 1,800,000, it does not work. So, who can tell us why? By the way, the version of Nutch we use is 0.9 and the version of Hadoop is 0.12. \nThanks for your help!", "comments": ["Sha Feng, could you please bring this up on the Nutch mailing list instead of JIRA?\nIt would also be good if you could upgrade your Nutch (including Hadoop) and see if it works then.  0.12 is VERY old version of Hadoop.\n", "According to Dennis Kubes's response on the mailing list, this has already been fixed.  Please try with the most recent versions of Nutch and Hadoop, Sha Feng.\n"], "tasks": {"summary": "Reduce tasks do not report their status and are killed by jobtracker", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Reduce tasks do not report their status and are killed by jobtracker"}, {"question": "What is the main context?", "answer": "We choose Fetcher2 as our fetcher. Map tasks of Fetcher2 fetches about 2,000,000 urls, but at reduce stage, all reduce tasks can not report their status and be killed by jobtracker. Although we change"}]}}
{"issue_id": "NUTCH-676", "project": "NUTCH", "title": "MapWritable is written inefficiently and confusingly", "status": "Closed", "priority": "Minor", "reporter": "Todd Lipcon", "assignee": "Dogacan Guney", "created": "2008-12-30T20:56:16.937+0000", "updated": "2009-04-10T12:29:07.862+0000", "description": "The MapWritable implemention in o.a.n.crawl is written confusingly - it maintains its own internal linked list which I think may have a bug somewhere (I'm getting an NPE in certain cases in the code, though it's hard to track down)\n\nCan anyone comment as to why MapWritable is written the way it is, rather than just using a HashMap or a LinkedHashMap if consistent ordering is important? I imagine that would improve performance.\n\nWhat about just using the Hadoop MapWritable? Obviously that would break some backwards compatibility but it may be a good idea at some point to reduce confusion (I didn't realize that Nutch had its own impl until a few minutes ago)", "comments": ["    NUTCH-676: Replace MapWritable implementation with the one from Hadoop, but retaining old class IDs from nutch\n    \n    Change to the test because the test assumes broken behavior in MapWritable\n", "Oops - please disregard above patch - it breaks backwards compatibility. Will send in a new one that is compatible later.", "Patch for the issue.\n\nBumps CrawlDatum version and starts using o.a.h.io.MapWritable in CrawlDatum. Compatibility\nis preserved by keeping nutch's MapWritable around and adding extra code for reading from nutch MapWritable if CrawlDatum version is 6.\n\nAlso changes CrawlDatum#toString as hadoop's MapWritable does not have a good toString method.", "New patch.\n\nIt seems we have to create a new MapWritable in every CrawlDatum#readFields call, otherwise\nwe run into a similar problem (in nutch's MapWritable).\n\nAlso, updates CrawlDatum#equals and CrawlDatum#hashCode as hadoop's MapWritable does not have an equals method (whereas nutch's MapWritable compares every entry with the other MapWritable).\n\nI am going to commit this soon if no objections. ", "Have you run some full crawls yet? I wrote pretty much this same patch but ran into a lot of issues when actually trying to run it in production. It seems like there's a bug in nutch's MapWritable where the classes of the keys are used for keys rather than the actual keys. I'll try to hunt down what I'm referring to and post  back later today.", "Hmm, I can't seem to find the bug I thought I remembered. Maybe the bug I ran into was actually due to the hashCode/equals issue.\n\nIf a crawl seems to go OK, I'm all for this.", "No, actually it is because we should create a new MapWritable in CrawlDatum#readFields. Because MapWritable \"remembers\" the id-class mappings it has already written, and does not rewrite them in a later #write call. So, if the order of keys you output in map is different than the order you receive keys in reduce, it fails. As MapWritable tries to map an id to a class but that id-class mapping is not read yet.\n\nSorry if the description is not very clear. ", "Patch committed as of rev. 736385. ", "Integrated in Nutch-trunk #701 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/701/])\n     - MapWritable is written inefficiently and confusingly.\n", "Integrated in Nutch-trunk #722 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/722/])\n    NUTCH-683 -  broke CrawlDbMerger\n"], "tasks": {"summary": "MapWritable is written inefficiently and confusingly", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "MapWritable is written inefficiently and confusingly"}, {"question": "What is the main context?", "answer": "The MapWritable implemention in o.a.n.crawl is written confusingly - it maintains its own internal linked list which I think may have a bug somewhere (I'm getting an NPE in certain cases in the code, "}]}}
{"issue_id": "NUTCH-677", "project": "NUTCH", "title": "Segment merge filering based on segment content", "status": "Closed", "priority": "Major", "reporter": "Marcin Okraszewski", "assignee": "Chris A. Mattmann", "created": "2009-01-08T22:03:42.863+0000", "updated": "2011-04-01T15:07:22.208+0000", "description": "I needed a segment filtering based on meta data detected during parse phase. Unfortunately current URL based filtering does not allow for this. So I have created a new SegmentMergeFilter extension which receives segment entry which is being merged and decides if it should be included or not. Even though I needed only ParseData for my purpose I have done it a bit more general purpose, so the filter receives all merged data.\n\nThe attached patch is for version 0.9 which I use. Unfortunately I didn't have time to check how it fits to trunk version. Sorry :(", "comments": ["The patch for 0.9", "The filter interface (referred by the patch).", "Merge filter aggregation which hides extension point, etc. It is referred by the patch.", "Moving this issue to 1.1.", "The patch ported to Nutch 1.0. The Java files remain unchanged, only patch has changed.", "Marcin - could you please include the Apache license on top of the code, like other Nutch classes do?\n", "Added Apache License.", "Added Apache license header.", "Sorry, I didn't notice the request for the license header. I've just uploaded files with the header.", "- pushing this out per http://bit.ly/c7tBv9", "Hi Marcin,\n\nI applied your patch, and was unit testing it, all ready to commit, when I ran into this:\n\n{noformat}\n    [junit] Test org.apache.nutch.segment.TestSegmentMerger FAILED\n    [junit] Running org.apache.nutch.util.TestEncodingDetector\n    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 2.408 sec\n    [junit] Running org.apache.nutch.util.TestGZIPUtils\n    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 2.521 sec\n    [junit] Running org.apache.nutch.util.TestNodeWalker\n    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.593 sec\n    [junit] Running org.apache.nutch.util.TestPrefixStringMatcher\n    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.452 sec\n    [junit] Running org.apache.nutch.util.TestStringUtil\n    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.076 sec\n    [junit] Running org.apache.nutch.util.TestSuffixStringMatcher\n    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.321 sec\n    [junit] Running org.apache.nutch.util.TestURLUtil\n    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 2.009 sec\n\nBUILD FAILED\n/Users/mattmann/src/nutch/build.xml:258: Tests failed!\n\nTotal time: 8 minutes 44 seconds\n[chipotle:~/src/nutch] mattmann%\n{noformat}\n\nThe root cause of the SegmentMerger test error is: (from build/test/TEST-org.apache.nutch.segment.TestSegmentMerger.txt):\n\n{noformat}\n432\n2010-07-14 13:45:33,085 INFO  mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(276)) - file:/tmp/hadoop-mattmann/merge-1279140109299/seg1/parse_text/part-00000/data:0+3355\n4432\n2010-07-14 13:45:33,445 INFO  mapred.MapTask (MapTask.java:flush(1115)) - Starting flush of map output\n2010-07-14 13:45:35,101 INFO  mapred.MapTask (MapTask.java:sortAndSpill(1295)) - Finished spill 2\n2010-07-14 13:45:35,107 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(256)) - job_local_0001\norg.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/jobcache/job_local_0001/attempt_local_0001_m_000000_0/output/spill0.out in any of the configured\n local directories\n        at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:389)\n        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:138)\n        at org.apache.hadoop.mapred.MapOutputFile.getSpillFile(MapOutputFile.java:94)\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:1443)\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1154)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:359)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)\n2010-07-14 13:45:35,879 INFO  mapred.JobClient (JobClient.java:monitorAndPrintJob(1343)) - Job complete: job_local_0001\n2010-07-14 13:45:35,883 INFO  mapred.JobClient (Counters.java:log(514)) - Counters: 9\n2010-07-14 13:45:35,884 INFO  mapred.JobClient (Counters.java:log(516)) -   FileSystemCounters\n2010-07-14 13:45:35,884 INFO  mapred.JobClient (Counters.java:log(518)) -     FILE_BYTES_READ=68360507\n2010-07-14 13:45:35,885 INFO  mapred.JobClient (Counters.java:log(518)) -     FILE_BYTES_WRITTEN=229824559\n2010-07-14 13:45:35,885 INFO  mapred.JobClient (Counters.java:log(516)) -   Map-Reduce Framework\n2010-07-14 13:45:35,885 INFO  mapred.JobClient (Counters.java:log(518)) -     Combine output records=0\n2010-07-14 13:45:35,886 INFO  mapred.JobClient (Counters.java:log(518)) -     Map input records=703319\n2010-07-14 13:45:35,886 INFO  mapred.JobClient (Counters.java:log(518)) -     Spilled Records=524287\n2010-07-14 13:45:35,887 INFO  mapred.JobClient (Counters.java:log(518)) -     Map output bytes=42791349\n2010-07-14 13:45:35,888 INFO  mapred.JobClient (Counters.java:log(518)) -     Map input bytes=0\n2010-07-14 13:45:35,888 INFO  mapred.JobClient (Counters.java:log(518)) -     Map output records=703319\n2010-07-14 13:45:35,889 INFO  mapred.JobClient (Counters.java:log(518)) -     Combine input records=0\n------------- ---------------- ---------------\n------------- Standard Error -----------------\nCreating large segment 1...\n - done: 1677722 records.\nCreating large segment 2...\n - done: 1677722 records.\n------------- ---------------- ---------------\n\nTestcase: testLargeMerge took 227.804 sec\n        Caused an ERROR\nJob failed!\njava.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1252)\n        at org.apache.nutch.segment.SegmentMerger.merge(SegmentMerger.java:639)\n        at org.apache.nutch.segment.TestSegmentMerger.testLargeMerge(TestSegmentMerger.java:87)\n{noformat}\n\nAny ideas? I'd be happy to commit this, provided we can get it to past regression....\n\nCheers,\nChris\n \n", "Hrm, OK, I figured it out. Heh. Umm there aren't any segment merger extensions that are loaded, I believe? I think you need to contribute a segment-merger plugin here, can you contribute an implementation?", "- patch co-locates all of your attachments, patched against 2.0 trunk. I'll use this as the basis for committing, once I get an implementation of a segment-merger plugin. Any contributions welcome there...", "Odd, OK, the patch now appears to work? I wonder if I had some temp directory that was messing stuff up before. Looking at the patch, it doesn't matter if any mergeFilters haven't been defined yet (which was my original thought as to why it wasn't working). If there aren't any mergeFilters defined, SegmentMergeFilters returns true which causes SegmentMerger to _not_ break out of the function (which was its old behavior) anyways, so this patch works great. Thanks! I'll commit it to trunk, and then backport to branch-1.2 and nutchbase shortly...", "- Okey dokey. I applied this patch to the current trunk (r978988) and backported it to the 1.2 branch (r978989). I'm hesitant to mark it for 2.0 though since when I tried to apply it to the Nutchbase branch, I noticed SegmentMerger.java is gone, so not sure this patch is applicable there, or if it is, then I don't know the Nutch 2.0 equivalent of SegmentMerger.\n\nAnyhoo, thanks very much for the patch, Marcin! The functionality will be there in the 1.2 release for sure. If one of the Nutchbasers wants to port to 2.0, by all means (but please don't reopen the issue, file a new one). Thanks!", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Segment merge filering based on segment content", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Segment merge filering based on segment content"}, {"question": "What is the main context?", "answer": "I needed a segment filtering based on meta data detected during parse phase. Unfortunately current URL based filtering does not allow for this. So I have created a new SegmentMergeFilter extension whi"}]}}
{"issue_id": "NUTCH-678", "project": "NUTCH", "title": "Hadoop 0.19 requires an update of jets3t", "status": "Closed", "priority": "Critical", "reporter": "Julien Nioche", "assignee": "Dogacan Guney", "created": "2009-01-14T21:23:00.056+0000", "updated": "2009-04-10T12:29:02.654+0000", "description": "jets3t-0.6.0.jar is currently in the lib directory. Hadoop 0.19 relies on the version 0.6.1\n\nI am getting java.lang.NoSuchMethodError: org.jets3t.service.S3Service.moveObject(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Lorg/jets3t/service/model/S3Object;Z)Ljava/util/Map;\n\tat org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.rename(Jets3tNativeFileSystemStore.java:228)\n\nwhen using distcp with jets3t-0.6.0.jar. I haven't tried with 0.6.1 yet but I suspect this is the cause. It won't hurt to upgrade jets3t anyway", "comments": ["I agree that it won't hurt to upgrade jets3t, but could you confirm that old version is the problem? ", "I confirm. Upgrading to 0.6.1 fixed the problem and I am not getting this exception any longer", "OK, fixed as of rev 735748.\n\nThanks!", "Integrated in Nutch-trunk #699 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/699/])\n     - Hadoop 0.19 requires an update of jets3t (julien nioche)\n"], "tasks": {"summary": "Hadoop 0.19 requires an update of jets3t", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Hadoop 0.19 requires an update of jets3t"}, {"question": "What is the main context?", "answer": "jets3t-0.6.0.jar is currently in the lib directory. Hadoop 0.19 relies on the version 0.6.1\n\nI am getting java.lang.NoSuchMethodError: org.jets3t.service.S3Service.moveObject(Ljava/lang/String;Ljava/l"}]}}
{"issue_id": "NUTCH-679", "project": "NUTCH", "title": "Fetcher2 implementing Tool", "status": "Closed", "priority": "Minor", "reporter": "Julien Nioche", "assignee": "Andrzej Bialecki", "created": "2009-01-15T13:33:35.392+0000", "updated": "2009-10-10T04:45:46.551+0000", "description": "The patch attached makes Fetcher2 implement Tool. As a result we should be able to override parameters on the command line e.g. \nbin/nutch fetch2 -Dfetcher.server.min.delay=1.0 -Dmapred.reduce.tasks=4 segments/20090115072836\ninstead of having to modify the *-site.xml files in conf/\n", "comments": ["Patch which makes Fetcher2 implement Tool interface", "Looks simple enough. I am going to commit it soon if no objections.\n\nBtw, please use 2-space tabs otherwise it messes up patches :)", "I'm not sure, but committing this may mess up Todd's work on merging Fetcher and Fetcher2.\n", "I can send a modified version of it once Todd has finished working on the Fetchers. Same for https://issues.apache.org/jira/browse/NUTCH-658  ", "Updated version of the patch", "Fixed in rev. 823600. Thanks!", "Integrated in Nutch-trunk #959 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/959/])\n     Fetcher2 implementing Tool.\n"], "tasks": {"summary": "Fetcher2 implementing Tool", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fetcher2 implementing Tool"}, {"question": "What is the main context?", "answer": "The patch attached makes Fetcher2 implement Tool. As a result we should be able to override parameters on the command line e.g. \nbin/nutch fetch2 -Dfetcher.server.min.delay=1.0 -Dmapred.reduce.tasks=4"}]}}
{"issue_id": "NUTCH-68", "project": "NUTCH", "title": "A tool to generate arbitrary fetchlists", "status": "Closed", "priority": "Minor", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2005-07-05T17:04:43.000+0000", "updated": "2007-01-17T19:56:38.326+0000", "description": "This is a tool to generate arbitrary fetchlists out of plain-text URL lists. I found it useful quite often, e.g. when I had to fetch certain specific pages without adding them to DB, or for testing purposes.", "comments": ["Works like a charm. I use this to do almost \"live\" fetches of submitted links.  How hard would it be to port this to .8?  it would almost be interesting to show how to do so on the wiki so people can have an idea how old stuff moves to mapreduce as i'm still wrapping my brain around the process :)", "Ported to use map-reduce, and added to trunk in rev. 497141."], "tasks": {"summary": "A tool to generate arbitrary fetchlists", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "A tool to generate arbitrary fetchlists"}, {"question": "What is the main context?", "answer": "This is a tool to generate arbitrary fetchlists out of plain-text URL lists. I found it useful quite often, e.g. when I had to fetch certain specific pages without adding them to DB, or for testing pu"}]}}
{"issue_id": "NUTCH-680", "project": "NUTCH", "title": "Update external jars to latest versions", "status": "Closed", "priority": "Minor", "reporter": "Dogacan Guney", "assignee": "Dogacan Guney", "created": "2009-01-19T14:00:18.759+0000", "updated": "2009-04-10T12:29:04.259+0000", "description": "This issue will be used to update external libraries nutch uses. \n\nThese are the libraries that are outdated (upon a quick glance):\n\nnekohtml (1.9.9)\nlucene-highlighter (2.4.0)\njdom (1.1)\ncarrot2 - as mentioned in another issue\njets3t - above\nicu4j (4.0.1)\njakarta-oro (2.0.8)\n\nWe should probably update tika to whatever the latest is as well before 1.0.\n\n\nPlease add ones  I missed in comments.\n\nAlso what exactly is pmd-ext? There is an extra jakarta-oro and jaxen there.....", "comments": ["I have committed updates (as of rev. 737325) to\n\nnekohtml (1.9.11)\nlucene-highlighter (2.4.0)\nicu4j (4.0.1)\njakarta-oro (2.0.8)\n\nI would also update jdom (in lib-xml) but libraries there changed too much, it seems. Library saxpath now apparently is a part of jaxen and I can't figure out what is jaxen-jdom or xercesImpl. Anyway, I will take another look and update those libraries as well.", "Integrated in Nutch-trunk #704 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/704/])\n     - Update external jars to latest versions\n\nUpdates:\n\nnekohtml\nlucene-highlighter\nicu4j\njakarta-oro\n", "Since no objections, I am removing pmd-ext directory for now. We should\nadd it back properly after 1.0. This also shaves a nice 2.2M from nutch's size.", "Integrated in Nutch-trunk #707 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/707/])\n     - Remove pmd-ext jars for now\n", "Updating jdom and jaxen causes parse-oo tests to fail. So I am closing this issue."], "tasks": {"summary": "Update external jars to latest versions", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Update external jars to latest versions"}, {"question": "What is the main context?", "answer": "This issue will be used to update external libraries nutch uses. \n\nThese are the libraries that are outdated (upon a quick glance):\n\nnekohtml (1.9.9)\nlucene-highlighter (2.4.0)\njdom (1.1)\ncarrot2 - as"}]}}
{"issue_id": "NUTCH-681", "project": "NUTCH", "title": "parse-mp3 compilation problem", "status": "Closed", "priority": "Major", "reporter": "Wildan Maulana", "assignee": "Dogacan Guney", "created": "2009-01-20T08:17:09.547+0000", "updated": "2009-04-10T12:29:04.503+0000", "description": "Due to API changes, the MP3 parser (which is not compiled by default due to licensing problem) doesn't compile anymore.\n\ncompile:\n     [echo] Compiling plugin: parse-mp3\n    [javac] Compiling 2 source files to /home/wildan/jobstuff/LIPI/Ngoprek/nutch/build/parse-mp3/classes\n    [javac] /home/wildan/jobstuff/LIPI/Ngoprek/nutch/src/plugin/parse-mp3/src/java/org/apache/nutch/parse/mp3/MP3Parser.java:53: org.apache.nutch.parse.mp3.MP3Parser is not abstract and does not override abstract method getParse(org.apache.nutch.protocol.Content) in org.apache.nutch.parse.Parser\n    [javac] public class MP3Parser implements Parser {\n    [javac]        ^\n    [javac] /home/wildan/jobstuff/LIPI/Ngoprek/nutch/src/plugin/parse-mp3/src/java/org/apache/nutch/parse/mp3/MP3Parser.java:58: getParse(org.apache.nutch.protocol.Content) in org.apache.nutch.parse.mp3.MP3Parser cannot implement getParse(org.apache.nutch.protocol.Content) in org.apache.nutch.parse.Parser; attempting to use incompatible return type\n    [javac] found   : org.apache.nutch.parse.Parse\n    [javac] required: org.apache.nutch.parse.ParseResult\n    [javac]   public Parse getParse(Content content) {\n    [javac]                ^\n    [javac] /home/wildan/jobstuff/LIPI/Ngoprek/nutch/src/plugin/parse-mp3/src/java/org/apache/nutch/parse/mp3/MetadataCollector.java:54: cannot find symbol\n    [javac] symbol  : constructor Outlink(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)\n    [javac] location: class org.apache.nutch.parse.Outlink\n    [javac]       links.add(new Outlink(value, \"\", this.conf));\n    [javac]                 ^\n    [javac] Note: /home/wildan/jobstuff/LIPI/Ngoprek/nutch/src/plugin/parse-mp3/src/java/org/apache/nutch/parse/mp3/MetadataCollector.java uses unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n    [javac] 3 errors\n\nBUILD FAILED\n/home/wildan/jobstuff/LIPI/Ngoprek/nutch/build.xml:113: The following error occurred while executing this line:\n/home/wildan/jobstuff/LIPI/Ngoprek/nutch/src/plugin/build.xml:55: The following error occurred while executing this line:\n/home/wildan/jobstuff/LIPI/Ngoprek/nutch/src/plugin/build-plugin.xml:111: Compile failed; see the compiler error output for details.\n", "comments": ["please re-check the patch that i have submitted above", "i've been test this patch, and it's works ..", "\"Resolve Issue\" is for when issue is resolved (i.e. when relevant commits are made to nutch trunk) not when\nthe patch submitted works.", "Btw, the patch itself is fine :)\n\nCommitted in rev. 736307 with some cleanup. I also updated parse-mp3's unit test.", "Sorry .., new people here :)\n\nThanks Güney ! ", "Integrated in Nutch-trunk #701 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/701/])\n     - parse-mp3 compilation problem. Patch by Wildan Maulana.\n"], "tasks": {"summary": "parse-mp3 compilation problem", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "parse-mp3 compilation problem"}, {"question": "What is the main context?", "answer": "Due to API changes, the MP3 parser (which is not compiled by default due to licensing problem) doesn't compile anymore.\n\ncompile:\n     [echo] Compiling plugin: parse-mp3\n    [javac] Compiling 2 source"}]}}
{"issue_id": "NUTCH-682", "project": "NUTCH", "title": "SOLR indexer does not set boost on the document", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2009-01-29T18:53:18.966+0000", "updated": "2009-04-10T12:29:02.324+0000", "description": "the method org.apache.nutch.indexer.solr.SolrWriter.write(NutchDocument doc) should call the method setDocumentBoost as illustrated below :  \n\nfor(final Entry<String, List<String>> e : doc) {\n      for (final String val : e.getValue()) {\n        inputDoc.addField(e.getKey(), val);\n      }\n    }\n    inputDoc.setDocumentBoost(doc.getScore());\n\nas done by the LuceneWriter.", "comments": ["This is obviously correct. Can't believe I missed that :)\n\nCommitted in rev. 738970.\n\nThanks!", "Integrated in Nutch-trunk #709 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/709/])\n     - SOLR indexer does not set boost on the document. Patch by julien nioche\n"], "tasks": {"summary": "SOLR indexer does not set boost on the document", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "SOLR indexer does not set boost on the document"}, {"question": "What is the main context?", "answer": "the method org.apache.nutch.indexer.solr.SolrWriter.write(NutchDocument doc) should call the method setDocumentBoost as illustrated below :  \n\nfor(final Entry<String, List<String>> e : doc) {\n      fo"}]}}
{"issue_id": "NUTCH-683", "project": "NUTCH", "title": "NUTCH-676 broke CrawlDbMerger", "status": "Closed", "priority": "Minor", "reporter": "Dogacan Guney", "assignee": "Dogacan Guney", "created": "2009-01-29T19:45:22.397+0000", "updated": "2009-04-10T12:29:07.094+0000", "description": "Switch to hadoop's MapWritable broke CrawlDbMerger. Part of the reason is that we reuse the same MapWritable instance during reduce\nwhich apparently is a big no-no for hadoop's MapWritable. Also, hadoop's MapWritable#putAll doesn't work (I think.... see HADOOP-5142),\nso we should also work around that.", "comments": ["Patch for issue", "+1.", "Fixed in rev. 743277.", "Integrated in Nutch-trunk #722 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/722/])\n     - NUTCH-676 broke CrawlDbMerger\n"], "tasks": {"summary": "NUTCH-676 broke CrawlDbMerger", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "NUTCH-676 broke CrawlDbMerger"}, {"question": "What is the main context?", "answer": "Switch to hadoop's MapWritable broke CrawlDbMerger. Part of the reason is that we reuse the same MapWritable instance during reduce\nwhich apparently is a big no-no for hadoop's MapWritable. Also, hado"}]}}
{"issue_id": "NUTCH-684", "project": "NUTCH", "title": "Dedup support for Solr", "status": "Closed", "priority": "Major", "reporter": "Dogacan Guney", "assignee": "Dogacan Guney", "created": "2009-01-30T16:34:10.062+0000", "updated": "2013-09-25T10:29:52.940+0000", "description": "After NUTCH-442, nutch now can index to both solr and lucene. However, duplicate deletion feature (based on digests) is only available in lucene. It should also be available for solr.", "comments": ["First version of a solr dedup feature. I haven't yet tested this patch much yet, so if you use it it may blow your computer.\n\nI first thought about trying to make duplicate deletion a generic class with solr and lucene backends. However, lucene and solr are so different in this regard that, it was much easier to just\nwrite a new solr dedup class.\n\nSince urls are assumed to be unique in solr, SolrDeleteDuplicates only deletes urls with the same digest based on score. If two urls have the same digest and the same score then the one with the later timestamp stays.", "This patch works for me too.", "patch for bin/nutch\n\nso we can write\n{{bin/nutch solrdedup <solrurl>}}", "Produce a little more log output for SolrDeleteDuplicates", "A few comments to this patch (and to other closely related classes in o.a.n.i.solr):\n\n* we need javadocs in this patch - both class-level and for public methods. The class-level javadoc should contain pseudo-code to illustrate the selection process (see o.a.n.i.DeleteDuplicates for an example).\n\n* there is a silent assumption that Solr schema uses \"id\" field as unique key, and that this field contains the URL of the document. First, shouldn't this be \"url\" field? Because as far as I can see the field name \"id\" is not used anywhere in SolrIndexer/SolrWriter - please correct me if I missed something. At least this assumption should be spelled out in javadocs, both on the indexing side and on the dedup side. (Actually, we should have added an example of the minimum required Solr schema when the original Nutch/Solr integration was committed)\n\n* field names should be constants and not magic literals, they should come either from o.a.n.metadata.Nutch or be defined in SolrConstants.\n\n* SolrServer.deleteById() creates and sends UpdateRequest containing just this single id. This is inefficient, especially in our case where the number of deletes may be significant. Perhaps this patch works sufficiently well for now, but it should be improved (either here or in a separate issue) by using a single UpdateRequest per reduce task, and calling SolrServer.request(UpdateRequest) with the accumulated id-s.", "bq. there is a silent assumption that Solr schema uses \"id\" field as unique key, and that this field contains the URL of the document. First, shouldn't this be \"url\" field? Because as far as I can see the field name \"id\" is not used anywhere in SolrIndexer/SolrWriter - please correct me if I missed something. At least this assumption should be spelled out in javadocs, both on the indexing side and on the dedup side. (Actually, we should have added an example of the minimum required Solr schema when the original Nutch/Solr integration was committed)\n\n\"id\" field defined in schema.xml (NUTCH-442)", "I wasn't thinking of putting this in for 1.0, but if people want this feature I will ready it for 1.0\n\nbq.    *  there is a silent assumption that Solr schema uses \"id\" field as unique key, and that this field contains the URL of the document. First, shouldn't this be \"url\" field? Because as far as I can see the field name \"id\" is not used anywhere in SolrIndexer/SolrWriter - please correct me if I missed something. At least this assumption should be spelled out in javadocs, both on the indexing side and on the dedup side. (Actually, we should have added an example of the minimum required Solr schema when the original Nutch/Solr integration was committed)\n\n    * field names should be constants and not magic literals, they should come either from o.a.n.metadata.Nutch or be defined in SolrConstants.\n\nThis is something I have been thinking for a while. My assumption was that you didn't have to use \"url\" field in\nyour solr server as the unique field so I added an extra \"id\" field (which in NUTCH-442's schema.xml is copied from \"url\"). But I am no longer sure the extra cost of a field is worth the flexibility.\n\nI agree with you that we should have an solr schema xml somewhere in our codebase that is officially blessed. I guess NUTCH-442's schema is a good starting point for that but I am open to suggestions. I will create a new issue for it.\n\nbq.  SolrServer.deleteById() creates and sends UpdateRequest containing just this single id. This is inefficient, especially in our case where the number of deletes may be significant. Perhaps this patch works sufficiently well for now, but it should be improved (either here or in a separate issue) by using a single UpdateRequest per reduce task, and calling SolrServer.request(UpdateRequest) with the accumulated id-s.\n\nGood point. I will send an improved patch.\n", "Oh, about javadocs. I agree with you on class-level javadocs, but do we really need javadocs for public methods? They are rather straightforward stuff; map, reduce, etc....", "IMHO it would be good to have this functionality in 1.0, and the patch is very close.\n\nOk, how about the following:\n\n* we make the name of the unique field configurable, and provide a default value in nutch-default.xml, which is consistent with the one provided in the example schema.xml (yes, we should add an example schema, and the one in NUTCH-442 looks good enough).\n\n* the UpdateRequest improvement: it's up to you whether to do it here or separately. It would be certainly a nice to have.\n\n* javadocs: yeah, map/reduce/configure are obvious, and good javadocs exist in superclasses. Same of bean-like getters/setters. Other public methods should be documented, so that in half a year we still know what they are for and we understand the arguments they expect.", "New version:\n\n* Added field names to SolrConstants. Although only SolrDeleteDuplicates uses these.\n\n* Also, I didn't implement a conf option for unique key. It is a very good idea but it also requires changes to SolrIndexer and other classes and I didn't want to do it this late in the release cycle.\n\n* #reduce now uses a UpdateRequest so that deletes are queued and sent to server in batches.\n\n* Added javadoc\n\n* Updated bin/nutch with new command solrdedup", "Just found this issue from Sami's post on Lucid blog. Are you guys aware of the Deduplication feature in Solr trunk?\n\nhttp://wiki.apache.org/solr/Deduplication and SOLR-799", "Yes, I'm aware of this functionality. At this point however I thought that it would only complicate things, because users would have to install Nutch classes on Solr in order to use Signature implementations that we use. This is of course an open issue that we should investigate after 1.0 release.", "Fixed as of rev. 751774.", "Integrated in Nutch-trunk #748 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/748/])\n     - Dedup support for Solr\n"], "tasks": {"summary": "Dedup support for Solr", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Dedup support for Solr"}, {"question": "What is the main context?", "answer": "After NUTCH-442, nutch now can index to both solr and lucene. However, duplicate deletion feature (based on digests) is only available in lucene. It should also be available for solr."}]}}
{"issue_id": "NUTCH-685", "project": "NUTCH", "title": "Content-level redirect status lost in ParseSegment", "status": "Open", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": null, "created": "2009-02-06T10:11:48.030+0000", "updated": "2025-07-09T20:25:59.635+0000", "description": "When Fetcher runs in parsing mode, content-level redirects (HTML meta tag \"Refresh\") are properly discovered and recorded in crawl_fetch under source URL and target URL. If Fetcher runs in non-parsing mode, and ParseSegment is run as a separate step, the content-level redirection data is used only to add the new (target) URL, but the status of the original URL is not reset to indicate a redirect. Consequently, status of the original URL will be different depending on the way you run Fetcher, whereas it should be the same.", "comments": ["Confirmed (for 1.x): content-level redirects (aka. meta refresh) do not result in a redirect status in CrawlDb (db_redir_perm or db_redir_temp).\n\nIn current trunk/1.x, they are even not recorded if Fetcher is parsing (fetcher.parse==true):\n* set status \"redir_perm\" was introduced with r492525 in Fetcher.java:\n{code}\n case ProtocolStatus.SUCCESS:        // got a page\n   pstatus = output(url, datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS);\n   if (pstatus != null && pstatus.isSuccess() &&\n       pstatus.getMinorCode() == ParseStatus.SUCCESS_REDIRECT) {\n...\n      // record that we were redirected\n      output(url, datum, null, status, CrawlDatum.STATUS_FETCH_REDIR_PERM);\n{code}\n* but lost with r593151 (since release 1.0 / NUTCH-547)\n\nThe problem is that pages containing a content-level redirect are indexed as successfully fetched pages. But usually they contain only a note like \"You will be redirected in 10 seconds. If not click here.\" Possible solutions to exclude those pages (for 1.x):\n# mark meta-refresh redirects as such (the status is arguable):\n** re-introduce that Fetcher emits a CrawlDatum with redirect status\n** try this also for ParseOutput (if fetcher.parse==false): principally possible, but with the price of lost information. If we emit a redirect CrawlDatum into crawl_parse it overwrites that from crawl_fetch. Status is then redirect, but we loose the fetch time and meta data. The original fetch datum is not available while parsing segments.\n# skip and delete content-level redirects during indexing (similar to robots=noindex)\n* check for {{parseData.getStatus().getMinorCode() == ParseStatus.SUCCESS_REDIRECT}} in IndexerMapReduce\n* additionally, (it may not harm!) try to add the metarefresh to CrawlDatum's meta"], "tasks": {"summary": "Content-level redirect status lost in ParseSegment", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Content-level redirect status lost in ParseSegment"}, {"question": "What is the main context?", "answer": "When Fetcher runs in parsing mode, content-level redirects (HTML meta tag \"Refresh\") are properly discovered and recorded in crawl_fetch under source URL and target URL. If Fetcher runs in non-parsing"}]}}
{"issue_id": "NUTCH-686", "project": "NUTCH", "title": "Russian Analysis Plugin", "status": "Closed", "priority": "Major", "reporter": "OpenTeam.ru", "assignee": null, "created": "2009-02-10T05:19:29.493+0000", "updated": "2009-02-10T05:29:52.193+0000", "description": "This patch creates Russian Analysis plugin", "comments": ["cd nutch\npatch -p0  < analysis-ru.patch", "NUTCH-666 already do this"], "tasks": {"summary": "Russian Analysis Plugin", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Russian Analysis Plugin"}, {"question": "What is the main context?", "answer": "This patch creates Russian Analysis plugin"}]}}
{"issue_id": "NUTCH-687", "project": "NUTCH", "title": "Add RAT", "status": "Closed", "priority": "Minor", "reporter": "Sami Siren", "assignee": "Sami Siren", "created": "2009-02-17T13:59:42.494+0000", "updated": "2009-04-10T12:29:07.063+0000", "description": "Add apache rat so we can easily see the situation with required headers", "comments": ["committed", "Integrated in Nutch-trunk #729 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/729/])\n     add RAT, also check plugins\n add RAT\n", "closing issues for released version"], "tasks": {"summary": "Add RAT", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add RAT"}, {"question": "What is the main context?", "answer": "Add apache rat so we can easily see the situation with required headers"}]}}
{"issue_id": "NUTCH-688", "project": "NUTCH", "title": "Fix missing/wrong headers in source files", "status": "Closed", "priority": "Blocker", "reporter": "Sami Siren", "assignee": "Sami Siren", "created": "2009-02-17T14:03:06.942+0000", "updated": "2009-04-10T12:29:03.207+0000", "description": null, "comments": ["Buildfile: build.xml\n\nrat-sources-typedef:\nTrying to override old definition of task javadoc\n\nrat-sources:\n[rat:report] \n[rat:report] *****************************************************\n[rat:report] Summary\n[rat:report] -------\n[rat:report] Notes: 0\n[rat:report] Binaries: 0\n[rat:report] Archives: 0\n[rat:report] Standards: 242\n[rat:report] \n[rat:report] Apache Licensed: 175\n[rat:report] Generated Documents: 8\n[rat:report] \n[rat:report] JavaDocs are generated and so license header is optional\n[rat:report] Generated files do not required license headers\n[rat:report] \n[rat:report] 59 Unknown Licenses\n[rat:report] \n[rat:report] *******************************\n[rat:report] \n[rat:report] Unapproved licenses:\n[rat:report] \n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/package.html\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/NutchWritable.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/package.html\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/fetcher/package.html\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/IndexerMapReduce.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/IndexerOutputFormat.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/NutchDocument.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/NutchIndexWriter.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/NutchIndexWriterFactory.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/AnchorFields.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/BasicFields.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/CustomFields.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldFilter.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldFilters.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldIndexer.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldType.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldWritable.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/Fields.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldsWritable.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/lucene/LuceneConstants.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/lucene/LuceneWriter.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/package.html\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/solr/SolrConstants.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/solr/SolrIndexer.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/solr/SolrWriter.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/metadata/package.html\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/package.html\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/LinkDatum.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/LinkDumper.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/LinkRank.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/LoopReader.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/Loops.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/Node.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/NodeDumper.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/NodeReader.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/ScoreUpdater.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/WebGraph.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/DistributedSearchBean.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/DistributedSegmentBean.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/RPCSearchBean.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/RPCSegmentBean.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/SearchBean.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/SegmentBean.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/package.html\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/RequestUtils.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/ResponseWriter.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/ResponseWriters.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/SearchResults.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/SearchServlet.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/segment/ContentAsTextInputFormat.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/ResolveUrls.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/SearchLoadTester.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/compat/ReprUrlFixer.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/EncodingDetector.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/FSUtils.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/GenericWritableConfigurable.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/NodeWalker.java\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/domain/package.html\n[rat:report]   /home/sam/workspace/nutch-trunk-eu/src/java/overview.html\n[rat:report] \n[rat:report] *******************************\n[rat:report] \n[rat:report] Archives (+ indicates readable, $ unreadable): \n[rat:report] \n[rat:report]  \n[rat:report] *****************************************************\n[rat:report]   Files with Apache License headers will be marked AL\n[rat:report]   Binary files (which do not require AL headers) will be marked B\n[rat:report]   Compressed archives will be marked A\n[rat:report]   Notices, licenses etc will be marked N\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/AnalyzerFactory.java\n[rat:report]   GEN   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/CharStream.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/CommonGrams.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/FastCharStream.java\n[rat:report]   GEN   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/NutchAnalysis.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/NutchAnalysis.jj\n[rat:report]   GEN   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/NutchAnalysisConstants.java\n[rat:report]   GEN   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/NutchAnalysisTokenManager.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/NutchAnalyzer.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/NutchDocumentAnalyzer.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/NutchDocumentTokenizer.java\n[rat:report]   GEN   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/ParseException.java\n[rat:report]   GEN   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/Token.java\n[rat:report]   GEN   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/TokenManager.java\n[rat:report]   GEN   /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/TokenMgrError.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/package.html\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/clustering/HitsCluster.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/clustering/OnlineClusterer.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/clustering/OnlineClustererFactory.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/AbstractFetchSchedule.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/AdaptiveFetchSchedule.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/Crawl.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/CrawlDatum.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/CrawlDb.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/CrawlDbFilter.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/CrawlDbMerger.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/CrawlDbReader.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/CrawlDbReducer.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/DefaultFetchSchedule.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/FetchSchedule.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/FetchScheduleFactory.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/Generator.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/Injector.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/Inlink.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/Inlinks.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/LinkDb.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/LinkDbFilter.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/LinkDbMerger.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/LinkDbReader.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/MD5Signature.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/MapWritable.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/NutchWritable.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/PartitionUrlByHost.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/Signature.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/SignatureComparator.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/SignatureFactory.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/TextProfileSignature.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/package.html\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/fetcher/Fetcher.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/fetcher/Fetcher2.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/fetcher/FetcherOutput.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/fetcher/FetcherOutputFormat.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/fetcher/package.html\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/html/Entities.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/DeleteDuplicates.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/FsDirectory.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/HighFreqTerms.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/IndexMerger.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/IndexSorter.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/Indexer.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/IndexerMapReduce.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/IndexerOutputFormat.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/IndexingException.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/IndexingFilter.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/IndexingFilters.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/NutchDocument.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/NutchIndexWriter.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/NutchIndexWriterFactory.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/NutchSimilarity.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/AnchorFields.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/BasicFields.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/CustomFields.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldFilter.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldFilters.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldIndexer.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldType.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldWritable.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/Fields.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldsWritable.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/lucene/LuceneConstants.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/lucene/LuceneWriter.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/package.html\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/solr/SolrConstants.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/solr/SolrIndexer.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/solr/SolrWriter.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/metadata/CreativeCommons.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/metadata/DublinCore.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/metadata/Feed.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/metadata/HttpHeaders.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/metadata/MetaWrapper.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/metadata/Metadata.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/metadata/Nutch.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/metadata/Office.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/metadata/SpellCheckedMetadata.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/metadata/package.html\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/net/URLFilter.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/net/URLFilterChecker.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/net/URLFilterException.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/net/URLFilters.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/net/URLNormalizer.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/net/URLNormalizerChecker.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/net/URLNormalizers.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/net/protocols/HttpDateFormat.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/net/protocols/ProtocolException.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/net/protocols/Response.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/ontology/Ontology.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/ontology/OntologyFactory.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/HTMLMetaTags.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/HtmlParseFilter.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/HtmlParseFilters.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/Outlink.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/OutlinkExtractor.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/Parse.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParseData.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParseException.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParseImpl.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParseOutputFormat.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParsePluginList.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParsePluginsReader.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParseResult.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParseSegment.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParseStatus.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParseText.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParseUtil.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/Parser.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParserChecker.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParserFactory.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/parse/ParserNotFound.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/CircularDependencyException.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/Extension.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/ExtensionPoint.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/MissingDependencyException.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/Pluggable.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/Plugin.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/PluginClassLoader.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/PluginDescriptor.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/PluginManifestParser.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/PluginRepository.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/PluginRuntimeException.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/package.html\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/protocol/Content.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/protocol/EmptyRobotRules.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/protocol/Protocol.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/protocol/ProtocolException.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/protocol/ProtocolFactory.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/protocol/ProtocolNotFound.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/protocol/ProtocolOutput.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/protocol/ProtocolStatus.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/protocol/RobotRules.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/ScoringFilter.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/ScoringFilterException.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/ScoringFilters.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/LinkDatum.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/LinkDumper.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/LinkRank.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/LoopReader.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/Loops.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/Node.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/NodeDumper.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/NodeReader.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/ScoreUpdater.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/WebGraph.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/DistributedSearch.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/DistributedSearchBean.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/DistributedSegmentBean.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/FetchedSegments.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/FieldQueryFilter.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/Hit.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/HitContent.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/HitDetailer.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/HitDetails.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/HitInlinks.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/HitSummarizer.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/Hits.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/IndexSearcher.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/LinkDbInlinks.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/LuceneQueryOptimizer.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/LuceneSearchBean.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/NutchBean.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/OpenSearchServlet.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/Query.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/QueryException.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/QueryFilter.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/QueryFilters.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/RPCSearchBean.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/RPCSegmentBean.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/RawFieldQueryFilter.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/SearchBean.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/Searcher.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/SegmentBean.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/SolrSearchBean.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/Summarizer.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/SummarizerFactory.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/Summary.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/package.html\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/RequestUtils.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/ResponseWriter.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/ResponseWriters.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/SearchResults.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/SearchServlet.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/segment/ContentAsTextInputFormat.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/segment/SegmentMerger.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/segment/SegmentPart.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/segment/SegmentReader.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/servlet/Cached.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/DmozParser.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/FreeGenerator.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/PruneIndexTool.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/ResolveUrls.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/SearchLoadTester.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/arc/ArcInputFormat.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/arc/ArcRecordReader.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/arc/ArcSegmentCreator.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/compat/CrawlDbConverter.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/compat/ReprUrlFixer.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/CommandRunner.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/DeflateUtils.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/DomUtil.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/EncodingDetector.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/FSUtils.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/GZIPUtils.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/GenericWritableConfigurable.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/HadoopFSUtil.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/LockUtil.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/LogUtil.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/MimeUtil.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/NodeWalker.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/NutchConfiguration.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/NutchJob.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/ObjectCache.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/PrefixStringMatcher.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/StringUtil.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/SuffixStringMatcher.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/TrieStringMatcher.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/URLUtil.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/domain/DomainStatistics.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/domain/DomainSuffix.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/domain/DomainSuffixes.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/domain/DomainSuffixesReader.java\n[rat:report]   AL    /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/domain/TopLevelDomain.java\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/domain/package.html\n[rat:report]  !????? /home/sam/workspace/nutch-trunk-eu/src/java/overview.html\n[rat:report]  \n[rat:report]  *****************************************************\n[rat:report]  Printing headers for files without AL header...\n[rat:report]  \n[rat:report]  \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/analysis/package.html\n[rat:report]  =======================================================================\n[rat:report] <html>\n[rat:report] <body>\n[rat:report] Tokenizer for documents and query parser.\n[rat:report] </body>\n[rat:report] </html>\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/NutchWritable.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.crawl;\n[rat:report] \n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] import org.apache.nutch.util.GenericWritableConfigurable;\n[rat:report] \n[rat:report] public class NutchWritable extends GenericWritableConfigurable {\n[rat:report]   \n[rat:report]   private static Class<? extends Writable>[] CLASSES = null;\n[rat:report]   \n[rat:report]   static {\n[rat:report]     CLASSES = (Class<? extends Writable>[]) new Class[] {\n[rat:report]       org.apache.hadoop.io.NullWritable.class, \n[rat:report]       org.apache.hadoop.io.LongWritable.class,\n[rat:report]       org.apache.hadoop.io.BytesWritable.class,\n[rat:report]       org.apache.hadoop.io.FloatWritable.class,\n[rat:report]       org.apache.hadoop.io.IntWritable.class,\n[rat:report]       org.apache.hadoop.io.Text.class,\n[rat:report]       org.apache.hadoop.io.MD5Hash.class,\n[rat:report]       org.apache.nutch.crawl.CrawlDatum.class,\n[rat:report]       org.apache.nutch.crawl.Inlink.class,\n[rat:report]       org.apache.nutch.crawl.Inlinks.class,\n[rat:report]       org.apache.nutch.crawl.MapWritable.class,\n[rat:report]       org.apache.nutch.fetcher.FetcherOutput.class,\n[rat:report]       org.apache.nutch.metadata.Metadata.class,\n[rat:report]       org.apache.nutch.parse.Outlink.class,\n[rat:report]       org.apache.nutch.parse.ParseText.class,\n[rat:report]       org.apache.nutch.parse.ParseData.class,\n[rat:report]       org.apache.nutch.parse.ParseImpl.class,\n[rat:report]       org.apache.nutch.parse.ParseStatus.class,\n[rat:report]       org.apache.nutch.protocol.Content.class,\n[rat:report]       org.apache.nutch.protocol.ProtocolStatus.class,\n[rat:report]       org.apache.nutch.searcher.Hit.class,\n[rat:report]       org.apache.nutch.searcher.HitDetails.class,\n[rat:report]       org.apache.nutch.searcher.Hits.class\n[rat:report]     };\n[rat:report]   }\n[rat:report] \n[rat:report]   public NutchWritable() { }\n[rat:report]   \n[rat:report]   public NutchWritable(Writable instance) {\n[rat:report]     set(instance);\n[rat:report]   }\n[rat:report] \n[rat:report]   @Override\n[rat:report]   protected Class<? extends Writable>[] getTypes() {\n[rat:report]     return CLASSES;\n[rat:report]   }\n[rat:report] \n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/crawl/package.html\n[rat:report]  =======================================================================\n[rat:report] <html>\n[rat:report] <body>\n[rat:report] Crawl control code.\n[rat:report] </body>\n[rat:report] </html>\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/fetcher/package.html\n[rat:report]  =======================================================================\n[rat:report] <html>\n[rat:report] <body>\n[rat:report] The Nutch robot.\n[rat:report] </body>\n[rat:report] </html>\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/IndexerMapReduce.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] import java.util.Collection;\n[rat:report] import java.util.Iterator;\n[rat:report] \n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configured;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] import org.apache.hadoop.mapred.FileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.Mapper;\n[rat:report] import org.apache.hadoop.mapred.OutputCollector;\n[rat:report] import org.apache.hadoop.mapred.Reducer;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileInputFormat;\n[rat:report] import org.apache.nutch.crawl.CrawlDatum;\n[rat:report] import org.apache.nutch.crawl.CrawlDb;\n[rat:report] import org.apache.nutch.crawl.Inlinks;\n[rat:report] import org.apache.nutch.crawl.LinkDb;\n[rat:report] import org.apache.nutch.crawl.NutchWritable;\n[rat:report] import org.apache.nutch.metadata.Metadata;\n[rat:report] import org.apache.nutch.metadata.Nutch;\n[rat:report] import org.apache.nutch.parse.Parse;\n[rat:report] import org.apache.nutch.parse.ParseData;\n[rat:report] import org.apache.nutch.parse.ParseImpl;\n[rat:report] import org.apache.nutch.parse.ParseText;\n[rat:report] import org.apache.nutch.scoring.ScoringFilterException;\n[rat:report] import org.apache.nutch.scoring.ScoringFilters;\n[rat:report] \n[rat:report] public class IndexerMapReduce extends Configured\n[rat:report] implements Mapper<Text, Writable, Text, NutchWritable>,\n[rat:report]           Reducer<Text, NutchWritable, Text, NutchDocument> {\n[rat:report] \n[rat:report]   public static final Log LOG = LogFactory.getLog(IndexerMapReduce.class);\n[rat:report] \n[rat:report]   private IndexingFilters filters;\n[rat:report]   private ScoringFilters scfilters;\n[rat:report] \n[rat:report]   public void configure(JobConf job) {\n[rat:report]     setConf(job);\n[rat:report]     this.filters = new IndexingFilters(getConf());\n[rat:report]     this.scfilters = new ScoringFilters(getConf());\n[rat:report]   }\n[rat:report] \n[rat:report]   public void map(Text key, Writable value,\n[rat:report]       OutputCollector<Text, NutchWritable> output, Reporter reporter) throws IOException {\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/IndexerOutputFormat.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.RecordWriter;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.util.Progressable;\n[rat:report] \n[rat:report] public class IndexerOutputFormat extends FileOutputFormat<Text, NutchDocument> {\n[rat:report] \n[rat:report]   @Override\n[rat:report]   public RecordWriter<Text, NutchDocument> getRecordWriter(FileSystem ignored,\n[rat:report]       JobConf job, String name, Progressable progress) throws IOException {\n[rat:report]     final NutchIndexWriter[] writers =\n[rat:report]       NutchIndexWriterFactory.getNutchIndexWriters(job);\n[rat:report] \n[rat:report]     for (final NutchIndexWriter writer : writers) {\n[rat:report]       writer.open(job, name);\n[rat:report]     }\n[rat:report]     return new RecordWriter<Text, NutchDocument>() {\n[rat:report] \n[rat:report]       public void close(Reporter reporter) throws IOException {\n[rat:report]         for (final NutchIndexWriter writer : writers) {\n[rat:report]           writer.close();\n[rat:report]         }\n[rat:report]       }\n[rat:report] \n[rat:report]       public void write(Text key, NutchDocument doc) throws IOException {\n[rat:report]         for (final NutchIndexWriter writer : writers) {\n[rat:report]           writer.write(doc);\n[rat:report]         }\n[rat:report]       }\n[rat:report]     };\n[rat:report]   }\n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/NutchDocument.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer;\n[rat:report] \n[rat:report] import java.io.DataInput;\n[rat:report] import java.io.DataOutput;\n[rat:report] import java.io.IOException;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.Collection;\n[rat:report] import java.util.HashMap;\n[rat:report] import java.util.Iterator;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Map;\n[rat:report] import java.util.Map.Entry;\n[rat:report] \n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.VersionMismatchException;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] import org.apache.hadoop.io.WritableUtils;\n[rat:report] import org.apache.nutch.metadata.Metadata;\n[rat:report] \n[rat:report] /** A {@link NutchDocument} is the unit of indexing.*/\n[rat:report] public class NutchDocument\n[rat:report] implements Writable, Iterable<Entry<String, List<String>>> {\n[rat:report] \n[rat:report]   public static final byte VERSION = 1;\n[rat:report] \n[rat:report]   private Map<String, List<String>> fields;\n[rat:report] \n[rat:report]   private Metadata documentMeta;\n[rat:report] \n[rat:report]   private float score;\n[rat:report] \n[rat:report]   public NutchDocument() {\n[rat:report]     fields = new HashMap<String, List<String>>();\n[rat:report]     documentMeta = new Metadata();\n[rat:report]     score = 0.0f;\n[rat:report]   }\n[rat:report] \n[rat:report]   public void add(String name, String value) {\n[rat:report]     List<String> fieldValues = fields.get(name);\n[rat:report]     if (fieldValues == null) {\n[rat:report]       fieldValues = new ArrayList<String>();\n[rat:report]     }\n[rat:report]     fieldValues.add(value);\n[rat:report]     fields.put(name, fieldValues);\n[rat:report]   }\n[rat:report] \n[rat:report]   private void addFieldUnprotected(String name, String value) {\n[rat:report]     fields.get(name).add(value);\n[rat:report]   }\n[rat:report] \n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/NutchIndexWriter.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] \n[rat:report] public interface NutchIndexWriter {\n[rat:report]   public void open(JobConf job, String name) throws IOException;\n[rat:report] \n[rat:report]   public void write(NutchDocument doc) throws IOException;\n[rat:report] \n[rat:report]   public void close() throws IOException;\n[rat:report] \n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/NutchIndexWriterFactory.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer;\n[rat:report] \n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] \n[rat:report] public class NutchIndexWriterFactory {\n[rat:report]   @SuppressWarnings(\"unchecked\")\n[rat:report]   public static NutchIndexWriter[] getNutchIndexWriters(Configuration conf) {\n[rat:report]     final String[] classes = conf.getStrings(\"indexer.writer.classes\");\n[rat:report]     final NutchIndexWriter[] writers = new NutchIndexWriter[classes.length];\n[rat:report]     for (int i = 0; i < classes.length; i++) {\n[rat:report]       final String clazz = classes[i];\n[rat:report]       try {\n[rat:report]         final Class<NutchIndexWriter> implClass =\n[rat:report]           (Class<NutchIndexWriter>) Class.forName(clazz);\n[rat:report]         writers[i] = implClass.newInstance();\n[rat:report]       } catch (final Exception e) {\n[rat:report]         throw new RuntimeException(\"Couldn't create \" + clazz, e);\n[rat:report]       }\n[rat:report]     }\n[rat:report]     return writers;\n[rat:report]   }\n[rat:report] \n[rat:report]   public static void addClassToConf(Configuration conf,\n[rat:report]                                     Class<? extends NutchIndexWriter> clazz) {\n[rat:report]     final String classes = conf.get(\"indexer.writer.classes\");\n[rat:report]     final String newClass = clazz.getName();\n[rat:report] \n[rat:report]     if (classes == null) {\n[rat:report]       conf.set(\"indexer.writer.classes\", newClass);\n[rat:report]     } else {\n[rat:report]       conf.set(\"indexer.writer.classes\", classes + \",\" + newClass);\n[rat:report]     }\n[rat:report] \n[rat:report]   }\n[rat:report] \n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/AnchorFields.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.field;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.Collections;\n[rat:report] import java.util.Comparator;\n[rat:report] import java.util.Iterator;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Random;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.conf.Configured;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.ObjectWritable;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] import org.apache.hadoop.mapred.FileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobClient;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.Mapper;\n[rat:report] import org.apache.hadoop.mapred.OutputCollector;\n[rat:report] import org.apache.hadoop.mapred.Reducer;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileOutputFormat;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.hadoop.util.Tool;\n[rat:report] import org.apache.hadoop.util.ToolRunner;\n[rat:report] import org.apache.nutch.scoring.webgraph.LinkDatum;\n[rat:report] import org.apache.nutch.scoring.webgraph.Node;\n[rat:report] import org.apache.nutch.scoring.webgraph.WebGraph;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] import org.apache.nutch.util.NutchJob;\n[rat:report] \n[rat:report] /**\n[rat:report]  * Creates FieldWritable objects for inbound anchor text.   These FieldWritable\n[rat:report]  * objects are then included in the input to the FieldIndexer to be converted\n[rat:report]  * to Lucene Field objects and indexed.\n[rat:report]  * \n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/BasicFields.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.field;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.Iterator;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Random;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.conf.Configured;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.ObjectWritable;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] import org.apache.hadoop.io.WritableUtils;\n[rat:report] import org.apache.hadoop.mapred.FileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobClient;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.Mapper;\n[rat:report] import org.apache.hadoop.mapred.OutputCollector;\n[rat:report] import org.apache.hadoop.mapred.Reducer;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileOutputFormat;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.hadoop.util.Tool;\n[rat:report] import org.apache.hadoop.util.ToolRunner;\n[rat:report] import org.apache.lucene.document.DateTools;\n[rat:report] import org.apache.nutch.crawl.CrawlDatum;\n[rat:report] import org.apache.nutch.metadata.Metadata;\n[rat:report] import org.apache.nutch.metadata.Nutch;\n[rat:report] import org.apache.nutch.parse.Parse;\n[rat:report] import org.apache.nutch.parse.ParseData;\n[rat:report] import org.apache.nutch.parse.ParseImpl;\n[rat:report] import org.apache.nutch.parse.ParseText;\n[rat:report] import org.apache.nutch.scoring.webgraph.LinkDatum;\n[rat:report] import org.apache.nutch.scoring.webgraph.Node;\n[rat:report] import org.apache.nutch.scoring.webgraph.WebGraph;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/CustomFields.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.field;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] import java.io.InputStream;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.Enumeration;\n[rat:report] import java.util.HashMap;\n[rat:report] import java.util.HashSet;\n[rat:report] import java.util.Iterator;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Map;\n[rat:report] import java.util.Properties;\n[rat:report] import java.util.Random;\n[rat:report] import java.util.Set;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.conf.Configured;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.LongWritable;\n[rat:report] import org.apache.hadoop.io.ObjectWritable;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] import org.apache.hadoop.mapred.FileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobClient;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.Mapper;\n[rat:report] import org.apache.hadoop.mapred.OutputCollector;\n[rat:report] import org.apache.hadoop.mapred.Reducer;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.TextInputFormat;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.hadoop.util.Tool;\n[rat:report] import org.apache.hadoop.util.ToolRunner;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] import org.apache.nutch.util.NutchJob;\n[rat:report] \n[rat:report] /**\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldFilter.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.field;\n[rat:report] \n[rat:report] import java.util.List;\n[rat:report] \n[rat:report] import org.apache.hadoop.conf.Configurable;\n[rat:report] import org.apache.lucene.document.Document;\n[rat:report] import org.apache.nutch.indexer.IndexingException;\n[rat:report] import org.apache.nutch.plugin.Pluggable;\n[rat:report] \n[rat:report] /**\n[rat:report]  * Filter to manipulate FieldWritable objects for a given url during indexing.\n[rat:report]  * \n[rat:report]  * Field filters are responsible for converting FieldWritable objects into \n[rat:report]  * lucene fields and adding those fields to the Lucene document.\n[rat:report]  */\n[rat:report] public interface FieldFilter\n[rat:report]   extends Pluggable, Configurable {\n[rat:report] \n[rat:report]   final static String X_POINT_ID = FieldFilter.class.getName();\n[rat:report] \n[rat:report]   /**\n[rat:report]    * Returns the document to which fields are being added or null if we are to\n[rat:report]    * stop processing for this url and not add anything to the index.  All \n[rat:report]    * FieldWritable objects for a url are aggregated from databases passed into\n[rat:report]    * the FieldIndexer and these fields are then passed into the Field filters.\n[rat:report]    * \n[rat:report]    * It is therefore possible for fields to be added, removed, and changed \n[rat:report]    * before being indexed.\n[rat:report]    * \n[rat:report]    * @param url The url to index.  \n[rat:report]    * @param doc The lucene document\n[rat:report]    * @param fields The list of FieldWritable objects representing fields for \n[rat:report]    * the index.\n[rat:report]    * @return The lucene Document or null to stop processing and not index any\n[rat:report]    * content for this url.\n[rat:report]    * \n[rat:report]    * @throws IndexingException If an error occurs during indexing\n[rat:report]    */\n[rat:report]   public Document filter(String url, Document doc, List<FieldWritable> fields)\n[rat:report]     throws IndexingException;\n[rat:report] \n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldFilters.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.field;\n[rat:report] \n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.HashMap;\n[rat:report] import java.util.List;\n[rat:report] \n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.lucene.document.Document;\n[rat:report] import org.apache.nutch.indexer.IndexingException;\n[rat:report] import org.apache.nutch.plugin.Extension;\n[rat:report] import org.apache.nutch.plugin.ExtensionPoint;\n[rat:report] import org.apache.nutch.plugin.PluginRepository;\n[rat:report] import org.apache.nutch.plugin.PluginRuntimeException;\n[rat:report] import org.apache.nutch.util.ObjectCache;\n[rat:report] \n[rat:report] /**\n[rat:report]  * The FieldFilters class provides a standard way to collect, order, and run\n[rat:report]  * all FieldFilter implementations that are active in the plugin system.\n[rat:report]  */\n[rat:report] public class FieldFilters {\n[rat:report] \n[rat:report]   public static final String FIELD_FILTER_ORDER = \"field.filter.order\";\n[rat:report] \n[rat:report]   public final static Log LOG = LogFactory.getLog(FieldFilters.class);\n[rat:report] \n[rat:report]   private FieldFilter[] fieldFilters;\n[rat:report] \n[rat:report]   /**\n[rat:report]    * Configurable constructor.\n[rat:report]    */\n[rat:report]   public FieldFilters(Configuration conf) {\n[rat:report] \n[rat:report]     // get the field filter order, the cache, and all field filters\n[rat:report]     String order = conf.get(FIELD_FILTER_ORDER);\n[rat:report]     ObjectCache objectCache = ObjectCache.get(conf);\n[rat:report]     this.fieldFilters = (FieldFilter[])objectCache.getObject(FieldFilter.class.getName());\n[rat:report]     \n[rat:report]     if (this.fieldFilters == null) {\n[rat:report] \n[rat:report]       String[] orderedFilters = null;\n[rat:report]       if (order != null && !order.trim().equals(\"\")) {\n[rat:report]         orderedFilters = order.split(\"\\\\s+\");\n[rat:report]       }\n[rat:report]       try {\n[rat:report] \n[rat:report]         // get the field filter extension point\n[rat:report]         ExtensionPoint point = PluginRepository.get(conf).getExtensionPoint(\n[rat:report]           FieldFilter.X_POINT_ID);\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldIndexer.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.field;\n[rat:report] \n[rat:report] import java.io.DataInput;\n[rat:report] import java.io.DataOutput;\n[rat:report] import java.io.IOException;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.Iterator;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Random;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.conf.Configured;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] import org.apache.hadoop.io.WritableComparable;\n[rat:report] import org.apache.hadoop.io.WritableUtils;\n[rat:report] import org.apache.hadoop.mapred.FileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobClient;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.Mapper;\n[rat:report] import org.apache.hadoop.mapred.OutputCollector;\n[rat:report] import org.apache.hadoop.mapred.RecordWriter;\n[rat:report] import org.apache.hadoop.mapred.Reducer;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileInputFormat;\n[rat:report] import org.apache.hadoop.util.Progressable;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.hadoop.util.Tool;\n[rat:report] import org.apache.hadoop.util.ToolRunner;\n[rat:report] import org.apache.lucene.document.Document;\n[rat:report] import org.apache.lucene.index.IndexWriter;\n[rat:report] import org.apache.nutch.analysis.AnalyzerFactory;\n[rat:report] import org.apache.nutch.analysis.NutchAnalyzer;\n[rat:report] import org.apache.nutch.analysis.NutchDocumentAnalyzer;\n[rat:report] import org.apache.nutch.indexer.IndexingException;\n[rat:report] import org.apache.nutch.indexer.NutchSimilarity;\n[rat:report] import org.apache.nutch.util.LogUtil;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldType.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.field;\n[rat:report] \n[rat:report] /**\n[rat:report]  * The different types of fields. Different types of fields will be handled by\n[rat:report]  * different FieldFilter implementations during indexing.\n[rat:report]  */\n[rat:report] public enum FieldType {\n[rat:report]   \n[rat:report]   CONTENT,\n[rat:report]   BOOST,\n[rat:report]   COMPUTATION,\n[rat:report]   ACTION;\n[rat:report]   \n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldWritable.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.field;\n[rat:report] \n[rat:report] import java.io.DataInput;\n[rat:report] import java.io.DataOutput;\n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] \n[rat:report] /** \n[rat:report]  * A class that holds a single field of content to be placed into an index.\n[rat:report]  * \n[rat:report]  * This class has options type of content as well as for how the field is to \n[rat:report]  * be indexed.\n[rat:report]  */\n[rat:report] public class FieldWritable\n[rat:report]   implements Writable {\n[rat:report] \n[rat:report]   private String name;\n[rat:report]   private String value;\n[rat:report]   private FieldType type = FieldType.CONTENT;\n[rat:report]   private float boost;\n[rat:report]   private boolean indexed = true;\n[rat:report]   private boolean stored = false;\n[rat:report]   private boolean tokenized = true;\n[rat:report] \n[rat:report]   public FieldWritable() {\n[rat:report] \n[rat:report]   }\n[rat:report] \n[rat:report]   public FieldWritable(String name, String value, FieldType type, float boost) {\n[rat:report]     this(name, value, type, boost, true, false, true);\n[rat:report]   }\n[rat:report] \n[rat:report]   public FieldWritable(String name, String value, FieldType type,\n[rat:report]     boolean indexed, boolean stored, boolean tokenized) {\n[rat:report]     this(name, value, type, 0.0f, indexed, stored, tokenized);\n[rat:report]   }\n[rat:report] \n[rat:report]   public FieldWritable(String name, String value, FieldType type, float boost,\n[rat:report]     boolean indexed, boolean stored, boolean tokenized) {\n[rat:report]     this.name = name;\n[rat:report]     this.value = value;\n[rat:report]     this.type = type;\n[rat:report]     this.boost = boost;\n[rat:report]     this.indexed = indexed;\n[rat:report]     this.stored = stored;\n[rat:report]     this.tokenized = tokenized;\n[rat:report]   }\n[rat:report] \n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/Fields.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.field;\n[rat:report] \n[rat:report] public interface Fields {\n[rat:report] \n[rat:report]   // names of common fields\n[rat:report]   public static final String ANCHOR = \"anchor\";\n[rat:report]   public static final String SEGMENT = \"segment\";\n[rat:report]   public static final String DIGEST = \"digest\";\n[rat:report]   public static final String HOST = \"host\";\n[rat:report]   public static final String SITE = \"site\";\n[rat:report]   public static final String URL = \"url\";\n[rat:report]   public static final String ORIG_URL = \"orig\";\n[rat:report]   public static final String SEG_URL = \"segurl\";\n[rat:report]   public static final String CONTENT = \"content\";\n[rat:report]   public static final String TITLE = \"title\";\n[rat:report]   public static final String CACHE = \"cache\";\n[rat:report]   public static final String TSTAMP = \"tstamp\";\n[rat:report]   public static final String BOOSTFACTOR = \"boostfactor\";\n[rat:report]   \n[rat:report]   // special fields for indexer\n[rat:report]   public static final String BOOST = \"boost\";\n[rat:report]   public static final String COMPUTATION = \"computation\";\n[rat:report]   public static final String ACTION = \"action\";\n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/field/FieldsWritable.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.field;\n[rat:report] \n[rat:report] import java.io.DataInput;\n[rat:report] import java.io.DataOutput;\n[rat:report] import java.io.IOException;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.List;\n[rat:report] \n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] \n[rat:report] /**\n[rat:report]  * A class that holds a grouping of FieldWritable objects.\n[rat:report]  */\n[rat:report] public class FieldsWritable\n[rat:report]   implements Writable {\n[rat:report] \n[rat:report]   private List<FieldWritable> fieldsList = new ArrayList<FieldWritable>();\n[rat:report] \n[rat:report]   public FieldsWritable() {\n[rat:report] \n[rat:report]   }\n[rat:report]   \n[rat:report]   public boolean hasField(String name) {\n[rat:report]     for (FieldWritable field : fieldsList) {\n[rat:report]       if (field.getName().equals(name)) {\n[rat:report]         return true;\n[rat:report]       }\n[rat:report]     }\n[rat:report]     return false;\n[rat:report]   }\n[rat:report]   \n[rat:report]   public FieldWritable getField(String name) {\n[rat:report]     for (FieldWritable field : fieldsList) {\n[rat:report]       if (field.getName().equals(name)) {\n[rat:report]         return field;\n[rat:report]       }\n[rat:report]     }\n[rat:report]     return null;\n[rat:report]   }\n[rat:report]   \n[rat:report]   public List<FieldWritable> getFields(String name) {\n[rat:report]     List<FieldWritable> named = new ArrayList<FieldWritable>();\n[rat:report]     for (FieldWritable field : fieldsList) {\n[rat:report]       if (field.getName().equals(name)) {\n[rat:report]         named.add(field);\n[rat:report]       }\n[rat:report]     }\n[rat:report]     return named.size() > 0 ? named : null;\n[rat:report]   }\n[rat:report]   \n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/lucene/LuceneConstants.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.lucene;\n[rat:report] \n[rat:report] public interface LuceneConstants {\n[rat:report]   public static final String LUCENE_PREFIX = \"lucene.\";\n[rat:report] \n[rat:report]   public static final String FIELD_PREFIX = LUCENE_PREFIX + \"field.\";\n[rat:report] \n[rat:report]   public static final String FIELD_STORE_PREFIX = FIELD_PREFIX + \"store.\";\n[rat:report] \n[rat:report]   public static final String FIELD_INDEX_PREFIX = FIELD_PREFIX + \"index.\";\n[rat:report] \n[rat:report]   public static final String FIELD_VECTOR_PREFIX = FIELD_PREFIX + \"vector.\";\n[rat:report] \n[rat:report]   public static final String STORE_YES = \"store.yes\";\n[rat:report] \n[rat:report]   public static final String STORE_NO = \"store.no\";\n[rat:report] \n[rat:report]   public static final String STORE_COMPRESS = \"store.compress\";\n[rat:report] \n[rat:report]   public static final String INDEX_NO = \"index.no\";\n[rat:report] \n[rat:report]   public static final String INDEX_NO_NORMS = \"index.no_norms\";\n[rat:report] \n[rat:report]   public static final String INDEX_TOKENIZED = \"index.tokenized\";\n[rat:report] \n[rat:report]   public static final String INDEX_UNTOKENIZED = \"index.untokenized\";\n[rat:report] \n[rat:report]   public static final String VECTOR_NO = \"vector.no\";\n[rat:report] \n[rat:report]   public static final String VECTOR_POS = \"vector.pos\";\n[rat:report] \n[rat:report]   public static final String VECTOR_OFFSET = \"vector.offset\";\n[rat:report] \n[rat:report]   public static final String VECTOR_POS_OFFSET = \"vector.pos_offset\";\n[rat:report] \n[rat:report]   public static final String VECTOR_YES = \"vector.yes\";\n[rat:report] \n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/lucene/LuceneWriter.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.lucene;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] import java.util.HashMap;\n[rat:report] import java.util.Iterator;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Map;\n[rat:report] import java.util.Random;\n[rat:report] import java.util.Map.Entry;\n[rat:report] \n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.lucene.document.Document;\n[rat:report] import org.apache.lucene.document.Field;\n[rat:report] import org.apache.lucene.index.IndexWriter;\n[rat:report] import org.apache.nutch.analysis.AnalyzerFactory;\n[rat:report] import org.apache.nutch.analysis.NutchAnalyzer;\n[rat:report] import org.apache.nutch.analysis.NutchDocumentAnalyzer;\n[rat:report] import org.apache.nutch.indexer.Indexer;\n[rat:report] import org.apache.nutch.indexer.NutchDocument;\n[rat:report] import org.apache.nutch.indexer.NutchIndexWriter;\n[rat:report] import org.apache.nutch.indexer.NutchSimilarity;\n[rat:report] import org.apache.nutch.metadata.Metadata;\n[rat:report] import org.apache.nutch.util.LogUtil;\n[rat:report] \n[rat:report] public class LuceneWriter implements NutchIndexWriter {\n[rat:report] \n[rat:report]   public static enum STORE { YES, NO, COMPRESS }\n[rat:report] \n[rat:report]   public static enum INDEX { NO, NO_NORMS, TOKENIZED, UNTOKENIZED }\n[rat:report] \n[rat:report]   public static enum VECTOR { NO, OFFSET, POS, POS_OFFSET, YES }\n[rat:report] \n[rat:report]   private IndexWriter writer;\n[rat:report] \n[rat:report]   private AnalyzerFactory analyzerFactory;\n[rat:report] \n[rat:report]   private Path perm;\n[rat:report] \n[rat:report]   private Path temp;\n[rat:report] \n[rat:report]   private FileSystem fs;\n[rat:report] \n[rat:report]   private final Map<String, Field.Store> fieldStore;\n[rat:report] \n[rat:report]   private final Map<String, Field.Index> fieldIndex;\n[rat:report] \n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/package.html\n[rat:report]  =======================================================================\n[rat:report] <html>\n[rat:report] <body>\n[rat:report] Maintain Lucene full-text indexes.\n[rat:report] </body>\n[rat:report] </html>\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/solr/SolrConstants.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.solr;\n[rat:report] \n[rat:report] public interface SolrConstants {\n[rat:report]   public static final String SOLR_PREFIX = \"solr.\";\n[rat:report] \n[rat:report]   public static final String SERVER_URL = SOLR_PREFIX + \"server.url\";\n[rat:report] \n[rat:report]   public static final String COMMIT_SIZE = SOLR_PREFIX + \"commit.size\";\n[rat:report] \n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/solr/SolrIndexer.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.solr;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Random;\n[rat:report] \n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.conf.Configured;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobClient;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.hadoop.util.Tool;\n[rat:report] import org.apache.hadoop.util.ToolRunner;\n[rat:report] import org.apache.nutch.indexer.IndexerMapReduce;\n[rat:report] import org.apache.nutch.indexer.NutchIndexWriterFactory;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] import org.apache.nutch.util.NutchJob;\n[rat:report] \n[rat:report] public class SolrIndexer extends Configured implements Tool {\n[rat:report] \n[rat:report]   public static Log LOG = LogFactory.getLog(SolrIndexer.class);\n[rat:report] \n[rat:report]   public SolrIndexer() {\n[rat:report]     super(null);\n[rat:report]   }\n[rat:report] \n[rat:report]   public SolrIndexer(Configuration conf) {\n[rat:report]     super(conf);\n[rat:report]   }\n[rat:report] \n[rat:report]   private void indexSolr(String solrUrl, Path crawlDb, Path linkDb,\n[rat:report]       List<Path> segments) throws IOException {\n[rat:report]     LOG.info(\"SolrIndexer: starting\");\n[rat:report] \n[rat:report]     final JobConf job = new NutchJob(getConf());\n[rat:report]     job.setJobName(\"index-solr \" + solrUrl);\n[rat:report] \n[rat:report]     IndexerMapReduce.initMRJob(crawlDb, linkDb, segments, job);\n[rat:report] \n[rat:report]     job.set(SolrConstants.SERVER_URL, solrUrl);\n[rat:report] \n[rat:report]     NutchIndexWriterFactory.addClassToConf(job, SolrWriter.class);\n[rat:report] \n[rat:report]     job.setReduceSpeculativeExecution(false);\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/indexer/solr/SolrWriter.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.indexer.solr;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Map.Entry;\n[rat:report] \n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.nutch.indexer.NutchDocument;\n[rat:report] import org.apache.nutch.indexer.NutchIndexWriter;\n[rat:report] import org.apache.solr.client.solrj.SolrServer;\n[rat:report] import org.apache.solr.client.solrj.SolrServerException;\n[rat:report] import org.apache.solr.client.solrj.impl.CommonsHttpSolrServer;\n[rat:report] import org.apache.solr.common.SolrInputDocument;\n[rat:report] \n[rat:report] public class SolrWriter implements NutchIndexWriter {\n[rat:report] \n[rat:report]   private SolrServer solr;\n[rat:report] \n[rat:report]   private final List<SolrInputDocument> inputDocs =\n[rat:report]     new ArrayList<SolrInputDocument>();\n[rat:report] \n[rat:report]   private int commitSize;\n[rat:report] \n[rat:report]   public void open(JobConf job, String name)\n[rat:report]   throws IOException {\n[rat:report]     solr = new CommonsHttpSolrServer(job.get(SolrConstants.SERVER_URL));\n[rat:report]     commitSize = job.getInt(SolrConstants.COMMIT_SIZE, 1000);\n[rat:report]   }\n[rat:report] \n[rat:report]   public void write(NutchDocument doc) throws IOException {\n[rat:report]     final SolrInputDocument inputDoc = new SolrInputDocument();\n[rat:report]     for(final Entry<String, List<String>> e : doc) {\n[rat:report]       for (final String val : e.getValue()) {\n[rat:report]         inputDoc.addField(e.getKey(), val);\n[rat:report]       }\n[rat:report]     }\n[rat:report]     inputDoc.setDocumentBoost(doc.getScore());\n[rat:report]     inputDocs.add(inputDoc);\n[rat:report]     if (inputDocs.size() > commitSize) {\n[rat:report]       try {\n[rat:report]         solr.add(inputDocs);\n[rat:report]       } catch (final SolrServerException e) {\n[rat:report]         throw makeIOException(e);\n[rat:report]       }\n[rat:report]       inputDocs.clear();\n[rat:report]     }\n[rat:report]   }\n[rat:report] \n[rat:report]   public void close() throws IOException {\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/metadata/package.html\n[rat:report]  =======================================================================\n[rat:report] <html>\n[rat:report] <body>\n[rat:report] A Multi-valued Metadata container, and set\n[rat:report] of constant fields for Nutch Metadata.\n[rat:report] </body>\n[rat:report] </html>\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/plugin/package.html\n[rat:report]  =======================================================================\n[rat:report] <html>\n[rat:report] <body>\n[rat:report] The Nutch {@link org.apache.nutch.plugin.Pluggable Plugin} System.\n[rat:report] <p>\n[rat:report] <b>The Nutch Plugin System provides a way to extend nutch functionality</b>.\n[rat:report] A large part of the functionality of Nutch are provided by plugins:\n[rat:report] All of the parsing, indexing and searching that nutch does is actually\n[rat:report] accomplished by various plugins.\n[rat:report] </p><p>\n[rat:report] In writing a plugin, you're actually providing one or more extensions of the\n[rat:report] existing extension-points (<i>hooks</i>).\n[rat:report] The core Nutch extension-points are themselves defined in a plugin,\n[rat:report] the <code>nutch-extensionpoints</code> plugin.\n[rat:report] Each extension-point defines an interface that must be implemented by the\n[rat:report] extension. The core extension-points and extensions available in Nutch are\n[rat:report] listed in the {@link org.apache.nutch.plugin.Pluggable} interface.\n[rat:report] </p>\n[rat:report] \n[rat:report] @see <a href=\"./doc-files/plugin.dtd\">Nutch plugin manifest DTD</a>\n[rat:report] \n[rat:report] @see <a href=\"http://wiki.apache.org/nutch/PluginCentral\">\n[rat:report]      Plugin Central\n[rat:report]      </a>\n[rat:report] @see <a href=\"http://wiki.apache.org/nutch/AboutPlugins\">\n[rat:report]      About Plugins\n[rat:report]      </a>\n[rat:report] @see <a href=\"http://wiki.apache.org/nutch/WhyNutchHasAPluginSystem\">\n[rat:report]      Why Nutch has a Plugin System?\n[rat:report]      </a>\n[rat:report] @see <a href=\"http://wiki.apache.org/nutch/WhichTechnicalConceptsAreBehindTheNutchPluginSystem\">\n[rat:report]      Which technical concepts are behind the nutch plugin system?\n[rat:report]      </a>\n[rat:report] @see <a href=\"http://wiki.apache.org/nutch/WhatsTheProblemWithPluginsAndClass-loading\">\n[rat:report]      What's the problem with Plugins and Class loading?\n[rat:report]      </a>\n[rat:report] @see <a href=\"http://wiki.apache.org/nutch/WritingPluginExample\">\n[rat:report]      Writing Plugin Example\n[rat:report]      </a>\n[rat:report] </body>\n[rat:report] </html>\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/LinkDatum.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.scoring.webgraph;\n[rat:report] \n[rat:report] import java.io.DataInput;\n[rat:report] import java.io.DataOutput;\n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] \n[rat:report] /**\n[rat:report]  * A class for holding link information including the url, anchor text, a score,\n[rat:report]  * the timestamp of the link and a link type.\n[rat:report]  */\n[rat:report] public class LinkDatum\n[rat:report]   implements Writable {\n[rat:report] \n[rat:report]   public final static byte INLINK = 1;\n[rat:report]   public final static byte OUTLINK = 2;\n[rat:report] \n[rat:report]   private String url = null;\n[rat:report]   private String anchor = \"\";\n[rat:report]   private float score = 0.0f;\n[rat:report]   private long timestamp = 0L;\n[rat:report]   private byte linkType = 0;\n[rat:report] \n[rat:report]   /**\n[rat:report]    * Default constructor, no url, timestamp, score, or link type.\n[rat:report]    */\n[rat:report]   public LinkDatum() {\n[rat:report] \n[rat:report]   }\n[rat:report] \n[rat:report]   /**\n[rat:report]    * Creates a LinkDatum with a given url. Timestamp is set to current time.\n[rat:report]    * \n[rat:report]    * @param url The link url.\n[rat:report]    */\n[rat:report]   public LinkDatum(String url) {\n[rat:report]     this(url, \"\", System.currentTimeMillis());\n[rat:report]   }\n[rat:report] \n[rat:report]   /**\n[rat:report]    * Creates a LinkDatum with a url and an anchor text. Timestamp is set to\n[rat:report]    * current time.\n[rat:report]    * \n[rat:report]    * @param url The link url.\n[rat:report]    * @param anchor The link anchor text.\n[rat:report]    */\n[rat:report]   public LinkDatum(String url, String anchor) {\n[rat:report]     this(url, anchor, System.currentTimeMillis());\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/LinkDumper.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.scoring.webgraph;\n[rat:report] \n[rat:report] import java.io.DataInput;\n[rat:report] import java.io.DataOutput;\n[rat:report] import java.io.IOException;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.Iterator;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Random;\n[rat:report] import java.util.Set;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.conf.Configured;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.MapFile;\n[rat:report] import org.apache.hadoop.io.ObjectWritable;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] import org.apache.hadoop.io.WritableUtils;\n[rat:report] import org.apache.hadoop.mapred.FileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobClient;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.MapFileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.Mapper;\n[rat:report] import org.apache.hadoop.mapred.OutputCollector;\n[rat:report] import org.apache.hadoop.mapred.Reducer;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.lib.HashPartitioner;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.hadoop.util.Tool;\n[rat:report] import org.apache.hadoop.util.ToolRunner;\n[rat:report] import org.apache.nutch.scoring.webgraph.Loops.LoopSet;\n[rat:report] import org.apache.nutch.util.FSUtils;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] import org.apache.nutch.util.NutchJob;\n[rat:report] \n[rat:report] /**\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/LinkRank.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.scoring.webgraph;\n[rat:report] \n[rat:report] import java.io.BufferedReader;\n[rat:report] import java.io.IOException;\n[rat:report] import java.io.InputStreamReader;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.HashSet;\n[rat:report] import java.util.Iterator;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Random;\n[rat:report] import java.util.Set;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.conf.Configured;\n[rat:report] import org.apache.hadoop.fs.FSDataInputStream;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.LongWritable;\n[rat:report] import org.apache.hadoop.io.ObjectWritable;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] import org.apache.hadoop.io.WritableUtils;\n[rat:report] import org.apache.hadoop.mapred.FileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobClient;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.MapFileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.Mapper;\n[rat:report] import org.apache.hadoop.mapred.OutputCollector;\n[rat:report] import org.apache.hadoop.mapred.Reducer;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.TextOutputFormat;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.hadoop.util.Tool;\n[rat:report] import org.apache.hadoop.util.ToolRunner;\n[rat:report] import org.apache.nutch.scoring.webgraph.Loops.LoopSet;\n[rat:report] import org.apache.nutch.util.FSUtils;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] import org.apache.nutch.util.NutchJob;\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/LoopReader.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.scoring.webgraph;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.MapFile;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.mapred.MapFileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.lib.HashPartitioner;\n[rat:report] import org.apache.nutch.scoring.webgraph.Loops.LoopSet;\n[rat:report] import org.apache.nutch.util.FSUtils;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] \n[rat:report] /**\n[rat:report]  * The LoopReader tool prints the loopset information for a single url.\n[rat:report]  */\n[rat:report] public class LoopReader {\n[rat:report] \n[rat:report]   private Configuration conf;\n[rat:report]   private FileSystem fs;\n[rat:report]   private MapFile.Reader[] loopReaders;\n[rat:report] \n[rat:report]   /**\n[rat:report]    * Prints loopset for a single url.  The loopset information will show any\n[rat:report]    * outlink url the eventually forms a link cycle.\n[rat:report]    * \n[rat:report]    * @param webGraphDb The WebGraph to check for loops\n[rat:report]    * @param url The url to check.\n[rat:report]    * \n[rat:report]    * @throws IOException If an error occurs while printing loopset information.\n[rat:report]    */\n[rat:report]   public void dumpUrl(Path webGraphDb, String url)\n[rat:report]     throws IOException {\n[rat:report] \n[rat:report]     // open the readers\n[rat:report]     conf = NutchConfiguration.create();\n[rat:report]     fs = FileSystem.get(conf);\n[rat:report]     loopReaders = MapFileOutputFormat.getReaders(fs, new Path(webGraphDb,\n[rat:report]       Loops.LOOPS_DIR), conf);\n[rat:report] \n[rat:report]     // get the loopset for a given url, if any\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/Loops.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.scoring.webgraph;\n[rat:report] \n[rat:report] import java.io.DataInput;\n[rat:report] import java.io.DataOutput;\n[rat:report] import java.io.IOException;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.HashSet;\n[rat:report] import java.util.Iterator;\n[rat:report] import java.util.LinkedHashSet;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Random;\n[rat:report] import java.util.Set;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.conf.Configured;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.ObjectWritable;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] import org.apache.hadoop.io.WritableUtils;\n[rat:report] import org.apache.hadoop.mapred.FileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobClient;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.MapFileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.Mapper;\n[rat:report] import org.apache.hadoop.mapred.OutputCollector;\n[rat:report] import org.apache.hadoop.mapred.Reducer;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileOutputFormat;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.hadoop.util.Tool;\n[rat:report] import org.apache.hadoop.util.ToolRunner;\n[rat:report] import org.apache.nutch.util.FSUtils;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] import org.apache.nutch.util.NutchJob;\n[rat:report] \n[rat:report] /**\n[rat:report]  * The Loops job identifies cycles of loops inside of the web graph. This is\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/Node.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.scoring.webgraph;\n[rat:report] \n[rat:report] import java.io.DataInput;\n[rat:report] import java.io.DataOutput;\n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] import org.apache.nutch.metadata.Metadata;\n[rat:report] \n[rat:report] /**\n[rat:report]  * A class which holds the number of inlinks and outlinks for a given url along\n[rat:report]  * with an inlink score from a link analysis program and any metadata.  \n[rat:report]  * \n[rat:report]  * The Node is the core unit of the NodeDb in the WebGraph.\n[rat:report]  */\n[rat:report] public class Node\n[rat:report]   implements Writable {\n[rat:report] \n[rat:report]   private int numInlinks = 0;\n[rat:report]   private int numOutlinks = 0;\n[rat:report]   private float inlinkScore = 1.0f;\n[rat:report]   private Metadata metadata = new Metadata();\n[rat:report] \n[rat:report]   public Node() {\n[rat:report] \n[rat:report]   }\n[rat:report] \n[rat:report]   public int getNumInlinks() {\n[rat:report]     return numInlinks;\n[rat:report]   }\n[rat:report] \n[rat:report]   public void setNumInlinks(int numInlinks) {\n[rat:report]     this.numInlinks = numInlinks;\n[rat:report]   }\n[rat:report] \n[rat:report]   public int getNumOutlinks() {\n[rat:report]     return numOutlinks;\n[rat:report]   }\n[rat:report] \n[rat:report]   public void setNumOutlinks(int numOutlinks) {\n[rat:report]     this.numOutlinks = numOutlinks;\n[rat:report]   }\n[rat:report] \n[rat:report]   public float getInlinkScore() {\n[rat:report]     return inlinkScore;\n[rat:report]   }\n[rat:report] \n[rat:report]   public void setInlinkScore(float inlinkScore) {\n[rat:report]     this.inlinkScore = inlinkScore;\n[rat:report]   }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/NodeDumper.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.scoring.webgraph;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] import java.util.Iterator;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.conf.Configured;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.FloatWritable;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.WritableUtils;\n[rat:report] import org.apache.hadoop.mapred.FileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobClient;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.Mapper;\n[rat:report] import org.apache.hadoop.mapred.OutputCollector;\n[rat:report] import org.apache.hadoop.mapred.Reducer;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.TextOutputFormat;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.hadoop.util.Tool;\n[rat:report] import org.apache.hadoop.util.ToolRunner;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] import org.apache.nutch.util.NutchJob;\n[rat:report] \n[rat:report] /**\n[rat:report]  * A tools that dumps out the top urls by number of inlinks, number of outlinks,\n[rat:report]  * or by score, to a text file. One of the major uses of this tool is to check\n[rat:report]  * the top scoring urls of a link analysis program such as LinkRank.\n[rat:report]  * \n[rat:report]  * For number of inlinks or number of outlinks the WebGraph program will need to\n[rat:report]  * have been run. For link analysis score a program such as LinkRank will need\n[rat:report]  * to have been run which updates the NodeDb of the WebGraph.\n[rat:report]  */\n[rat:report] public class NodeDumper\n[rat:report]   extends Configured\n[rat:report]   implements Tool {\n[rat:report] \n[rat:report]   public static final Log LOG = LogFactory.getLog(NodeDumper.class);\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/NodeReader.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.scoring.webgraph;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.MapFile;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.mapred.MapFileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.lib.HashPartitioner;\n[rat:report] import org.apache.nutch.util.FSUtils;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] \n[rat:report] /**\n[rat:report]  * Reads and prints to system out information for a single node from the NodeDb \n[rat:report]  * in the WebGraph.\n[rat:report]  */\n[rat:report] public class NodeReader {\n[rat:report] \n[rat:report]   private Configuration conf;\n[rat:report]   private FileSystem fs;\n[rat:report]   private MapFile.Reader[] nodeReaders;\n[rat:report] \n[rat:report]   /**\n[rat:report]    * Prints the content of the Node represented by the url to system out.\n[rat:report]    * \n[rat:report]    * @param webGraphDb The webgraph from which to get the node.\n[rat:report]    * @param url The url of the node.\n[rat:report]    * \n[rat:report]    * @throws IOException If an error occurs while getting the node.\n[rat:report]    */\n[rat:report]   public void dumpUrl(Path webGraphDb, String url)\n[rat:report]     throws IOException {\n[rat:report] \n[rat:report]     conf = NutchConfiguration.create();\n[rat:report]     fs = FileSystem.get(conf);\n[rat:report]     nodeReaders = MapFileOutputFormat.getReaders(fs, new Path(webGraphDb,\n[rat:report]       WebGraph.NODE_DIR), conf);\n[rat:report] \n[rat:report]     // open the readers, get the node, print out the info, and close the readers\n[rat:report]     Text key = new Text(url);\n[rat:report]     Node node = new Node();\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/ScoreUpdater.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.scoring.webgraph;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] import java.util.Iterator;\n[rat:report] import java.util.Random;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.conf.Configured;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.ObjectWritable;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] import org.apache.hadoop.mapred.FileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobClient;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.MapFileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.Mapper;\n[rat:report] import org.apache.hadoop.mapred.OutputCollector;\n[rat:report] import org.apache.hadoop.mapred.Reducer;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileInputFormat;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.hadoop.util.Tool;\n[rat:report] import org.apache.hadoop.util.ToolRunner;\n[rat:report] import org.apache.nutch.crawl.CrawlDatum;\n[rat:report] import org.apache.nutch.crawl.CrawlDb;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] import org.apache.nutch.util.NutchJob;\n[rat:report] \n[rat:report] /**\n[rat:report]  * Updates the score from the WebGraph node database into the crawl database.\n[rat:report]  * Any score that is not in the node database is set to the clear score in the \n[rat:report]  * crawl database.\n[rat:report]  */\n[rat:report] public class ScoreUpdater\n[rat:report]   extends Configured\n[rat:report]   implements Tool, Mapper<Text, Writable, Text, ObjectWritable>,\n[rat:report]   Reducer<Text, ObjectWritable, Text, CrawlDatum> {\n[rat:report] \n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/scoring/webgraph/WebGraph.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.scoring.webgraph;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.HashSet;\n[rat:report] import java.util.Iterator;\n[rat:report] import java.util.LinkedHashMap;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Map;\n[rat:report] import java.util.Random;\n[rat:report] import java.util.Set;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.conf.Configured;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] import org.apache.hadoop.io.WritableUtils;\n[rat:report] import org.apache.hadoop.mapred.FileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobClient;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.MapFileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.Mapper;\n[rat:report] import org.apache.hadoop.mapred.OutputCollector;\n[rat:report] import org.apache.hadoop.mapred.Reducer;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileInputFormat;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.hadoop.util.Tool;\n[rat:report] import org.apache.hadoop.util.ToolRunner;\n[rat:report] import org.apache.nutch.metadata.Nutch;\n[rat:report] import org.apache.nutch.net.URLNormalizers;\n[rat:report] import org.apache.nutch.parse.Outlink;\n[rat:report] import org.apache.nutch.parse.ParseData;\n[rat:report] import org.apache.nutch.util.FSUtils;\n[rat:report] import org.apache.nutch.util.LockUtil;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] import org.apache.nutch.util.NutchJob;\n[rat:report] import org.apache.nutch.util.URLUtil;\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/DistributedSearchBean.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.searcher;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] import java.net.InetSocketAddress;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.Arrays;\n[rat:report] import java.util.Collections;\n[rat:report] import java.util.Comparator;\n[rat:report] import java.util.List;\n[rat:report] import java.util.PriorityQueue;\n[rat:report] import java.util.concurrent.Callable;\n[rat:report] import java.util.concurrent.ExecutionException;\n[rat:report] import java.util.concurrent.ExecutorService;\n[rat:report] import java.util.concurrent.Executors;\n[rat:report] import java.util.concurrent.Future;\n[rat:report] import java.util.concurrent.ScheduledExecutorService;\n[rat:report] import java.util.concurrent.TimeUnit;\n[rat:report] \n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.ipc.RPC;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] \n[rat:report] public class DistributedSearchBean implements SearchBean {\n[rat:report] \n[rat:report]   private static final ExecutorService executor =\n[rat:report]     Executors.newCachedThreadPool();\n[rat:report] \n[rat:report]   private final ScheduledExecutorService pingService;\n[rat:report] \n[rat:report]   private class SearchTask implements Callable<Hits> {\n[rat:report]     private int id;\n[rat:report] \n[rat:report]     private Query query;\n[rat:report]     private int numHits;\n[rat:report]     private String dedupField;\n[rat:report]     private String sortField;\n[rat:report]     private boolean reverse;\n[rat:report] \n[rat:report]     public SearchTask(int id) {\n[rat:report]       this.id = id;\n[rat:report]     }\n[rat:report] \n[rat:report]     public Hits call() throws Exception {\n[rat:report]       if (!liveServers[id]) {\n[rat:report]         return null;\n[rat:report]       }\n[rat:report]       return beans[id].search(query, numHits, dedupField, sortField, reverse);\n[rat:report]     }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/DistributedSegmentBean.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.searcher;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] import java.net.InetSocketAddress;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.Iterator;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Map;\n[rat:report] import java.util.concurrent.Callable;\n[rat:report] import java.util.concurrent.ConcurrentHashMap;\n[rat:report] import java.util.concurrent.ConcurrentMap;\n[rat:report] import java.util.concurrent.ExecutorService;\n[rat:report] import java.util.concurrent.Executors;\n[rat:report] import java.util.concurrent.Future;\n[rat:report] import java.util.concurrent.ScheduledExecutorService;\n[rat:report] import java.util.concurrent.TimeUnit;\n[rat:report] \n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.ipc.RPC;\n[rat:report] import org.apache.nutch.parse.ParseData;\n[rat:report] import org.apache.nutch.parse.ParseText;\n[rat:report] \n[rat:report] public class DistributedSegmentBean implements SegmentBean {\n[rat:report] \n[rat:report]   private static final ExecutorService executor =\n[rat:report]     Executors.newCachedThreadPool();\n[rat:report] \n[rat:report]   private final ScheduledExecutorService pingService;\n[rat:report] \n[rat:report]   private class DistSummmaryTask implements Callable<Summary[]> {\n[rat:report]     private int id;\n[rat:report] \n[rat:report]     private HitDetails[] details;\n[rat:report]     private Query query;\n[rat:report] \n[rat:report]     public DistSummmaryTask(int id) {\n[rat:report]       this.id = id;\n[rat:report]     }\n[rat:report] \n[rat:report]     public Summary[] call() throws Exception {\n[rat:report]       if (details == null) {\n[rat:report]         return null;\n[rat:report]       }\n[rat:report]       return beans[id].getSummary(details, query);\n[rat:report]     }\n[rat:report] \n[rat:report]     public void setSummaryArgs(HitDetails[] details, Query query) {\n[rat:report]       this.details = details;\n[rat:report]       this.query = query;\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/RPCSearchBean.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.searcher;\n[rat:report] \n[rat:report] import org.apache.hadoop.ipc.VersionedProtocol;\n[rat:report] \n[rat:report] public interface RPCSearchBean extends SearchBean, VersionedProtocol {\n[rat:report] \n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/RPCSegmentBean.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.searcher;\n[rat:report] \n[rat:report] import org.apache.hadoop.ipc.VersionedProtocol;\n[rat:report] \n[rat:report] public interface RPCSegmentBean extends SegmentBean, VersionedProtocol {\n[rat:report] \n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/SearchBean.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.searcher;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] \n[rat:report] public interface SearchBean extends Searcher, HitDetailer {\n[rat:report]   public static final Log LOG = LogFactory.getLog(SearchBean.class);\n[rat:report] \n[rat:report]   public boolean ping() throws IOException ;\n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/SegmentBean.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.searcher;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] public interface SegmentBean extends HitContent, HitSummarizer {\n[rat:report] \n[rat:report]   public String[] getSegmentNames() throws IOException;\n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/package.html\n[rat:report]  =======================================================================\n[rat:report] <html>\n[rat:report] <body>\n[rat:report] Search API\n[rat:report] </body>\n[rat:report] </html>\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/RequestUtils.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.searcher.response;\n[rat:report] \n[rat:report] import javax.servlet.http.HttpServletRequest;\n[rat:report] \n[rat:report] import org.apache.commons.lang.StringUtils;\n[rat:report] \n[rat:report] /**\n[rat:report]  * A set of utility methods for getting request paramters.\n[rat:report]  */\n[rat:report] public class RequestUtils {\n[rat:report] \n[rat:report]   public static boolean parameterExists(HttpServletRequest request, String param) {\n[rat:report]     String value = request.getParameter(param);\n[rat:report]     return value != null;\n[rat:report]   }\n[rat:report] \n[rat:report]   public static Integer getIntegerParameter(HttpServletRequest request,\n[rat:report]     String param) {\n[rat:report]     if (parameterExists(request, param)) {\n[rat:report]       String value = request.getParameter(param);\n[rat:report]       if (StringUtils.isNotBlank(value) && StringUtils.isNumeric(value)) {\n[rat:report]         return new Integer(value);\n[rat:report]       }\n[rat:report]     }\n[rat:report]     return null;\n[rat:report]   }\n[rat:report] \n[rat:report]   public static Integer getIntegerParameter(HttpServletRequest request,\n[rat:report]     String param, Integer def) {\n[rat:report]     Integer value = getIntegerParameter(request, param);\n[rat:report]     return (value == null) ? def : value;\n[rat:report]   }\n[rat:report] \n[rat:report]   public static String getStringParameter(HttpServletRequest request,\n[rat:report]     String param) {\n[rat:report]     if (parameterExists(request, param)) {\n[rat:report]       return request.getParameter(param);\n[rat:report]     }\n[rat:report]     return null;\n[rat:report]   }\n[rat:report] \n[rat:report]   public static String getStringParameter(HttpServletRequest request,\n[rat:report]     String param, String def) {\n[rat:report]     String value = getStringParameter(request, param);\n[rat:report]     return (value == null) ? def : value;\n[rat:report]   }\n[rat:report] \n[rat:report]   public static Boolean getBooleanParameter(HttpServletRequest request,\n[rat:report]     String param) {\n[rat:report]     if (parameterExists(request, param)) {\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/ResponseWriter.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.searcher.response;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] import javax.servlet.http.HttpServletRequest;\n[rat:report] import javax.servlet.http.HttpServletResponse;\n[rat:report] \n[rat:report] import org.apache.hadoop.conf.Configurable;\n[rat:report] import org.apache.nutch.plugin.Pluggable;\n[rat:report] \n[rat:report] /**\n[rat:report]  * Nutch extension point which allow writing search results in many different\n[rat:report]  * output formats.\n[rat:report]  */\n[rat:report] public interface ResponseWriter\n[rat:report]   extends Pluggable, Configurable {\n[rat:report] \n[rat:report]   public final static String X_POINT_ID = ResponseWriter.class.getName();\n[rat:report]   \n[rat:report]   /**\n[rat:report]    * Sets the returned content MIME type.  Populated though variables set in\n[rat:report]    * the plugin.xml file of the ResponseWriter.  This allows easily changing\n[rat:report]    * output content types, for example for JSON from text/plain during tesing\n[rat:report]    * and debugging to application/json in production.\n[rat:report]    * \n[rat:report]    * @param contentType The MIME content type to set.\n[rat:report]    */\n[rat:report]   public void setContentType(String contentType);\n[rat:report] \n[rat:report]   /**\n[rat:report]    * Writes out the search results response to the HttpServletResponse.\n[rat:report]    * \n[rat:report]    * @param results The SearchResults object containing hits and other info.\n[rat:report]    * @param request The HttpServletRequest object.\n[rat:report]    * @param response The HttpServletResponse object.\n[rat:report]    * \n[rat:report]    * @throws IOException If an error occurs while writing out the response.\n[rat:report]    */\n[rat:report]   public void writeResponse(SearchResults results, HttpServletRequest request,\n[rat:report]     HttpServletResponse response)\n[rat:report]     throws IOException;\n[rat:report] \n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/ResponseWriters.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.searcher.response;\n[rat:report] \n[rat:report] import java.util.HashMap;\n[rat:report] import java.util.Map;\n[rat:report] \n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.nutch.plugin.Extension;\n[rat:report] import org.apache.nutch.plugin.ExtensionPoint;\n[rat:report] import org.apache.nutch.plugin.PluginRepository;\n[rat:report] import org.apache.nutch.plugin.PluginRuntimeException;\n[rat:report] import org.apache.nutch.util.ObjectCache;\n[rat:report] \n[rat:report] /**\n[rat:report]  * Utility class for getting all ResponseWriter implementations and for\n[rat:report]  * returning the correct ResponseWriter for a given request type.\n[rat:report]  */\n[rat:report] public class ResponseWriters {\n[rat:report] \n[rat:report]   private Map<String, ResponseWriter> responseWriters;\n[rat:report] \n[rat:report]   /**\n[rat:report]    * Constructor that configures the cache of ResponseWriter objects.\n[rat:report]    * \n[rat:report]    * @param conf The Nutch configuration object.\n[rat:report]    */\n[rat:report]   public ResponseWriters(Configuration conf) {\n[rat:report] \n[rat:report]     // get the cache and the cache key\n[rat:report]     String cacheKey = ResponseWriter.class.getName();\n[rat:report]     ObjectCache objectCache = ObjectCache.get(conf);\n[rat:report]     this.responseWriters = (Map<String, ResponseWriter>)objectCache.getObject(cacheKey);\n[rat:report] \n[rat:report]     // if already populated do nothing\n[rat:report]     if (this.responseWriters == null) {\n[rat:report] \n[rat:report]       try {\n[rat:report] \n[rat:report]         // get the extension point and all ResponseWriter extensions\n[rat:report]         ExtensionPoint point = PluginRepository.get(conf).getExtensionPoint(\n[rat:report]           ResponseWriter.X_POINT_ID);\n[rat:report]         if (point == null) {\n[rat:report]           throw new RuntimeException(ResponseWriter.X_POINT_ID + \" not found.\");\n[rat:report]         }\n[rat:report] \n[rat:report]         // populate content type on the ResponseWriter classes, each response\n[rat:report]         // writer can handle more than one response type\n[rat:report]         Extension[] extensions = point.getExtensions();\n[rat:report]         Map<String, ResponseWriter> writers = new HashMap<String, ResponseWriter>();\n[rat:report]         for (int i = 0; i < extensions.length; i++) {\n[rat:report]           Extension extension = extensions[i];\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/SearchResults.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.searcher.response;\n[rat:report] \n[rat:report] import org.apache.nutch.searcher.Hit;\n[rat:report] import org.apache.nutch.searcher.HitDetails;\n[rat:report] import org.apache.nutch.searcher.Summary;\n[rat:report] \n[rat:report] public class SearchResults {\n[rat:report] \n[rat:report]   private String[] fields;\n[rat:report]   private String responseType;\n[rat:report]   private String query;\n[rat:report]   private String lang;\n[rat:report]   private String sort;\n[rat:report]   private boolean reverse;\n[rat:report]   private boolean withSummary = true;\n[rat:report]   private int start;\n[rat:report]   private int rows;\n[rat:report]   private int end;\n[rat:report]   private long totalHits;\n[rat:report]   private Hit[] hits;\n[rat:report]   private HitDetails[] details;\n[rat:report]   private Summary[] summaries;\n[rat:report] \n[rat:report]   public SearchResults() {\n[rat:report] \n[rat:report]   }\n[rat:report] \n[rat:report]   public String[] getFields() {\n[rat:report]     return fields;\n[rat:report]   }\n[rat:report] \n[rat:report]   public void setFields(String[] fields) {\n[rat:report]     this.fields = fields;\n[rat:report]   }\n[rat:report] \n[rat:report]   public boolean isWithSummary() {\n[rat:report]     return withSummary;\n[rat:report]   }\n[rat:report] \n[rat:report]   public void setWithSummary(boolean withSummary) {\n[rat:report]     this.withSummary = withSummary;\n[rat:report]   }\n[rat:report] \n[rat:report]   public String getResponseType() {\n[rat:report]     return responseType;\n[rat:report]   }\n[rat:report] \n[rat:report]   public void setResponseType(String responseType) {\n[rat:report]     this.responseType = responseType;\n[rat:report]   }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/searcher/response/SearchServlet.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.searcher.response;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] import javax.servlet.ServletConfig;\n[rat:report] import javax.servlet.ServletException;\n[rat:report] import javax.servlet.http.HttpServlet;\n[rat:report] import javax.servlet.http.HttpServletRequest;\n[rat:report] import javax.servlet.http.HttpServletResponse;\n[rat:report] \n[rat:report] import org.apache.commons.lang.StringUtils;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.nutch.searcher.Hit;\n[rat:report] import org.apache.nutch.searcher.HitDetails;\n[rat:report] import org.apache.nutch.searcher.Hits;\n[rat:report] import org.apache.nutch.searcher.NutchBean;\n[rat:report] import org.apache.nutch.searcher.Query;\n[rat:report] import org.apache.nutch.searcher.Summary;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] \n[rat:report] /**\n[rat:report]  * Servlet that allows returning search results in multiple different formats\n[rat:report]  * through a ResponseWriter Nutch extension point.\n[rat:report]  * \n[rat:report]  * @see org.apache.nutch.searcher.response.ResponseWriter\n[rat:report]  */\n[rat:report] public class SearchServlet\n[rat:report]   extends HttpServlet {\n[rat:report] \n[rat:report]   public static final Log LOG = LogFactory.getLog(SearchServlet.class);\n[rat:report]   private NutchBean bean;\n[rat:report]   private Configuration conf;\n[rat:report]   private ResponseWriters writers;\n[rat:report] \n[rat:report]   private String defaultRespType = \"xml\";\n[rat:report]   private String defaultLang = null;\n[rat:report]   private int defaultNumRows = 10;\n[rat:report]   private String defaultDedupField = \"site\";\n[rat:report]   private int defaultNumDupes = 1;\n[rat:report] \n[rat:report]   public static final String RESPONSE_TYPE = \"rt\";\n[rat:report]   public static final String QUERY = \"query\";\n[rat:report]   public static final String LANG = \"lang\";\n[rat:report]   public static final String START = \"start\";\n[rat:report]   public static final String ROWS = \"rows\";\n[rat:report]   public static final String SORT = \"sort\";\n[rat:report]   public static final String REVERSE = \"reverse\";\n[rat:report]   public static final String DEDUPE = \"ddf\";\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/segment/ContentAsTextInputFormat.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.segment;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.mapred.FileSplit;\n[rat:report] import org.apache.hadoop.mapred.InputSplit;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.RecordReader;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileRecordReader;\n[rat:report] import org.apache.nutch.protocol.Content;\n[rat:report] \n[rat:report] /**\n[rat:report]  * An input format that takes Nutch Content objects and converts them to text\n[rat:report]  * while converting newline endings to spaces.  This format is useful for working\n[rat:report]  * with Nutch content objects in Hadoop Streaming with other languages.\n[rat:report]  */\n[rat:report] public class ContentAsTextInputFormat\n[rat:report]   extends SequenceFileInputFormat<Text, Text> {\n[rat:report] \n[rat:report]   private static class ContentAsTextRecordReader\n[rat:report]     implements RecordReader<Text, Text> {\n[rat:report] \n[rat:report]     private final SequenceFileRecordReader<Text, Content> sequenceFileRecordReader;\n[rat:report] \n[rat:report]     private Text innerKey;\n[rat:report]     private Content innerValue;\n[rat:report] \n[rat:report]     public ContentAsTextRecordReader(Configuration conf, FileSplit split)\n[rat:report]       throws IOException {\n[rat:report]       sequenceFileRecordReader = new SequenceFileRecordReader<Text, Content>(\n[rat:report]         conf, split);\n[rat:report]       innerKey = (Text)sequenceFileRecordReader.createKey();\n[rat:report]       innerValue = (Content)sequenceFileRecordReader.createValue();\n[rat:report]     }\n[rat:report] \n[rat:report]     public Text createKey() {\n[rat:report]       return new Text();\n[rat:report]     }\n[rat:report] \n[rat:report]     public Text createValue() {\n[rat:report]       return new Text();\n[rat:report]     }\n[rat:report] \n[rat:report]     public synchronized boolean next(Text key, Text value)\n[rat:report]       throws IOException {\n[rat:report]       \n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/ResolveUrls.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.tools;\n[rat:report] \n[rat:report] import java.io.BufferedReader;\n[rat:report] import java.io.File;\n[rat:report] import java.io.FileReader;\n[rat:report] import java.net.InetAddress;\n[rat:report] import java.util.concurrent.ExecutorService;\n[rat:report] import java.util.concurrent.Executors;\n[rat:report] import java.util.concurrent.TimeUnit;\n[rat:report] import java.util.concurrent.atomic.AtomicInteger;\n[rat:report] import java.util.concurrent.atomic.AtomicLong;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.nutch.util.URLUtil;\n[rat:report] \n[rat:report] /**\n[rat:report]  * A simple tool that will spin up multiple threads to resolve urls to ip\n[rat:report]  * addresses. This can be used to verify that pages that are failing due to\n[rat:report]  * UnknownHostException during fetching are actually bad and are not failing due\n[rat:report]  * to a dns problem in fetching.\n[rat:report]  */\n[rat:report] public class ResolveUrls {\n[rat:report] \n[rat:report]   public static final Log LOG = LogFactory.getLog(ResolveUrls.class);\n[rat:report] \n[rat:report]   private String urlsFile = null;\n[rat:report]   private int numThreads = 100;\n[rat:report]   private ExecutorService pool = null;\n[rat:report]   private static AtomicInteger numTotal = new AtomicInteger(0);\n[rat:report]   private static AtomicInteger numErrored = new AtomicInteger(0);\n[rat:report]   private static AtomicInteger numResolved = new AtomicInteger(0);\n[rat:report]   private static AtomicLong totalTime = new AtomicLong(0L);\n[rat:report] \n[rat:report]   /**\n[rat:report]    * A Thread which gets the ip address of a single host by name.\n[rat:report]    */\n[rat:report]   private static class ResolverThread\n[rat:report]     extends Thread {\n[rat:report] \n[rat:report]     private String url = null;\n[rat:report] \n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/SearchLoadTester.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.tools;\n[rat:report] \n[rat:report] import java.io.BufferedReader;\n[rat:report] import java.io.File;\n[rat:report] import java.io.FileReader;\n[rat:report] import java.io.IOException;\n[rat:report] import java.util.concurrent.ExecutorService;\n[rat:report] import java.util.concurrent.Executors;\n[rat:report] import java.util.concurrent.TimeUnit;\n[rat:report] import java.util.concurrent.atomic.AtomicInteger;\n[rat:report] import java.util.concurrent.atomic.AtomicLong;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.nutch.searcher.Hits;\n[rat:report] import org.apache.nutch.searcher.NutchBean;\n[rat:report] import org.apache.nutch.searcher.Query;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] \n[rat:report] /**\n[rat:report]  * <p>A simple tool to perform load testing on configured search servers.  A \n[rat:report]  * queries file can be specified with a list of different queries to run against\n[rat:report]  * the search servers.  The number of threads used to perform concurrent\n[rat:report]  * searches is also configurable.</p>\n[rat:report]  * \n[rat:report]  * <p>This tool will output approximate times for running all queries in the \n[rat:report]  * queries file.  If configured it will also print out individual queries times\n[rat:report]  * to the log.</p>\n[rat:report]  */\n[rat:report] public class SearchLoadTester {\n[rat:report] \n[rat:report]   public static final Log LOG = LogFactory.getLog(SearchLoadTester.class);\n[rat:report] \n[rat:report]   private String queriesFile = null;\n[rat:report]   private int numThreads = 100;\n[rat:report]   private boolean showTimes = false;\n[rat:report]   private ExecutorService pool = null;\n[rat:report]   private static AtomicInteger numTotal = new AtomicInteger(0);\n[rat:report]   private static AtomicInteger numErrored = new AtomicInteger(0);\n[rat:report]   private static AtomicInteger numResolved = new AtomicInteger(0);\n[rat:report]   private static AtomicLong totalTime = new AtomicLong(0L);\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/tools/compat/ReprUrlFixer.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.tools.compat;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] import java.net.MalformedURLException;\n[rat:report] import java.net.URL;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.Iterator;\n[rat:report] import java.util.List;\n[rat:report] import java.util.Random;\n[rat:report] \n[rat:report] import org.apache.commons.cli.CommandLine;\n[rat:report] import org.apache.commons.cli.CommandLineParser;\n[rat:report] import org.apache.commons.cli.GnuParser;\n[rat:report] import org.apache.commons.cli.HelpFormatter;\n[rat:report] import org.apache.commons.cli.Option;\n[rat:report] import org.apache.commons.cli.OptionBuilder;\n[rat:report] import org.apache.commons.cli.Options;\n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.conf.Configured;\n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.MapWritable;\n[rat:report] import org.apache.hadoop.io.Text;\n[rat:report] import org.apache.hadoop.io.WritableUtils;\n[rat:report] import org.apache.hadoop.mapred.FileInputFormat;\n[rat:report] import org.apache.hadoop.mapred.FileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.JobClient;\n[rat:report] import org.apache.hadoop.mapred.JobConf;\n[rat:report] import org.apache.hadoop.mapred.MapFileOutputFormat;\n[rat:report] import org.apache.hadoop.mapred.OutputCollector;\n[rat:report] import org.apache.hadoop.mapred.Reducer;\n[rat:report] import org.apache.hadoop.mapred.Reporter;\n[rat:report] import org.apache.hadoop.mapred.SequenceFileInputFormat;\n[rat:report] import org.apache.hadoop.util.StringUtils;\n[rat:report] import org.apache.hadoop.util.Tool;\n[rat:report] import org.apache.hadoop.util.ToolRunner;\n[rat:report] import org.apache.nutch.crawl.CrawlDatum;\n[rat:report] import org.apache.nutch.crawl.CrawlDb;\n[rat:report] import org.apache.nutch.metadata.Nutch;\n[rat:report] import org.apache.nutch.scoring.webgraph.Node;\n[rat:report] import org.apache.nutch.util.FSUtils;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] import org.apache.nutch.util.NutchJob;\n[rat:report] import org.apache.nutch.util.URLUtil;\n[rat:report] \n[rat:report] /**\n[rat:report]  * <p>\n[rat:report]  * Significant changes were made to representative url logic used for redirects.\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/EncodingDetector.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.util;\n[rat:report] \n[rat:report] import java.io.BufferedInputStream;\n[rat:report] import java.io.ByteArrayOutputStream;\n[rat:report] import java.io.FileInputStream;\n[rat:report] import java.io.IOException;\n[rat:report] import java.nio.charset.Charset;\n[rat:report] import java.util.ArrayList;\n[rat:report] import java.util.HashMap;\n[rat:report] import java.util.HashSet;\n[rat:report] import java.util.List;\n[rat:report] \n[rat:report] import org.apache.commons.logging.Log;\n[rat:report] import org.apache.commons.logging.LogFactory;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.nutch.metadata.Metadata;\n[rat:report] import org.apache.nutch.net.protocols.Response;\n[rat:report] import org.apache.nutch.protocol.Content;\n[rat:report] import org.apache.nutch.util.LogUtil;\n[rat:report] import org.apache.nutch.util.NutchConfiguration;\n[rat:report] \n[rat:report] import com.ibm.icu.text.CharsetDetector;\n[rat:report] import com.ibm.icu.text.CharsetMatch;\n[rat:report] \n[rat:report] /**\n[rat:report]  * A simple class for detecting character encodings.\n[rat:report]  *\n[rat:report]  * <p>\n[rat:report]  * Broadly this encompasses two functions, which are distinctly separate:\n[rat:report]  *\n[rat:report]  * <ol>\n[rat:report]  *  <li>Auto detecting a set of \"clues\" from input text.</li>\n[rat:report]  *  <li>Taking a set of clues and making a \"best guess\" as to the\n[rat:report]  *      \"real\" encoding.</li>\n[rat:report]  * </ol>\n[rat:report]  * </p>\n[rat:report]  *\n[rat:report]  * <p>\n[rat:report]  * A caller will often have some extra information about what the\n[rat:report]  * encoding might be (e.g. from the HTTP header or HTML meta-tags, often\n[rat:report]  * wrong but still potentially useful clues). The types of clues may differ\n[rat:report]  * from caller to caller. Thus a typical calling sequence is:\n[rat:report]  * <ul>\n[rat:report]  *    <li>Run step (1) to generate a set of auto-detected clues;</li>\n[rat:report]  *    <li>Combine these clues with the caller-dependent \"extra clues\"\n[rat:report]  *        available;</li>\n[rat:report]  *    <li>Run step (2) to guess what the most probable answer is.</li>\n[rat:report]  * </p>\n[rat:report]  */\n[rat:report] public class EncodingDetector {\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/FSUtils.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.util;\n[rat:report] \n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] import org.apache.hadoop.fs.FileSystem;\n[rat:report] import org.apache.hadoop.fs.Path;\n[rat:report] import org.apache.hadoop.io.MapFile;\n[rat:report] import org.apache.hadoop.io.SequenceFile;\n[rat:report] \n[rat:report] /**\n[rat:report]  * Utility methods for common filesystem operations.\n[rat:report]  */\n[rat:report] public class FSUtils {\n[rat:report] \n[rat:report]   /**\n[rat:report]    * Replaces the current path with the new path and if set removes the old\n[rat:report]    * path. If removeOld is set to false then the old path will be set to the\n[rat:report]    * name current.old.\n[rat:report]    * \n[rat:report]    * @param fs The FileSystem.\n[rat:report]    * @param current The end path, the one being replaced.\n[rat:report]    * @param replacement The path to replace with.\n[rat:report]    * @param removeOld True if we are removing the current path.\n[rat:report]    * \n[rat:report]    * @throws IOException If an error occurs during replacement.\n[rat:report]    */\n[rat:report]   public static void replace(FileSystem fs, Path current, Path replacement,\n[rat:report]     boolean removeOld)\n[rat:report]     throws IOException {\n[rat:report] \n[rat:report]     // rename any current path to old\n[rat:report]     Path old = new Path(current + \".old\");\n[rat:report]     if (fs.exists(current)) {\n[rat:report]       fs.rename(current, old);\n[rat:report]     }\n[rat:report] \n[rat:report]     // rename the new path to current and remove the old path if needed\n[rat:report]     fs.rename(replacement, current);\n[rat:report]     if (fs.exists(old) && removeOld) {\n[rat:report]       fs.delete(old, true);\n[rat:report]     }\n[rat:report]   }\n[rat:report] \n[rat:report]   /**\n[rat:report]    * Closes a group of SequenceFile readers.\n[rat:report]    * \n[rat:report]    * @param readers The SequenceFile readers to close.\n[rat:report]    * @throws IOException If an error occurs while closing a reader.\n[rat:report]    */\n[rat:report]   public static void closeReaders(SequenceFile.Reader[] readers)\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/GenericWritableConfigurable.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.util;\n[rat:report] \n[rat:report] import java.io.DataInput;\n[rat:report] import java.io.IOException;\n[rat:report] \n[rat:report] import org.apache.hadoop.conf.Configurable;\n[rat:report] import org.apache.hadoop.conf.Configuration;\n[rat:report] import org.apache.hadoop.io.GenericWritable;\n[rat:report] import org.apache.hadoop.io.Writable;\n[rat:report] \n[rat:report] /** A generic Writable wrapper that can inject Configuration to {@link Configurable}s */ \n[rat:report] public abstract class GenericWritableConfigurable extends GenericWritable \n[rat:report]                                                   implements Configurable {\n[rat:report] \n[rat:report]   private Configuration conf;\n[rat:report]   \n[rat:report]   public Configuration getConf() {\n[rat:report]     return conf;\n[rat:report]   }\n[rat:report] \n[rat:report]   public void setConf(Configuration conf) {\n[rat:report]     this.conf = conf;\n[rat:report]   }\n[rat:report]   \n[rat:report]   @Override\n[rat:report]   public void readFields(DataInput in) throws IOException {\n[rat:report]     byte type = in.readByte();\n[rat:report]     Class clazz = getTypes()[type];\n[rat:report]     try {\n[rat:report]       set((Writable) clazz.newInstance());\n[rat:report]     } catch (Exception e) {\n[rat:report]       e.printStackTrace();\n[rat:report]       throw new IOException(\"Cannot initialize the class: \" + clazz);\n[rat:report]     }\n[rat:report]     Writable w = get();\n[rat:report]     if (w instanceof Configurable)\n[rat:report]       ((Configurable)w).setConf(conf);\n[rat:report]     w.readFields(in);\n[rat:report]   }\n[rat:report]   \n[rat:report] }\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/NodeWalker.java\n[rat:report]  =======================================================================\n[rat:report] package org.apache.nutch.util;\n[rat:report] \n[rat:report] import java.util.Stack;\n[rat:report] \n[rat:report] import org.w3c.dom.Node;\n[rat:report] import org.w3c.dom.NodeList;\n[rat:report] \n[rat:report] /**\n[rat:report]  * <p>A utility class that allows the walking of any DOM tree using a stack \n[rat:report]  * instead of recursion.  As the node tree is walked the next node is popped\n[rat:report]  * off of the stack and all of its children are automatically added to the \n[rat:report]  * stack to be called in tree order.</p>\n[rat:report]  * \n[rat:report]  * <p>Currently this class is not thread safe.  It is assumed that only one\n[rat:report]  * thread will be accessing the <code>NodeWalker</code> at any given time.</p>\n[rat:report]  */\n[rat:report] public class NodeWalker {\n[rat:report] \n[rat:report]   // the root node the the stack holding the nodes\n[rat:report]   private Node currentNode;\n[rat:report]   private NodeList currentChildren;\n[rat:report]   private Stack<Node> nodes;\n[rat:report]   \n[rat:report]   /**\n[rat:report]    * Starts the <code>Node</code> tree from the root node.\n[rat:report]    * \n[rat:report]    * @param rootNode\n[rat:report]    */\n[rat:report]   public NodeWalker(Node rootNode) {\n[rat:report] \n[rat:report]     nodes = new Stack<Node>();\n[rat:report]     nodes.add(rootNode);\n[rat:report]   }\n[rat:report]   \n[rat:report]   /**\n[rat:report]    * <p>Returns the next <code>Node</code> on the stack and pushes all of its\n[rat:report]    * children onto the stack, allowing us to walk the node tree without the\n[rat:report]    * use of recursion.  If there are no more nodes on the stack then null is\n[rat:report]    * returned.</p>\n[rat:report]    * \n[rat:report]    * @return Node The next <code>Node</code> on the stack or null if there\n[rat:report]    * isn't a next node.\n[rat:report]    */\n[rat:report]   public Node nextNode() {\n[rat:report]     \n[rat:report]     // if no next node return null\n[rat:report]     if (!hasNext()) {\n[rat:report]       return null;\n[rat:report]     }\n[rat:report]     \n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/org/apache/nutch/util/domain/package.html\n[rat:report]  =======================================================================\n[rat:report] <html>\n[rat:report] <body>\n[rat:report] <h2> org.apache.nutch.util.domain</h2>\n[rat:report] \n[rat:report] <p>This package contains classes for domain analysis.</p>\n[rat:report] \n[rat:report] for information please refer to following urls : \n[rat:report] <ul>\n[rat:report] <li><a href=\"http://en.wikipedia.org/wiki/DNS\">http://en.wikipedia.org/wiki/DNS</a></li>\n[rat:report] <li><a href=\"http://en.wikipedia.org/wiki/Top-level_domain\">http://en.wikipedia.org/wiki/Top-level_domain</a></li>\n[rat:report] <li><a href=\"http://wiki.mozilla.org/TLD_List\">http://wiki.mozilla.org/TLD_List</a></li>\n[rat:report] <li><a href=\"http://publicsuffix.org/\">http://publicsuffix.org/</a></li>\n[rat:report] </ul>\n[rat:report] \n[rat:report] </body>\n[rat:report] </html>\n[rat:report] \n[rat:report]  =======================================================================\n[rat:report]  ==/home/sam/workspace/nutch-trunk-eu/src/java/overview.html\n[rat:report]  =======================================================================\n[rat:report] <html>\n[rat:report] <head>\n[rat:report]    <title>Nutch</title>\n[rat:report] </head>\n[rat:report] <body>\n[rat:report] Nutch is the open-source search engine.<p>\n[rat:report] </body>\n[rat:report] </html>\n[rat:report] \n\nBUILD SUCCESSFUL\nTotal time: 1 second", "I think we are done with this.", "Integrated in Nutch-trunk #729 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/729/])\n     add missing headers, part 2 rest\n add missing headers, part 1 core\n", "closing issues for released version"], "tasks": {"summary": "Fix missing/wrong headers in source files", "classification": "bug", "qa_pairs": []}}
{"issue_id": "NUTCH-689", "project": "NUTCH", "title": "Swf parser doesn't seem to handle relative links", "status": "Closed", "priority": "Major", "reporter": "Peter Sparks", "assignee": null, "created": "2009-02-17T20:54:37.999+0000", "updated": "2011-04-01T15:26:38.179+0000", "description": "I was using the swf parser to extract links from flash files on the site www.arnoldworldwide.com and I was getting an malformed url exception because an outlink was found and it was a relative link that wasn't being resolved. I was able to fix it by resolving all links as they are added to the list of outlinks.", "comments": ["I'm new to both open source and nutch so I'm not sure of the right way to submit corrections... but anyways, I have attached my fix to SwfParser.java here.", "about development: check url http://wiki.apache.org/nutch/Becoming%20A%20Nutch%20Developer for instructions about developing nutch, in particular section \"Step Three: Using the JIRA and Developing\"\n\nYou should attach pacthes instead of full java source files because it's much easier to see what changed by looking at diffs.", "Here is the patch.", "for some reason I cannot apply the patch:\n\npatching file src/java/org/apache/nutch/parse/swf/SWFParser.java\nHunk #2 FAILED at 94.\n\n", "I used Tortoise SVN 1.5.2 to make the patch... and I just tested it by getting a fresh version of the 0.9 branch from subversion and then applying the patch myself and it worked for me, so I don't know what to say. Is there maybe an issue because I'm using Tortoise?", "Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Swf parser doesn't seem to handle relative links", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Swf parser doesn't seem to handle relative links"}, {"question": "What is the main context?", "answer": "I was using the swf parser to extract links from flash files on the site www.arnoldworldwide.com and I was getting an malformed url exception because an outlink was found and it was a relative link th"}]}}
{"issue_id": "NUTCH-69", "project": "NUTCH", "title": "fetcher.threads.per.host ignored", "status": "Closed", "priority": "Major", "reporter": "Matthias Jaekle", "assignee": null, "created": "2005-07-08T23:18:40.000+0000", "updated": "2007-04-18T15:46:16.264+0000", "description": "Fetcher ignores 'maximum threads per host'.\nIf you fetch less domains with multiple threads, some webservers feel attacked or could not serve you any more.\nSo you loose lots of existing pages in your segments.", "comments": ["This behaviour is caused by improper configuration. When crawling less hosts than (fetcher threads / threads per host), some threads will always be blocked. Solution: change configuration to use less threads, or more threads per host, or increase the max.http.delay so that blocked threads would wait longer.."], "tasks": {"summary": "fetcher.threads.per.host ignored", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "fetcher.threads.per.host ignored"}, {"question": "What is the main context?", "answer": "Fetcher ignores 'maximum threads per host'.\nIf you fetch less domains with multiple threads, some webservers feel attacked or could not serve you any more.\nSo you loose lots of existing pages in your "}]}}
{"issue_id": "NUTCH-690", "project": "NUTCH", "title": "bug in DomContentUtils.shouldThrowAwayLink?", "status": "Open", "priority": "Minor", "reporter": "Peter Sparks", "assignee": null, "created": "2009-02-17T21:08:24.767+0000", "updated": "2025-07-09T20:25:51.610+0000", "description": "I found a potential bug in DomContentUtils.shouldThrowAwayLink. It returns true for the 5 links at the top of the home page for www.aksteel.com. Here are the links in the source:\n            <a id=\"Search\" href=\"/search/default.aspx\"></a>\n            <a id=\"Investor\" style=\"height: 15px;\" href=\"/investor_information/\"></a>\n            <a id=\"Markets\" href=\"/markets_products/\"></a>\n            <a id=\"Production\" href=\"/production_facilities/\"></a>\n            <a id=\"News\" href=\"/news/\"></a>\nPerhaps I am just ignorant of what this function is supposed to do but returning true for these 5 links on that site make that site impossible to crawl.", "comments": ["I see these url's as outlinks with 1.4-dev and parse-tika and also using 1.3 and parse-html. Is this still a valid issue?"], "tasks": {"summary": "bug in DomContentUtils.shouldThrowAwayLink?", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "bug in DomContentUtils.shouldThrowAwayLink?"}, {"question": "What is the main context?", "answer": "I found a potential bug in DomContentUtils.shouldThrowAwayLink. It returns true for the 5 links at the top of the home page for www.aksteel.com. Here are the links in the source:\n            <a id=\"Se"}]}}
{"issue_id": "NUTCH-691", "project": "NUTCH", "title": "Update jakarta poi jars to the most relevant version", "status": "Closed", "priority": "Major", "reporter": "Dmitry Lihachev", "assignee": null, "created": "2009-02-18T04:31:28.084+0000", "updated": "2009-04-10T12:29:05.292+0000", "description": "Update  jakarta poi jars to the most relevant version closes bug NUTCH-591.", "comments": ["Steps to reproduce NUTCH-591 (you must have trunk version of nutch in the nutch dir)\n\n{code}\ncd nutch\nwget https://issues.apache.org/jira/secure/attachment/12400380/NUTCH-691-v1-test.patch\npatch -p0 < NUTCH-691-v1-test.patch\nrm NUTCH-691-v1-test.patch\nwget https://issues.apache.org/bugzilla/attachment.cgi?id=19200 \\\n     -O src/plugin/parse-msword/sample/bugzilla_bug_41076_att_19200.doc\nwget https://issues.apache.org/bugzilla/attachment.cgi?id=22957 \\\n     -O src/plugin/parse-msword/sample/bugzilla_bug_41076_att_22957.doc\nant clean compile-core\ncd src/plugin/parse-msword/; ant clean test; cd ../../..\n{code}\n\nWith new version of jakarta poi this test passes.\nSteps to upgrade jakarta poi to 3.5 beta 4.\n\n1. download new version of POI and update jars in src/plugin/lib-jakarta-poi\n{code}\nwget http://www.sai.msu.su/apache/poi/dev/bin/poi-bin-3.5-beta4-20081128.tar.gz\ntar -xzf poi-bin-3.5-beta4-20081128.tar.gz\ncp poi-3.5-beta4/poi{,-scratchpad}-3.5-beta4-20081128.jar src/plugin/lib-jakarta-poi/lib/\nrm src/plugin/lib-jakarta-poi/lib/poi{,-scratchpad}-3.0-alpha1-20050704.jar\n# cleanup downloaded stuff\nrm poi-bin-3.5-beta4-20081128.tar.gz\nrm -r poi-3.5-beta4\n{code}\n\n2. API has changed in new version of POI so we need some patching [^NUTCH-691-v1-poi.patch]\n{code}\nwget https://issues.apache.org/jira/secure/attachment/12400378/NUTCH-691-v1-poi.patch\npatch -p0 < NUTCH-691-v1-poi.patch\nrm NUTCH-691-v1-poi.patch\n{code}\n\n3. Ensure that *.doc files are read properly.\n{code}\ncd src/plugin/lib-jakarta-poi/; ant clean; cd ../../..\ncd src/plugin/parse-msword/; ant clean test; cd ../../..\n{code}", "committed, Thanks Dmitry", "Integrated in Nutch-trunk #729 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/729/])\n     - Update jakarta poi jars to the most relevant version, contributed by Dmitry Lihachev\n", "closing issues for released version"], "tasks": {"summary": "Update jakarta poi jars to the most relevant version", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Update jakarta poi jars to the most relevant version"}, {"question": "What is the main context?", "answer": "Update  jakarta poi jars to the most relevant version closes bug NUTCH-591."}]}}
{"issue_id": "NUTCH-692", "project": "NUTCH", "title": "AlreadyBeingCreatedException with Hadoop 0.19", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2009-02-18T12:30:19.227+0000", "updated": "2011-04-13T23:48:08.666+0000", "description": "I have been using the SVN version of Nutch on an EC2 cluster and got some AlreadyBeingCreatedException during the reduce phase of a parse. For some reason one of my tasks crashed and then I ran into this AlreadyBeingCreatedException when other nodes tried to pick it up.\n\nThere was recently a discussion on the Hadoop user list on similar issues with Hadoop 0.19 (see http://markmail.org/search/after+upgrade+to+0%2E19%2E0). I have not tried using 0.18.2 yet but will do if the problems persist with 0.19\n\nI was wondering whether anyone else had experienced the same problem. Do you think 0.19 is stable enough to use it for Nutch 1.0?\nI will be running a crawl on a super large cluster in the next couple of weeks and I will confirm this issue  \n\nJ.  \n\n", "comments": ["Have you seen this outside of EC2? Only in multinode setup?", "I have seen this only in multinode setup and on EC2.", "I have been investigating this a bit more. Same problem : some reduce tasks fail during the parsing and when the mapred.task.timeout is reached the new tasks can't get a lease for the files and we get the AlreadyBeingCreatedException. \n\nThis is clearly a Hadoop issue; I have not tried with a previous version and don't know whether this will be fixed in the 0.19.1 release. Could this be due to the fact that the RecordWriter in ParseOutputFormat holds multiple Writers internally?\n\nI had a look at the other side of the problem and found that for some documents the tasks were blocking on : \n\n\tat org.apache.oro.text.regex.Util.substitute(Unknown Source)\n\tat org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer.substituteUnnecessaryRelativePaths(BasicURLNormalizer.java:166)\n\tat org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer.normalize(BasicURLNormalizer.java:125)\n\tat org.apache.nutch.net.URLNormalizers.normalize(URLNormalizers.java:286)\n\tat org.apache.nutch.parse.ParseOutputFormat$1.write(ParseOutputFormat.java:223)\n\tat org.apache.nutch.parse.ParseOutputFormat$1.write(ParseOutputFormat.java:114)\n\nand the following regex used in the regexurlfilter \n\n-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/\n\nI haven't dumped the  actual URLS in the logs but I suspect that they come from the JSParser. I will remove both the regex-urlfilter and the BasicURLNormalizer and see what I get.\n\nJ.\n\n\n\n", "The AlreadyBeingCreatedException appears when a reduce task fails at a first attempt and leaves the output files open for the next. I have a patch for it. The reduce task won't stop with an AlreadyBeingCreatedException on the second run. However this is sometimes caused by other bugs - on of them being the regexp match hang caused by a Java Regex bug and even if you won't get the AlreadyBeingCreatedException you still need to deal with the regexp infinite loop. ", "As I pointed out in my previous message the root of the problem in my case was related to some dodgy URLs coming from the Javascript parser which put the basic normalizer into a spin. This would repeat in subsequent attempts indeed.\n\nHowever the AlreadyBeingCreatedException should not happen and we should not have output files left open. If you patch fixes that I am sure that this will be a very welcome contribution.", "This just checks the destination file existence before attempting to create a new output MapFile for the reduce task in the FetcherOutputFormat and ParseOutputFormat. If the destination files exist it deletes them. \nThe AlreadyBeingCreatedException is thrown when a MapFile creation attempt fails to create the same file as the previous failed task. \n", "Thanks for the patch.\n\nPatch looks good to me. Can you confirm if this fixes the problem (or tell me how to trigger the problem without patch)?", "The AlreadyBeingCreatedException appears at the second attempt for a reduce task. In our case, in the reduce phase of the fetch process sometimes we had hanging processes that initially reported something like \"Task attempt_200903031109_0007_r_000002_0 failed to report status for 603 seconds. Killing!\". When the task is retried the ParseOutputFormat, FetcherOutputFormat try to create the MapFile, but this already exists so the task fails again with AlreadyBeingCreatedException and there's no way to recover unless the files are deleted. \n\nThe patch fixes the issue with a second reduce attempt, and yes, it works. Without the patch there will be no second reduce attempt since it will stop at \nnew MapFile.Writer(job,...) with the AlreadyBeingCreatedException\n\nIt's not trivial to reproduce the \"failed to report status for X seconds. Killing!\" problem, unless you have some bad regexp to feed the crawler with :). However I believe it could be reproduced by stopping and starting the tasktracker with hadoop-daemon.sh stop/start tasktracker. \n\nAnother way to reproduce just the HDFS exception is to try to create the same file twice. \n \n However it should be known that there are many reasons for \"failed to report status for 603 seconds. Killing!\". One of them is due to the regex problem stated above when the regex.match process loops forever taking 100% of the CPU. If for some reason the reduce task will hit a problem like this this patch won't help much, except that it will let the reduce phase go through the entire reducer process again and not fail when it starts. \n\n    ", "setting mapred.task.timeout to a small value (e.g. 60000) and trying to parse a page containing a stupidly long link, with for instance 2000 \\ chars should be sufficient to put the basic-normalizer into trouble during a parsing. This should cause the task to fail and illustrate the issue", "Julien, have you tried it with the patch? Can you confirm the behavior with a unpatched/patched nutch?", "I haven't had the time to try it on the SVN version.  Will try to do so when I have more time. Thanks!", "OK I had the same problem again on my main cluster, one of the nodes lost contact with the master during a parsing and the subsequent attempts failed with AlreadyBeingCreatedException.\n\nI managed to reproduce the problem locally using a fresh copy from SVN by hacking  the BasicURLNormalizer to make it sleep for 5 mins everytime it gets a URL, which gave me plenty of time to fail a reduce task with \n\n./hadoop job -fail-task attempt_200904241525_0007_r_000000_0\n\nas expected the following attempts failed with AlreadyBeingCreatedException.\n\nI did the same experiment using your patch and can confirm that it solves the problem. \n\nThanks\n\nJ.", "Sorry for the late answer.\n\nSince Julien confirmed that it fixes the problem, I will commit this patch if there are no objections.", "I've been using this patch for a while now and confirm that it fixes the problem. Could someone have a look at it and commit it so that we can close this issue?\nThanks\nJ.", "We should review this issue after the upgrade to Hadoop 0.20 - task output mgmt differs there, and the problem may be nonexistent.", "Ok let's leave it open for now", "I cannot reproduce the issue since we moved to the Hadoop 0.20., which is good news", "Closing all resolved issues with a non-fixed status."], "tasks": {"summary": "AlreadyBeingCreatedException with Hadoop 0.19", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "AlreadyBeingCreatedException with Hadoop 0.19"}, {"question": "What is the main context?", "answer": "I have been using the SVN version of Nutch on an EC2 cluster and got some AlreadyBeingCreatedException during the reduce phase of a parse. For some reason one of my tasks crashed and then I ran into t"}]}}
{"issue_id": "NUTCH-693", "project": "NUTCH", "title": "Add configurable option for treating nofollow behaviour.", "status": "Closed", "priority": "Minor", "reporter": "Andrew McCall", "assignee": null, "created": "2009-02-18T20:58:16.182+0000", "updated": "2013-08-23T20:21:44.752+0000", "description": "For my purposes I'd like to follow links even if they're marked nofollow- Ideally I'd like to follow them, but not pass the link juice between them. \n\nI've attached a patch that adds a configuration element parser.html.outlinks.ignore_nofollow which allows the parser to ignore the nofollow elements on a page. ", "comments": ["Here is the patch.", "I think I see some formatting that's a bit off (looks off in the patch itself at least), but more importantly, is everyone OK with allowing this behaviour?\n\n+1 from me -- let the operators decide.\n", "This patch is controversial in the sense that a) Nutch strives to adhere to Internet standards and netiquette, which says that robots should obey nofollow, and b) most Nutch users want a well-behaved robot. You are free of course to modify the source as you did. Therefore I think that this functionality is not applicable to majority of Nutch users, and I vote -1 on including it in Nutch.", "[http://en.wikipedia.org/wiki/Nofollow]\n\nI don't think there is really any consensus on this standard to be honest. Most search engines don't index no-follow links per se, but they do follow them for crawling. Even Google, who first proposed the nofollow, sometimes actually do follow according to some tests linked in the wikipedia article. The results show that if the link is already in the index (eg has been followed elsewhere) then it does get followed and indexed. \n\nThe nofollow is really just a keyword to point out that the link isn't being endorsed by the author - It's more a content guideline than a strict order for robots to obey. So I disagree that you're breaking standards or creating a robot that's not well behaved by ignoring it. \n\nI would have liked to have done a bit more with this so that I could have respected nofollows, but injected the URL as a brand new seed URL but other commitments took over and I never got around to it. Since the ideal nofollow behaviour is somewhere between ignoring them and not ignoring them I figured the option to ignore them was a good start and submitted the patch, but I'm not precious about it.", "Thanks for the pointer to the article. Indeed, the issue is muddy at best. So far Nutch adhered to a strict interpretation, where the links with this attribute are deleted from page outlinks immediately (so they are not only not followed but also don't affect out-degree metrics). If there is a general agreement in Nutch community towards relaxing this behavior we can further develop this patch - at the moment I don't see such support. Consequently, I propose to discuss it and in the meantime to move this issue to a later release.", "Vote for `won't fix`. We also don't implement an ignore.robotstxt option for the above reasons.", "+1 Markus. Please close off when you can.", "This is completely different from an hypothetical \"ignore.robots.txt\" option. \"robots.txt\" is controlled by the site owner, and it tells us explicitely not to access/index some parts of the website. rel=nofollow is usually controlled by third-parties and it's not supposed to restrict crawling. It's just for preventing the link from adding up in link scoring algorithms (or, as Andrew put it, non-endorsement).\n\nBut what is more important: What happpens when your seeds use rel=nofollow? Then Nutch cannot crawl anything. For example, most MediaWiki setups include rel=nofollow for all external links. That means that, if you need to use a MediaWiki-based site as a seed, Nutch will not be able to extract links for further crawling.", "Hi Santiago, if you would like to update the patch then please do so. Patch against trunk and/or 2.x HEAD and we will see where this goes."], "tasks": {"summary": "Add configurable option for treating nofollow behaviour.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add configurable option for treating nofollow behaviour."}, {"question": "What is the main context?", "answer": "For my purposes I'd like to follow links even if they're marked nofollow- Ideally I'd like to follow them, but not pass the link juice between them. \n\nI've attached a patch that adds a configuration e"}]}}
{"issue_id": "NUTCH-694", "project": "NUTCH", "title": "Distributed Search Server fails", "status": "Closed", "priority": "Blocker", "reporter": "Dr. Nadine Hochstotter", "assignee": "Sami Siren", "created": "2009-02-19T08:37:42.560+0000", "updated": "2011-06-08T21:34:21.684+0000", "description": "I run Nutch on a single server, I have two crawl directories, that's why I use Nutch  in distributed search server mode as described in the hadoop manual.\nBut since I have a new Trunk Version (04.02.2009) it fails. Local search on one index works fine. But distributed search throws following exception:\nIn catalina.out (server)\n2009-02-18 17:08:14,906 ERROR NutchBean - org.apache.hadoop.ipc.RemoteException: java.io.IOException: Unknown Protocol classname:org.apache.nutch.searcher.RPCSegmentBean\n       at org.apache.nutch.searcher.NutchBean.getProtocolVersion(NutchBean.java:403)\n       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n       at java.lang.reflect.Method.invoke(Method.java:597)\n       at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)\n       at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)\n\n       at org.apache.hadoop.ipc.Client.call(Client.java:696)\n       at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)\n       at $Proxy4.getProtocolVersion(Unknown Source)\n       at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:319)\n       at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:306)\n       at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:343)\n       at org.apache.nutch.searcher.DistributedSegmentBean.<init>(DistributedSegmentBean.java:103)\n       at org.apache.nutch.searcher.NutchBean.<init>(NutchBean.java:111)\n       at org.apache.nutch.searcher.NutchBean.<init>(NutchBean.java:80)\n       at org.apache.nutch.searcher.NutchBean$NutchBeanConstructor.contextInitialized(NutchBean.java:422)\n       at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:3843)\n       at org.apache.catalina.core.StandardContext.start(StandardContext.java:4350)\n       at org.apache.catalina.core.StandardContext.reload(StandardContext.java:3099)\n       at org.apache.catalina.manager.ManagerServlet.reload(ManagerServlet.java:913)\n       at org.apache.catalina.manager.HTMLManagerServlet.reload(HTMLManagerServlet.java:536)\n       at org.apache.catalina.manager.HTMLManagerServlet.doGet(HTMLManagerServlet.java:114)\n       at javax.servlet.http.HttpServlet.service(HttpServlet.java:690)\n       at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)\n       at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)\n       at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n       at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n       at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175)\n       at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:525)\n       at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)\n       at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n       at org.apache.catalina.valves.RequestFilterValve.process(RequestFilterValve.java:269)\n       at org.apache.catalina.valves.RemoteAddrValve.invoke(RemoteAddrValve.java:81)\n       at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n       at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)\n       at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844)\n       at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583)\n       at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447)\n       at java.lang.Thread.run(Thread.java:619)\n\nAnd in Hadoop.log:\n2009-02-18 17:07:52,847 INFO  ipc.Server - IPC Server handler 48 on 13001: starting\n2009-02-18 17:07:52,847 INFO  ipc.Server - IPC Server handler 49 on 13001: starting\n2009-02-18 17:07:52,847 INFO  ipc.Server - IPC Server handler 40 on 13001: starting\n2009-02-18 17:08:14,675 INFO  ipc.RPC - Call: getProtocolVersion(org.apache.nutch.searcher.RPCS...\n2009-02-18 17:08:14,857 INFO  ipc.RPC - Return: 1\n2009-02-18 17:08:14,878 INFO  ipc.RPC - Call: getProtocolVersion(org.apache.nutch.searcher.RPCS...\n2009-02-18 17:08:14,879 INFO  ipc.Server - IPC Server handler 0 on 13001, call getProtocolVersion(org.apache.nutch.searcher.RPCSegmentBean, 1) from 78.46.86.99:40851: error: java.io.IOException: Unknown Protocol classname:org.apache.nutch.searcher.RPCSegmentBean\njava.io.IOException: Unknown Protocol classname:org.apache.nutch.searcher.RPCSegmentBean\n       at org.apache.nutch.searcher.NutchBean.getProtocolVersion(NutchBean.java:403)\n       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n       at java.lang.reflect.Method.invoke(Method.java:597)\n       at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)\n       at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)\n2009-02-18 17:08:14,879 INFO  ipc.RPC - Call: ping()\n2009-02-18 17:08:14,938 INFO  ipc.RPC - Return: true\n2009-02-18 17:08:24,876 INFO  ipc.RPC - Call: ping()\n\nWe do not run Nutch in PseudoDistributedMode. We only use the distributed search mode. With Nutch-0.9 this was working properly.\n", "comments": ["This fixed the problem for me.", "Hi Sami,\nI tried the patch, but it didn't work for me. Probably, because I don't use Nutch in PseudoDistributedMode on a single server? ", "Strange, did you update both ends (the server and the client?), normally the web application (.war) is the client.\n\nAfter patching you should run \n\n1. \"ant clean job\"\n\n2. deploy & run server + client", "Hi Sami,\nI tried my best, but still it doesn't work, I updated all, but whenever I have a search query which would return results I get a blank page and even if I try this in terminal, there are no results.\nBut now I have a new exception. Any ideas? Is this because we do not run Nutch in PseudoDistrubutedMode. But it worked so fine before. I have no idea.\nThank you for any help.\n\nClient log:\n2009-02-19 18:15:33,819 INFO  ipc.RPC - Return: true\n2009-02-19 18:15:43,819 INFO  ipc.RPC - Call: ping()\n2009-02-19 18:15:43,820 INFO  ipc.RPC - Return: true\n2009-02-19 18:15:46,426 INFO  ipc.RPC - Call: getProtocolVersion(org.apache.nutch.searcher.RPCS...\n2009-02-19 18:15:46,426 INFO  ipc.RPC - Return: 1\n2009-02-19 18:15:46,462 INFO  ipc.RPC - Call: getProtocolVersion(org.apache.nutch.searcher.RPCS...\n2009-02-19 18:15:46,462 INFO  ipc.RPC - Return: 1\n2009-02-19 18:15:46,462 INFO  ipc.RPC - Call: ping()\n2009-02-19 18:15:46,463 INFO  ipc.RPC - Return: true\n2009-02-19 18:15:46,481 INFO  ipc.RPC - Call: getSegmentNames()\n2009-02-19 18:15:46,481 INFO  ipc.RPC - Return: null\n2009-02-19 18:15:46,886 INFO  ipc.RPC - Call: search(\"DEFAULT der\", 10, null, null, false)\n2009-02-19 18:15:46,896 INFO  ipc.RPC - Return: org.apache.nutch.searcher.Hits@786c1a82\n2009-02-19 18:15:53,819 INFO  ipc.RPC - Call: ping()\n2009-02-19 18:15:53,820 INFO  ipc.RPC - Return: true\n2009-02-19 18:16:03,816 INFO  ipc.RPC - Call: ping()\n2009-02-19 18:16:03,816 INFO  ipc.RPC - Return: true\n2009-02-19 18:16:13,820 INFO  ipc.RPC - Call: ping()\n2009-02-19 18:16:13,820 INFO  ipc.RPC - Return: true\n2009-02-19 18:16:23,816 INFO  ipc.RPC - Call: ping()\n2009-02-19 18:16:23,816 INFO  ipc.RPC - Return: true\n\nAnd Tomcat:\nINFO: Reloading this Context has started\nFeb 19, 2009 6:14:22 PM org.apache.catalina.core.StandardContext addApplicationListener\nINFO: The listener \"org.apache.nutch.searcher.NutchBean$NutchBeanConstructor\" is already configured for this context. The duplicate definition has been ignored.\n2009-02-19 18:14:22,818 INFO  NutchBean - creating new bean\nFeb 19, 2009 6:14:26 PM org.apache.catalina.loader.WebappClassLoader loadClass\nINFO: Illegal access: this web application instance has been stopped already.  Could not load org.apache.log4j.spi.NOPLoggerRepository.  The eventual following stack trace is caused by an error thrown for debugging purposes as well as to attempt to terminate the thread which caused the illegal access, and has no functional impact.\njava.lang.IllegalStateException\n\tat org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1246)\n\tat org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1206)\n\tat java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)\n\tat org.apache.log4j.LogManager.getLoggerRepository(LogManager.java:175)\n\tat org.apache.log4j.LogManager.getLogger(LogManager.java:199)\n\tat org.apache.log4j.Logger.getLogger(Logger.java:105)\n\tat org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229)\n\tat org.apache.commons.logging.impl.Log4JLogger.isDebugEnabled(Log4JLogger.java:239)\n\tat org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:211)\n\tat $Proxy1.ping(Unknown Source)\n\tat org.apache.nutch.searcher.DistributedSearchBean$PingWorker.run(DistributedSearchBean.java:94)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)\nException in thread \"IPC Client (47) connection to pp1.hetzner.itroot.de/78.46.86.99:13001 from an unknown user\" java.lang.NoClassDefFoundError: org/apache/log4j/spi/NOPLoggerRepository\n\tat org.apache.log4j.LogManager.getLoggerRepository(LogManager.java:175)\n\tat org.apache.log4j.LogManager.getLogger(LogManager.java:199)\n\tat org.apache.log4j.Logger.getLogger(Logger.java:105)\n\tat org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229)\n\tat org.apache.commons.logging.impl.Log4JLogger.isDebugEnabled(Log4JLogger.java:239)\n\tat org.apache.hadoop.ipc.Client$Connection.close(Client.java:560)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:441)\n\n", "This exception may occur if you only reload a webapp without restarting Tomcat, and it's not directly related to Nutch - please restart Tomcat (using shutdown/startup) and try again.", "I rechecked this again and there was also something else wrong, I am attaching a new patch that is now manually tested (we lost the testcase somewhere) with local and nutch rpc search.\n", "Hi,\nI reinstalled both sides (client and server), again.\nI restarted Tomcat, I reloaded my Nutch web application, and I deleted any cookies.\nAnd thanks, it is working fine, now.\nThank you for your help and time.\nCheers, Nadine.\n\n", "Committed. Thanks for testing it.", "Integrated in Nutch-trunk #734 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/734/])\n     - Distributed Search Server fails\n", "closing issues for released version"], "tasks": {"summary": "Distributed Search Server fails", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Distributed Search Server fails"}, {"question": "What is the main context?", "answer": "I run Nutch on a single server, I have two crawl directories, that's why I use Nutch  in distributed search server mode as described in the hadoop manual.\nBut since I have a new Trunk Version (04.02.2"}]}}
{"issue_id": "NUTCH-695", "project": "NUTCH", "title": "incorrect mime type detection by MoreIndexingFilter plugin", "status": "Closed", "priority": "Major", "reporter": "Dmitry Lihachev", "assignee": "Sami Siren", "created": "2009-02-19T10:04:43.035+0000", "updated": "2009-04-10T12:29:01.906+0000", "description": "When server sends {{Content-Type}} header with optional params like {{Content-Type: text/html; charset=UTF-8}} MoreIndexingFilter returns null in {{type}} field.", "comments": ["Test case for this bug in [^NUTCH-695_TestMoreIndexingFilter.patch]", "[^NUTCH-695_MoreIndexingFilter.patch] fixes this bug", "committed, thanks", "thank you, Sami", "Integrated in Nutch-trunk #730 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/730/])\n     - incorrect mime type detection by MoreIndexingFilter plugin, contributed by Dmitry Lihachev\n", "closing issues for released version"], "tasks": {"summary": "incorrect mime type detection by MoreIndexingFilter plugin", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "incorrect mime type detection by MoreIndexingFilter plugin"}, {"question": "What is the main context?", "answer": "When server sends {{Content-Type}} header with optional params like {{Content-Type: text/html; charset=UTF-8}} MoreIndexingFilter returns null in {{type}} field."}]}}
{"issue_id": "NUTCH-696", "project": "NUTCH", "title": "Timeout for Parser", "status": "Closed", "priority": "Minor", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2009-02-19T16:56:16.596+0000", "updated": "2013-05-22T03:54:54.234+0000", "description": "I found that the parsing sometimes crashes due to a problem on a specific document, which is a bit of a shame as this blocks the rest of the segment and Hadoop ends up finding that the node does not respond. I was wondering about whether it would make sense to have a timeout mechanism for the parsing so that if a document is not parsed after a time t, it is simply treated as an exception and we can get on with the rest of the process.\n\nDoes that make sense? Where do you think we should implement that, in ParseUtil?\n", "comments": ["This makes perfect sense, but I am not sure how to implement it. We can push parsing to a different thread and kill the thread after a while but IIRC forcefully shutting down threads is a looked down practice in java. Maybe we can push parsing to another process on the same machine and kill the process. This is cleaner but is more difficult to implement.\n\nDo you have a suggestion on how to implement the timeout mechanism?", "I was thinking along the lines of your first option i.e do the parsing in a separate thread and kill it if we pass the time out. Am not the most experienced person when it comes to threads in Java so someone else will probably have a better idea.", "The simplest way of not being blocked by such issues is simply to skip the problematic records so that even if a document fails a task the other records will still be processed in a subsequent re-run. This can be done with the following Hadoop options :\n\nskipRecordsOptions=\"-D mapred.skip.attempts.to.start.skipping=2 -D mapred.skip.map.max.skip.records=1\"\n\nThe time out mechanism could be interesting but is not really straightforward to implement so probably best to forget about it for now.", "A simple patch that implements the strategy outlined here http://bit.ly/bdTYrS - I've been recently suffering from this issue, so this is better than nothing. Julien's strategy would work, too, but then the job takes much longer to execute.", "Every day, I love java.util.concurrent even more :)\n\nI think using a ExecutorService (and a thread pool) may be better than creating and releasing a thread for each URL.", "Yes - this patch is a quick solution that allowed me to complete a crawl. If people feel this is useful, let's polish it.", "This may be useful after all - let's gather more comments.", "+1 : this is definitely useful. Hopefully the underlying parsers in Tika are constantly improved to prevent loops and crashes but having the parser timeout on top would be great \n\nI suggest we mark it for 2.0 and 1.2", "Another +1 from me. Maybe tika has improved parsers, but back in the day, many many parse jobs failed due to infinite loops and crashes :)", "FWIW, so far I haven't run into issues with creating/releasing threads for each document being parsed, for a 20M page crawl. Or at least relative to the overhead of all that happens during parsing, it hasn't been noticeable.\n", "Hey Guys,\n\nWhy don't we flow this patch into Tika, per Ken's original suggestion, then flow it back into Nutch via the parse-tika plugin? Do we need a Nutch-specific solution to this? Maybe, just checking...\n\nCheers,\nChris", "I agree, ultimately that's the way to go. However, I needed something _now_, and the patch helps to solve the problem that I have now - and until this problem is solved in Tika this patch provides some kind of band-aid for us poor Nutch-ers...", "I hear ya! Welp, +1 to commit, no objections from me. I'll work on implementing Ken's Parser in Tika and get that end going so we can integrate later.\n\nCheers,\nChris\n", "Hey Chris - let me know if you want me to file a Tika issue and attach my current code.\n\nI never heard anything back re the general solution I'd proposed, but if you want to run with the ball that would be great.", "Hey Ken:\n\n+1, please file a Tika issue and attach your code if that works for you. I'll happily commit it and start moving over in Tika ville. +1 on your general approach. It sounds reasonable to me...\n\nCheers,\nChris\n", "It would be great to have that in Tika but I think it would make sense to commit Andrzej's patch in Nutch as well as it covers all types of parsers and not only Tika.\n\nIf there are no objections I will commit it next week", "Trunk : Committed revision 981829.\nNutchBase : Committed revision 981835.\n1.2 : Committed revision 981844."], "tasks": {"summary": "Timeout for Parser", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Timeout for Parser"}, {"question": "What is the main context?", "answer": "I found that the parsing sometimes crashes due to a problem on a specific document, which is a bit of a shame as this blocks the rest of the segment and Hadoop ends up finding that the node does not r"}]}}
{"issue_id": "NUTCH-697", "project": "NUTCH", "title": "Generate log output for solr indexer and dedup", "status": "Closed", "priority": "Trivial", "reporter": "Dmitry Lihachev", "assignee": "Chris A. Mattmann", "created": "2009-02-20T08:09:25.249+0000", "updated": "2013-05-22T03:54:44.423+0000", "description": null, "comments": ["- this was resolved as part of NUTCH-838", "Added to chanelog for 1.2 in r964122 and for trunk in r964117."], "tasks": {"summary": "Generate log output for solr indexer and dedup", "classification": "task", "qa_pairs": []}}
{"issue_id": "NUTCH-698", "project": "NUTCH", "title": "CrawlDb is corrupted after a few crawl cycles", "status": "Closed", "priority": "Blocker", "reporter": "Dogacan Guney", "assignee": "Dogacan Guney", "created": "2009-02-20T08:53:10.691+0000", "updated": "2009-04-10T12:29:02.743+0000", "description": "After change to hadoop's MapWritable, crawldb becomes corrupted after some fetch cycles. For more details see this discussion thread:\n\nhttp://www.nabble.com/Fetcher2-crashes-with-current-trunk-td21978049.html", "comments": ["Patch for the issue.\n\nAgain, hadoop's MapWritable#putAll is broken (see HADOOP-5142 ). So another workaround for that....", "Oops. Wrong priority. Elevating this to blocker.", "committed. thanks guys", "Integrated in Nutch-trunk #735 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/735/])\n     - CrawlDb is corrupted after a few crawl cycles, contributed by dogacan\n", "closing issues for released version"], "tasks": {"summary": "CrawlDb is corrupted after a few crawl cycles", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "CrawlDb is corrupted after a few crawl cycles"}, {"question": "What is the main context?", "answer": "After change to hadoop's MapWritable, crawldb becomes corrupted after some fetch cycles. For more details see this discussion thread:\n\nhttp://www.nabble.com/Fetcher2-crashes-with-current-trunk-td21978"}]}}
{"issue_id": "NUTCH-699", "project": "NUTCH", "title": "Add an \"official\" solr schema for solr integration", "status": "Closed", "priority": "Major", "reporter": "Dogacan Guney", "assignee": "Dogacan Guney", "created": "2009-02-20T10:32:32.032+0000", "updated": "2009-04-10T12:29:04.880+0000", "description": "See Andrzej's comments on NUTCH-684 for more info.", "comments": ["Schema in NUTCH-442 may be a good starting point.\n\nQuestion, where do you think is a good place for this schema in nutch codebase?", "I think we must extends field set for each plugin just like code code below \n{noformat} \n<fields>\n  <field name=\"id\" type=\"string\" stored=\"true\" indexed=\"true\"/>\n\n  <!-- core fields -->\n  <field name=\"segment\" type=\"string\" stored=\"true\" indexed=\"false\"/>\n  <field name=\"digest\" type=\"string\" stored=\"true\" indexed=\"false\"/>\n  <field name=\"boost\" type=\"float\" stored=\"true\" indexed=\"false\"/>\n\n  <!-- fields for index-basic plugin -->\n  <field name=\"host\" type=\"url\" stored=\"false\" indexed=\"true\"/>\n  <field name=\"site\" type=\"string\" stored=\"false\" indexed=\"true\"/>\n  <field name=\"url\" type=\"url\" stored=\"true\" indexed=\"true\" required=\"true\"/>\n  <field name=\"content\" type=\"text\" stored=\"false\" indexed=\"true\"/>\n  <field name=\"title\" type=\"text\" stored=\"true\" indexed=\"true\"/>\n  <field name=\"cache\" type=\"string\" stored=\"true\" indexed=\"false\"/>\n  <field name=\"tstamp\" type=\"long\" stored=\"true\" indexed=\"false\"/>\n\n  <!-- fields for index-anchor plugin -->\n  <field name=\"anchor\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/>\n\n  <!-- fields for index-more plugin -->\n  <field name=\"type\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/>\n  <field name=\"contentLength\" type=\"long\" stored=\"true\" indexed=\"false\"/>\n  <field name=\"lastModified\" type=\"long\" stored=\"true\" indexed=\"false\"/>\n  <field name=\"date\" type=\"string\" stored=\"true\" indexed=\"true\"/>\n\n  <!-- fields for languageidentifier plugin -->\n  <field name=\"lang\" type=\"string\" stored=\"true\" indexed=\"true\"/>\n\n  <!-- fields for subcollection plugin -->\n  <field name=\"subcollection\" type=\"string\" stored=\"true\" indexed=\"true\"/>\n\n  <!-- fields for feed plugin -->\n  <field name=\"author\" type=\"string\" stored=\"true\" indexed=\"true\"/>\n  <field name=\"tag\" type=\"string\" stored=\"true\" indexed=\"true\"/>\n  <field name=\"feed\" type=\"string\" stored=\"true\" indexed=\"true\"/>\n  <field name=\"publishedDate\" type=\"string\" stored=\"true\" indexed=\"true\"/>\n  <field name=\"updatedDate\" type=\"string\" stored=\"true\" indexed=\"true\"/>\n\n</fields>\n\n{noformat} ", "We could put it under conf/ ?", "+1. Perhaps add a comment on top that explains what users are supposed to do with this file.", "committed", "Integrated in Nutch-trunk #738 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/738/])\n     - Add an \"official\" solr schema for solr integration. Contributed by dogacan, Dmitry Lihachev\n", "In some cases (eg. when using MoreIndexingFilter) the' title'  field might have more than one value. This causes exception when indexing with solr.  Adding attribute multiValued=\"true\" to title definition solves the problem", "closing issues for released version"], "tasks": {"summary": "Add an \"official\" solr schema for solr integration", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add an \"official\" solr schema for solr integration"}, {"question": "What is the main context?", "answer": "See Andrzej's comments on NUTCH-684 for more info."}]}}
{"issue_id": "NUTCH-7", "project": "NUTCH", "title": "analyze tool takes up all the disk space when there are circular links", "status": "Closed", "priority": "Major", "reporter": "Phoebe Miller", "assignee": null, "created": "2005-03-11T12:52:23.000+0000", "updated": "2005-08-09T05:00:28.000+0000", "description": "It is repeatable by running an instance with these seeds:\nhttp://www.acf.hhs.gov/programs/ofs/forms.htm/grants/grants/grants/grants/data/grants/data/data/data/data/grants/data/grants/grants/grants/process.htm\nhttp://www.acf.hhs.gov/programs/ofs/\n\nand limit it (for best effect) to just:\n*.acf.hhs.gov/*\n\nLet it go for about 12 cycles to build it up and the temp file size roughly doubles with each segment.\n\n]$ ls -l /db/tmpdir2344la/\n...\n\n1503641425 Mar 10 17:42 scoreEdits.0.unsorted\n\nfor a very small db:\n\nStats for net.nutch.db.WebDBReader@89cf1e\n-------------------------------\nNumber of pages: 6916\nNumber of links: 8085\n\nscoreEdits.0.sorted.0 contains rows of links that looked like the first seed url, but with more grants/ and data/ in the sub dirs.\n\n\nIn the File:\n.DistributedAnalysisTool.java\n 345                     if (curIndex - startIndex > extent) {\n 346                         break;\n 347                     }\nis the hard stop.\n\nFurther down the score is written:\n381  for (int i = 0; i < outLinks.length; i++) {\n...\n385     scoreWriter.append(outLinks[i].getURL(), score);\n\nPutting a check here stops the tmpdir.../scoreEdits.0 file growth\nbut the links themselves should not be produced in the generation either.\n\n", "comments": ["I have fixed this problem by changing the update database tool, basically, links from a page is not added if the page has already been processed and current (same MD5). Now link analysis won't run into these infinite chains of links.\n\nHere is the diff in UpdateDatabaseTool.java.\n\n\n64d63\n<     private IWebDBReader webdbread;\n72c71\n<     public UpdateDatabaseTool(IWebDBWriter webdb,  IWebDBReader webdbread, boolean additionsAllowed, int maxCount) {\n---\n>     public UpdateDatabaseTool(IWebDBWriter webdb, boolean additionsAllowed, int maxCount) {\n74d72\n<         this.webdbread = webdbread;\n229,231d226\n< // If the page is already in the db, so are the links,\n< // This should take care of relative links and symlinks to itself.\n<                                 if (!webdbread.pageExists(newPage.getMD5()))  // page not seen before\n365,366c360\n<       IWebDBReader webdbread = new WebDBReader(nfs, root);\n<       UpdateDatabaseTool tool = new UpdateDatabaseTool(webdb, webdbread, additionsAllowed, max);\n---\n>       UpdateDatabaseTool tool = new UpdateDatabaseTool(webdb, additionsAllowed, max);\n", "The link analysis tool is not actively maintained.  It's use is optional, so, if you have problems with it, you can just stop using it.  To get some of its effects (prioritizing pages when crawling and searching) without using the analyze command, set both fetchlist.score.by.link.count and indexer.boost.by.link.count to true.  This \"poor man's link analysis\" implementation works surprisingly well.\n", "I am attaching a patch that should fix this issue.\n\nI was trying to reproduce the bug  on URL submitted in bug description but failed to do so (I am not sure why). But I run at exactly this problem when computing PageRank for my WebDB (~70mln of pages, ~150mln of links).\nThe most problematic site in my WebDB was starting from this url:\nhttp://www.scotlandsheritagehotels.co.uk/darroch/special-promotions/wine-and-dine/\nMajority of links on this page will add one additional component in URL but point to to exactly the same page content.\nI have over 1mln of pages with this url pattern in my development WebDB :(. After running PageRank computation on it I was running out of disk space (my WebDB is about 48GB and I had ~600GB free before starting PageRank computation) and process was terminated in the middle.\n\nI debugged the problem and it appears that it is not a problem with cyclic links but with many pages having the same MD5.\nBecause all 1mln pages in my WebDB have the same content and thus the same MD5  - all links from these pages are grouped and returned  when PageRank computation tries to gather outlinks from currently processed page.\nSo for each of 1mln bad pages it gets over a milion of outlinks and tries to write url and scores to file. In my opinion each link should be used exactly once in PageRank computation not as many times as  pages with the same md5 exist.\n\nBecause during PageRank computation iteration is done over pages sorted by MD5 the change is simple to implement.\nI have added a check so only first page of the set of pages having the same MD5 is actually processed - all others are skipped.\nThis guarantees that number of processed links should be equal to number of links in WebDB (not greater as previously).\n\nAdditionaly I have added a warning in logs if number of outlinks per page is greater than some limit (10000 right now - but can be set to smaller value). It allows to identify some problematic sites in WebDB easily - but should be treated as a hint only as for example if dmoz data is used to seed the WebDB www.dmoz.org will have huge number of outlinks.\n\nThis patch should improve performance of PageRank computation as less data is actually written to disk and processed. I was surprised to see many small groups of duplicated Pages in my WebDB in addition to this huge one - so I encourage others to test if they have performance improvements even though they have not run out of disk space yet.\nIn my opinion it will also improve the quality of results because each link would be used exactly once in PageRank computation.\n\nI also agree that we should avoid adding such pages to WebDB in first place - I will investigate some ideas that will help as to do so in future.\n\n\n", "Patch applied."], "tasks": {"summary": "analyze tool takes up all the disk space when there are circular links", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "analyze tool takes up all the disk space when there are circular links"}, {"question": "What is the main context?", "answer": "It is repeatable by running an instance with these seeds:\nhttp://www.acf.hhs.gov/programs/ofs/forms.htm/grants/grants/grants/grants/data/grants/data/data/data/data/grants/data/grants/grants/grants/pro"}]}}
{"issue_id": "NUTCH-70", "project": "NUTCH", "title": "duplicate pages - virtual hosts in db.", "status": "Closed", "priority": "Major", "reporter": "YourSoft", "assignee": null, "created": "2005-07-11T18:09:11.000+0000", "updated": "2008-03-14T23:58:10.663+0000", "description": "Dear Developers,\n\nI have a problem with nutch:\n- There are many sites duplicates in the webdb and in the segments.\nThe source of this problem is:\n- If the site make 'virtual hosts' (like Apache), e.g. www.origo.hu, origo.hu, origo.matav.hu, origo.matavnet.hu etc.: the result pages are the same, only the inlinks are differents.\n- The ip address is the same.\n- When search, all virtualhosts are in the results.\n\nGoogle only show one of these virtual hosts, the nutch show all. The result nutch db is larger, and this case slower, than google.\n\nHave any idea, how to remove these duplicates?\n\nRegards,\n    Ferenc", "comments": ["Is the content exactly the same? Maybe could the page be checked  against an already existing one by an MD5 on the content? But I'm not sure if there is a clean way to workaround the problem - what if all pages are the same except one, on the other vhost? Would have to crawl all anyway, wouldn't you?", "These issues are partially addressed in 0.9 / 1.0. Also, 0.7 branch is in End Of Life status."], "tasks": {"summary": "duplicate pages - virtual hosts in db.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "duplicate pages - virtual hosts in db."}, {"question": "What is the main context?", "answer": "Dear Developers,\n\nI have a problem with nutch:\n- There are many sites duplicates in the webdb and in the segments.\nThe source of this problem is:\n- If the site make 'virtual hosts' (like Apache), e.g."}]}}
{"issue_id": "NUTCH-700", "project": "NUTCH", "title": "Neko1.9.11 goes into a loop", "status": "Closed", "priority": "Critical", "reporter": "Julien Nioche", "assignee": "Sami Siren", "created": "2009-02-20T10:44:02.125+0000", "updated": "2009-04-10T12:29:05.123+0000", "description": "Neko1.9.11 goes into a loop on some documents e.g. \n\nhttp://mediacet.com/Archive/FourYorkshiremen/bb/post.htm\nhttp://cizel.co.kr/main.php\n\nreverting to 0.9.4 seems to fix the problem\n\nThe approach mentioned in https://issues.apache.org/jira/browse/NUTCH-696 could be a way to alleviate similar issues\n\nPS: haven't had time to report to the Neko people yet, will do at some stage", "comments": ["Reported to CyberNeko\n\nhttps://sourceforge.net/tracker/index.php?func=detail&aid=2619582&group_id=195122&atid=952178", "This one just bit me - the effect is that parsing hangs forever. I am promoting it to be fixed in  1.0.", "reverted to 0.9.4", "Integrated in Nutch-trunk #742 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/742/])\n    ", "closing issues for released version"], "tasks": {"summary": "Neko1.9.11 goes into a loop", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Neko1.9.11 goes into a loop"}, {"question": "What is the main context?", "answer": "Neko1.9.11 goes into a loop on some documents e.g. \n\nhttp://mediacet.com/Archive/FourYorkshiremen/bb/post.htm\nhttp://cizel.co.kr/main.php\n\nreverting to 0.9.4 seems to fix the problem\n\nThe approach men"}]}}
{"issue_id": "NUTCH-701", "project": "NUTCH", "title": "Replace Fetcher with Fetcher2", "status": "Closed", "priority": "Major", "reporter": "Sami Siren", "assignee": "Sami Siren", "created": "2009-02-24T10:06:03.729+0000", "updated": "2009-04-10T12:29:02.470+0000", "description": "Currently there are two fetcher implementation within nutch, one too many. This task tracks the process of promoting Fetcher2.\n\nmy plan is basically to\n-remove Fetcher all together and rename Fetcher2 to Fetcher\n-fix crawl class so it works with F2 api.\n\nIf there are no objections I will proceed with this soon.", "comments": ["This is a duplicate of NUTCH-669. Please follow-up with Todd to finalize that issue instead.", "closing issues for released version"], "tasks": {"summary": "Replace Fetcher with Fetcher2", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Replace Fetcher with Fetcher2"}, {"question": "What is the main context?", "answer": "Currently there are two fetcher implementation within nutch, one too many. This task tracks the process of promoting Fetcher2.\n\nmy plan is basically to\n-remove Fetcher all together and rename Fetcher2"}]}}
{"issue_id": "NUTCH-702", "project": "NUTCH", "title": "Lazy Instanciation of Metadata in CrawlDatum", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2009-02-25T13:05:55.712+0000", "updated": "2009-09-19T07:20:00.886+0000", "description": "CrawlDatum systematically instanciates its metadata, which is quite wasteful especially in the case of CrawlDBReducer when it generates a new CrawlDatum for each incoming link before storing it in a List.\n\nInitial testing on the lazy instanciation shows an improvement in both speed and memory consumption. I will generate a patch for it ASAP", "comments": ["patch for lazy instanciation of metadata in crawldatum", "patch for lazy instanciation of metadata in crawldatum (replaces previous one)", "I have encountered OutOfMemoryError in CrawlDBReducer constantly on a system with 7GB main memory. I agree that the CrawlDatum for each incoming link is the cause.", "I catched NPE when using this patch\n{code}\njava.lang.NullPointerException\n        at org.apache.nutch.crawl.CrawlDatum.putAllMetaData(CrawlDatum.java:216)\n        at org.apache.nutch.crawl.CrawlDbReducer.reduce(CrawlDbReducer.java:134)\n        at org.apache.nutch.crawl.CrawlDbReducer.reduce(CrawlDbReducer.java:35)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:436)\n        at org.apache.hadoop.mapred.Child.main(Child.java:158)\n{code}\nat line 216 we can see this part of code\n{code}\n     metaData.put(e.getKey(), e.getValue());\n{code}\nbut metadata is yet not initialized.\n\nI think problem can be soved if change this code part to \n{code}\n     getMetaData().put(e.getKey(), e.getValue());\n{code}\n", "Fixed bug reported by Dmitry Lihachev", "Reattaching Julien's last patch. I have made some minor code cleanups.\n\nCould anyone confirm if that NPE is gone? (and my cleanup did not break anything ? :)", "There have been quite a few related questions on the mailing-lists since releasing the patch. As pointed out by Andrzej, the best way of avoiding memory exceptions is to limit the number of URLs per inlink (see nutch-default.xml), however this patch allows to set a higher value for this parameter as it reduces the memory footprint. \n\nThe main advantage of this patch is that it speeds up all phases of a crawl involving  the crawlDB. As an illustration I did a bit of comparison between the default version and the patched one, the average results being : \n\nInjection (1M URLs)\noriginal : 98 secs\npatched : 65 secs\n\ngeneration : \noriginal : 37 secs\npatched : 37 secs\n\nstats: \noriginal : 13 secs\npatched : 19 secs\n\nupdate: \noriginal : 40 secs\npatched : 19 secs\n\n\nI find a bit surprising that the patched version is not faster on the generation (any idea?). On the other phases it seems to be 50% faster, except for the updating where it is twice as fast.\n\nJ.\n", "of course it was meant to be \n\nstats:\noriginal : 19 secs\npatched : 13 secs", "Committed.\n\nThanks for the patch, Julien.", "Integrated in Nutch-trunk #929 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/929/])\n     - Lazy Instanciation of Metadata in CrawlDatum. Contributed by Julien Nioche.\n", "I used NUTCH1.1 Integrated in Nutch-trunk #929,but still outmemory.\njava.lang.OutOfMemoryError: Java heap space\n        at java.util.Arrays.copyOf(Unknown Source)\n        at java.util.ArrayList.ensureCapacity(Unknown Source)\n        at java.util.ArrayList.add(Unknown Source)\n        at org.apache.nutch.crawl.CrawlDbReducer.reduce(CrawlDbReducer.java:97)\n        at org.apache.nutch.crawl.CrawlDbReducer.reduce(CrawlDbReducer.java:35)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:436)\n        at org.apache.hadoop.mapred.Child.main(Child.java:158)"], "tasks": {"summary": "Lazy Instanciation of Metadata in CrawlDatum", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Lazy Instanciation of Metadata in CrawlDatum"}, {"question": "What is the main context?", "answer": "CrawlDatum systematically instanciates its metadata, which is quite wasteful especially in the case of CrawlDBReducer when it generates a new CrawlDatum for each incoming link before storing it in a L"}]}}
{"issue_id": "NUTCH-703", "project": "NUTCH", "title": "Upgrade to Hadoop 0.19.1", "status": "Closed", "priority": "Blocker", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2009-02-25T16:45:42.433+0000", "updated": "2009-04-10T12:29:07.923+0000", "description": "From release notes: \"Release 0.19.1 fixes many critical bugs in 0.19.0, including ***some data loss issues***.\".", "comments": ["Andrzej, are you working with this now?", "Fixed in rev. 748637.", "Integrated in Nutch-trunk #738 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/738/])\n     Upgrade to Hadoop 0.19.1.\n"], "tasks": {"summary": "Upgrade to Hadoop 0.19.1", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade to Hadoop 0.19.1"}, {"question": "What is the main context?", "answer": "From release notes: \"Release 0.19.1 fixes many critical bugs in 0.19.0, including ***some data loss issues***.\"."}]}}
{"issue_id": "NUTCH-704", "project": "NUTCH", "title": "ensure that more important pages are crawled first", "status": "Closed", "priority": "Minor", "reporter": "kr", "assignee": null, "created": "2009-02-26T06:50:32.642+0000", "updated": "2009-02-26T08:29:01.540+0000", "description": "To implement url ordering algorithms mentioned in the paper \"Efficient crawling through url ordering\" by Lawrence Page et,al.. for crawling to ensure that more \"important\" pages are crawled first.This is important as even the most powerful and successful search engines have crawled only 15% of the WWW. ", "comments": ["Please see the ScoringFilter framework, and the org.apache.nutch.scoring.webgraph package."], "tasks": {"summary": "ensure that more important pages are crawled first", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "ensure that more important pages are crawled first"}, {"question": "What is the main context?", "answer": "To implement url ordering algorithms mentioned in the paper \"Efficient crawling through url ordering\" by Lawrence Page et,al.. for crawling to ensure that more \"important\" pages are crawled first.This"}]}}
{"issue_id": "NUTCH-705", "project": "NUTCH", "title": "parse-rtf plugin", "status": "Closed", "priority": "Minor", "reporter": "Dmitry Lihachev", "assignee": null, "created": "2009-02-27T04:16:33.093+0000", "updated": "2013-05-22T03:54:47.744+0000", "description": "Demoting this issue and moving to 1.1 - current patch is not suitable due to LGPL licensed parts.", "comments": ["This parser correctly handles non ascii input", "I think that the patch contains some lgpl code that we cannot commit into apache repository.", "Yes, it looks a bit like a problem... How can we handle this?", "I think we should start looking at Apache Tika for most (or all) of our parsers.", "RTF parsing is now handled by the TikaPlugin (NUTCH-766). Please open an issue on Tika if  the original problem with non-ascii chars still occurs"], "tasks": {"summary": "parse-rtf plugin", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "parse-rtf plugin"}, {"question": "What is the main context?", "answer": "Demoting this issue and moving to 1.1 - current patch is not suitable due to LGPL licensed parts."}]}}
{"issue_id": "NUTCH-706", "project": "NUTCH", "title": "Url regex normalizer: default pattern for session id removal not to match \"newsId\"", "status": "Closed", "priority": "Minor", "reporter": "Meghna Kukreja", "assignee": null, "created": "2009-02-27T18:47:06.587+0000", "updated": "2013-05-22T03:54:47.306+0000", "description": "Hey,\n\nI encountered the following problem while trying to crawl a site using\nnutch-trunk. In the file regex-normalize.xml, the following regex is\nused to remove session ids:\n\n<pattern>([;_]?((?i)l|j|bv_)?((?i)sid|phpsessid|sessionid)=.*?)(\\?|&amp;|#|$)</pattern>.\n\nThis pattern also transforms a url, such as,\n\"&newsId=2000484784794&newsLang=en\" into \"&new&newsLang=en\" (since it\nmatches 'sId' in the 'newsId'), which is incorrect and hence does not\nget fetched. This expression needs to be changed to prevent this.\n\nThanks,\nMeghna", "comments": ["The pattern should be changed to:\n<pattern>([;_\\?&amp;]((?i)l|j|bv_)?((?i)sid|phpsessid|sessionid)=.*?)(\\?|&amp;|#|$)</pattern>\n\nCan someone please verify this? I am not very good with regular expressions :)", "I think this must be changed to \n\n{code:xml}\n<regex>\n  <pattern>(\\?|&amp;)([;_]?((?i)l|j|bv_|ps_)?((?i)s|sid|phpsessid|sessionid|conversationid|sess_id)=.*?)(\\?|&amp;|#|$)</pattern>\n  <substitution>$1$5</substitution>\n</regex>\n{code}", "Both variants of the substitution rule above break existing tests. More work will be needed to get a pattern which covers the case described by Meghna *and* is compatible with the existing test cases.\nMoving it to post-1.1", "Two comments about this:\n\n1. From my experiences with Nutch & Bixo, I think that URL normalization ultimately needs to be more structured - ie first break the URL into pieces, then apply rules against the pieces. Trying to craft regular expressions to handle target cases leads to big, hairy, hard-to-understand strings.\n\n2. URL normalization is something that makes a lot of sense for crawler-commons. If somebody from the Nutch side wants to define a target API, I could look at porting existing Bixo code to crawler-commons.\n", "- fix the pattern by adding an anchor prohibiting inner-word matches such as in New{color:red}sId{color}\n- add test", "Second trial for patch. The first one does not remove:\n{code}\n?_sessionID=...\n{code}\nAdded more tests to cover more types of real session ids and a further counterexample:\n{code}\n?addressid=...\n{code}", "Are there objections to apply and commit the patch? Tests pass for both trunk and 2.x.\nThe problem is reported twice. Until there is a more sophisticated URL normalizer (see Ken Krugler's comment) there is no real alternative then improving the regex pattern.\n", "If tests pass and this solves the problem and you're comfortable with it i believe it's fine to commit the patch. ", "committed to trunk (revision 1396796) and 2.x (revision 1396795)", "First commit erroneously with wrong patch.\nCorrect patch (NUTCH-706-2.patch) now committed to trunk (revision 1396817) and 2.x (revision 1396822).", "Integrated in nutch-trunk-maven #449 (See [https://builds.apache.org/job/nutch-trunk-maven/449/])\n    NUTCH-706 (applied correct patch) (Revision 1396817)\nNUTCH-706 Url regex normalizer: pattern for session id removal not to match \"newsId\" (Revision 1396796)\n\n     Result = SUCCESS\nsnagel : \nFiles : \n* /nutch/trunk/conf/regex-normalize.xml.template\n* /nutch/trunk/src/plugin/urlnormalizer-regex/sample/regex-normalize-default.test\n* /nutch/trunk/src/plugin/urlnormalizer-regex/sample/regex-normalize-default.xml\n\nsnagel : \nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/conf/regex-normalize.xml.template\n* /nutch/trunk/src/plugin/urlnormalizer-regex/sample/regex-normalize-default.test\n* /nutch/trunk/src/plugin/urlnormalizer-regex/sample/regex-normalize-default.xml\n", "Integrated in Nutch-nutchgora #375 (See [https://builds.apache.org/job/Nutch-nutchgora/375/])\n    NUTCH-706 (applied correct patch) (Revision 1396822)\nNUTCH-706 Url regex normalizer: pattern for session id removal not to match \"newsId\" (Revision 1396795)\n\n     Result = SUCCESS\nsnagel : \nFiles : \n* /nutch/branches/2.x/conf/regex-normalize.xml.template\n* /nutch/branches/2.x/src/plugin/urlnormalizer-regex/sample/regex-normalize-default.test\n* /nutch/branches/2.x/src/plugin/urlnormalizer-regex/sample/regex-normalize-default.xml\n\nsnagel : \nFiles : \n* /nutch/branches/2.x/CHANGES.txt\n* /nutch/branches/2.x/conf/regex-normalize.xml.template\n* /nutch/branches/2.x/src/plugin/urlnormalizer-regex/sample/regex-normalize-default.test\n* /nutch/branches/2.x/src/plugin/urlnormalizer-regex/sample/regex-normalize-default.xml\n"], "tasks": {"summary": "Url regex normalizer: default pattern for session id removal not to match \"newsId\"", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Url regex normalizer: default pattern for session id removal not to match \"newsId\""}, {"question": "What is the main context?", "answer": "Hey,\n\nI encountered the following problem while trying to crawl a site using\nnutch-trunk. In the file regex-normalize.xml, the following regex is\nused to remove session ids:\n\n<pattern>([;_]?((?i)l|j|b"}]}}
{"issue_id": "NUTCH-707", "project": "NUTCH", "title": "Generation of multiple segments in multiple runs returns only 1 segment", "status": "Closed", "priority": "Major", "reporter": "Michael Chan", "assignee": "Andrzej Bialecki", "created": "2009-02-28T17:40:14.211+0000", "updated": "2009-10-10T04:45:45.610+0000", "description": "To generate multiple segments, generator.update.crawldb is set to true and -topN is defined to be the size of the segments. However, only one segment of size N is generated.\n\nFor example, I've tried it with a db containing 10,000+ links according to dump. When generator.update.crawldb is set to true and -topN is set to 5, only 1 segment of size 5 is produced.\n\nIt seems to me the problem is due to an incorrect recording of generation time. Selector.map assigns the generation time to each URL, even reduce only collects N many. It's perfectly fine if the generator was run once and that the db isn't updated. In the situation where the generator is run again within genDelay, all the remaining URLs will be excluded. So, I suggest the generation time should be assigned in reduce rather than map.", "comments": ["Fixed - the bug was actually present in CrawlDbUpdater. Thanks!", "Integrated in Nutch-trunk #959 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/959/])\n     Generation of multiple segments in multiple runs returns only 1 segment.\n"], "tasks": {"summary": "Generation of multiple segments in multiple runs returns only 1 segment", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Generation of multiple segments in multiple runs returns only 1 segment"}, {"question": "What is the main context?", "answer": "To generate multiple segments, generator.update.crawldb is set to true and -topN is defined to be the size of the segments. However, only one segment of size N is generated.\n\nFor example, I've tried i"}]}}
{"issue_id": "NUTCH-708", "project": "NUTCH", "title": "NutchBean: OOM due to searcher.max.hits and dedup.", "status": "Closed", "priority": "Major", "reporter": "Aaron Binns", "assignee": null, "created": "2009-03-01T20:11:51.153+0000", "updated": "2011-06-08T21:34:21.931+0000", "description": "When searching an index we built for the National Archives, this one in particular: http://webharvest.gov/collections/congress110th/\nWe ran into an interesting situation.\n\nWe were using searcher.max.hits=1000 in order to get faster searches.  Since our index is sorted, the \"best\" documents are \"at the front\" and setting searcher.max.hits=1000 would give us a nice trade-off of search quality vs. response time.\n\nWhat I discovered was that with dedup (on site) enabled, we would get into this loop where the searcher.max.hits would limit the raw hits to 1000 and the deduplication code would get to the end of those 1000 results and still need more as it hadn't found enough de-dup'd results to satisfy the query.\n\nThe first 6 pages of results would be fine, but when we got to page 7, the NutchBean would need more than 1000 raw results in order to get 60 de-duped results.\n\nThe code:\n    for (int rawHitNum = 0; rawHitNum < hits.getTotal(); rawHitNum++) {\n      // get the next raw hit                                                                                                                                                                                    \n      if (rawHitNum >= hits.getLength())\n        {\n        // optimize query by prohibiting more matches on some excluded values                                                                                                                                    \n        Query optQuery = (Query)query.clone();\n        for (int i = 0; i < excludedValues.size(); i++) {\n          if (i == MAX_PROHIBITED_TERMS)\n            break;\n          optQuery.addProhibitedTerm(((String)excludedValues.get(i)),\n                                     dedupField);\n        }\n        numHitsRaw = (int)(numHitsRaw * rawHitsFactor);\n        if (LOG.isInfoEnabled()) {\n          LOG.info(\"re-searching for \"+numHitsRaw+\" raw hits, query: \"+optQuery);\n        }\n        hits = searcher.search(optQuery, numHitsRaw,\n                               dedupField, sortField, reverse);\n        if (LOG.isInfoEnabled()) {\n          LOG.info(\"found \"+hits.getTotal()+\" raw hits\");\n        }\n        rawHitNum = -1;\n        continue;\n      }\n\nThe loop constraints were never satisfied as rawHitNum and hits.getLength() are capped by searcher.max.hits (1000).  The numHitsRaw keeps increasing by a factor of 2 (rawHitsFactor) until it gets to 2^31 or so and deep down in the search library code an array is allocated using that value as the size and you get an OOM.\n\nWe worked around the problem by abandoning the use of searcher.max.hits.  I suppose we could have increased the value, but the index was small enough (~10GB) that disabling searcher.max.hits didn't degrade the response time too much.", "comments": ["Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "NutchBean: OOM due to searcher.max.hits and dedup.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "NutchBean: OOM due to searcher.max.hits and dedup."}, {"question": "What is the main context?", "answer": "When searching an index we built for the National Archives, this one in particular: http://webharvest.gov/collections/congress110th/\nWe ran into an interesting situation.\n\nWe were using searcher.max.h"}]}}
{"issue_id": "NUTCH-709", "project": "NUTCH", "title": "JSParseFilter gets into an infinate loop and ets all the stack ", "status": "Closed", "priority": "Major", "reporter": "Tim Hawkins", "assignee": null, "created": "2009-03-03T13:26:47.876+0000", "updated": "2011-04-01T15:03:17.454+0000", "description": "When crawling pages with seperate fetch and parse, I see processes die becuase of stack overflow. \n\nOutput is generaly.\njava.lang.StackOverflowError\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:146)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\n\nInspection of the code shows that this is a recursive call to walk(.....) \n", "comments": ["do you know the URL of the document causing this problem?", "afraid not, its happened several hours into a crawl, so i'm not sure  \nwhere it was\n\n\n\n\n", "I've had the same issue (see mail http://www.mail-archive.com/nutch-user@lucene.apache.org/msg13030.html).\n\nURL: http://www.marktplatz-osterholz.de/Freizeit_Tourismus/index.html?navid=15\n\nI introduced a counter to determine how many recursive steps, the parser has already done. The counter increases for every recursive call of the method walk and decreases when the walk method is left. If the counter reaches a specified limit (I used 900), it ignores the deeper levels. For me that worked. i don't know, if that's the correct approach, but at least the parser finishes without failure.", "This patch catches errors in the walk method of JSParser and returns instead of breaking the whole job", "the patch above does not fix the issue but prevents it from crashing the whole parsing", "I was going to do exactly the same thing for now :-)\n\n\n\n", "Ill install this on our rig and test it and get back to you\n\n\n\n", "I can now confirm that this workaround allows the parse to complete\n\nSorry it took so long to get back to you.\n\n\n\n", "Hi Tim, \n\ndid you have a look at the logs to see which URL was causing the problem in the first place? do you specify a custom max length for the content to be fetched? \n\nMartina's example above is 428.42 kB which is far beyond the default max length, I am wondering whether the problem you found could be related to the fact that long documents are trimmed to the max length.", "I can confirm that this patch works when applied to nutch-1.0 (release) running on hadoop 0.19.1.   I encountered the same stack overflow error with a custom max content length of 4MB.   Julien, I tried looking through the logs to find the URL that was causing the problem, but did not see anything entries of interest.  (I fetch first, and then parse, so perhaps I won't see the URL in that case? The only output I get from the parser is related to PDF files)\n\n", "-1 for the patch. As Julien notes, the patch simply catches the error. I would suggest adding a depth counter instead. I don't think that there will be any half-sane HTML page out there with more than, say, 100 levels.\n\nAlternatively, we may try using NodeWalker.", "Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "JSParseFilter gets into an infinate loop and ets all the stack ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "JSParseFilter gets into an infinate loop and ets all the stack "}, {"question": "What is the main context?", "answer": "When crawling pages with seperate fetch and parse, I see processes die becuase of stack overflow. \n\nOutput is generaly.\njava.lang.StackOverflowError\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JS"}]}}
{"issue_id": "NUTCH-71", "project": "NUTCH", "title": "Search web page doesn't not focus on query input", "status": "Closed", "priority": "Minor", "reporter": "Christophe Noel", "assignee": "Jerome Charron", "created": "2005-07-12T21:17:29.000+0000", "updated": "2011-06-08T21:34:03.985+0000", "description": "In search.html and search.jsp , keyboard cursor does not focus in the form query input.\n\nI've made a patch for en and fr search.html and for search.jsp.", "comments": ["Search.html (fr,en) and search.jsp focus patch.", "(Patch is build with svn diff >>)", "Solved by revision 233312\n(http://svn.apache.org/viewcvs.cgi?rev=233312&view=rev)\n\n(I will close this issue as soon as I have the privileges)\n", "Thanks Christophe for reporting it and for your piece of code."], "tasks": {"summary": "Search web page doesn't not focus on query input", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Search web page doesn't not focus on query input"}, {"question": "What is the main context?", "answer": "In search.html and search.jsp , keyboard cursor does not focus in the form query input.\n\nI've made a patch for en and fr search.html and for search.jsp."}]}}
{"issue_id": "NUTCH-710", "project": "NUTCH", "title": "Support for rel=\"canonical\" attribute", "status": "Open", "priority": "Minor", "reporter": "Frank McCown", "assignee": null, "created": "2009-03-03T14:49:46.759+0000", "updated": "2025-07-09T20:25:46.019+0000", "description": "There is a the new rel=\"canonical\" attribute which is\nnow being supported by Google, Yahoo, and Live:\n\nhttp://googlewebmastercentral.blogspot.com/2009/02/specify-your-canonical.html\n\nAdding support for this attribute value will potentially reduce the number of URLs crawled and indexed and reduce duplicate page content.", "comments": ["Great idea. Won't be included in 1.1 though so moving to *fix : unknown*\n", "Shall we treat pages with a canonical metatag as a form of redirection? We know that there is no point indexing the page and that we'd be better off making sure that the page it refers to is fetched, parsed and indexed. That will not prevent these entries to be put in the crawlDB but should limit the size of the index and more importantly its quality. \n\nAlternatively we could keep the content of the page for indexing and rely on the de-duplication later. This would allow to have something returned in the search even if the target of the canonical tag has not been indexed yet (or if it does not exist). \n\nThe first option would be easier to implement. The second option would require some adaptation to the DeleteDuplicates and SolrDeleteDuplicates classes\n\nAny thoughts on this?", "As suggested previously we could either treat canonicals as redirections or during deduplication. Neither are satisfactory solutions.\n\nRedirection : we want to index the document if/when the target of the canonical is not available for indexing. We also want to follow the outlinks. \nDedup : could modify the *DeleteDuplicates code but canonical are more complex due to fact that we need to follow redirections\n\nWe probably need a third approach: prefilter by going through the crawldb & detect URLs which have a canonical target already indexed or ready to be indexed. We need to follow up to X levels of redirection e.g. doc A marked as canonical representation doc B, doc B redirects to doc C etc...if end of redirection chain exists and is valid then mark A as duplicate of C (intermediate redirs will not get indexed anyway)\n\nAs we don't know if has been indexed yet we would give it a special marker (e.g. status_duplicate) in the crawlDB. Then\n-> if indexer comes across such an entry : skip it\n-> make so that *deleteDuplicates can take a list of URLs with status_duplicate as an additional source of input OR have a custom resource that deletes such entries in SOLR or Lucene indices\n\nThe implementation would be as follows :\n\nGo through all redirections and generate all redirection chains e.g.\n\nA -> B\nB -> C\nD -> C\n\nwhere C is an indexable document (i.e. has been fetched and parsed - it may have been already indexed.\n\nwill yield\n\nA -> C\nB -> C\nD -> C\n\nbut also\n\nC -> C\n\nOnce we have all possible redirections : go through the crawlDB in search of canonicals. if the target of a canonical is the source of a valid alias (e.g. A - B - C - D) mark it as 'status:duplicate'\n\nThis design implies generating quite a few intermediate structures + scanning the whole crawlDB twice (once of the aliases then for the canonical) + rewrite the whole crawlDB to mark some of the entries as duplicates.\n\nThis would be much easier to do when we have Nutch2/HBase : could simply follow the redirs from the initial URL having a canonical tag instead of generating these intermediate structures. We can then modify the entries one by one instead of regenerating the whole crawlDB.\n\nWDYT?\n\n", "Hello -\n\nI am having two problems with Nutch and am hoping that you can help me out.\n\na) Crawling does not use link rel=\"canonical\" to index the links.\n\nb) Crawling ignores robots.txt.\n\nI am currently using Nutch 1.1 for crawling my local company site\n\nI have tried various settings from the web forums but am unable to get the above \nissues working.\n\nCan you tell me how to enable these while crawling.\n\nAppreciate your answer\n\nThanks in advance,\nRegards\nPraveen.\n\n\n\n      \n", "Putting a useful issue back on the radar. Fix for 2.0?", "Set and Classify ", "It seems the Fix Version of this issue keeps getting higher, while being created back in 2009. Can we say the 1.6/2.2 is final? I don't think anyone disagrees when I say the feature would be definitely useful.", "Iwan : sure, feel free to send a patch if you want to help it happen", "Currently looking into creating a patch which should handle the different Canonical features, will post again when I have a stable working version. For others, Google webmaster central's page about canonical pages and links is revised and more information is added: http://support.google.com/webmasters/bin/answer.py?hl=nl&answer=139394.", "I have implemented non-canonical page detection by means of modifying parse-html plugin. Though I am not that familiar with Nutch architecture and not sure if my implementation is in line with it. What I did is to have utility method boolean isCanonical(Node root, String baseUrl) which returns status of currently parsed html page: true if it is proper page, false if it is non canonical. Then in parse-html plugin's HtmlParser.getParse I call isCanonical and return from the method with ParseStatus\n\nif(!utils.isCanonical(root, baseUrl)\n return ParseStatusUtils.getEmptyParse(9999, \"Non canonical page\", getConf());\n\nIs this the right way to do it (of cause this needs to be made configurable)? If someone more familiar with Nutch would confirm or suggest more proper way I'd submit a patch. ", "If you could submit a working patch for this issue we can get cracking on testing and passing comments. Something is better than nothing. Thank you.", "Attaching the patch works with 2x branch (1x has different API). See comments in the code and my previous message for details how it works and issues to be solved.", "This feature would be awesome, noticed it got put on 1.8, is this actually coming out with 1.8?", "Nope. The version tag is more of a reminder that we'd like to push it to that release, not a firm commitment that it will be so. If an issue is marked  as resolved then you can be sure it will be part of said release though.\n\n", "hi [~jnioche] [~lewismc], I want to work about this issue for 2x branch. What is the last decision about the issue.", "hi [~msertacturkel], by the looks of the patch attached, it requires to be possibly extended to also cover parse-tika plugin as well as unit tests for both plugins. That would probably be it I suppose. We can then port it to 1.X API. If you were able to take this on it would be great.", "Hi [~lewismc] ,\n\nI prepared a patch file to solve this issue for 2.x branch. Patch file also covers test cases for tika parser and html parser plugins.Could you review my patch files.", "Thanks,  [~Sertac Turkel]! My comments:\n* every page containing a canonical link is now rejected. That's a rather hard decision. It should be configurable whether pages containing correct (non-empty, not self-referential, etc.) canonical links\n*# are unconditionally rejected\n*# are removed later only if the target is indexed. It's close to deduplication, and it's what canonical links are intended for: give web masters a chance to support and influence deduplication.\n*# are only recorded (as outlinks and/or as indexed fields)\nThis point is the most challenging one: you need to take care for all nasty situations \"in the wild\", e.g. a canonical link pointing to a redirect which leads you back to the current page, etc. It's required to \"resolve\" chains of canonical links in combination with redirects, see Julien's comment and [1|http://mail-archives.apache.org/mod_mbox/nutch-user/201203.mbox/%3CCA+-fM0sg=rvuNxzoez5NLFmhNJHta=qP5qHTfRJ8ii55fB2mJA@mail.gmail.com%3E].\n* is it really necessary to handle canonical links explicitely in DbUpdateMapper and mark as injected? Couldn't this be done by adding them simply as outlinks? Per default links of \"link\" elements are added as outlinks, cf. parser.html.outlinks.ignore_tags. Of course, canonical links should be added even if \"link\" elements are ignored.\n* extraction of canonical links: at least, the following points are missing: relative URLs, and canonical link inside HTTP headers (required for anything which is not HTML). I'll try support you in this point because there's already some work done.\n* keep names in parallel?\n{code}src/plugin/parse-html/.../TestDOMContentUtils.java\nsrc/plugin/parse-tika/.../DOMContentUtilsTest.java\n{code}\n\n... and some useful references:\n[http://en.wikipedia.org/wiki/Canonical_link_element]\n[http://tools.ietf.org/html/rfc6596]\n[https://support.google.com/webmasters/answer/139066]\n[http://www.mattcutts.com/blog/rel-canonical-html-head/]\n[http://googlewebmastercentral.blogspot.de/2011/06/supporting-relcanonical-http-headers.html]\n"], "tasks": {"summary": "Support for rel=\"canonical\" attribute", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Support for rel=\"canonical\" attribute"}, {"question": "What is the main context?", "answer": "There is a the new rel=\"canonical\" attribute which is\nnow being supported by Google, Yahoo, and Live:\n\nhttp://googlewebmastercentral.blogspot.com/2009/02/specify-your-canonical.html\n\nAdding support fo"}]}}
{"issue_id": "NUTCH-711", "project": "NUTCH", "title": "Indexer failing after upgrade to Hadoop 0.19.1", "status": "Closed", "priority": "Minor", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2009-03-04T10:52:40.204+0000", "updated": "2009-04-10T12:29:07.629+0000", "description": "After upgrade to Hadoop 0.19.1 Reducer is initialized in a different order than before (see http://svn.apache.org/viewvc?view=rev&revision=736239). IndexingFilters populate current JobConf with field options that are required for IndexerOutputFormat to function properly. However, the filters are instantiated in Reducer.configure(), which is now called after the OutputFormat is initialized, and not before as previously.\n\nThe workaround for now is to instantiate IndexinigFilters once again inside IndexerOutputFormat.  This issue should be revisited before 1.1 in order to find a better solution.\n\nSee this thread for more information: http://www.lucidimagination.com/search/document/7c62c625c7ea17fe/problem_with_crawling_using_the_latest_1_0_trunk", "comments": ["This patch instantiates IndexingFilters in IndexerOutputFormat, and thus fixes the issue. If there are not objections I will commit it shortly.", "+1", "Applied the patch in rev. 750037. I'm not closing this issue, because this needs to be solved in a better way after 1.0.", "Lowering priority because it works now, it just needs a cleanup.", "Integrated in Nutch-trunk #743 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/743/])\n     - Indexer failing after upgrade to Hadoop 0.19.1. This is a temporary\nfix, to be revisited later.\n", "closing issues for released version"], "tasks": {"summary": "Indexer failing after upgrade to Hadoop 0.19.1", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Indexer failing after upgrade to Hadoop 0.19.1"}, {"question": "What is the main context?", "answer": "After upgrade to Hadoop 0.19.1 Reducer is initialized in a different order than before (see http://svn.apache.org/viewvc?view=rev&revision=736239). IndexingFilters populate current JobConf with field "}]}}
{"issue_id": "NUTCH-712", "project": "NUTCH", "title": "ParseOutputFormat should catch java.net.MalformedURLException coming from normalizers", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Andrzej Bialecki", "created": "2009-03-06T11:57:43.443+0000", "updated": "2009-11-29T04:08:33.386+0000", "description": "ParseOutputFormat should catch java.net.MalformedURLException coming from normalizers otherwise the whole parsing step crashes instead of simply ignoring dodgy outlinks", "comments": ["I'm not sure that ignoring this exception is the right thing to do ... if we fail to normalize the url, we also fail to filter it. This means that if we proceed as if nothing happened (which your patch does) we could end up with many unfiltered junk urls.\n\nI think a better alternative is to return, i.e. to skip this record without further processing.", "Modified version of the patch : if normalizers indicate that a redirection URL is not correct then skip the filtering  and treat this redirection URL as filtered. ", "Fixed in rev. 885159. Thank you!", "Integrated in Nutch-trunk #996 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/996/])\n     ParseOutputFormat should catch java.net.MalformedURLException\ncoming from normalizers.\n"], "tasks": {"summary": "ParseOutputFormat should catch java.net.MalformedURLException coming from normalizers", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "ParseOutputFormat should catch java.net.MalformedURLException coming from normalizers"}, {"question": "What is the main context?", "answer": "ParseOutputFormat should catch java.net.MalformedURLException coming from normalizers otherwise the whole parsing step crashes instead of simply ignoring dodgy outlinks"}]}}
{"issue_id": "NUTCH-713", "project": "NUTCH", "title": "Config options for webgraph Scoring not documented", "status": "Closed", "priority": "Minor", "reporter": "Eric J. Christeson", "assignee": null, "created": "2009-03-09T15:41:43.855+0000", "updated": "2012-04-24T14:11:20.775+0000", "description": "There are a number of properties for webgraph scoring that are only documented in code.  I have found these:\n\nlink.ignore.internal.host\nlink.ignore.internal.domain\nlink.ignore.limit.domain\nlink.ignore.limit.host\nlink.ignore.limit.page\nlink.loops.depth\nlink.analyze.initial.score\nlink.analyze.damping.factor\nlink.analyze.rank.one\nlink.analyze.iteration\nlink.analyze.num.iterations\n\nI have a patch to add these to conf/nutch-default.xml with the best description I could find.", "comments": ["Patch to add config options to conf/nutch-default.xml", "Is it deemed necessary to add these properties to nutch-default.xml? We have some documentation on the WebGraph and associated classes on the wiki [1] [2], however I am not sure by adding the number of properties suggested in the patch that we would necessarily clear up any discrepancies when trying to configure the WebGraph classes properly.\n\nAny thoughts...\n\n[1] http://wiki.apache.org/nutch/NewScoring\n[2] http://wiki.apache.org/nutch/NewScoringIndexingExample", "Yes i believe these must be added to the default config. The nutch-default file is a good piece of option documentation and since linkrank is part of Nutch these should be added.", "Thanks Eric!\n\nThese configuration directives have already been added via another issue."], "tasks": {"summary": "Config options for webgraph Scoring not documented", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Config options for webgraph Scoring not documented"}, {"question": "What is the main context?", "answer": "There are a number of properties for webgraph scoring that are only documented in code.  I have found these:\n\nlink.ignore.internal.host\nlink.ignore.internal.domain\nlink.ignore.limit.domain\nlink.ignore"}]}}
{"issue_id": "NUTCH-714", "project": "NUTCH", "title": "Need a SFTP and SCP Protocol Handler", "status": "Closed", "priority": "Major", "reporter": "Sanjoy Ghosh", "assignee": "Chris A. Mattmann", "created": "2009-03-10T00:39:52.035+0000", "updated": "2017-09-22T11:26:07.150+0000", "description": "An SFTP and SCP Protocol handler is needed to fetch intranet content on an SFTP or SCP server.", "comments": ["Hi Sanjoy,\n\nWhen you get a patch, let me know and I will work to integrate it. For reference, you were intending this as an upgrade for 0.8.2? I think we should probably do this as a post 1.0 upgrade (maybe 1.1)?\n\nCheers,.\nChris\n", "I am attaching the package for a Sftp protocol handler.  Please let me know if it needs any changes.", "Changing fix version to 'unknown'", "- patch contributed by Sanjoy Ghosh, and contributed in r1026729. I upgraded the patch to the latest Protocol interface using o.a.nutch.storage.WebPage and fixed some stuff here and there, and added ASL headers. If there is interest in a 1.3 release, I would be willing to backport to 1.3. Thanks Sanjoy!\n", "I believe a 1.3 patch would be very welcome. Nutch 2.0 is perhaps too different from 1.x series that for many users the transition may be overwhelming. This is a useful feature so a 1.3 release with this may benefit many users.", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Need a SFTP and SCP Protocol Handler", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Need a SFTP and SCP Protocol Handler"}, {"question": "What is the main context?", "answer": "An SFTP and SCP Protocol handler is needed to fetch intranet content on an SFTP or SCP server."}]}}
{"issue_id": "NUTCH-715", "project": "NUTCH", "title": "Subcollection plugin doesn't work with default subcollections.xml file", "status": "Closed", "priority": "Major", "reporter": "Dmitry Lihachev", "assignee": "Sami Siren", "created": "2009-03-10T05:54:35.544+0000", "updated": "2009-04-10T12:29:05.647+0000", "description": "Subcollection plugin cann't parse his configuration file because it contatins top level comment (ASF notice) and DomUtil doesn't carry about of top-level comments", "comments": ["committed, thanks Dmitry!", "Integrated in Nutch-trunk #749 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/749/])\n     - Subcollection plugin doesn't work with default subcollections.xml file. Contributed by Dmitry Lihachev\n", "closing issues for released version"], "tasks": {"summary": "Subcollection plugin doesn't work with default subcollections.xml file", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Subcollection plugin doesn't work with default subcollections.xml file"}, {"question": "What is the main context?", "answer": "Subcollection plugin cann't parse his configuration file because it contatins top level comment (ASF notice) and DomUtil doesn't carry about of top-level comments"}]}}
{"issue_id": "NUTCH-716", "project": "NUTCH", "title": "Make subcollection index filed multivalued", "status": "Closed", "priority": "Major", "reporter": "Dmitry Lihachev", "assignee": null, "created": "2009-03-10T08:42:19.168+0000", "updated": "2011-04-01T15:07:22.412+0000", "description": "Looks like a reasonable thing to do. Marking as 1.2 and will commit if no one objects", "comments": ["- pushing this out per http://bit.ly/c7tBv9", "Some hunks fail while patching the 1.1 release:\n\n# cd apache-nutch-1.1-bin/\n# wget wget https://issues.apache.org/jira/secure/attachment/12401819/NUTCH-716_multivalued_subcollection.patch\n# patch -p0 < NUTCH-716_multivalued_subcollection.patch\n\npatching file conf/schema.xml\npatching file src/plugin/subcollection/src/java/org/apache/nutch/collection/CollectionManager.java\nHunk #4 FAILED at 172.\n1 out of 4 hunks FAILED -- saving rejects to file src/plugin/subcollection/src/java/org/apache/nutch/collection/CollectionManager.java.rej\npatching file src/plugin/subcollection/src/java/org/apache/nutch/indexer/subcollection/SubcollectionIndexingFilter.java", "Applied to nutch-1.2", "trunk => Committed revision 989726 & 989728\n1.2 => Committed revision 989727\n\nThanks!", "EDIT: new issue NUTCH-716 has been created. This comment can be ignored!\n\nThis patch concatenates multiple values in a single string instead of adding single values to a multi valued field. For a test crawl i have defined the following two subcollection definitions:\n\n <subcollection>\n  <name>asdf</name>\n  <id>asdf-site</id>\n  <whitelist>http://asdf/</whitelist>\n  <blacklist/>\n </subcollection>\n\n <subcollection>\n  <name>news</name>\n  <id>asdf-news</id>\n  <whitelist>http://asdf/news/</whitelist>\n  <blacklist/>\n </subcollection>\n\nReindexing the segments by sending them to Solr will yield the following results for a news URL:\n\n<doc>\n  <arr name=\"subcollection\">\n    <str>asdf</str>\n  </arr>\n  <str name=\"url\">http://asdf/home/</str>\n</doc>\n<doc>\n  <arr name=\"subcollection\">\n    <str>asdf news</str>\n  </arr>\n  <str name=\"url\">http://asdf/news/</str>\n</doc>\n\nInstead, i expected the following result for the second document:\n\n<doc>\n  <arr name=\"subcollection\">\n    <str>asdf</str>\n    <str>news</str>\n  </arr>\n  <str name=\"url\">http://asdf/news/</str>\n</doc>\n\nMy Solr schema.xml has the following declaration for the subcollection field:\n\n<field name=\"subcollection\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\" />\n\nThe latest nightly build i could find:\nnutch-2010-07-07_04-49-04\n", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Make subcollection index filed multivalued", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Make subcollection index filed multivalued"}, {"question": "What is the main context?", "answer": "Looks like a reasonable thing to do. Marking as 1.2 and will commit if no one objects"}]}}
{"issue_id": "NUTCH-717", "project": "NUTCH", "title": "Make Nutch Solr integration easier", "status": "Open", "priority": "Major", "reporter": "Sami Siren", "assignee": null, "created": "2009-03-10T09:56:58.127+0000", "updated": "2025-07-09T20:25:46.837+0000", "description": "Erik Hatcher proposed we should provide a full solr config dir to be used with Nutch-Solr. Now we only provide index schema. It would be considerably easier to setup nutch-solr if we provided the whole conf dir that you could use with solr like:\n\njava -Dsolr.solr.home=<Nutch's Solr Home> -jar start.jar\n", "comments": ["+1\n\nI've just tried to integrate solr and Nutch following a fairly clear explanation. However I failed to get it working and have no obvious errors in the log files to tell me what went wrong. Anything which can be done to simplify this process would be helpful. ", "+1 from me too.\n\nI have another proposition: Would it make sense to add full solr jar (and webapp) to our code base and make something like this work?\n\nbin/nutch solrserver\n\nThis would make integration much easier, but may make nutch size very big.", "- pushing this out per http://bit.ly/c7tBv9", "Back on the radar for 2.0?\n", "After having had a chance to work through the updated tutorial: http://wiki.apache.org/nutch/RunningNutchAndSolr#A4._Setup_Solr_for_search I think the Solr step is a bit awkward as well.   One of the reasons Solr has seen great adoption is that the /example app is so well thought out, and easy to get started.   \n\nWith Nutch being decoupled from Solr, but depending on Solr, I wonder if there is an issue of a user downloading nutch, and then downloading Solr, and the versions being out of whack?  Like if Nutch depends on Solr 3.3, but Solr 4 has been released.   \n\nI could see taking a version of Solr and checking it in.  Strip it down to JUST what Nutch needs, and then you could include in the Nutch version a nice Velocity based UI for just browsing through the data that Nutch returns.", "We can add a Solr instance with Jetty and deploy it in the runtime directory. If a user can simply go to runtime/solr directory and run with java -jar start.jar it greatly reduces the hassle for new users. We can then also move our schema.xml to the proper location.", "Maybe we could make the indexing backends pluggable first and move the SOLR-related stuff to a new plugin? The plugin would have a custom task (e.g. startSOLR) as you described but this would not affect the common build.xml + the various config files would be kept separated from the content of the main conf dir. Makes sense? ", "Makes sense indeed! Same would be true for ES, bundle it in the plugi-to-be-made.", "Are we to provide any support for users wishing to use Solr within a container such as Tomcat? e.g. is it going to be necessary/required for us to ship a .WAR file to incorporate suggestions here? Personally I first began using a solr.war with Tomcat due to the production environment I was in and the requirement to monitor and run everything through Tomcat, however I now find using Solr independently inside Jetty as a more suitable option. What are the thoughts here?\n\nRegarding your comment Julien, I am very much in favour of making a Solr indexing backend pluggable. It would establish a nice structure/precedence for any future options we wish to support as stated by Markus.\n\nThis is on the Radar for both 1.4 and 2.0 though... what and where are the differences? I think we can only begin to make progress with this when both incorporated issues as above are resolved.", "20120304-push-1.6"], "tasks": {"summary": "Make Nutch Solr integration easier", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Make Nutch Solr integration easier"}, {"question": "What is the main context?", "answer": "Erik Hatcher proposed we should provide a full solr config dir to be used with Nutch-Solr. Now we only provide index schema. It would be considerably easier to setup nutch-solr if we provided the whol"}]}}
{"issue_id": "NUTCH-718", "project": "NUTCH", "title": "urlfilter-subnets plugin", "status": "Closed", "priority": "Minor", "reporter": "Dmitry Lihachev", "assignee": null, "created": "2009-03-12T09:49:26.908+0000", "updated": "2019-10-13T22:36:09.592+0000", "description": "This plugin filter urls by netmasks in CIDR-notation", "comments": ["{code}\ncd nutch-trunk\npatch -p0 < NUTCH-718_urlfilter_subnets.patch\nmkdir src/plugin/urlfilter-subnets/lib/\ncp $COMMONS_NET_DIR/commons-net-2.0.jar src/plugin/urlfilter-subnets/lib/\n{code}", "new version with impooved logging and excetpion handling", "applied for nutchbase", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "urlfilter-subnets plugin", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "urlfilter-subnets plugin"}, {"question": "What is the main context?", "answer": "This plugin filter urls by netmasks in CIDR-notation"}]}}
{"issue_id": "NUTCH-719", "project": "NUTCH", "title": "fetchQueues.totalSize incorrect in Fetcher2", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2009-03-12T14:41:02.982+0000", "updated": "2010-02-23T08:49:39.091+0000", "description": "I had a look at the logs generated by Fetcher2 and found cases where there were no active fetchQueues but fetchQueues.totalSize was != 0\n\nfetcher.Fetcher2 - -activeThreads=200, spinWaiting=200, fetchQueues.totalSize=1, fetchQueues=0\n\nsince the code relies on fetchQueues.totalSize to determine whether the work is finished or not the task is blocked until the abortion mechanism kicks in\n\n2009-03-12 09:27:38,977 WARN  fetcher.Fetcher2 - Aborting with 200 hung threads.\n\ncould that be a synchronisation issue? any ideas?", "comments": ["I've done some investigation on this.\n\nIt looks to me as if queues can get reaped to early. I've put in some debug and this is what I see:\n\n2009-07-09 04:39:50,704 DEBUG fetcher.Fetcher -   FetchItemQueue::getFetchItemQueue() id=http://125.168.254.20\n2009-07-09 04:39:50,704 DEBUG fetcher.Fetcher - Created queue: http://125.168.254.20\n....\n2009-07-09 04:39:50,704 DEBUG fetcher.Fetcher - reaping: http://125.168.254.20\n.....\n2009-07-09 04:39:50,705 DEBUG fetcher.Fetcher - addFetchItem: adding item - http://www.callidan.com/ma100.htm\n2009-07-09 04:39:50,883 DEBUG fetcher.Fetcher - totalSize++:2 http://125.168.254.20 http://www.callidan.com/ma100.htm queuesize: 1 queuecount: 11\n2009-07-09 04:39:50,883 DEBUG fetcher.Fetcher - * queue: http://61.9.216.193, size: 0\n2009-07-09 04:39:50,883 DEBUG fetcher.Fetcher - * queue: http://216.184.34.250, size: 0\n2009-07-09 04:39:50,883 DEBUG fetcher.Fetcher - * queue: http://139.146.150.23, size: 0\n2009-07-09 04:39:50,883 DEBUG fetcher.Fetcher - * queue: http://203.29.78.68, size: 0\n2009-07-09 04:39:50,883 DEBUG fetcher.Fetcher - * queue: http://150.101.91.39, size: 0\n2009-07-09 04:39:50,883 DEBUG fetcher.Fetcher - * queue: http://209.212.110.211, size: 0\n2009-07-09 04:39:50,883 DEBUG fetcher.Fetcher - * queue: http://123.176.112.44, size: 0\n2009-07-09 04:39:50,883 DEBUG fetcher.Fetcher - * queue: http://117.104.160.130, size: 0\n2009-07-09 04:39:50,883 DEBUG fetcher.Fetcher - * queue: http://196.25.73.205, size: 0\n2009-07-09 04:39:50,884 DEBUG fetcher.Fetcher - * queue: http://202.53.7.145, size: 0\n2009-07-09 04:39:50,884 DEBUG fetcher.Fetcher - * queue: http://202.60.67.145, size: 1\n\nNote that the queue is created and then immediately reaped, and after totalSize is incremented, that queue does not appear in the list, even though it supposedly has the item added to it.\n\nThe upshot is that the url is never fetched (as the queue has gone) so totalSize never = 0, and eventually the abort will happen.\n\nIn short I'd say this is a sync issue, but I'm not sure where the best place to lock would be.\n\nAny comments from the author?\n", "I've changed line 324 of src/java/org/apache/nutch/fetcher/Fetcher.java to \n\npublic void synchronized void addFetchItem(FetchItem it) {\n\n(added the synchronized) and initial testing looks good.\n\n", "Thanks for looking into this bug.\n\nI wonder if this is the cause of the performance problem so many people are facing with Fetcher in nutch-1.0. Can it be that QueueFeeder stops feeding new URLs into FetchQueues because of this bug?", "I'm not sure, as far as I can tell, the feeder has always finished feeding the urls, it's just that a proportion are \"lost\".\n\nHowever, there are two things I've noted re performance (if you just look at url's crawled per second)\n\n1) When this situation arrises, the fetcher will time out and \"Abort with N hung threads\". The timeout occurs after \"mapred.task.timeout\"/2 or seconds (default 5 mins), so any timing on a crawl that aborted will be extended by 5 mins. One a small crawl this could skew the figures\n\n2) DNS look up can take a while. I know this has been noted before, but on my test system (admittedly only a vm on our network, with nothing special in terms of DNS), some of the look ups were taking 5-6 seconds. THis is possibley the wrong place to discuss given NUTCH-721, but I put in some debug arround the feeder thread and got:\n\n2009-07-10 04:01:35,296 INFO  fetcher.Fetcher - Fed 500 urls in 186 secs = 2.7url/s\n2009-07-10 04:04:18,343 INFO  fetcher.Fetcher - Fed 499 urls in 163 secs = 3.1url/s\n2009-07-10 04:06:57,109 INFO  fetcher.Fetcher - Fed 498 urls in 158 secs = 3.2url/s\n2009-07-10 04:10:38,282 INFO  fetcher.Fetcher - Fed 499 urls in 221 secs = 2.3url/s\n2009-07-10 04:12:58,371 INFO  fetcher.Fetcher - Fed 498 urls in 140 secs = 3.6url/s\n2009-07-10 04:16:12,275 INFO  fetcher.Fetcher - Fed 499 urls in 193 secs = 2.6url/s\n2009-07-10 04:19:20,162 INFO  fetcher.Fetcher - Fed 499 urls in 187 secs = 2.7url/s\n2009-07-10 04:21:25,846 INFO  fetcher.Fetcher - Fed 499 urls in 125 secs = 4.0url/s\n2009-07-10 04:24:16,049 INFO  fetcher.Fetcher - Fed 495 urls in 170 secs = 2.9url/s\n2009-07-10 04:27:01,944 INFO  fetcher.Fetcher - Fed 499 urls in 165 secs = 3.0url/s\n2009-07-10 04:29:26,247 INFO  fetcher.Fetcher - Fed 499 urls in 144 secs = 3.5url/s\n2009-07-10 04:32:02,590 INFO  fetcher.Fetcher - Fed 499 urls in 156 secs = 3.2url/s\n2009-07-10 04:34:49,985 INFO  fetcher.Fetcher - Fed 498 urls in 167 secs = 3.0url/s\n2009-07-10 04:37:28,367 INFO  fetcher.Fetcher - Fed 498 urls in 158 secs = 3.2url/s\n2009-07-10 04:40:09,865 INFO  fetcher.Fetcher - Fed 499 urls in 161 secs = 3.1url/s\n2009-07-10 04:42:55,203 INFO  fetcher.Fetcher - Fed 499 urls in 165 secs = 3.0url/s\n\nobviously when I'm only feeding 3-4 urls/sec, i'll only every be able to fetch that. That test was one a crawldb just initialised with 11,000 urls (unique sites).\n\nHowever, on the next iteration where I'm feeding urls from non-unique sites, I see 5-7 times that rate.\n", "perhaps i spoke too soon\n\n10 threads, 15520 pages, 723 errors, 3.7 pages/s, 2972 kb/s, \n-activeThreads=10, spinWaiting=10, fetchQueues.totalSize=0, fetchQueues.count=0\nAborting with 10 hung threads.\nUnable to resolve: www.countryenergy.com.au, skipping.\nException in thread \"QueueFeeder\" java.lang.NullPointerException\n\tat org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:48)\n\tat org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:41)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:206)\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:238)\n\tat org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:177)\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:111)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:370)\n\tat org.apache.hadoop.io.SequenceFile$Reader.readRecordLength(SequenceFile.java:1895)\n\tat org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1925)\n\tat org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2062)\n\tat org.apache.hadoop.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:76)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:192)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:176)\n\tat org.apache.nutch.fetcher.Fetcher$QueueFeeder.run(Fetcher.java:418)\n\n\nIt apears that the feeder hung, but I'm not sure whether the exception raised is the cause or the effect (i suspect it's the effect of the thread aborting)\n\nI'm also not sure if any of these issues are vm related. Hopefully our real hardware will turn up soon....", "Committed revision 911905.\nThanks to S. Dennis for investigating the issue + R. Schwab for testing it ", "Integrated in Nutch-trunk #1074 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1074/])\n     fetchQueues.totalSize incorrect in Fetcher\n", "I notice the other addFetchItem method of FetchItemQueues  and FetchItemQueue in Fetcher.java should these also be synchronized?", "the other addFetchItem method of FetchItemQueues uses the synchronized one internally so there is no need for synch there + the one in FetchItemQueue is not necessary as it uses a synchronized collection internally. "], "tasks": {"summary": "fetchQueues.totalSize incorrect in Fetcher2", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "fetchQueues.totalSize incorrect in Fetcher2"}, {"question": "What is the main context?", "answer": "I had a look at the logs generated by Fetcher2 and found cases where there were no active fetchQueues but fetchQueues.totalSize was != 0\n\nfetcher.Fetcher2 - -activeThreads=200, spinWaiting=200, fetchQ"}]}}
{"issue_id": "NUTCH-72", "project": "NUTCH", "title": "Query basic filter with correction feature", "status": "Closed", "priority": "Major", "reporter": "Christophe Noel", "assignee": null, "created": "2005-07-15T20:26:29.000+0000", "updated": "2011-06-08T21:34:03.721+0000", "description": "This plugin improves query-basic plugin with a correction feature.\n\nLucene includes FuzzyQuery feature which consists of searching not only for matching terms, but searching for very similar terms too.\nThis plugin should be used instead of query-basic, for people looking for an easy solution about users query requests correction.\n\nCorrection Query Plugin can be used as follows :\nSolution 1 :  If you want to search for very similar terms, add autocorrectionmod as the first term of the query (example : 'nutch engine' -> 'autocorrectionmod nutch engine')\nSolution 2 : Create a new search.jsp page which include a \"correction\" checkbox management (<input type=\"checkbox\" name=\"autocorrection\" value=\"true\"> may automatically add 'autocorrectionmod' as the first term of the query) \n\nQueryFuzzy knows a big problem : it is very slow for large index !\n\nSo Correction Query Plugin works as follows :\n- it is not useful for big indexes\n- it only works for 5 characters and more words\n- it only look for words matching with the 2 first characters (to improve performance this should be set to 3/4)\n- it only works for 65 % matching suffixes (algorithm is levenstein)\n\nPLease give your opinion about it.", "comments": ["Here is the CVS diff patch.\n\nI can send the search.jsp if requested.", "Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Query basic filter with correction feature", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Query basic filter with correction feature"}, {"question": "What is the main context?", "answer": "This plugin improves query-basic plugin with a correction feature.\n\nLucene includes FuzzyQuery feature which consists of searching not only for matching terms, but searching for very similar terms too"}]}}
{"issue_id": "NUTCH-720", "project": "NUTCH", "title": "site: search operator with no query term", "status": "Closed", "priority": "Minor", "reporter": "Frank McCown", "assignee": null, "created": "2009-03-12T18:53:49.933+0000", "updated": "2009-04-10T12:29:00.888+0000", "description": "Google, Yahoo, and Live list all pages they have indexed for the \"site:www.example.com\" query.  But Nutch returns back 0 results unless a query term is also supplied (e.g., \"site:www.example.com term\"). It would be helpful to make Nutch support the site: operator in a consistent manor.\n\n", "comments": ["This issue can be solved by changing the query.site.boost property in nutch-default.xml from 0.0 to 0.1. ", "closing issues for released version"], "tasks": {"summary": "site: search operator with no query term", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "site: search operator with no query term"}, {"question": "What is the main context?", "answer": "Google, Yahoo, and Live list all pages they have indexed for the \"site:www.example.com\" query.  But Nutch returns back 0 results unless a query term is also supplied (e.g., \"site:www.example.com term\""}]}}
{"issue_id": "NUTCH-721", "project": "NUTCH", "title": "Fetcher2 Slow", "status": "Closed", "priority": "Major", "reporter": "Roger Dunk", "assignee": "Dogacan Guney", "created": "2009-03-17T20:29:45.799+0000", "updated": "2009-08-25T05:46:46.329+0000", "description": "Fetcher2 fetches far more slowly than Fetcher1.\n\nConfig options:\nfetcher.threads.fetch = 80\nfetcher.threads.per.host = 80\nfetcher.server.delay = 0\ngenerate.max.per.host = 1\n\nWith a queue size of ~40,000, the result is:\n\nactiveThreads=80, spinWaiting=79, fetchQueues.totalSize=0\n\nwith maybe a download of 1 page per second.\n\nRuning with -noParse makes little difference.\n\nCPU load average is around 0.2. With Fetcher1 CPU load is around 2.0 - 3.0\n\nHosts already cached by local caching NS appear to download quickly upon a re-fetch, so possible issue relating to NS lookups, however all things being equal Fetcher1 runs fast without pre-caching hosts.\n", "comments": ["OK, there is clearly a problem with the new fetcher. \n\nFirst, let's make sure that there is indeed a problem with the new fetcher and this is not the side effect of some other code we introduced between 0.9 and 1.0. So I suggest that we re-commit old fetcher back into trunk and do a side-by-side comparison to make sure that the problem is with the new fetcher. \n\nIf it is with the new fetcher, then we may try to salvage Todd's work (I remember that he said that his fetcher was faster, right?).", "I've committed nutch 0.9 fetcher as OldFetcher. So can you test with trunk and OldFetcher so that we can find out if this is related to new fetcher or is the side effect of some other change?", "For the following tests I've used the same segment containing 5000 URLs. I cleaned the named cache before the first two tests.\n\n[root@server1 trunk]# time bin/nutch org.apache.nutch.fetcher.OldFetcher newcrawl/segments/20090402130655/\n\nreal    3m38.084s\nuser    2m20.887s\nsys     0m7.470s\n\n[root@server1 trunk]# time bin/nutch org.apache.nutch.fetcher.Fetcher newcrawl/segments/20090402130655/\n\n[...]\n\nFetcher: done\n\nreal    53m44.800s\nuser    2m20.070s\nsys     0m9.527s\n\nFor this next test, I used the same segment but didn't clear the named cache from the previous test, so all resolvable hosts should still be cached. This appeared to help greatly, as often times out of 80 active threads, only 60 were spinwaiting (as opposed to 79 in the non-cached test), but there were still plenty of times where at least 30 consecutive log entries showed 80 threads spinwaiting. And clearly as can be seen from the times below, still nowhere in the league of OldFetcher.\n\n[root@server1 trunk]# time bin/nutch org.apache.nutch.fetcher.Fetcher newcrawl/segments/20090402130655/\n\n[...]\n\nAborting with 80 hung threads.\nFetcher: done\n\nreal    22m5.420s\nuser    2m39.407s\nsys     0m8.192s", "Integrated in Nutch-trunk #772 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/772/])\n     - Commit old fetcher as OldFetcher for now so that we can test Fetcher2 performance.\n", "Wow, 53 min vs 3 min !?\n\nThanks a lot for testing and that is indeed very worrying.\n\nWhich 5000 url set did you use? I think the crawl_generate you attached to this issue has 13K urls?\n\nPS: One small thing new Fetcher requires less threads than OldFetcher. If you have time can you try with\nsmaller number of threads (say, 15-20)?", "I did a -\"topN 5000\", so only a subset of the attached, but still only 1 URL per host. The following is with 20 threads and also no parsing.\n\n[root@server1 trunk]# time bin/nutch org.apache.nutch.fetcher.Fetcher newcrawl/segments/20090402130655 -threads 20 -noParsing\n\n[...]\n\nAborting with 20 hung threads.\nFetcher: done\n\nreal    60m14.926s\nuser    0m38.671s\nsys     0m6.134s", "The message about the \"Aborted hung threads\" looks like what I described in https://issues.apache.org/jira/browse/NUTCH-719 except that in this case there are active queues but fetchQueues.totalSize=0 \n\nRoger : can you confirm that the parameter fetcher.threads.per.host.by.ip is set to false?\n\n", "Julien, yes, fetcher.threads.per.host.by.ip was set to false in the above tests. I have also tried it with true, which certainly didn't help the speed issue, but I can't comment on the hung threads as I didn't bother letting the fetch complete. I'd say there are two, likely unrelated problems with Fetcher2.", "Questions:\nHas anyone tried profiling this? (may be relevant: http://markmail.org/message/4ixrnvfycpgmkdno )\n\nOr maybe simply debugged/timed various blocks of code using something as simple as print statements and simple timers?\n\nOr maybe running just a single thread and then doing kill -QUIT a number of times to simply try and spot the method where the code seems to spend a lot of its time?\n", "Ken's thoughts: http://ken-blog.krugler.org/2009/05/19/performance-problems-with-verticalfocused-web-crawling/\n", "My tests were done on a segment with only 1 URL per host (generate.max.per.host = 1), so I don't believe what Ken has to say is the reason, at least in my case, for Fetcher2 performing slowly.", "I've done some testing on this and looked at the number of pages being fed, as this obvioulsy limits the number of pages you can fetch:\n\n2009-07-10 04:01:35,296 INFO fetcher.Fetcher - Fed 500 urls in 186 secs = 2.7url/s\n2009-07-10 04:04:18,343 INFO fetcher.Fetcher - Fed 499 urls in 163 secs = 3.1url/s\n2009-07-10 04:06:57,109 INFO fetcher.Fetcher - Fed 498 urls in 158 secs = 3.2url/s\n2009-07-10 04:10:38,282 INFO fetcher.Fetcher - Fed 499 urls in 221 secs = 2.3url/s\n2009-07-10 04:12:58,371 INFO fetcher.Fetcher - Fed 498 urls in 140 secs = 3.6url/s\n2009-07-10 04:16:12,275 INFO fetcher.Fetcher - Fed 499 urls in 193 secs = 2.6url/s\n2009-07-10 04:19:20,162 INFO fetcher.Fetcher - Fed 499 urls in 187 secs = 2.7url/s\n2009-07-10 04:21:25,846 INFO fetcher.Fetcher - Fed 499 urls in 125 secs = 4.0url/s\n2009-07-10 04:24:16,049 INFO fetcher.Fetcher - Fed 495 urls in 170 secs = 2.9url/s\n2009-07-10 04:27:01,944 INFO fetcher.Fetcher - Fed 499 urls in 165 secs = 3.0url/s\n2009-07-10 04:29:26,247 INFO fetcher.Fetcher - Fed 499 urls in 144 secs = 3.5url/s\n2009-07-10 04:32:02,590 INFO fetcher.Fetcher - Fed 499 urls in 156 secs = 3.2url/s\n2009-07-10 04:34:49,985 INFO fetcher.Fetcher - Fed 498 urls in 167 secs = 3.0url/s\n2009-07-10 04:37:28,367 INFO fetcher.Fetcher - Fed 498 urls in 158 secs = 3.2url/s\n2009-07-10 04:40:09,865 INFO fetcher.Fetcher - Fed 499 urls in 161 secs = 3.1url/s\n2009-07-10 04:42:55,203 INFO fetcher.Fetcher - Fed 499 urls in 165 secs = 3.0url/s\n\nThat test was one a crawldb just initialised with 11,000 urls (unique sites).\n\nHowever, on the next iteration where I'm feeding urls from non-unique sites, I see 5-7 times that rate. (My test system is a vm on our network, with nothing special in terms of DNS. Someof the look ups were taking 5-6 seconds).", "Steven, if you have time/hardware, can you retry your use-case with OIdFetcher in trunk?", "I had another look at this issue after applying the patch from Nutch-719. I can easily reproduce the situation from the original post by setting fetcher.threads.per.host.by.ip to true. The nutch-site file sent by Rodger does not specify it so it would rely on this value by default. Once setting it to false all threads are active and the fetching is much faster. \n\nI have used the first 5K URLs from the fetchlist sent by Rodger and compared the perfs with by.ip set to false :  \n\nOldFetcher :  \nreal\t32m26.003s\nuser\t1m11.768s\nsys\t0m10.337s\n\nOldFetcher :  \nreal\t30m52.965s\nuser\t1m10.696s\nsys\t0m10.425s\n\nFetcher :  \nreal\t31m21.924s\nuser\t1m12.725s\nsys\t0m10.797s\n\nFetcher :\nreal\t30m3.017s\nuser\t1m15.509s\nsys\t0m10.909s\n\nI ran each step twice and as we can see the results are comparable.\n\nThis explanation is also compliant with Steven's observation that we get 5-7 times the rate as we would hit the DNS cache for subsequent calls for URLs from non unique sites. The IP resolution is done by the QueueFeeder which explains why it is slowing down the number of URLs being available for fetching.\n\nI don't think that the oldFetcher allows to group URLs by IP for politeness in which case why not making fetcher.threads.per.host.by.ip default to false in the new fetcher?\n", "+1. Current defaults are sub-optimal due to backward-compatibility issues with early Nutch 0.8. This should be no longer a concern.", "Thanks for the analysis, Julien! Can you make a patch for the conf changes so we can commit it with your name?", "Sets the default value for fetcher.threads.per.host.by.ip to false", "Code committed as of rev. 807485.\n\nI am closing this issue. Of course, there may be other reasons why Fetcher2 is slow, so feel free to create new issues if so."], "tasks": {"summary": "Fetcher2 Slow", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fetcher2 Slow"}, {"question": "What is the main context?", "answer": "Fetcher2 fetches far more slowly than Fetcher1.\n\nConfig options:\nfetcher.threads.fetch = 80\nfetcher.threads.per.host = 80\nfetcher.server.delay = 0\ngenerate.max.per.host = 1\n\nWith a queue size of ~40,0"}]}}
{"issue_id": "NUTCH-722", "project": "NUTCH", "title": "Nutch contains jars that we cannot redistribute", "status": "Closed", "priority": "Blocker", "reporter": "Sami Siren", "assignee": null, "created": "2009-03-19T13:18:27.308+0000", "updated": "2009-04-10T12:29:06.949+0000", "description": "It seems that we have some jars (as part of pdf parser) that we cannot redistribute.\n\nJukkas comment from email:\n\"\nThe release contains the Java Advanced Imaging libraries (jai_core.jar and jai_codec.jar) which are licensed under Sun's Binary Code License. We can't redistribute those libraries.\n\"\n\n\n", "comments": ["I added these libs when upgrading PDFBox. During my tests I discovered that they are needed to correctly parse PDFs with certain types of images. If these libs are absent PDFBox throws a RuntimeException.\n\nOf course we should remove the libraries from our svn, but I wonder whether we shouldn't still download them on the fly.", "See PDFBOX-381 for how the JAI dependency issues was solved in the currently incubating Apache PDFBox. Unfortunately we don't yet have an official release of Apache PDFBox.", "One acceptable alternative for now is to drop the jars and add a note to end users that they should explicitly get and add the JAI libraries if they want support for PDF documents with rotated pages or embedded TIFF images.", "+1 for this solution.", "+1, i am fine with this solution too", "if there are no objections I will commit this change tomorrow morning (EET)", "removed the jars and added note about this in README.txt", "Integrated in Nutch-trunk #762 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/762/])\n     remove JAI libs\n", "closing issues for released version"], "tasks": {"summary": "Nutch contains jars that we cannot redistribute", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Nutch contains jars that we cannot redistribute"}, {"question": "What is the main context?", "answer": "It seems that we have some jars (as part of pdf parser) that we cannot redistribute.\n\nJukkas comment from email:\n\"\nThe release contains the Java Advanced Imaging libraries (jai_core.jar and jai_codec."}]}}
{"issue_id": "NUTCH-723", "project": "NUTCH", "title": "LICENCE.txt is lacking info that should be there", "status": "Closed", "priority": "Major", "reporter": "Sami Siren", "assignee": null, "created": "2009-03-19T13:23:24.400+0000", "updated": "2009-04-10T12:29:06.531+0000", "description": "Jukkas comment from email:\n\n* The LICENSE.txt file should have at least references to the licenses of the bundled libraries.", "comments": ["added licenses of 3rd party software", "Looks good to me.\n\nPS. There's not really a need to repeat the ALv2 for all Apache components, the first copy at the beginning is enough to cover them all (except of course any non-ALv2 parts). But it's no problem to repeat the license if you think it's clearer to explicitly mention the full licensing terms of each bundled library.", "Integrated in Nutch-trunk #758 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/758/])\n    \n", "yeah, true. the good thing is that  we have a baseline now and we can enhance it from now on every time somebody touches a jar."], "tasks": {"summary": "LICENCE.txt is lacking info that should be there", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "LICENCE.txt is lacking info that should be there"}, {"question": "What is the main context?", "answer": "Jukkas comment from email:\n\n* The LICENSE.txt file should have at least references to the licenses of the bundled libraries."}]}}
{"issue_id": "NUTCH-724", "project": "NUTCH", "title": "Drop the JAI libraries", "status": "Closed", "priority": "Blocker", "reporter": "Jukka Zitting", "assignee": null, "created": "2009-03-19T13:23:54.390+0000", "updated": "2009-04-10T12:29:05.490+0000", "description": "The PDF parser plugin contains Java Advanced Imaging (JAI) libraries (jai_core.jar and jai_codec.jar) that are licensed under the Sun Binary Code License. The license is incompatible with Apache policies, so we need to drop those libraries.\n\nAFAIK (see PDFBOX-381) PDFBox only uses the JAI libraries for handling page rotations and tiff images, so simply dropping the JAI jars shouldn't have too much impact. A better solution would be to switch to using Apache PDFBox that has a proper workaround for this issue, but the first Apache PDFBox release has not yet been made.", "comments": ["closing issues for released version"], "tasks": {"summary": "Drop the JAI libraries", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Drop the JAI libraries"}, {"question": "What is the main context?", "answer": "The PDF parser plugin contains Java Advanced Imaging (JAI) libraries (jai_core.jar and jai_codec.jar) that are licensed under the Sun Binary Code License. The license is incompatible with Apache polic"}]}}
{"issue_id": "NUTCH-725", "project": "NUTCH", "title": "NOTICE.txt is lacking info that should be there", "status": "Closed", "priority": "Major", "reporter": "Sami Siren", "assignee": null, "created": "2009-03-19T13:25:09.697+0000", "updated": "2009-04-10T12:29:00.086+0000", "description": "Jukkas comment from email:\n\n* The NOTICE.txt file should start with the the following lines:\n\n          Apache Nutch\n          Copyright 2009 The Apache Software Foundation\n\n* The NOTICE.txt file should contain the required copyright notices\nfrom all bundled libraries.", "comments": ["went through the libs and added copyright notices", "Looks good.", "Integrated in Nutch-trunk #758 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/758/])\n    \n\n"], "tasks": {"summary": "NOTICE.txt is lacking info that should be there", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "NOTICE.txt is lacking info that should be there"}, {"question": "What is the main context?", "answer": "Jukkas comment from email:\n\n* The NOTICE.txt file should start with the the following lines:\n\n          Apache Nutch\n          Copyright 2009 The Apache Software Foundation\n\n* The NOTICE.txt file shou"}]}}
{"issue_id": "NUTCH-726", "project": "NUTCH", "title": "README.txt is lacking info that should be there", "status": "Closed", "priority": "Major", "reporter": "Sami Siren", "assignee": null, "created": "2009-03-19T13:26:10.442+0000", "updated": "2009-04-10T12:29:03.560+0000", "description": "from Jukkas email:\n\n* The README.txt should start with \"Apache Nutch\" instead of \"Nutch\"", "comments": ["committed", "Integrated in Nutch-trunk #758 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/758/])\n    \n", "closing issues for released version"], "tasks": {"summary": "README.txt is lacking info that should be there", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "README.txt is lacking info that should be there"}, {"question": "What is the main context?", "answer": "from Jukkas email:\n\n* The README.txt should start with \"Apache Nutch\" instead of \"Nutch\""}]}}
{"issue_id": "NUTCH-727", "project": "NUTCH", "title": "Add KEYS file to release artifact", "status": "Closed", "priority": "Major", "reporter": "Sami Siren", "assignee": null, "created": "2009-03-19T13:37:41.537+0000", "updated": "2009-04-10T12:29:02.137+0000", "description": "comment from Grant:\n\n>> Where's the KEYS file for Nutch?\n>\n> hi,\n>\n> the keys file is at the top level nutch directory (eg: http://www.nic.funet.fi/pub/mirrors/apache.org/lucene/nutch/KEYS)\n\nOK, I think it should be in the tarball, too., at the top \n", "comments": ["KEYS file will now be packaged inside tgz", "Integrated in Nutch-trunk #758 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/758/])\n    \n"], "tasks": {"summary": "Add KEYS file to release artifact", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add KEYS file to release artifact"}, {"question": "What is the main context?", "answer": "comment from Grant:\n\n>> Where's the KEYS file for Nutch?\n>\n> hi,\n>\n> the keys file is at the top level nutch directory (eg: http://www.nic.funet.fi/pub/mirrors/apache.org/lucene/nutch/KEYS)\n\nOK, I thi"}]}}
{"issue_id": "NUTCH-728", "project": "NUTCH", "title": "Improve nutch release packaging", "status": "Closed", "priority": "Major", "reporter": "Sami Siren", "assignee": null, "created": "2009-03-19T17:13:32.500+0000", "updated": "2013-05-22T03:53:19.048+0000", "description": "see the discussion from http://www.lucidimagination.com/search/document/aa4d52cbd9af026a/discuss_contents_of_nutch_release_artifact", "comments": ["add simple target to generate source release tgz from svn tag\n\n-did not touch to the binary one", "Is there a particular reason that repository is svn.eu.apache.org and not svn.apache.org?", "not really, it just happens to be the mirror I use.", "OK. I tested it, it works fine.\n\n+1", "Updated patches for trunk and Nutchgora", "Ok to commit?", "Looking at this, then at what we have available on our mirrors, I don't really see the need at the moment (unless it would make release process easier) of including this code. Chris already provides us with src.tar.gz with every release?\nI suppose this ones really down to release manager's opinion. ", "Agreed, Lewis. I push *-src (the only *required* release artifact at the ASF), as well as *-bin, so I think we are OK to close this one.", "As per comments below. Closing this old dog off and putting him to bed."], "tasks": {"summary": "Improve nutch release packaging", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Improve nutch release packaging"}, {"question": "What is the main context?", "answer": "see the discussion from http://www.lucidimagination.com/search/document/aa4d52cbd9af026a/discuss_contents_of_nutch_release_artifact"}]}}
{"issue_id": "NUTCH-729", "project": "NUTCH", "title": "NPE in FieldIndexer when BasicFields url doesn't exist", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2009-03-25T14:53:22.649+0000", "updated": "2011-07-19T13:18:42.044+0000", "description": "There is a NullPointerException during a logging call in FieldIndexer when there isn't a url for a document.  Documents shouldn't be without urls but since the FieldIndexer doesn't validate fields it is possible for it to occur.  Most often this happens when BasicFields is run with the wrong segments directory and doesn't complain.  It could also occur if using the FieldIndexer to index things other than basic fields.", "comments": ["Simple patch.  Changes the logging to use the key (which should be url and which should always exist).", "Where do you change the logging to use a url key?", "- pushing this out per http://bit.ly/c7tBv9", "Closed for legacy. FieldIndexer no longer exists."], "tasks": {"summary": "NPE in FieldIndexer when BasicFields url doesn't exist", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "NPE in FieldIndexer when BasicFields url doesn't exist"}, {"question": "What is the main context?", "answer": "There is a NullPointerException during a logging call in FieldIndexer when there isn't a url for a document.  Documents shouldn't be without urls but since the FieldIndexer doesn't validate fields it "}]}}
{"issue_id": "NUTCH-73", "project": "NUTCH", "title": "A page for CSV results", "status": "Closed", "priority": "Minor", "reporter": "Christophe Noel", "assignee": "Chris A. Mattmann", "created": "2005-07-15T21:04:36.000+0000", "updated": "2011-04-01T15:07:23.075+0000", "description": "This jsp page allow users to get a CSV results pages.\n\nI don't know if it's very useful but as someone ask XML results page, i think it's nearly the same.", "comments": ["The searchcsv.jsp web page.", "Thanks for this patch, Christophe. I think we can resolve this as fixed, if my patch to SOLR-1925 gets committed (then this would be supported via Nutch-Solr integration). In the meanwhile, it might be good to implement this as a servlet, in that way we don't have to add another jsp.\n\nI'll take a look at this.\n\nCheers,\nChris\n", "Note: SOLR-1925 was committed, so there is now CSV output from Solr. I'm tempted to resolve this as-is, so we don't have to maintain two CSV web services.\n\nIf I don't hear any objections in the next 24 hours, I'll resolve this issue as such. Thanks Christophe for your contribution!", "- classify and schedule", "With SOLR-1925, we get this same functionality for free. Thanks for your contribution, Christophe, regardless!", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "A page for CSV results", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "A page for CSV results"}, {"question": "What is the main context?", "answer": "This jsp page allow users to get a CSV results pages.\n\nI don't know if it's very useful but as someone ask XML results page, i think it's nearly the same."}]}}
{"issue_id": "NUTCH-730", "project": "NUTCH", "title": "NPE in LinkRank if no nodes with which to create the WebGraph", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Andrzej Bialecki", "created": "2009-03-26T03:13:28.661+0000", "updated": "2009-10-10T04:45:46.917+0000", "description": "For LinkRank, if there are no nodes to process, then a NullPointerException is thrown when trying to count number of nodes.", "comments": ["Throws a more detailed error message if there are no nodes to process.  This shouldn't happen on large web graphs but may happen on smaller webgraphs or webgraphs that are all inside one domain (including subdomains).", "Fixed in rev. 823532, thanks!", "Integrated in Nutch-trunk #959 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/959/])\n     NPE in LinkRank if no nodes with which to create the WebGraph.\n"], "tasks": {"summary": "NPE in LinkRank if no nodes with which to create the WebGraph", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "NPE in LinkRank if no nodes with which to create the WebGraph"}, {"question": "What is the main context?", "answer": "For LinkRank, if there are no nodes to process, then a NullPointerException is thrown when trying to count number of nodes."}]}}
{"issue_id": "NUTCH-731", "project": "NUTCH", "title": "Redirection of robots.txt in RobotRulesParser", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Andrzej Bialecki", "created": "2009-04-03T17:53:49.644+0000", "updated": "2018-09-17T15:59:42.275+0000", "description": "The patch attached allows to follow one level of redirection for robots.txt files. A similar issue was mentioned in NUTCH-124 and has been marked as fixed a long time ago but the problem remained, at least when using Fetcher2 . Mathijs Homminga pointed to the problem in a mail to the nutch-dev list in March.\n\nI have been using this patch for a while now on a large cluster and noticed that the ratio of robots_denied per fetchlist went up, meaning that at least we are now getting restrictions we would not have had before (and getting less complaints from webmasters at the same time)", "comments": ["People have redirects on their robots.txt?  Wow.  Do you have an example of that handy, by any chance?", "I don't have a specific example now, in all the cases I have seen the redirections were between two variants of a hostname with and without www.\n", "Here is an example which the patch helps addressing \n\ncurl http://wizardhq.com/robots.txt\n\n<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n<html><head>\n<title>301 Moved Permanently</title>\n</head><body>\n<h1>Moved Permanently</h1>\n<p>The document has moved <a href=\"http://www.wizardhq.com/robots.txt\">here</a>.</p>\n</body></html>\n\nagain, the ratio of robots_denied status started going up after I wrote the patch which means that such cases are not so rare", "This is definitely an issue - I've been pinging various domains while testing robots.txt handling in bixo, and many of them will do a redirect if you use http://<domain>/robots.txt, to http://www.<domain>/robots.txt.", "Fixed in rev. 823540 - I applied a slightly modified version of the patch, to account for common errors in redirects. Thank you!", "Integrated in Nutch-trunk #959 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/959/])\n     Redirection of robots.txt in RobotRulesParser.\n"], "tasks": {"summary": "Redirection of robots.txt in RobotRulesParser", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Redirection of robots.txt in RobotRulesParser"}, {"question": "What is the main context?", "answer": "The patch attached allows to follow one level of redirection for robots.txt files. A similar issue was mentioned in NUTCH-124 and has been marked as fixed a long time ago but the problem remained, at "}]}}
{"issue_id": "NUTCH-732", "project": "NUTCH", "title": "Subcollection plugin not working on Nutch-1.0", "status": "Closed", "priority": "Critical", "reporter": "Filipe Antunes", "assignee": "Andrzej Bialecki", "created": "2009-04-07T12:07:37.444+0000", "updated": "2010-04-27T18:17:15.571+0000", "description": "I am trying to get subcollections working, using Nutch-1.0 !\nI configured subcolections.xml then I added the plugin on nutch-site.xml.\nWhen the index finishes, I opened lucene luke to check if the database was working properly.\nThe field subcollection is populated as it should, but searching for any subcollection, on the search tab of luke, returns no results.\nIf I do a search on the url field, I can see that every record has a subcollection associated, yet i can't search for using the  subcollection field.\nsearch examples on luke:\nsubcollection:sub1 -> no results\nurl:sub1 -> results with field subcollection populated -> sub1\n\nSame results using:\n./bin/nutch org.apache.nutch.searcher.NutchBean \"subcollection:sub1 sub\"\n\nIf i use the \"explain\", subcollection field is there with the correct word.\n\nIt makes no sense so i beleive it's a bug.\n", "comments": ["Turns out this was due to a way the list of applicable collections is created, and how that field is added to the indexing backend. First, it appends a leading space, creating collection names like ' nutch' instead of 'nutch'. Then, instead of tokenizing this field it passes it as is, so the leading space is kept and prevents you from running a query.\n\nI changed the collection name appending logic, and turned the field into tokenized.\n\nI'll commit the patch shortly.", "Fixed in rev. 938592. Thanks!"], "tasks": {"summary": "Subcollection plugin not working on Nutch-1.0", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Subcollection plugin not working on Nutch-1.0"}, {"question": "What is the main context?", "answer": "I am trying to get subcollections working, using Nutch-1.0 !\nI configured subcolections.xml then I added the plugin on nutch-site.xml.\nWhen the index finishes, I opened lucene luke to check if the dat"}]}}
{"issue_id": "NUTCH-733", "project": "NUTCH", "title": "plain text view of cached files ignores HTML encoding", "status": "Closed", "priority": "Major", "reporter": "Ilguiz Latypov", "assignee": "Chris A. Mattmann", "created": "2009-04-30T19:53:00.399+0000", "updated": "2011-04-13T23:48:08.325+0000", "description": "The plain text view of cached HitDetails is sent as raw text under the Content-Type: text/html header.\n\nEither the content type should be changed to text/plain (patch attached) or the text should be HTML-encoded (perhaps, using http://commons.apache.org/lang/api/org/apache/commons/lang/StringEscapeUtils.html).\n", "comments": ["Set the Content-Type: \"text/plain; charset=UTF-8\" header.  Ignore HTML-rich i18n message taglib for now.  Perhaps, there is a plain-text version of the \"note\" tag.  I do not know.\n\n", "My patch does not wrap long lines.", "I think I prefer HTML-encoding but this also looks good to me. So I will commit it if there are no objections.", "- Nutch 2.0 doesn't have a webapp right now, but will before release. My guess is that it'll look different than the current one, or at least be implemented in a different way. We'll make sure that the spirit of this patch makes its way into the webapp...thanks!", "Closing all resolved issues with a non-fixed status."], "tasks": {"summary": "plain text view of cached files ignores HTML encoding", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "plain text view of cached files ignores HTML encoding"}, {"question": "What is the main context?", "answer": "The plain text view of cached HitDetails is sent as raw text under the Content-Type: text/html header.\n\nEither the content type should be changed to text/plain (patch attached) or the text should be H"}]}}
{"issue_id": "NUTCH-734", "project": "NUTCH", "title": "option to filter \"a\" tag text", "status": "Closed", "priority": "Major", "reporter": "ron", "assignee": null, "created": "2009-05-02T11:47:59.903+0000", "updated": "2013-05-22T03:53:23.564+0000", "description": "Motivation:\nWhen fetching pages with \"menue links\" the menues (for example search) appear on all pages of the site. Searching for the word \"search\" then returns all pages of the site, instead of just returning the the search page.\n\nChange request:\nAdd options to filter texts of \"a\" tags, or more generally add filters to avoid texts within specific tags.\n\nI have worked around this by changing DOMContentUtils.getTextHelper : \n\n     if (nodeType == Node.TEXT_NODE && !(currentNode.getParentNode() != null && \"a\".equalsIgnoreCase(currentNode.getParentNode().getNodeName()))) \n\n- Ron", "comments": ["This is simply not required and dated. Plus I assume by referring to \"a\", we mean stop words. These are filtered during the IR process in (all?) modern indexing servers. "], "tasks": {"summary": "option to filter \"a\" tag text", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "option to filter \"a\" tag text"}, {"question": "What is the main context?", "answer": "Motivation:\nWhen fetching pages with \"menue links\" the menues (for example search) appear on all pages of the site. Searching for the word \"search\" then returns all pages of the site, instead of just "}]}}
{"issue_id": "NUTCH-735", "project": "NUTCH", "title": "crawl-tool.xml must be read before nutch-site.xml when invoked using crawl command", "status": "Closed", "priority": "Minor", "reporter": "Susam Pal", "assignee": "Dogacan Guney", "created": "2009-05-09T06:55:13.440+0000", "updated": "2009-06-08T04:14:09.094+0000", "description": "The inline documentation of 'conf/crawl-tool.xml' mentions:\n\n{code:xml}\n<!-- Do not modify this file directly.  Instead, copy entries that you -->\n<!-- wish to modify from this file into nutch-site.xml and change them -->\n<!-- there.  If nutch-site.xml does not already exist, create it.      -->\n{code}\n\nHowever, I don't see any way of overriding the properties defined in 'conf/crawl-tool.xml' as 'conf/nutch-site.xml' is added to the configuration before 'conf/crawl-tool.xml' in the code. Here are the relevant code snippets:\n\n*src/org/apache/nutch/crawl/Crawl.java:*\n\n{code:java}\nConfiguration conf = NutchConfiguration.create();\nconf.addResource(\"crawl-tool.xml\");\nJobConf job = new NutchJob(conf);\n{code}\n\n*src/org/apache/nutch/tool/NutchConfiguration.java:*\n\n{code:java}\nconf.addResource(\"nutch-default.xml\");\nconf.addResource(\"nutch-site.xml\");\n{code}\n\nI have fixed this in the attached patch. 'crawl-tool.xml' is now added to the configuration before 'nutch-site.xml' only if crawl is invoked using the 'bin/nutch crawl' command.", "comments": ["Attached patch.", "Committed in rev. 782412.\n\nThanks!", "Integrated in Nutch-trunk #838 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/838/])\n     - crawl-tool.xml must be read before nutch-site.xml when invoked using crawl command. Patch by Susam Pal.\n"], "tasks": {"summary": "crawl-tool.xml must be read before nutch-site.xml when invoked using crawl command", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "crawl-tool.xml must be read before nutch-site.xml when invoked using crawl command"}, {"question": "What is the main context?", "answer": "The inline documentation of 'conf/crawl-tool.xml' mentions:\n\n{code:xml}\n<!-- Do not modify this file directly.  Instead, copy entries that you -->\n<!-- wish to modify from this file into nutch-site.xm"}]}}
{"issue_id": "NUTCH-736", "project": "NUTCH", "title": "how long it takes nutch 1.0 to fetch", "status": "Closed", "priority": "Major", "reporter": "Filipe Antunes", "assignee": "Otis Gospodnetic", "created": "2009-05-14T09:38:36.189+0000", "updated": "2011-04-13T23:48:08.521+0000", "description": "I need an opinion about how long it takes nuch 1.0 to fetch a web site.\n\nAt the moment I'm indexing 3000 sites (medical area). university's, clinics, hospitals, associations, journals (html, docs, PDF, txt, xls).\nSo far I have 5 segments (64Gb) and the its fetching the 6th.\nUsing an Intel 2.8 Core2Duo OS X 10.5.6 with 4Mbit internet connection (the machine is throttled to 64Kbytes during the day (8 hours)) and this fetching started one month ago.\n\nDoes anyone have statistics of how long a site (# of pages) nutch 1.0 takes? ", "comments": ["Please ask questions on nutch-user mailing list (mailing list info is on the site).", "Closing all resolved issues with a non-fixed status."], "tasks": {"summary": "how long it takes nutch 1.0 to fetch", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "how long it takes nutch 1.0 to fetch"}, {"question": "What is the main context?", "answer": "I need an opinion about how long it takes nuch 1.0 to fetch a web site.\n\nAt the moment I'm indexing 3000 sites (medical area). university's, clinics, hospitals, associations, journals (html, docs, PDF"}]}}
{"issue_id": "NUTCH-737", "project": "NUTCH", "title": "urlnormalizer-unalias plugin", "status": "Closed", "priority": "Minor", "reporter": "Dmitry Lihachev", "assignee": null, "created": "2009-05-26T04:15:19.525+0000", "updated": "2013-05-22T03:54:50.003+0000", "description": "I tried to search any whole site duplication detection tools without success. This plugin allows to do domain name transformation (for example www.google.com -> google.com). It is very stupid, but can be useful when fighting with site aliases. For detect site aliases I use my own ugly class (based on SolrDeleteDuplicates).", "comments": [], "tasks": {"summary": "urlnormalizer-unalias plugin", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "urlnormalizer-unalias plugin"}, {"question": "What is the main context?", "answer": "I tried to search any whole site duplication detection tools without success. This plugin allows to do domain name transformation (for example www.google.com -> google.com). It is very stupid, but can"}]}}
{"issue_id": "NUTCH-738", "project": "NUTCH", "title": "Close SegmentUpdater when FetchedSegments is closed", "status": "Closed", "priority": "Minor", "reporter": "Martina Koch", "assignee": "Andrzej Bialecki", "created": "2009-05-26T06:40:17.488+0000", "updated": "2011-06-08T21:34:22.678+0000", "description": "Currently FetchedSegments starts a SegmentUpdater, but never closes it when FetchedSegments is closed.\n\n(The problem was described in this mailing: http://www.mail-archive.com/nutch-user@lucene.apache.org/msg13823.html)", "comments": ["Patch adds close method for SegmentUpdater.", "Alternative implementation, that uses Thread interruption as cancel semantics.   Because I'm not sure that the project properly handles interruption everywhere, I capture the interruption request to ensure the shutdown is not lost.\n\nIf this is not patch acceptable, I believe that the original patch needs to make the boolean used become volatile to conform to the Java Memory Model.\n\nThe this.$0 in the SegmentUpdater, the segmentUpdater and the fact that segmentUpdater is a GC root (all threads are GC roots), appears to confuse the GC into not releasing this cluster of objects, thus holding this the class loader in memory.\n\nIf segmentUpdater needs to stay final, just pass the values from FetchSegments and make SegmentUpdater become a static class.  I believe that will also resolve the problem, but I didn't actually try it.\n\nUsing this, and the patch from NUTCH-746, and adding \"-XX:+CMSClassUnloadingEnabled\" to Tomcat's 6.0.18 startup script will allow the classes to be unloaded.\n", "Fixed in rev. 885150. Thank you!", "Integrated in Nutch-trunk #996 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/996/])\n     Close SegmentUpdater when FetchedSegments is closed.\n"], "tasks": {"summary": "Close SegmentUpdater when FetchedSegments is closed", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Close SegmentUpdater when FetchedSegments is closed"}, {"question": "What is the main context?", "answer": "Currently FetchedSegments starts a SegmentUpdater, but never closes it when FetchedSegments is closed.\n\n(The problem was described in this mailing: http://www.mail-archive.com/nutch-user@lucene.apache"}]}}
{"issue_id": "NUTCH-739", "project": "NUTCH", "title": "SolrDeleteDuplications too slow when using hadoop", "status": "Closed", "priority": "Major", "reporter": "Dmitry Lihachev", "assignee": "Andrzej Bialecki", "created": "2009-05-28T04:32:51.265+0000", "updated": "2009-11-29T04:08:33.013+0000", "description": "in my environment i always have many warnings like this on the dedup step\n{noformat}\nTask attempt_200905270022_0212_r_000003_0 failed to report status for 600 seconds. Killing!\n{noformat}\nsolr logs:\n{noformat}\nINFO: [] webapp=/solr path=/update params={wt=javabin&waitFlush=true&optimize=true&waitSearcher=true&maxSegments=1&version=2.2} status=0 QTime=173741\nMay 27, 2009 10:29:27 AM org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: {optimize=} 0 173599\nMay 27, 2009 10:29:27 AM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/solr path=/update params={wt=javabin&waitFlush=true&optimize=true&waitSearcher=true&maxSegments=1&version=2.2} status=0 QTime=173599\nMay 27, 2009 10:29:27 AM org.apache.solr.search.SolrIndexSearcher close\nINFO: Closing Searcher@2ad9ac58 main\nMay 27, 2009 10:29:27 AM org.apache.solr.core.JmxMonitoredMap$SolrDynamicMBean getMBeanInfo\nWARNING: Could not getStatistics on info bean org.apache.solr.search.SolrIndexSearcher\norg.apache.lucene.store.AlreadyClosedException: this IndexReader is closed\n....\n{noformat}\n\nSo I think the problem in the piece of code on line 301 of SolrDeleteDuplications ( solr.optimize() ). Because we have few job tasks each of ones tries to optimize solr indexes before closing.\nThe simplest way to avoid this bug - removing this line and sending \"<optimize/>\" message directly to solr server after dedup step\n", "comments": ["This simple patch decrease dedup time from 20 min to 2 min plus 4 min for curl indexer_host \"<optimize/> and reduce solr server load", "I think there are a few issues here.\n# multiple tasks trying to optimize the same index (I'm assuming you are correct about this) -- yes, this should not be happening\n# tasks timing out -- not sure how to handle that, since one never knows how long the optimize call will take\n# your patch simply removed the optimize call -- but now where/how is the index going to get optimized after dups are deleted?\n", "in my recrawl script I have following lines\n{code}\nserver=http://some.server.org\nbin/nutch solrdedup $server\ncurl -so /dev/null -H 'Content-Type: text/xml' -d \"<optimize/>\" $server/update\n{code}\n\nYou can always send commands to solr without Java", "There's another approach that works well here, and that's to start up a thread that calls the Hadoop reporter while the optimize is happening.\n\nWe ran into the same issue when optimizing large Lucene indexes from our Bixo IndexScheme tap for Cascading. You can find that code on GitHub, but the skeleton is to do something like this in the reducer's close() method - assuming you've stashed the reporter from the reduce() call:\n\n{code:java}\n// Hadoop needs to know we still working on it.\nThread reporterThread = new Thread() {\n\tpublic void run() {\n\t\twhile (!isInterrupted()) {\n\t\t\treporter.progress();\n\t\t\ttry {\n\t\t\t\tsleep(10 * 1000);\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tinterrupt();\n\t\t\t}\n\t\t}\n\t}\n};\nreporterThread.start();\n\nindexWriter.optimize();\n<and other lengthy tasks here>\nreporterThread.interrupt();\n{code}\n\n", "Yes, external optimize calls will work, I was just wondering if we could avoid that.  I like Ken's suggestion that tells Hadoop the task is still alive.  Do you think you could do that in your patch, Dmitry?", "with this approach we still have few optimize calls (so many so we have reducers), but we need exactly one optimize call after dedup", "am I wrong?", "I think that optimizing solr - is not hadoop job. it does not need parallelization.", "I agree with Dmitry. We should not need more than 1 optimize call, it was my mistake to not\nconsider the case of multiple tasks all trying to optimize at the same time. I am ready to be\nproven wrong (or right, depending on your POV :)\n\nHowever, I still believe that we should not require users to use curl directly. Can't we just\nmove the optimize call to somewhere after the job is finished? ", "Doğacan, I agree with you about curl usage. May by we must write Tool SolrOpimizer in org.apache.nutch.indexer.solr and call this tool from bin/nutch? ", "Ooops, sorry... Tool is Map/Reduce application.\nOk, we can write standard Java application with main method and run it from bin/nutch", "I can do this work, but only after June, 15", "Yeah, sounds right.  That Tool should make use of SolrJ then, we already have it as a dependency in lib/.\n", "Fixed in rev. 885152. Thank you!", "Integrated in Nutch-trunk #996 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/996/])\n     SolrDeleteDuplications too slow when using hadoop.\n"], "tasks": {"summary": "SolrDeleteDuplications too slow when using hadoop", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "SolrDeleteDuplications too slow when using hadoop"}, {"question": "What is the main context?", "answer": "in my environment i always have many warnings like this on the dedup step\n{noformat}\nTask attempt_200905270022_0212_r_000003_0 failed to report status for 600 seconds. Killing!\n{noformat}\nsolr logs:\n{"}]}}
{"issue_id": "NUTCH-74", "project": "NUTCH", "title": "French Analyzer Plugin", "status": "Closed", "priority": "Major", "reporter": "Christophe Noel", "assignee": "Jerome Charron", "created": "2005-07-19T21:30:27.000+0000", "updated": "2009-04-10T12:29:06.813+0000", "description": "This is DRAFT for a new plugin for French Analysis (all java file come from Lucene project sandbox)... This includes ISO LATIN1 accent filter, plurial forms removing, ...\n\nAnalyze-frech should be used instead of NutchDocumentAnalysis as described by Jerome Charron in New Language Identifier project. It should be used also as a query-parser in Nutch searcher.\n\nWe miss an EXTENSION-POINT to include this kind of plugin in Nutch. Could anyone help me to build this new Extension Point please ?", "comments": ["A zip file for this plugin\n\n(plugin.xml is not right)", "Christophe,\n\nI already done such plugin for French and German in order to test the Analyzer Factory. The difference with your approach is that instead of copying the luncene's analyzer code, I added some dependencies on the lucene libs. I think it is a better approach since it avoids to duplicate the code.\nI added an analysis extension point too in order to plug the analysis plugins.\nBut for now, these plugins are called by the AnalysisFactory depending on the language identifier result. And as I explained in a previous mail, the language identifier failed (bad language identification) due to an enconding problem in Nutch. I'm currently working on this issue, and I can't submit my code in its current state.\nBut if you want, I can send you some parts of the code.\n\nRegards\n\nJerome\n", "Christophe,\n\nHere is the patch (I hope nothing is missing, since I have a lot of modified code on local for other purposes) that add the NutchAnalyzer extension point used by the AnalyzerFactory. There's also a french plugin that uses the lucene french analyzer.\nThis is a basis framework to plug some language specific analyzers.\nTry it, give me your feed back, please.\n\nJerome", "Hi Jerome\nDo let me know what work needs to be done on this plugin. I am keen to participate\n\nthanks\nilango", "Hi :\n\nThe following issues have been moved from 0.8 to 0.9 and I am sure it will be moved to 0.10 cos its seems to me that the comitter/asignee doesn't have the time to fix it. I suggest that these issues shouldn't be carried around from versions to version i.e. 0.8 to 0.9 to 0.10. No offense .. just fact - The current code base is so different from 0.8 and the supplied patch doesn't work.. \n\n=============\n\nNew Feature  \t NUTCH-74  \t French Analyzer Plugin   \t Jerome Charron  \tChristophe Noel  \t Major   \t Open  Open  \t 25/Jul/06\nNew Feature \tNUTCH-261 \tMulti Language Support \tJerome Charron \tJerome Charron \tMajor \tIn Progress In Progress \t16/Nov/06\nSub-task \tNUTCH-262 \tNUTCH-261\nSummary excerpts and highlights problems \tJerome Charron \tJerome Charron \tMajor \tOpen Open \t25/Jul/06\nImprovement \tNUTCH-86 \tLanguageIdentifier API enhancements \tJerome Charron \tJerome Charron \tMinor \tOpen Open \t25/Jul/06\nImprovement \tNUTCH-310 \tReview Log Levels \tJerome Charron \tJerome Charron \tMinor \tOpen Open \t25/Jul/06\nImprovement \tNUTCH-309 \tUses commons logging Code Guards \tJerome Charron \tJerome Charron \tMinor \tReopened Reopened \t28/Jul/06", "This was fixed long time ago as a part of NUTCH-261"], "tasks": {"summary": "French Analyzer Plugin", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "French Analyzer Plugin"}, {"question": "What is the main context?", "answer": "This is DRAFT for a new plugin for French Analysis (all java file come from Lucene project sandbox)... This includes ISO LATIN1 accent filter, plurial forms removing, ...\n\nAnalyze-frech should be used"}]}}
{"issue_id": "NUTCH-740", "project": "NUTCH", "title": "Configuration option to override default language for fetched pages.", "status": "Closed", "priority": "Minor", "reporter": "Marcin Okraszewski", "assignee": "Julien Nioche", "created": "2009-05-28T21:12:26.061+0000", "updated": "2010-03-23T04:11:49.545+0000", "description": "By default \"Accept-Language\" HTTP request header is set to English. Unfortunately this value is hard coded and seems there is no way to override it. As a result you may index English version of pages even though you would prefer it in different language. ", "comments": ["The patch which allows overriding of \"Accept-Language\" header. The patch is done on 1.0 code. ", "+1 for the idea but your patch does not apply to trunk.", "It does apply, but with Fuzz factor set to 2. Here is the ported patch.", "A nice contribution but should not this be applied to the *protocol-http* plugin as well e.g. in HttpResponse?", "Slightly modified version of the patch with modifs for protocol-http.\nwill commit shortly", "Committed in rev 926003\nThanks Marcin for contributing this patch", "Integrated in Nutch-trunk #1104 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1104/])\n     Configuration option to override default language for fetched pages\n"], "tasks": {"summary": "Configuration option to override default language for fetched pages.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Configuration option to override default language for fetched pages."}, {"question": "What is the main context?", "answer": "By default \"Accept-Language\" HTTP request header is set to English. Unfortunately this value is hard coded and seems there is no way to override it. As a result you may index English version of pages "}]}}
{"issue_id": "NUTCH-741", "project": "NUTCH", "title": "Job file includes multiple copies of nutch config files.", "status": "Closed", "priority": "Minor", "reporter": "Kirby Bohling", "assignee": "Andrzej Bialecki", "created": "2009-05-29T19:59:43.772+0000", "updated": "2009-11-29T04:08:32.876+0000", "description": "From a clean checkout, running \"ant tar\" will create a .job file.  The .job file includes two copies of the nutch-site.xml and nutch-default.xml file.", "comments": ["Simple patch that excludes the conf files in ${build.classes}, and just uses the ones from the conf directory.", "Fixed in rev. 885156. Thank you!", "Integrated in Nutch-trunk #996 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/996/])\n     Job file includes multiple copies of nutch config files.\n"], "tasks": {"summary": "Job file includes multiple copies of nutch config files.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Job file includes multiple copies of nutch config files."}, {"question": "What is the main context?", "answer": "From a clean checkout, running \"ant tar\" will create a .job file.  The .job file includes two copies of the nutch-site.xml and nutch-default.xml file."}]}}
{"issue_id": "NUTCH-742", "project": "NUTCH", "title": "Checksum Error ", "status": "Closed", "priority": "Major", "reporter": "mawanqiang", "assignee": null, "created": "2009-06-20T09:17:48.730+0000", "updated": "2011-04-01T15:07:19.362+0000", "description": "Approximately 1 million data used to create index when nutch1.0 error.\nThe error is:\njava.lang.RuntimeException: problem advancing post rec#6758513\nat org.apache.hadoop.mapred.Task$ValuesIterator.next(Task.java:883)\nat org.apache.hadoop.mapred.ReduceTask$ReduceValuesIterator.moveToNext(ReduceTask.java:237)\nat org.apache.hadoop.mapred.ReduceTask$ReduceValuesIterator.next(ReduceTask.java:233)\nat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:79)\nat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:50)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:436)\nat org.apache.hadoop.mapred.Child.main(Child.java:158)\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum Error\nat org.apache.hadoop.mapred.IFileInputStream.doRead(IFileInputStream.java:153)\nat org.apache.hadoop.mapred.IFileInputStream.read(IFileInputStream.java:90)\nat org.apache.hadoop.mapred.IFile$Reader.readData(IFile.java:301)\nat org.apache.hadoop.mapred.IFile$Reader.rejigData(IFile.java:331)\nat org.apache.hadoop.mapred.IFile$Reader.readNextBlock(IFile.java:315)\nat org.apache.hadoop.mapred.IFile$Reader.next(IFile.java:377)\nat org.apache.hadoop.mapred.Merger$Segment.next(Merger.java:174)\nat org.apache.hadoop.mapred.Merger$MergeQueue.adjustPriorityQueue(Merger.java:277)\nat org.apache.hadoop.mapred.Merger$MergeQueue.next(Merger.java:297)\nat org.apache.hadoop.mapred.Task$ValuesIterator.readNextKey(Task.java:922)\nat org.apache.hadoop.mapred.Task$ValuesIterator.next(Task.java:881)\n... 6 more\n", "comments": ["Could you please post more detailed information to nutch-user mailing list first?", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Checksum Error ", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Checksum Error "}, {"question": "What is the main context?", "answer": "Approximately 1 million data used to create index when nutch1.0 error.\nThe error is:\njava.lang.RuntimeException: problem advancing post rec#6758513\nat org.apache.hadoop.mapred.Task$ValuesIterator.next"}]}}
{"issue_id": "NUTCH-743", "project": "NUTCH", "title": "Site search powered by Lucene/Solr", "status": "Closed", "priority": "Minor", "reporter": "Sami Siren", "assignee": "Sami Siren", "created": "2009-06-23T16:26:39.785+0000", "updated": "2013-05-22T03:54:51.367+0000", "description": "Replace current Nutch site search with Lucene/Solr powered search hosted by Lucid Imagination (http://www.lucidimagination.com/search).  It allows one to search all of the Nutch (content from other parts of the Lucene ecosystem is also available) content from a single place, including web, wiki, JIRA and mail archives. Lucid has a fault tolerant setup with replication and fail over as well as monitoring services in place. \n\nA preview of the site with the new search enabled is available at http://people.apache.org/~siren/site/\n", "comments": ["If there are no objections I will commit this within a week or so.", "+1, based on the outcome of a thorough discussion of pros/cons of the same subject on the Lucene lists.", "committed", "Integrated in Nutch-trunk #864 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/864/])\n     - Site search powered by Lucene/Solr\n"], "tasks": {"summary": "Site search powered by Lucene/Solr", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Site search powered by Lucene/Solr"}, {"question": "What is the main context?", "answer": "Replace current Nutch site search with Lucene/Solr powered search hosted by Lucid Imagination (http://www.lucidimagination.com/search).  It allows one to search all of the Nutch (content from other pa"}]}}
{"issue_id": "NUTCH-744", "project": "NUTCH", "title": "indexing items in rss-feed in seperate page", "status": "Closed", "priority": "Trivial", "reporter": "Tarun", "assignee": null, "created": "2009-07-09T14:29:33.005+0000", "updated": "2009-07-09T14:40:10.513+0000", "description": "the rss feed have many items.\ni want to index each item seperately. So that when we search the query should take place in each item. and the content displayed as search result should show the result from that item only.\nNot the whole rss page that is crawled.\n ", "comments": ["Use feed plugin to parse individual rss items.\n\nAlso please use nutch-user to ask questions.", "thanx\n\nAnd from next i will ask ques from nutch-user.\nthanx again\n\nTarun\n"], "tasks": {"summary": "indexing items in rss-feed in seperate page", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "indexing items in rss-feed in seperate page"}, {"question": "What is the main context?", "answer": "the rss feed have many items.\ni want to index each item seperately. So that when we search the query should take place in each item. and the content displayed as search result should show the result f"}]}}
{"issue_id": "NUTCH-745", "project": "NUTCH", "title": "MyHtmlParser getParse return not null，so all Analyzer-(zh|fr) cannot run", "status": "Closed", "priority": "Major", "reporter": "Tian Xia", "assignee": null, "created": "2009-07-10T05:38:39.845+0000", "updated": "2013-05-22T03:53:21.416+0000", "description": "MyHtmlParser getParse return not null，so all Analyzer-(zh|fr) cannot run\n\n\tpublic ParseResult getParse(Content content) {\n    \treturn ParseResult.createParseResult(content.getUrl(), new ParseStatus(ParseStatus.FAILED, \n                ParseStatus.FAILED_MISSING_CONTENT, \n        \"No textual content available\").getEmptyParse(conf)); \n\t\t\n\t\t// return null;\n\t}\n\n========nutch-site.xml=======\n<property>\n  <name>plugin.includes</name>\n  <value>protocol-http|urlfilter-regex|parse-(myHtml|html|text|js)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|language-identifier|analysis-(zh)</value>\n  <description><![CDATA[\n  \n  ]]>  </description>\n</property>\n==========parse-plugins.xml============\n<mimeType name=\"text/html\">\n\t\t<plugin id=\"parse-myHtml\" />\n\t\t<plugin id=\"parse-html\" />\n\t</mimeType>\n<alias name=\"parse-myHtml\"\n\t\t\textension-id=\"org.apache.nutch.parse.html.MyHtmlParser\" />\n\n===src/plugin/parse-html/src/java/org/apache/nutch/parse/html/HtmlParser.java========\n public ParseResult getParse(Content content) {\n.....\n// cannot run the code:\n  ParseResult filteredParse = this.htmlParseFilters.filter(content, parseResult, \n                                                             metaTags, root);\n.......\n\n", "comments": ["close of legacy issue"], "tasks": {"summary": "MyHtmlParser getParse return not null，so all Analyzer-(zh|fr) cannot run", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "MyHtmlParser getParse return not null，so all Analyzer-(zh|fr) cannot run"}, {"question": "What is the main context?", "answer": "MyHtmlParser getParse return not null，so all Analyzer-(zh|fr) cannot run\n\n\tpublic ParseResult getParse(Content content) {\n    \treturn ParseResult.createParseResult(content.getUrl(), new ParseStatus(Pa"}]}}
{"issue_id": "NUTCH-746", "project": "NUTCH", "title": "NutchBeanConstructor does not close NutchBean upon contextDestroyed, causing resource leak in the container.", "status": "Closed", "priority": "Major", "reporter": "Kirby Bohling", "assignee": "Andrzej Bialecki", "created": "2009-07-26T23:13:18.667+0000", "updated": "2009-11-29T04:08:32.691+0000", "description": "NutchBeanConstructor is not cleaning up upon application shutdown (contextDestroyed()).   It leaves open the SegmentUpdater, and potentially other resources.  This causes the WebApp's classloader to not be able to GC'ed in Tomcat, which after repeated restarts will lead to a PermGen error.", "comments": ["Patch to address this issue.  When combined with my NUTCH-738 patch will address the shutdown not releasing the classes and classloaders.", "Fixed in rev. 885148. Thanks!", "Integrated in Nutch-trunk #996 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/996/])\n     NutchBeanConstructor does not close NutchBean upon\ncontextDestroyed, causing resource leak in the container.\n"], "tasks": {"summary": "NutchBeanConstructor does not close NutchBean upon contextDestroyed, causing resource leak in the container.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "NutchBeanConstructor does not close NutchBean upon contextDestroyed, causing resource leak in the container."}, {"question": "What is the main context?", "answer": "NutchBeanConstructor is not cleaning up upon application shutdown (contextDestroyed()).   It leaves open the SegmentUpdater, and potentially other resources.  This causes the WebApp's classloader to n"}]}}
{"issue_id": "NUTCH-747", "project": "NUTCH", "title": "inject&Index metadatas and inherit these metadatas to all matching suburls", "status": "Closed", "priority": "Major", "reporter": "Marko Bauhardt", "assignee": null, "created": "2009-08-06T10:35:25.811+0000", "updated": "2013-05-22T03:53:35.187+0000", "description": "Hi.\nthe following two patches supports\n+ inject metadatas to url's into a metadatadb\nurl.com <TAB> <METAKEY> : <TAB> <METAVALUE> <TAB> <METAVALUE> <METAKEY> : <METAVALUE> ...\n...\n+ updates the parse_data metadata from a shard and write the metadatas to all fetched urls that starts with an url from the metadatadb\n+ this patch support's metadata to all matching suburls inheritance\n\nthe second patch implements a index-metadata plugin.\n+ this plugin extract all metadats from the parse_data of a shard and index it. which metadats you can configure in the plugin.properties.\n+ to index for example the lang you have to configure the plugin.properties: lang=STORE,UNTOKENIZED\n+ that means that the index plugin exract metadata values with key \"lang\". if exists, all values are indexed stored and untokenized\n\nExample\n\ncreate start url's in \"/tmp/urls/start/urls.txt\"\nhttp://lucene.apache.org/nutch/apidocs-1.0/index.html\nhttp://lucene.apache.org/nutch/apidocs-0.9/index.html\n\ncreate metadata url's in \"/tmp/urls/metadata/urls.txt\"\nhttp://lucene.apache.org/nutch/apidocs-1.0/     version:        1.0\nhttp://lucene.apache.org/nutch/apidocs-0.9/     version:        0.9\n\nInject Urls\nbin/nutch inject crawldb /tmp/urls/start/\nbin/nutch org.apache.nutch.crawl.metadata.MetadataInjector metadatadb /tmp/urls/metadata/\n\nFetch & Parse & Update\nbin/nutch generate crawldb segments\nbin/nutch fetch segments/20090806105717/\nbin/nutch org.apache.nutch.crawl.metadata.ParseDataUpdater metadatadb segments/20090806105717\nbin/nutch updatedb crawldb/ segments/20090806105717/\n\nFetch & Parse & Update Again\n...\n\nIndex\nbin/nutch invertlinks linkdb -dir segments/\nbin/nutch index index crawldb/ linkdb/ segments/20090806105717 segments/20090806110127\n\nCheck your Index\nAll urls starting with \"http://lucene.apache.org/nutch/apidocs-1.0/ \" are indexed with \"version:1.0\".\nAll urls starting with \"http://lucene.apache.org/nutch/apidocs-0.9/ \" are indexed with \"version:0.9\".\n\nThis issue is some related to NUTCH-655\n\n\n", "comments": ["To update the parse_data.metadata is maybe not the best solution, but i don't want to change the input path's for the indexer.\nso maybe a new crawl_metadata or fetch_metadata path is better to store metadatas. but then we have to change the input path for the indexer. \nand i think metadatas should be stored in a path inside the shard instead in the crawldb.", "- pushing this out per http://bit.ly/c7tBv9", "This has been made possible since thanks to : \n- Metadata injection (https://issues.apache.org/jira/browse/NUTCH-655)\n- urlmeta plugin\n- index-metadata plugin\n", "Excellent Julien. I thought this was the case but thanks for confirming. :)"], "tasks": {"summary": "inject&Index metadatas and inherit these metadatas to all matching suburls", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "inject&Index metadatas and inherit these metadatas to all matching suburls"}, {"question": "What is the main context?", "answer": "Hi.\nthe following two patches supports\n+ inject metadatas to url's into a metadatadb\nurl.com <TAB> <METAKEY> : <TAB> <METAVALUE> <TAB> <METAVALUE> <METAKEY> : <METAVALUE> ...\n...\n+ updates the parse_d"}]}}
{"issue_id": "NUTCH-748", "project": "NUTCH", "title": "DiskChecker  Could not find", "status": "Closed", "priority": "Major", "reporter": "mawanqiang", "assignee": null, "created": "2009-08-18T06:28:04.472+0000", "updated": "2009-10-09T13:57:14.486+0000", "description": "2009-08-17 19:08:17,286 WARN  mapred.TaskTracker - getMapOutput(attempt_200908171832_0012_m_000013_0,55) failed :\norg.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/jobcache/job_200908171832_0012/attempt_200908171832_0012_m_000013_0/output/file.out.index in any of the configured local directories\n\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:381)\n\tat org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:138)\n\tat org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:2840)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:689)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n\tat org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)\n\tat org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)\n\tat org.mortbay.http.HttpContext.handle(HttpContext.java:1565)\n\tat org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)\n\tat org.mortbay.http.HttpContext.handle(HttpContext.java:1517)\n\tat org.mortbay.http.HttpServer.service(HttpServer.java:954)\n\tat org.mortbay.http.HttpConnection.service(HttpConnection.java:814)\n\tat org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)\n\tat org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)\n\tat org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)\n\tat org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)\n\tat org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)", "comments": ["This is a Hadoop-specific error, and it's usually related to out of disk space or out of file descriptors conditions."], "tasks": {"summary": "DiskChecker  Could not find", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "DiskChecker  Could not find"}, {"question": "What is the main context?", "answer": "2009-08-17 19:08:17,286 WARN  mapred.TaskTracker - getMapOutput(attempt_200908171832_0012_m_000013_0,55) failed :\norg.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/jobc"}]}}
{"issue_id": "NUTCH-749", "project": "NUTCH", "title": "Fetching the url from crawldb", "status": "Closed", "priority": "Major", "reporter": "salima abdulsalam", "assignee": null, "created": "2009-08-21T13:36:23.346+0000", "updated": "2009-08-21T15:37:13.410+0000", "description": "Hi,\n Iam new to using the nutch with solr.I followed the link  http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/  for integration.Iam getting an error while fetching the url from crawldb.\n\nI used the below command\n\n  bin/nutch fetch $SEGMENT -noParsing and i set the SEGMENT as  export SEGMENT=crawl/segments/`ls -tr crawl/segments|tail -1`\n\nafter running the command, iam getting the error as\n\n\nFetcher: Your 'http.agent.name' value should be listed first in 'http.robots.agents' property.\nFetcher: starting\nFetcher: segment: crawl/segments/20090821062021\nException in thread \"main\" java.io.IOException: Illegal file pattern: Expecting set closure character or end of range, or } for glob 20090821062021 at 30\n        at org.apache.hadoop.fs.FileSystem$GlobFilter.error(FileSystem.java:1086)\n        at org.apache.hadoop.fs.FileSystem$GlobFilter.setRegex(FileSystem.java:1071)\n        at org.apache.hadoop.fs.FileSystem$GlobFilter.<init>(FileSystem.java:989)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:955)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:904)\n        at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:868)\n        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:159)\n        at org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:39)\n        at org.apache.nutch.fetcher.Fetcher$InputFormat.getSplits(Fetcher.java:101)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:797)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1142)\n        at org.apache.nutch.fetcher.Fetcher.fetch(Fetcher.java:969)\n        at org.apache.nutch.fetcher.Fetcher.main(Fetcher.java:1003)\n\nCan anyone help in this.\n\nThanks,\nSalima\n\n\n ", "comments": ["Please use the nutch-user mailing list to ask questions.\n\nAs for your problem, you need to add to your nutch-site.xml something like this:\n\n<property>\n  <name>http.robots.agents</name>\n  <value>nutch-solr-integration,*</value>\n</property>\n\nChange nutch-solr-integration to your robot name."], "tasks": {"summary": "Fetching the url from crawldb", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fetching the url from crawldb"}, {"question": "What is the main context?", "answer": "Hi,\n Iam new to using the nutch with solr.I followed the link  http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/  for integration.Iam getting an error while fetching the url from crawldb.\n\nI"}]}}
{"issue_id": "NUTCH-75", "project": "NUTCH", "title": "Patch for WebDBReader to get more detailed information about WebDBs", "status": "Closed", "priority": "Minor", "reporter": "Matthias Jaekle", "assignee": null, "created": "2005-07-21T00:19:26.000+0000", "updated": "2008-03-31T05:25:48.773+0000", "description": "The patch offers information to watch\n* all outlinks from a specific url\n* all links to a specific url as csv-file\n* most links to a specific domain as csv-file\n* detailed stats with the amount of deleted links, pages without outlinks, links with anchor texts, links without anchor texts, average length of anchortexts.", "comments": ["moreWebDBReaderStats.patch", "WebDBReader is part of Version 0.7 which is no longer supported."], "tasks": {"summary": "Patch for WebDBReader to get more detailed information about WebDBs", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Patch for WebDBReader to get more detailed information about WebDBs"}, {"question": "What is the main context?", "answer": "The patch offers information to watch\n* all outlinks from a specific url\n* all links to a specific url as csv-file\n* most links to a specific domain as csv-file\n* detailed stats with the amount of del"}]}}
{"issue_id": "NUTCH-750", "project": "NUTCH", "title": "HtmlParser plugin - page title extraction", "status": "Open", "priority": "Minor", "reporter": "Alexey Torochkov", "assignee": null, "created": "2009-08-29T09:20:23.814+0000", "updated": "2025-07-09T20:25:52.319+0000", "description": "A little improvement to trying to extract <title> tag in body if it doesn't exist in head.\nIn current version DOMContentUtils just skip all after <body> in getTitle() method.\n\nAttached patch allows to change this behavior (for default it doesn't change anything) and can cope with webmasters mistakes", "comments": ["Configurable skip-body patch", "- pushing this out per http://bit.ly/c7tBv9"], "tasks": {"summary": "HtmlParser plugin - page title extraction", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "HtmlParser plugin - page title extraction"}, {"question": "What is the main context?", "answer": "A little improvement to trying to extract <title> tag in body if it doesn't exist in head.\nIn current version DOMContentUtils just skip all after <body> in getTitle() method.\n\nAttached patch allows to"}]}}
{"issue_id": "NUTCH-751", "project": "NUTCH", "title": "Upgrade version of HttpClient ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2009-09-04T19:00:44.614+0000", "updated": "2011-04-01T15:07:24.079+0000", "description": "The existing version of commons http-client (3.01) should be replaced with the latest version from http://hc.apache.org/.\nCurrently the only way of using the https protocol is to enable http-client. The version 3.01 is bugged and causes a lot of issues which have been reported before. Apparently the new version has been redesigned and should fix them. The old v3.01 is too unstable to be used on a large scale.\n \nI will try to send a patch in the next couple of weeks but would love to hear your thoughts on this.\n\nJ.\n", "comments": ["In general, if new version of a third-party package doesn't cause regression then we should upgrade.", "I'm using HttpClient 4.0 in Bixo, and I agree that Nutch should upgrade.\n\nBut the API has been changed significantly, as I'm sure Julien has seen. Lots of improvements, but this will be a non-trivial patch.\n\nThere was a recent (Sept 2nd) post on the HttpClient list by Gerald Turner, and a response by Oleg, that contained a lot of useful info about migrating from 3.1 to 4.0", "Thanks for the pointer Ken, what will be very useful when I start looking into this ", "The changes in the underlying API are quite substantial and this would need a bit of work. Maybe this could be done as part of crawler-commons? In the meantime I'll just mark it as 'later' ", "i agree that this should be in crawler-commons. E.g. I've recently made changes to avoid synchronization bottlenecks with HttpClient 4.0, and identified a few places in HC where things should be improved.\n\nThough I'm concerned that the level of customization each crawler wants could result in a pretty ugly ball of code. For example, in Bixo I'm looking at how to use a streaming disk buffer for reads, to avoid OOM errors when many threads x big responses. How would that get implemented in a way that's friendly to Nutch, Droids & Heritrix?\n\nIf we could define some least-common-denominator API, that would be a good starting point. E.g. here are the set of config values, here are the set of parameters required when making a request, and here's the format of the response from a request.\n", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Upgrade version of HttpClient ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade version of HttpClient "}, {"question": "What is the main context?", "answer": "The existing version of commons http-client (3.01) should be replaced with the latest version from http://hc.apache.org/.\nCurrently the only way of using the https protocol is to enable http-client. T"}]}}
{"issue_id": "NUTCH-752", "project": "NUTCH", "title": "how to index data from databse(ect oracle)", "status": "Closed", "priority": "Major", "reporter": "zhengfang", "assignee": null, "created": "2009-09-07T09:46:01.544+0000", "updated": "2009-09-10T07:56:26.226+0000", "description": "I am new user for nutch.I need some advice about the databases used by Nutch-1.0.\nmy application has 3 datasource:\n1. website\n2. oracle\n3. word.\nNow, I know nutch can solve problem 1 and 3, but I didn't find solution for problem 2, anyone can help me? thanks!!", "comments": ["Please use the nutch-user mailing list for questions."], "tasks": {"summary": "how to index data from databse(ect oracle)", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "how to index data from databse(ect oracle)"}, {"question": "What is the main context?", "answer": "I am new user for nutch.I need some advice about the databases used by Nutch-1.0.\nmy application has 3 datasource:\n1. website\n2. oracle\n3. word.\nNow, I know nutch can solve problem 1 and 3, but I didn"}]}}
{"issue_id": "NUTCH-753", "project": "NUTCH", "title": "Prevent new Fetcher to retrieve the robots twice", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Andrzej Bialecki", "created": "2009-09-07T18:32:34.432+0000", "updated": "2009-11-28T14:08:02.514+0000", "description": "The new Fetcher which is now used by default handles the robots file directly instead of relying on the protocol. The options Protocol.CHECK_BLOCKING and Protocol.CHECK_ROBOTS are set to false to prevent fetching the robots.txt twice (in Fetcher + in protocol), which avoids calling robots.isAllowed. However in practice the robots file is still fetched as there is a call to robots.getCrawlDelay() a bit further which is not covered by the if (Protocol.CHECK_ROBOTS).\n", "comments": ["Patch which prevents fetching the robots file twice with the new Fetcher", "Fixed in rev. 884203 - thanks!", "Integrated in Nutch-trunk #995 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/995/])\n     Prevent new Fetcher from retrieving the robots twice.\n"], "tasks": {"summary": "Prevent new Fetcher to retrieve the robots twice", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Prevent new Fetcher to retrieve the robots twice"}, {"question": "What is the main context?", "answer": "The new Fetcher which is now used by default handles the robots file directly instead of relying on the protocol. The options Protocol.CHECK_BLOCKING and Protocol.CHECK_ROBOTS are set to false to prev"}]}}
{"issue_id": "NUTCH-754", "project": "NUTCH", "title": "Use GenericOptionsParser instead of FileSystem.parseArgs()", "status": "Closed", "priority": "Minor", "reporter": "Julien Nioche", "assignee": "Andrzej Bialecki", "created": "2009-09-16T13:46:41.426+0000", "updated": "2009-10-10T04:45:46.359+0000", "description": "FileSystem.parseArgs() should be replaced with GenericOptionsParser. Doing this allows to compile Nutch with the 0.20.* branch of Hadoop", "comments": ["Fixed in rev. 823553. Thanks!", "Integrated in Nutch-trunk #959 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/959/])\n     Use GenericOptionsParser instead of FileSystem.parseArgs().\n"], "tasks": {"summary": "Use GenericOptionsParser instead of FileSystem.parseArgs()", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Use GenericOptionsParser instead of FileSystem.parseArgs()"}, {"question": "What is the main context?", "answer": "FileSystem.parseArgs() should be replaced with GenericOptionsParser. Doing this allows to compile Nutch with the 0.20.* branch of Hadoop"}]}}
{"issue_id": "NUTCH-755", "project": "NUTCH", "title": "DomainURLFilter crashes on malformed URL", "status": "Closed", "priority": "Major", "reporter": "Mike Baranczak", "assignee": "Andrzej Bialecki", "created": "2009-09-17T02:39:33.515+0000", "updated": "2009-12-29T22:30:27.496+0000", "description": "\n2009-09-16 21:54:17,001 ERROR [Thread-156] DomainURLFilter - Could not apply filter on url: http:/comments.php\njava.lang.NullPointerException\n        at org.apache.nutch.urlfilter.domain.DomainURLFilter.filter(DomainURLFilter.java:173)\n        at org.apache.nutch.net.URLFilters.filter(URLFilters.java:88)\n        at org.apache.nutch.parse.ParseOutputFormat$1.write(ParseOutputFormat.java:200)\n        at org.apache.nutch.parse.ParseOutputFormat$1.write(ParseOutputFormat.java:113)\n        at org.apache.nutch.fetcher.FetcherOutputFormat$1.write(FetcherOutputFormat.java:96)\n        at org.apache.nutch.fetcher.FetcherOutputFormat$1.write(FetcherOutputFormat.java:70)\n        at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:410)\n        at org.apache.hadoop.mapred.lib.IdentityReducer.reduce(IdentityReducer.java:39)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:436)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:170)\n\n\nExpected behavior would be to recognize the URL as malformed, and reject it.\n", "comments": ["Update: this class also chokes on legal URLs that specify the port number, like:\n\nhttp://localhost:9999/", "in the first case, the \"url\" is rejected or?\nthe filter method will return null.\n\ncatch (Exception e) {\n\n      // if an error happens, allow the url to pass\n      LOG.error(\"Could not apply filter on url: \" + url + \"\\n\"\n        + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      return null;\n    }\n\nthe comment in the code is wrong.\nif the method returns null, the url does not pass.\n\nthe malformed check is done by java.net.URL constructor.\nit accepts http:/comments.php", "I could not verify that the filter indeed crashes - it simply prints the exception and then returns null, as you suggested.", "You are both correct - it doesn't crash, it prints the exception and rejects the URL. I misunderstood what was going on.\n\nWe still have the other problem that I mentioned: a URL that specifies the port number is not parsed correctly. I'm attaching an alternate DomainURLFilter that I wrote, which fixes this problem. (It  doesn't check for the attribute \"file\" in plugin.xml, otherwise the functionality is the same as the original.)"], "tasks": {"summary": "DomainURLFilter crashes on malformed URL", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "DomainURLFilter crashes on malformed URL"}, {"question": "What is the main context?", "answer": "\n2009-09-16 21:54:17,001 ERROR [Thread-156] DomainURLFilter - Could not apply filter on url: http:/comments.php\njava.lang.NullPointerException\n        at org.apache.nutch.urlfilter.domain.DomainURLFil"}]}}
{"issue_id": "NUTCH-756", "project": "NUTCH", "title": "CrawlDatum.set() does not reset Metadata if it is null", "status": "Closed", "priority": "Blocker", "reporter": "Julien Nioche", "assignee": "Andrzej Bialecki", "created": "2009-09-29T10:40:01.108+0000", "updated": "2009-10-10T04:45:47.160+0000", "description": "The patch Nutch-702 implemented the lazy instanciation of CrawlDatum objects, but when using the method set(CrawlDatum) to copy the content of an instance into the current object we did not reset the metadata of the current object if it was null in the instance passed as argument. As a result, the metadata of the current object might be kept from a previous CrawlDatum and won't correspond to the other fields of the instance.\nThe patch attached fixes this issue.", "comments": ["Fixes issue with metadata not being properly overridden for CrawlDatum when calling the set() method", "Fixed in rev. 823557 - thanks!", "Integrated in Nutch-trunk #959 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/959/])\n     CrawlDatum.set() does not reset Metadata if it is null.\n"], "tasks": {"summary": "CrawlDatum.set() does not reset Metadata if it is null", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "CrawlDatum.set() does not reset Metadata if it is null"}, {"question": "What is the main context?", "answer": "The patch Nutch-702 implemented the lazy instanciation of CrawlDatum objects, but when using the method set(CrawlDatum) to copy the content of an instance into the current object we did not reset the "}]}}
{"issue_id": "NUTCH-757", "project": "NUTCH", "title": "RequestUtils getBooleanParameter() always returns false", "status": "Closed", "priority": "Minor", "reporter": "Niall Pemberton", "assignee": "Andrzej Bialecki", "created": "2009-09-30T14:40:58.782+0000", "updated": "2011-06-08T21:34:22.957+0000", "description": "RequestUtils getBooleanParameter() always returns false because it tests the name of the parameter rather than the value!\n\nYou can see this problem with the Nutch webapp, if you try adding a \"summary\" parameter the summary is never included, whatever value you specify (either \"1\", \"true\" or \"yes\"", "comments": ["Fixed in rev. 823547 - special thanks for the comprehensive unit test!", "Integrated in Nutch-trunk #959 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/959/])\n     RequestUtils getBooleanParameter() always returns false.\n"], "tasks": {"summary": "RequestUtils getBooleanParameter() always returns false", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "RequestUtils getBooleanParameter() always returns false"}, {"question": "What is the main context?", "answer": "RequestUtils getBooleanParameter() always returns false because it tests the name of the parameter rather than the value!\n\nYou can see this problem with the Nutch webapp, if you try adding a \"summary\""}]}}
{"issue_id": "NUTCH-758", "project": "NUTCH", "title": "Set subversion eol-style to \"native\"", "status": "Closed", "priority": "Minor", "reporter": "Niall Pemberton", "assignee": "Andrzej Bialecki", "created": "2009-09-30T18:44:21.540+0000", "updated": "2009-10-10T04:45:47.768+0000", "description": "It would be really nice to set the subversion eol-style (end-of-line style) to \"native\" - makes it much easier for different contributors on different OS's to contribute patches.", "comments": ["Fixed in rev. 823614 - I did propset using a script, so if I missed something please open a separate issue to track it. Thanks!", "Integrated in Nutch-trunk #959 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/959/])\n     Set subversion eol-style to \"native\".\n"], "tasks": {"summary": "Set subversion eol-style to \"native\"", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Set subversion eol-style to \"native\""}, {"question": "What is the main context?", "answer": "It would be really nice to set the subversion eol-style (end-of-line style) to \"native\" - makes it much easier for different contributors on different OS's to contribute patches."}]}}
{"issue_id": "NUTCH-759", "project": "NUTCH", "title": "Removal of deprecated APIs", "status": "Closed", "priority": "Minor", "reporter": "Stephen Norman", "assignee": "Chris A. Mattmann", "created": "2009-10-14T01:28:44.399+0000", "updated": "2011-04-01T15:07:23.411+0000", "description": "The Nutch project is using a number of deprecated APIs.\n\nIt would be nice to clean this up in to maintain compatibility in the future.", "comments": ["This issue isn't clear at all. What deprecated APIs are you talking about? If you have some concrete places you think the APIs could be improved, please file new issues that clearly identify where, and patches welcome.\n\nThanks!", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Removal of deprecated APIs", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Removal of deprecated APIs"}, {"question": "What is the main context?", "answer": "The Nutch project is using a number of deprecated APIs.\n\nIt would be nice to clean this up in to maintain compatibility in the future."}]}}
{"issue_id": "NUTCH-760", "project": "NUTCH", "title": "Allow field mapping from nutch to solr index", "status": "Closed", "priority": "Major", "reporter": "David Stuart", "assignee": "Andrzej Bialecki", "created": "2009-10-15T10:43:49.939+0000", "updated": "2009-11-28T14:08:02.837+0000", "description": "I am using nutch to crawl sites and have combined it\nwith solr pushing the nutch index using the solrindex command. I have\nset it up as specified on the wiki using the copyField url to id in the\nschema. Whilst this works fine it is stuff's up my inputs from other\nsources in solr (e.g. using the solr data import handler) as they have\nboth id's and url's. I have patch that implements a nutch xml schema\ndefining what basic nutch fields map to in your solr push.", "comments": ["First pass at a schema reader for mapping basic nutch fields to solr", "oops left out schema file", "Thanks David, this is a good start. We also need to address the searching part, i.e. SolrSearchBean, where Nutch hardcodes the same field names.", "Updated patch with the modifications to the SolrSearchBean. Have also re factored a wee bit to allow other classes to hook into the solr index schema", "A few comments to the latest patch:\n\n* the description of the property in nutch-default.xml could be more descriptive ;)\n\n* <schema> element has name and version attributes - do we really need these? It's not a Solr schema.xml anyway, so we don't have to pretend that we follow the same format.\n\n* SolrSchemaReader uses static instance of NutchConfiguration - this is a big no-no, the whole point of using the property in nutch-default.xml is that you could set different values, and making this field static basically pins down the configuration to the version set on the first instantiation of the class ... Please do as other similar classes do - implement Configurable, or add Configuration to the constructor, and pass the current job configuration where appropriate.\n\n* consequently, static references to SolrSchemaReader need to be un-staticized in other places.\n\n* minor nits: code formatting should use 2 literal spaces indents. There are some accidental changes in NutchBean and SolrWriter.", "Thanks,\n\nI will have another go. It quite a big task getting my head around all of the\nins and outs of nutch but its good to help to contribute to a great product\n\nRegards,\n\nDave\n\n\n\n\n", "Have updated patch as per comment below\n    *  the description of the property in nutch-default.xml could be more descriptive\n\n    * <schema> element has name and version attributes - do we really need these? It's not a Solr schema.xml anyway, so we don't have to pretend that we follow the same format.\n\n    * SolrSchemaReader uses static instance of NutchConfiguration - this is a big no-no, the whole point of using the property in nutch-default.xml is that you could set different values, and making this field static basically pins down the configuration to the version set on the first instantiation of the class ... Please do as other similar classes do - implement Configurable, or add Configuration to the constructor, and pass the current job configuration where appropriate.\n\n    * consequently, static references to SolrSchemaReader need to be un-staticized in other places.\n\n    * minor nits: code formatting should use 2 literal spaces indents. There are some accidental changes in NutchBean and SolrWriter.\n", "Hi Andrzej,\n\nI have amended the patch to incorporate your suggestions\nhttps://issues.apache.org/jira/browse/NUTCH-760\n\nRegards,\n\n\nDave \n\n", "I reworked the patch to get rid of any left-overs of static Configuration, and changed the concept of \"schema\" (which was misleading) to \"mapping\" throughout the patch and class names.\n\nThis is now committed in rev. 884269 - thanks!", "Integrated in Nutch-trunk #995 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/995/])\n    Add part of .\n Allow field mapping from nutch to solr index.\n"], "tasks": {"summary": "Allow field mapping from nutch to solr index", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Allow field mapping from nutch to solr index"}, {"question": "What is the main context?", "answer": "I am using nutch to crawl sites and have combined it\nwith solr pushing the nutch index using the solrindex command. I have\nset it up as specified on the wiki using the copyField url to id in the\nschem"}]}}
{"issue_id": "NUTCH-761", "project": "NUTCH", "title": "Avoid cloningCrawlDatum in CrawlDbReducer ", "status": "Closed", "priority": "Minor", "reporter": "Julien Nioche", "assignee": "Andrzej Bialecki", "created": "2009-11-03T14:39:38.527+0000", "updated": "2009-11-28T14:08:03.115+0000", "description": "In the huge majority of cases the CrawlDbReducer gets unique CrawlData in its reduce phase and these will be the entries coming from the crawlDB and not present in the segments.\nThe patch attached optimizes the reduce step by avoid an unnecessary cloning of the CrawlDatum fields when there is only one CrawlDatum in the values. This has more impact has the crawlDB gets larger,  we noticed an improvement of around 25-30% in the time spent in the reduce phase.", "comments": ["I applied the patch with some changes - reverted the logic in the name of the boolean var, and applied the same method to other cases of non-multiple values. Committed in rev. 884224 - thanks!", "Integrated in Nutch-trunk #995 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/995/])\n    Fix a bug resulting from over-eager optimization in .\n Avoid cloning CrawlDatum in CrawlDbReducer.\n"], "tasks": {"summary": "Avoid cloningCrawlDatum in CrawlDbReducer ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Avoid cloningCrawlDatum in CrawlDbReducer "}, {"question": "What is the main context?", "answer": "In the huge majority of cases the CrawlDbReducer gets unique CrawlData in its reduce phase and these will be the entries coming from the crawlDB and not present in the segments.\nThe patch attached opt"}]}}
{"issue_id": "NUTCH-762", "project": "NUTCH", "title": "Alternative Generator which can generate several segments in one parse of the crawlDB", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2009-11-03T15:03:42.045+0000", "updated": "2011-09-07T09:53:22.837+0000", "description": "When using Nutch on a large scale (e.g. billions of URLs), the operations related to the crawlDB (generate - update) tend to take the biggest part of the time. One solution is to limit such operations to a minimum by generating several fetchlists in one parse of the crawlDB then update the Db only once on several segments. The existing Generator allows several successive runs by generating a copy of the crawlDB and marking the URLs to be fetched. In practice this approach does not work well as we need to read the whole crawlDB as many time as we generate a segment.\n\nThe patch attached contains an implementation of a MultiGenerator  which can generate several fetchlists by reading the crawlDB only once. The MultiGenerator differs from the Generator in other aspects: \n* can filter the URLs by score\n* normalisation is optional\n* IP resolution is done ONLY on the entries which have been selected for  fetching (during the partitioning). Running the IP resolution on the whole crawlDb is too slow to be usable on a large scale\n* can max the number of URLs per host or domain (but not by IP)\n* can choose to partition by host, domain or IP\n\nTypically the same unit (e.g. domain) would be used for maxing the URLs and for partitioning; however as we can't count the max number of URLs by IP another unit must be chosen while partitioning by IP. \nWe found that using a filter on the score can dramatically improve the performance as this reduces the amount of data being sent to the reducers.\n\nThe MultiGenerator is called via : nutch org.apache.nutch.crawl.MultiGenerator ...\nwith the following options :\nMultiGenerator <crawldb> <segments_dir> [-force] [-topN N] [-numFetchers numFetchers] [-adddays numDays] [-noFilter] [-noNorm] [-maxNumSegments num]\n\nwhere most parameters are similar to the default Generator - apart from : \n-noNorm (explicit)\n-topN : max number of URLs per segment\n-maxNumSegments : the actual number of segments generated could be less than the max value select e.g. not enough URLs are available for fetching and fit in less segments\n\nPlease give it a try and less me know what you think of it\n\nJulien Nioche\nhttp://www.digitalpebble.com\n\n\n\n\n ", "comments": ["Patch for the MultiGenerator", "This class offers a strict superset of the current Generator functionality. Maintaining both tools would be cumbersome and error-prone. I propose to replace Generator with MultiGenerator (under the current name Generator).", "It would be handy if it output a list of the segments it generated, either one at a time or a list at the end of generating all of them. This would be very useful for automation scripts that rely on parsed output for further processing.", "Improved version of the patch : \n\n- fixed a few minor bugs\n- renamed Generator into OldGenerator\n- renamed MultiGenerator into Generator\n- fixed test classes to use new Generator\n- documented parameters in nutch-default.xml\n- add names of segments to the LOG to facilitate integration in scripts\n- PartitionUrlByHost is replaced by URLPartitioner which is more generic\n\nI decided to keep the old version for the time being but we might as well get rid of it altogether. The new version is now used in the Crawl class. \n\nWould be nice if people could give it a good try before we put it in 1.1\n\nThanks\n\nJulien ", "It appears this class is not a strict superset - the generate.update.crawldb functionality is not there. This is a regression in a useful functionality, so I think it needs to be added back.", "If I am not mistaken the point of having  _generate.update.crawldb_ was to marke the URLs put in a fetchlist in order to be able to do another round of generation. This is not necessary now as we can generate several segments without writing a new crawldb.\nAm I missing something?  ", "In case of users generating just 1 segment at a time it's an unexpected loss of flexibility. You can't run this version of Generator twice without first completing _both_ fetching & updating of all segments from the previous run - because some of the same urls would be generated in the next round. The point of generate.update.crawldb is to be able to freely interleave generate/update steps.\n\nE.g. the following scenario breaks in a non-obvious way:\n\n* generate 10 segments\n* fetch & update 8 of them\n* realize you need more rounds due to e.g. gone pages\n* generate additional 10 segments\n\n..kaboom! now the new segments partially overlap with the unfetched 2 segments from the previous generation, and you are going to fetch some urls twice. ", "OK, there was indeed an assumption that the generator would not need to be called again before an update.  Am happy to add back generate.update.crawldb. \n\nNote that this version of the Generator also differs from the original version in that \n\n{quote}\n*IP resolution is done ONLY on the entries which have been selected for fetching (during the partitioning). Running the IP resolution on the whole crawlDb is too slow to be usable on a large scale\n*can max the number of URLs per host or domain (but not by IP)\n{quote}\n\nWe could allow more flexibility by counting per IP, again at the expense of performance. Not sure it is very useful in practice though. Since the way we count the URLs is now decoupled from the way we partition them, we can have an hybrid approach e.g. count per domain THEN partition by IP. \n\nAny thoughts on whether or not we should reintroduce the counting per IP?", "In my experience the IP-based fetching was only (rarely) needed when there was a large number of urls from virtual hosts hosted at the same ISP. In other words, not a common case - others may have different experience depending on their typical crawl targets... IMHO I think we don't have to reimplement this.", "Yes, I came across that situation too on a large crawl where a single machine was used to host a whole range of unrelated domain names (needless to say the host of the domains was not very pleased). We can now handle such cases that simply by partitioning by IP (and counting by domain).\n\nI will have a look at reintroducing *generate.update.crawldb* tomorrow.\n\n\n\n ", "new patch which reintroduces the 'generator.update.crawldb' functionality ", "I just noticed that the new Generator uses different config property names (\"generator.\" vs. \"generate.\"), and the older versions are now marked with \"(Deprecated)\". However, this doesn't reflect the reality - properties with old names are simply ignored now, whereas \"deprecated\" implies that they should still work. For back-compat reason I think they should still work - the current (admittedly awkward) prefix is good enough, and I think that changing it in a minor release would create confusion. I suggest reverting to the old names where appropriate, and add new properties with the same prefix, i.e. \"generate.\".", "{quote}\nI just noticed that the new Generator uses different config property names (\"generator.\" vs. \"generate.\"), and the older versions are now marked with \"(Deprecated)\". However, this doesn't reflect the reality - properties with old names are simply ignored now, whereas \"deprecated\" implies that they should still work\n{quote}\n\nThey will still work if we keep the old Generator as OldGenerator - which is what we assume in the patch. If we decide to get shot of the OldGenerator then yes, they should not be marked  with \"(Deprecated)\"\n\n{quote}\nFor back-compat reason I think they should still work - the current (admittedly awkward) prefix is good enough, and I think that changing it in a minor release would create confusion. I suggest reverting to the old names where appropriate, and add new properties with the same prefix, i.e. \"generate.\".\n{quote}\n\nthe original assumption was that we'd keep both this version of the generator and the old one in which case we could have used a different prefix for the properties. If we want to *replace* the old generator altogether - which I think would be a good option - then indeed we should discuss whether or not to align on the old prefix. \n\nI don't have strong feelings on whether or not to modify the prefix in a minor release.  \n\n\n", "bq. If we want to replace the old generator altogether - which I think would be a good option\n\nI think this makes sense now, since the new Generator in your latest patch is a strict superset of the old one. \n\nbq. I don't have strong feelings on whether or not to modify the prefix in a minor release. \n\nI do :) , see also here: http://en.wikipedia.org/wiki/Principle_of_least_astonishment\n\nIMHO it's all about breaking or not breaking existing installs after a minor upgrade. I suspect most users won't be aware of a subtle change between \"generate.\" and \"generator.\", especially since the command-line of the new Generator is compatible with the old one. So they will try to use the new Generator while keeping their existing configs.", "The change of prefix also reflected that we now use 2 different parameters so specify how to count the URLs (host or domain) and the max number of URLs.  We can of course maintain the old parameters as well for the sake of compatibility, except that _generate.max.per.host.by.ip_ won't be of much use anymore as we don't count per IP.\n\nHave just noticed  that 'crawl.gen.delay' is not documented in nutch-default.xml, and does not seem to be used outside the Generator. What is it supposed to be used for? ", "bq. The change of prefix also reflected that we now use 2 different parameters so specify how to count the URLs (host or domain) and the max number of URLs. We can of course maintain the old parameters as well for the sake of compatibility, except that generate.max.per.host.by.ip  won't be of much use anymore as we don't count per IP.\n\nOk.\n\nbq. Have just noticed that 'crawl.gen.delay' is not documented in nutch-default.xml, and does not seem to be used outside the Generator. What is it supposed to be used for? \n\nAh, a bit of ancient magic .. ;) This value, expressed in days, defines how long we should keep the lock on records in CrawlDb that were just selected for fetching. If these records are not updated in the meantime, the lock is canceled, i.e. the become eligible for selecting. Default value of this is 7 days.", "Committed revision 926155\n\nHave reverted the prefix for params to 'generate.' + added description of crawl.gen.delay on nutch-default + added warning when user specified generate.max.per.host.by.ip + param generate.max.per.host is now supported\n\nThanks Andzrej for your reviewing it ", "Integrated in Nutch-trunk #1104 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1104/])\n    fixed NPE introduced in \n : Generator can generate several segments in one parse of the crawlDB\n"], "tasks": {"summary": "Alternative Generator which can generate several segments in one parse of the crawlDB", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Alternative Generator which can generate several segments in one parse of the crawlDB"}, {"question": "What is the main context?", "answer": "When using Nutch on a large scale (e.g. billions of URLs), the operations related to the crawlDB (generate - update) tend to take the biggest part of the time. One solution is to limit such operations"}]}}
{"issue_id": "NUTCH-763", "project": "NUTCH", "title": "Separate configuration files from resources to be included in the job file", "status": "Closed", "priority": "Minor", "reporter": "Julien Nioche", "assignee": null, "created": "2009-11-05T18:33:34.589+0000", "updated": "2010-07-12T09:19:31.989+0000", "description": "One of the things I found confusing when I was learning Nutch was the fact that the conf/ directory contains at the same time : \n- configuration files for Hadoop / Nutch which are put in the jar files but not used there\n- resource files (e.g. filtering rules) which MUST be up to date in the job file\n\nI would separate the conf/ directory from say a resources/ directory which would contain the rule files and other things to put in the job file. Unless I am mistaken none of the configuration files need to be in the job file. I know it is a very minor point, but that would probably simplify things and make it easier for beginners to understand what has to be modified where. ", "comments": ["- pushing this out per http://bit.ly/c7tBv9", "NUTCH-843 made things a lot simpler and clearer"], "tasks": {"summary": "Separate configuration files from resources to be included in the job file", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Separate configuration files from resources to be included in the job file"}, {"question": "What is the main context?", "answer": "One of the things I found confusing when I was learning Nutch was the fact that the conf/ directory contains at the same time : \n- configuration files for Hadoop / Nutch which are put in the jar files"}]}}
{"issue_id": "NUTCH-764", "project": "NUTCH", "title": "Add support for vfsfile:// loading of plugins for JBoss", "status": "Closed", "priority": "Trivial", "reporter": "tcurran@approachingpi.com", "assignee": null, "created": "2009-11-10T01:23:32.942+0000", "updated": "2011-06-08T21:34:22.422+0000", "description": "In the file:\n/src/java/org/apache/nutch/plugin/PluginManifestParser.java\n\nThere is a check to make sure that the plugin file location is a url formatted like \"file://path/plugins\".\n\nWhen deployed on Jboss, the file protocol will sometimes be: \"vfsfile://path/plugins\".  The code with vfsfile can operate the same so I propose a change to the check to also allow this protocol.  This would allow Nutch to be deployed on the newer versions of JBoss without any modification.\n\nHere is a simple patch:\n\nIndex: src/java/org/apache/nutch/plugin/PluginManifestParser.java\n===================================================================\n--- src/java/org/apache/nutch/plugin/PluginManifestParser.java\tMon Nov 09 20:20:51 EST 2009\n+++ src/java/org/apache/nutch/plugin/PluginManifestParser.java\tMon Nov 09 20:20:51 EST 2009\n@@ -121,7 +121,8 @@\n       } else if (url == null) {\n         LOG.warn(\"Plugins: directory not found: \" + name);\n         return null;\n-      } else if (!\"file\".equals(url.getProtocol())) {\n+      } else if (!\"file\".equals(url.getProtocol()) &&\n+        !\"vfsfile\".equals(url.getProtocol())) {\n         LOG.warn(\"Plugins: not a file: url. Can't load plugins from: \" + url);\n         return null;\n       }\n\n", "comments": ["First question is: why is it sometimes vfsfile:// ? What you propose is just hiding the problem, and not solving it - Nutch does not support this protocol, so we should not pretend to do so. More specifically, plugins can only be loaded from locations accessible through the file:// protocol, and pretending that we can load them from vfsfile:// (whatever it may be) is just not true, even if it works in your specific case.\n\nSecond question: does it work with this fix, i.e. is Nutch able to load the plugins using this protocol?", "I should have said, by default, it will always be vfsfile. Jboss 5.0+ uses this virtual file system for any files access performed via the context class loaders.  For files outside the deployment directory can still be loaded with file:.  It is just a cache and abstracts that sometimes the file will be in memory or a different physical location during application runtime.\n\nfor your second question: yes, this does fix Nutch and it is then able to load all of the plugins properly.\n\nI understand your concerns and it was hasty of me to assume that this was a trivial issue to implement.  I will spend some time investigating and see if there are any other aspects of Nutch that are impacted by this protocol.\n\n", "Thanks for the explanation. Well, it's surprising that it works - in that method in the manifest parser we return a java.io.File constructed from this URL, and we expect that it's a regular java.io.File directory. Next question is: if this is a cache, then perhaps the first time you obtain a resource it returns a file:// but on subsequent calls it returns vfsfile:// and you didn't experience any problems because plugins were loaded just once? I'm guessing here - I don't use JBoss.", "Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Add support for vfsfile:// loading of plugins for JBoss", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add support for vfsfile:// loading of plugins for JBoss"}, {"question": "What is the main context?", "answer": "In the file:\n/src/java/org/apache/nutch/plugin/PluginManifestParser.java\n\nThere is a check to make sure that the plugin file location is a url formatted like \"file://path/plugins\".\n\nWhen deployed on J"}]}}
{"issue_id": "NUTCH-765", "project": "NUTCH", "title": "Allow Crawl class to call Either Solr or Lucene Indexer", "status": "Closed", "priority": "Minor", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2009-11-12T20:59:24.916+0000", "updated": "2009-11-28T14:08:02.297+0000", "description": "Change to the crawl class to have a -solr option which will call the solr indexer instead of the lucene indexer.  This also allows it to ignore dedup and merge for solr indexing and to point to a specific solr instance.", "comments": ["Committed.", "Integrated in Nutch-trunk #995 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/995/])\n     - Allow Crawl class to call Either Solr or Lucene Indexer.\n"], "tasks": {"summary": "Allow Crawl class to call Either Solr or Lucene Indexer", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Allow Crawl class to call Either Solr or Lucene Indexer"}, {"question": "What is the main context?", "answer": "Change to the crawl class to have a -solr option which will call the solr indexer instead of the lucene indexer.  This also allows it to ignore dedup and merge for solr indexing and to point to a spec"}]}}
{"issue_id": "NUTCH-766", "project": "NUTCH", "title": "Tika parser", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Chris A. Mattmann", "created": "2009-11-18T14:49:46.551+0000", "updated": "2010-09-28T00:48:52.313+0000", "description": "Tika handles a lot of different formats under the bonnet and exposes them nicely via SAX events. What is described here is a tika-parser plugin which delegates the pasring mechanism of Tika but can still coexist with the existing parsing plugins which is useful for formats partially handled by Tika (or not at all). Some of the elements below have already been discussed on the mailing lists. Note that this is work in progress, your feedback is welcome.\n\nTika is already used by Nutch for its MimeType implementations. Tika comes as different jar files (core and parsers), in the work described here we decided to put the libs in 2 different places\nNUTCH_HOME/lib : tika-core.jar\nNUTCH_HOME/tika-plugin/lib : tika-parsers.jar\nTika being used by the core only for its Mimetype functionalities we only need to put tika-core at the main lib level whereas the tika plugin obviously needs the tika-parsers.jar + all the jars used internally by Tika\n\nDue to limitations in the way Tika loads its classes, we had to duplicate the TikaConfig class in the tika-plugin. This might be fixed in the future in Tika itself or avoided by refactoring the mimetype part of Nutch using extension points.\n\nUnlike most other parsers, Tika handles more than one Mime-type which is why we are using \"*\" as its mimetype value in the plugin descriptor and have modified ParserFactory.java so that it considers the tika parser as potentially suitable for all mime-types. In practice this means that the associations between a mime type and a parser plugin as defined in parse-plugins.xml are useful only for the cases where we want to handle a mime type with a different parser than Tika. \n\nThe general approach I chose was to convert the SAX events returned by the Tika parsers into DOM objects and reuse the utilities that come with the current HTML parser i.e. link detection,  metatag handling but also means that we can use the HTMLParseFilters in exactly the same way. The main difference though is that HTMLParseFilters are not limited to HTML documents anymore as the XHTML tags returned by Tika can correspond to a different format for the original document. There is a duplication of code with the html-plugin which will be resolved by either a) getting rid of the html-plugin altogether or b) exporting its jar and make the tika parser depend on it.\n\nThe following libraries are required in the lib/ directory of the tika-parser : \n\n      <library name=\"asm-3.1.jar\"/>\n      <library name=\"bcmail-jdk15-144.jar\"/>\n      <library name=\"commons-compress-1.0.jar\"/>\n      <library name=\"commons-logging-1.1.1.jar\"/>\n      <library name=\"dom4j-1.6.1.jar\"/>\n      <library name=\"fontbox-0.8.0-incubator.jar\"/>\n      <library name=\"geronimo-stax-api_1.0_spec-1.0.1.jar\"/>\n      <library name=\"hamcrest-core-1.1.jar\"/>\n      <library name=\"jce-jdk13-144.jar\"/>\n      <library name=\"jempbox-0.8.0-incubator.jar\"/>\n      <library name=\"metadata-extractor-2.4.0-beta-1.jar\"/>\n      <library name=\"mockito-core-1.7.jar\"/>\n      <library name=\"objenesis-1.0.jar\"/>\n      <library name=\"ooxml-schemas-1.0.jar\"/>\n      <library name=\"pdfbox-0.8.0-incubating.jar\"/>\n      <library name=\"poi-3.5-FINAL.jar\"/>\n      <library name=\"poi-ooxml-3.5-FINAL.jar\"/>\n      <library name=\"poi-scratchpad-3.5-FINAL.jar\"/>\n      <library name=\"tagsoup-1.2.jar\"/>\n      <library name=\"tika-parsers-0.5-SNAPSHOT.jar\"/>\n      <library name=\"xml-apis-1.0.b2.jar\"/>\n      <library name=\"xmlbeans-2.3.0.jar\"/>\n\nThere is a small test suite which needs to be improved. We will need to have a look at each individual format and check that it is covered by Tika and if so to the same extent; the Wiki is probably the right place for this. The language identifier (which is a HTMLParseFilter) seemed to work fine.\n \nAgain, your comments are welcome. Please bear in mind that this is just a first step. \n\nJulien\nhttp://www.digitalpebble.com\n\n\n\n", "comments": ["patch for the Tika-plugin", "Patch for the ParserFactory to allow * as mimetype value for a parser plugin", "Hi Julien:\n\nI have had a look and was trying to test it out but got sidetracked. Give me this week to try and put together a final reviewable/commitable patch, otherwise, it's all yours.\n\nCheers,\nChris\n", "Hi Chris, \n\nNo worries, I'd rather wait for you to have a look at it. It's quite a big change and it would be better if someone else had a look at it. Being the author I might miss something obvious\n\nThanks\n\nJ.", "I took a brief look into the proposed patch, some somments:\n\nThe public API footprint of new classes should be smaller, eg use private, package private or protected methods/classes as much as possible.\n\nI think the end result of this plugin should be replacing all Tika supported parsers (or the parsers we choose to replace) with the TikaParser and not to build a parallel ways to parse same formats. So I think we need to copy all of the the existing test files and move&adapt the existing testcases fully before committing this. That is a good way of seeing that the parse result is what is expected and also find out about possible differences with old vs. Tika version.\n", "> I think the end result of this plugin should be replacing all Tika supported parsers (or the parsers we choose to replace) with the TikaParser and not to build a parallel ways to parse same formats. \n\nThat's how I see it - it's just that we have the option of choosing when to use Tika or not for a given mimetype. It is used by default unless an association is created between a parser implementation and   a mimetype in the parse-plugins.xml\n\n> So I think we need to copy all of the the existing test files and move&adapt the existing testcases fully before committing this. That is a good way of seeing that the parse result is what is expected and also find out about possible differences with old vs. Tika version.\n\nSure, but it would be silly to block the whole Tika plugin because Tika does not support such or such format as well as the original Nutch plugins. As I explained above we can configure which parser to use for which mimetype and use the Tika-plugin by default.   Hopefully the Tika implementation will get better and better and there will be no need for keeping the old plugins.\n\nBTW http://wiki.apache.org/nutch/TikaPlugin lists the differences between the current version of Tika and the existing Nutch parsers\n\nEven if we decide to keep using the old plugins for some of the formats to start with, we'd still be able to the Tika plugin by default for the ones which have already the same coverage\n", "> Sure, but it would be silly to block the whole Tika plugin because Tika does not support such or such format as well as the original Nutch plugins. As I explained above we can configure which parser to use for which mimetype and use the Tika-plugin by default. Hopefully the Tika implementation will get better and better and there will be no need for keeping the old plugins.\n\nI meant test files for the parsers we replace, not all\n\n> BTW http://wiki.apache.org/nutch/TikaPlugin lists the differences between the current version of Tika and the existing Nutch parsers\n\nok, I had misses that one. ", "{quote}\nSure, but it would be silly to block the whole Tika plugin because Tika does not support such or such format as well as the original Nutch plugins. As I explained above we can configure which parser to use for which mimetype and use the Tika-plugin by default. Hopefully the Tika implementation will get better and better and there will be no need for keeping the old plugins.\n{quote}\n\n+1, I'm going to agree on this one here Julien. Other communities ;) have convinced me of the need for backwards compat and unobtrusiveness when bringing in new functionality or results. +1 to at least in Nutch 1.1 leaving the old plugins (perhaps mentioning they should be deprecated and replaced by the Tika functionality) and then removing them in 1.2 or 1.3.\n\nI got bogged down with my paid job, but I found some Apache time recently so this is tops on my list to tackle.\n\nCheers,\nChris\n\n", ">+1, I'm going to agree on this one here Julien. Other communities  have convinced me of the need for backwards compat and unobtrusiveness when bringing in new functionality or results. +1 to at least in Nutch 1.1 leaving the old plugins (perhaps mentioning they should be deprecated and replaced by the Tika functionality) and then removing them in 1.2 or 1.3.\n\nChris, can you please explain me how keeping two components doing identical work would be more backwards compatible than having only 1? \n\n", "Hi Sami:\n\n{quote}\nChris, can you please explain me how keeping two components doing identical work would be more backwards compatible than having only 1?\n{quote}\n\nSure, it's more of a configuration backwards-compat issue. For those folks who have gone to the trouble of customizing their nutch configuration (nuch-site.xml, or nutch-default.xml, or even parse-plugins), to remove out the parsing plugins (e.g., basically say they don't exist anymore and update your deployed configuration to use the tika-plugin), this patch would require a configuration update in their deployed environments. Because of that, why don't we ease them into that upgrade with at least one released version before the plugins go away. It would make it easier from a configuration backwards-compat perspective.\n\nHTH,\nChris\n", "I agree with Chris, +1 on keeping the old plugins in 1.1 with a prominent deprecation note, but I feel equally strongly that we should not prolong their life-cycle beyond what we can support, i.e. I'm +1 on removing them in 1.2/1.3. We simply don't have resources to maintain so many duplicate plugins, and instead we should direct our efforts to improve those in Tika.", "{quote}\nSure, it's more of a configuration backwards-compat issue. For those folks who have gone to the trouble of customizing their nutch configuration (nuch-site.xml, or nutch-default.xml, or even parse-plugins), to remove out the parsing plugins (e.g., basically say they don't exist anymore and update your deployed configuration to use the tika-plugin), this patch would require a configuration update in their deployed environments. Because of that, why don't we ease them into that upgrade with at least one released version before the plugins go away. It would make it easier from a configuration backwards-compat perspective.\n{quote}\n\nOk, so you mean that we need to have duplicate parser plugins because we don't want to ask people already using nutch to reconfigure the bits this involves now even though we have to do it later? How is postponing going to ease the task they need to do anyway at some point? I still don't understand the (longer term) benefit.\n\nI am not strongly against the idea of keeping duplicate plugins, I mean it's just another ~20M in the .job, what I am worried about is that the history will repeat itself and we will end up having one more case of duplicate components (in this case many of them) doing the same work and no interest in cleaning up afterwards. Doing it the way I suggested would guarantee that this will not happen.\n", "Here is a slightly better version of the patch which : \n• fixes a small bug in the Tika parser (the API has changed slightly between 1.5beta and 1.5)\n• fixes a bug with the TestParserFactory\n• adds the tika-plugin to the list of plugins to be built in src/plugin/build.xml\n• limits public exposure of methods and classes (see Sami's comment)\n• modified parse-plugins.xml : added parse-tika and commented out associations between some mime-types and the old parsers\n\nI've also added an ANT script which uses IVY to pull the dependencies and copies them into the lib dir. Obviously this won't be needed when the plugin is committed but should simplify the initial testing. All you need to do after applying the patch is to :\n\ncd src/plugin/parse-tika/\nant -f build-ivy.xml\n\nAm also attaching the content of the sample directory as an archive - just unzip onto the src/plugin/parse-tika/ before calling ant test-plugins\n\nJulien\n\n\n", "new version of the patch + archive containing the binary docs used for testing", "Updated version of the plugin : uses Tika 0.6", "+1 to commit this - please remember to update nutch-default.xml to switch to the tika plugin, perhaps add a comment about the deprecated parse-* plugins - most people look here and not in the parse-plugins, where this change is documented...", "{quote}\n+1 to commit this...\n{quote}\n\nAwesome, Andrzej. Will do so tonight, PST, if I don't hear any objections between now and then...\n\nThanks!\n\nCheers,\nChris\n", "I'm going to hold off on committing this tonight. I've updated the docs per Andrzej, and I've also updated CHANGES.txt, but when running:\n\n{code}\nant clean compile-core test\n{code}\n\nI'm seeing these messages during plugin testing for parse-tika:\n\n{noformat}\n2010-02-10 22:39:16,593 ERROR tika.TikaParser (TikaParser.java:getParse(63)) - Can't retrieve Tika parser for mime-type application/pdf\n------------- ---------------- ---------------\n\nTestcase: testIt took 2.684 sec\n        FAILED\nnull\njunit.framework.AssertionFailedError\n        at org.apache.nutch.tika.TestPdfParser.testIt(TestPdfParser.java:79)\n{noformat}\n\nIt seems that the TikaConfig is not being found? I was looking at TikaParser#setConf and it seems that a default config is being created for Tika, but maybe not being loaded correctly? I need to look into this more...", "I suggest that we would still drive this a bit further an use. currently this patch does not use Tika for pkg formats nor html.\n\nJulien: was there a reason not to use AutoDetect parser? The only thing that I could come with was that the mime type detection would be done twice. We could get around this by implementing somethin simlilar to what composite parser does (it uses a parser (AutodetectParser) class from the context to do further parsing) to cover all supported pkg formats.\n\nAlso was there a reson not to parse html wtih tika?\n\nI have a patch nearby to demonstrate some of the improvements that I will try to post briefly.", "Extended TikaConfig that is able to load parsers and can be used with existing tika classes. The call to (super) cannot load parser but then the config is porcessed again locally. This is a hack and hopefully at some point we can drop the class alltogether.", "Modified parser that can process package formats too. To get rid of the mime type detection happening twice we have to extend AutoDetectParser so that skips the intitial detection but does the detection for the rest of the content (in pkg formats)", "@Chris : I just did a fresh co from svn, applied the patch v3 and unzipped sample.tar.gz onto  the directory parse-tika and ran the test just as you did but could not reproduce the problem.  Could there be a difference between your version and the trunk?\n\n@Sami :  \n\n{quote} was there a reason not to use AutoDetect parser?  {quote} \nI suppose we could as long we give it a clue about the MimeType obtained from the Content.  As you pointed out, there could be a duplication with the detection done by Mime-Util. I suppose one way to do would be to add a new version of the method getParse(Content conte, MimeType type). That's an interesting point.\n\n{quote} Also was there a reson not to parse html wtih tika?  {quote} \nIt is supposed to do so, if it does not then it's a bug which needs urgent fixing.\n\nRegarding parsing package formats, I think the plan is that Tika will handle that in the future but we could try to do that now if we find a relatively clean mechanism for doing so. BTW could you please send a diff and not the full code of the class you posted earlier, that would make the comparison much easier.\n\n\n", "I had a closer look at the HTML parsing issue. What happens  is that the association between the mime-type and the parser implementation is not explicitely set in parse-plugins.xml so the ParserFactory goes through all the plugins and gets the ones with a matching mimetype (or * for Tika). The Tika parser takes no precedence over the default HTML parser and the latter gets first in the list and is used for parsing.\n\nOf course that does not happen if parse-html is not specified in plugin.includes or if an explicit mapping is set in parse-plugins.xml.  I don't think we want to have to specify explicitely that tika should be used in all the mappings and reserve cases for when a parser must be used instead of Tika.\n\nWhat we could do though is that in the cases where no explicit mapping is set for a mimetype, Tika (or any parser marked as supporting any mimetype) will be put first in the list of discovered parsers so it would remain the default choice unless an explicit mapping is set (even if a plugin is loaded and can handle the type).\n\nMakes sense?\n\nThe ParserFactory section of the patch v3 can be replaced by :  \n\nIndex: src/java/org/apache/nutch/parse/ParserFactory.java\n===================================================================\n--- src/java/org/apache/nutch/parse/ParserFactory.java\t(revision 909059)\n+++ src/java/org/apache/nutch/parse/ParserFactory.java\t(working copy)\n@@ -348,11 +348,23 @@\n                 contentType)) {\n           extList.add(extensions[i]);\n         }\n+        else if (\"*\".equals(extensions[i].getAttribute(\"contentType\"))){\n+          // default plugins get the priority\n+          extList.add(0, extensions[i]);\n+        }\n       }\n       \n       if (extList.size() > 0) {\n         if (LOG.isInfoEnabled()) {\n-          LOG.info(\"The parsing plugins: \" + extList +\n+          StringBuffer extensionsIDs = new StringBuffer(\"[\");\n+          boolean isFirst = true;\n+          for (Extension ext : extList){\n+        \t  if (!isFirst) extensionsIDs.append(\" - \");\n+        \t  else isFirst=false;\n+        \t  extensionsIDs.append(ext.getId());\n+          }\n+    \t  extensionsIDs.append(\"]\");\n+          LOG.info(\"The parsing plugins: \" + extensionsIDs.toString() +\n                    \" are enabled via the plugin.includes system \" +\n                    \"property, and all claim to support the content type \" +\n                    contentType + \", but they are not mapped to it  in the \" +\n@@ -369,7 +381,7 @@\n \n   private boolean match(Extension extension, String id, String type) {\n     return ((id.equals(extension.getId())) &&\n-            (type.equals(extension.getAttribute(\"contentType\")) ||\n+            (type.equals(extension.getAttribute(\"contentType\")) || extension.getAttribute(\"contentType\").equals(\"*\") ||\n              type.equals(DEFAULT_PLUGIN)));\n   }\n   \n\n", "Hi Julien:\n\n{quote}\n@Chris : I just did a fresh co from svn, applied the patch v3 and unzipped sample.tar.gz onto the directory parse-tika and ran the test just as you did but could not reproduce the problem. Could there be a difference between your version and the trunk? \n{quote}\n\nI tried this process last night:\n\n1. SVN up to r908832\n2. download patch v3\n3. download sample.tgz\n4. apply patch v3 to r908832\n5. untar sample.tgz into src/plugin/parse-tika, creating a sample folder in that dir\n6. ant clean compile-core test\n\nAny idea why I'm seeing the error?\n\nCheers,\nChris\n", "@Chris : did you do \n\nant -f src/plugin/parse-tika/build-ivy.xml \n\nbetween 5 and 6? This is required in order to populate the lib directory automatically", "@Julien:\n\nSigh, no I didn't! :(\n\nThat's probably why! Thanks for the help. I'll try it later today. If that passes, my +1 to commit. \n\n@Sami, regarding your updates, would you be OK with me creating another issue to track them, attaching your diffs as patches against this issue, once committed to the trunk? That way we'll make sure they get into 1.1, but we won't block this issue anymore from getting in. Let me know what you think, thanks.\n\nCheers,\nChris\n", "- committed in r909268. Added in the nutch-default.xml comments near the parse-tika plugin.includes enable block. Sami, I'll create a new issue now to track your proposed updates to the Tika parser. I ran unit tests with the patch i committed, and they all passed.\n\nThanks, Julien!", "- forgot to add in dep libs, added in r909269. Thanks!", "Integrated in Nutch-trunk #1067 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1067/])\n    - 2nd part of  Tika parser\n- fix for  Tika parser\n", "Have added small improvement in revision 910187 (Prioritise default Tika parser when discovering plugins matching mime-type).\nThanks to Chris for testing and committing it + Andrzej and Sami for their comments and suggestions", "Integrated in Nutch-trunk #1071 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1071/])\n    ", "Hello,\nI'm concerned on the TikaParser.java efficiency.\nCouple issues to comment on:\n\n1/ If I understood correctly, it seems that for each meta tag, it traverses the DOM object beginning from its root. \n(I mean the DOMContentUtils methods calls).\nWouldnt it be more efficient to traverse once and go getting each of the meta tags required?\n\n2/ I want to develop a custom parser. Is there an efficiency penalty of doing this back-and-forth from Tika to Nutch as opposed to developing the \nparser as just a nutch-compliant one?\nThanks a lot."], "tasks": {"summary": "Tika parser", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Tika parser"}, {"question": "What is the main context?", "answer": "Tika handles a lot of different formats under the bonnet and exposes them nicely via SAX events. What is described here is a tika-parser plugin which delegates the pasring mechanism of Tika but can st"}]}}
{"issue_id": "NUTCH-767", "project": "NUTCH", "title": "Update Tika to v0.5  for the MimeType detection", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Andrzej Bialecki", "created": "2009-11-18T14:56:42.676+0000", "updated": "2010-01-12T04:47:09.356+0000", "description": "The version 0.5 of TIka requires a few changes to the MimeType implementation. Tika is now split in several jars, we need to place the tika-core.jar in the main nutch lib.\n", "comments": ["Hi Julien,\n\nThanks for pushing this forward. I'll take a look at this patch...\n\nCheers,\nChris\n", "Fixed in rev. 885869. Thank you!", "Reopening this issue, because TestContent is failing now - after fixing a trivial compilation problem, now the problem seems to be that the type for empty content is auto-detected as \"text/plain\" and this value overrides the hint from the Content-Type header.", "Fixes compilation issues for test class src/test/org/apache/nutch/protocol/TestContent.java and temporarily make sure that the expected mime-type values correspond to what is returned by Tika. I will investigate why Tika does not return txt/html when the HTML document has no text", "I applied the patch, and I'm closing this issue - we will track the test failures when we upgrade to Tika 0.6, which is imminent.", "Integrated in Nutch-trunk #1002 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1002/])\n     Fix a failing test - still needs more work.\n", "the problem with the test class has been investigated. am reopening the issue so that we can mark it as definitely fixed ", "the problems with the test comes from the fact that tika's detection of the mimetypes based on content returns \"text/plain\"  when no mimetype can be identified, e.g. in our case because we have an empty byte array as content.\n\nTika's MimeTypes used to have a default value which was used in MimeUtil to determine when to use the type guessed by Tika but it has been removed since. The best course of action is probably to take into account Tika's guess only if it is not  \"text/plain\" or \"application/octet-stream\", which is what this patch implements.\n\nThe expected mime types in the test class are set to their original values (pre patch v2) apart from the one which used Tika's default Mime Type.  \n\nJ.", "Committed revision 897825", "Integrated in Nutch-trunk #1037 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1037/])\n    fix for  : reverted original expected values for test + treat text/plain as a default mime-type from Tika\n"], "tasks": {"summary": "Update Tika to v0.5  for the MimeType detection", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Update Tika to v0.5  for the MimeType detection"}, {"question": "What is the main context?", "answer": "The version 0.5 of TIka requires a few changes to the MimeType implementation. Tika is now split in several jars, we need to place the tika-core.jar in the main nutch lib.\n"}]}}
{"issue_id": "NUTCH-768", "project": "NUTCH", "title": "Upgrade Nutch 1.0 to use Hadoop 0.20", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2009-11-21T23:38:05.857+0000", "updated": "2009-12-19T05:14:11.077+0000", "description": "Upgrade Nutch 1.0 to use the Hadoop 0.20 release.  ", "comments": ["I have tested the upgrade with Hadoop 0.20.  To upgrade this correctly we do need to upgrade Xerces both in the main lib jars and within the lib-xml plugin.  I have upgraded to the most recent version of Xerces 2.9.x.  Having run through multiple full crawl and index cycles both on the new and old indexing frameworks, including the webgraphdb, and the solr indexing process, I didn't find any errors within the process.  If no one has any objections I will commit these changes within the next 24 hours.", "Are there any source code changes involved? If so, please upload a patch.\n\nDid you check this in local, distributed or pseudo-distributed mode? In the past there have been errors related to local (or distributed) mode that wouldn't occur when running in other modes.", "I thought I was going to be able to do this without code changes.  No such luck.  \n\nThere are many, many deprecations as a result of this upgrade.  Anything that used the old Mapper and Reducer interfaces seems to have deprecated methods in it.  The NutchBean class needed to implement the two RPC*Bean interfaces to handle changes in Hadoop RPC (that could have been a leftover from 1.0 changes but I don't think so).  Also there are numerous changes to build scripts and the nutch bin script to support different hadoop jars.\n\nThere are also many new files for the conf directory as Hadoop has split out files and has new configuration files for new capabilities.\n\nAfter all changes I was able to run everything in local and pseudo-distributed mode as well as test out local and distributed searching.  Everything seems to work fine.  After we make this upgrade I would recommend going back and updating all of the tool interfaces for the most recent APIs.", "If no objections I will commit this tomorrow sometime?", "+1.\n\nMinor nit: file lib/hsqldb-1.8.0.10.LICENSE.txt uses Windows EOL style, this should be probably corrected before commit.", "Weird.  The hsqldb License file was the same checksum as that pulled from hadoop.  It must have had the windows EOL in hadoop distribution as well.  I changed it anyways.  Everything committed with revision 885778.", "The older jetty jar file was not removed with this patch.  It will need to be removed from the nutch lib directory if applying the patch versus pulling from trunk.  There is also a second patch that updates unit tests for the Jetty interfaces.  Neither of these will need to be applied if pulling from Trunk as those problems have been corrected.", "Integrated in Nutch-trunk #1015 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1015/])\n    "], "tasks": {"summary": "Upgrade Nutch 1.0 to use Hadoop 0.20", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade Nutch 1.0 to use Hadoop 0.20"}, {"question": "What is the main context?", "answer": "Upgrade Nutch 1.0 to use the Hadoop 0.20 release.  "}]}}
{"issue_id": "NUTCH-769", "project": "NUTCH", "title": "Fetcher to skip queues for URLS getting repeated exceptions  ", "status": "Closed", "priority": "Minor", "reporter": "Julien Nioche", "assignee": "Andrzej Bialecki", "created": "2009-11-23T11:05:24.134+0000", "updated": "2009-12-01T15:15:58.156+0000", "description": "As discussed on the mailing list (see http://www.mail-archive.com/nutch-user@lucene.apache.org/msg15360.html) this patch allows to clear URLs queues in the Fetcher when more than a set number of exceptions have been encountered in a row. This can speed up the fetching substantially in cases where target hosts are not responsive (as a TimeoutException would be thrown) and limits cases where a whole Fetch step is slowed down because of a few queues.\n\nby default the parameter fetcher.max.exceptions.per.queue has a value of -1 and is deactivated.  ", "comments": ["The patch contains a new method, checkExceptionThreshold,which seems to do the right thing, but this method is never used in Fetcher. I think the idea was to call it in FetchItemQueues.finishItem()?", "Missed a couple of lines indeed when I was trying to untangle this functionality from my (heavily modified) local copy.\ncheckExceptionThreshold is called after the line 664\n\n              case ProtocolStatus.EXCEPTION:\n                logError(fit.url, status.getMessage());\n                int killedURLs = fetchQueues.checkExceptionThreshold(fit.getQueueID());\n                reporter.incrCounter(\"FetcherStatus\", \"Exceptions\", killedURLs);\n\nI'll attach a modified version of the patch\n\nThanks\n\nJ.", "I had to apply this patch by hand, due to NUTCH-770. I also added conf/nutch-default.xml documentation. This was committed in rev. 885785 - thanks!"], "tasks": {"summary": "Fetcher to skip queues for URLS getting repeated exceptions  ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fetcher to skip queues for URLS getting repeated exceptions  "}, {"question": "What is the main context?", "answer": "As discussed on the mailing list (see http://www.mail-archive.com/nutch-user@lucene.apache.org/msg15360.html) this patch allows to clear URLs queues in the Fetcher when more than a set number of excep"}]}}
{"issue_id": "NUTCH-77", "project": "NUTCH", "title": "Project URL in JIRA", "status": "Closed", "priority": "Trivial", "reporter": "Stephan Strittmatter", "assignee": null, "created": "2005-08-02T21:02:24.000+0000", "updated": "2005-08-09T02:40:23.000+0000", "description": "The project URL on JIRA should be updated from\n    http://incubator.apache.org/nutch/\nto\n    http://lucene.apache.org/nutch/", "comments": ["Changed. Thanks."], "tasks": {"summary": "Project URL in JIRA", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Project URL in JIRA"}, {"question": "What is the main context?", "answer": "The project URL on JIRA should be updated from\n    http://incubator.apache.org/nutch/\nto\n    http://lucene.apache.org/nutch/"}]}}
{"issue_id": "NUTCH-770", "project": "NUTCH", "title": "Timebomb for Fetcher", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Andrzej Bialecki", "created": "2009-11-23T11:23:11.287+0000", "updated": "2009-12-05T16:50:15.044+0000", "description": "This patch provides the Fetcher with a timebomb mechanism. By default the timebomb is not activated; it can be set using the parameter fetcher.timebomb.mins. The number of minutes is relative to the start of the Fetch job. When the number of minutes is reached, the QueueFeeder skips all remaining entries then all active queues are purged. This allows to keep the Fetch step under comtrol and works well in combination with NUTCH-769\n", "comments": ["Please find the logs of the patch... I did effectively try it but I could not compile after it.", "The log simply shows that the patch has not been applied properly. \nSee http://markmail.org/message/wbd3r3t5bfxzkbpn for a discussion on how to apply patches\n\nShould work fine from the root directory of Nutch with \npatch -p0 < ~/Desktop/NUTCH-770.patch", "That's what I did  and just retried ... so I'm a bit suprised too.\nOther patches worked fine so far.\n\nChanged my method and used patching by Eclipse and I get the following compiling error :\n992: cannot find symbol\n    [javac] symbol  : method checkTimeBomb()\n    [javac] location: class org.apache.nutch.fetcher.Fetcher.FetchItemQueues\n    [javac]         int timeBombed  =fetchQueues.checkTimeBomb();\n    [javac]                                     ^\n    [javac] 1 error\n\n", "I propose to change the name of this functionality - \"timebomb\" is not self-explanatory, and it suggests that if you misbehave then your cluster may explode ;) Instead I would use \"time limit\", rename all vars and methods to follow this naming, and document it properly in nutch-default.xml.\n\nA few comments to the patch:\n\n* it has some overlap with NUTCH-769 (the emptyQueue() method), but that's easy to resolve, see also the next point.\n\n* why change the code in FetchQueues at all? Time limit is a global condition, we could just break the main loop in run() and ignore the QueueFeeder (or don't start it if the time limit already passed when starting run() ).\n\n* the patch does not follow the code style (notably whitespace in for/while loops and assignments).", "\"time limit\" is definitely better than timebomb (but not as amusing). \nFetchQueues : having it there has the advantage that we can count how many URLs have been skipped due to the time limit. That's in the same spirit as https://issues.apache.org/jira/browse/NUTCH-658 which I have been using for a while.  It's very useful to know what happens to the URLs as input and reveals quite a lot about the behaviour of the fetch. \nCodestyle : I suppose the following Eclipse codestyle is the one to use ? (http://wiki.apache.org/lucene-java/HowToContribute?action=AttachFile&do=view&target=Eclipse-Lucene-Codestyle.xml)", "bq.   \"time limit\" is definitely better than timebomb (but not as amusing). \n\n:) let's got for \"informative\" and \"less confusing\" now ... Could you please also add the nutch-default.xml property and its documentation.\n\nRe: FetchQueues - ok, you have a point here.\n\nRe: code style - yes.", "* renamed timebomb into timelimit\n* added parameter and its description in nutch-default.xml\n* applied Lucene codestyle from http://wiki.apache.org/lucene-java/HowToContribute?action=AttachFile&do=view&target=Eclipse-Lucene-Codestyle.xml", "the v2 applied the Lucene code formatting to the whole java file which caused far too many changes, the v3 does the same as the v2 (add param and description to nutch default + change timebomb to timelimit) but applies the code formatting only to the relevant portions of code", "Fixed in rev. 885776. Thank you!", "Tried it succesfully on a windows platform.\n\nIt does not work on a Ubuntu, pseudo-distributed hadoop configuration with two mappers running in parallel ????\n\n"], "tasks": {"summary": "Timebomb for Fetcher", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Timebomb for Fetcher"}, {"question": "What is the main context?", "answer": "This patch provides the Fetcher with a timebomb mechanism. By default the timebomb is not activated; it can be set using the parameter fetcher.timebomb.mins. The number of minutes is relative to the s"}]}}
{"issue_id": "NUTCH-771", "project": "NUTCH", "title": "Add WebGraph classes to the bin/nutch script", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Markus Jelsma", "created": "2009-11-24T20:47:39.133+0000", "updated": "2011-09-13T21:26:59.294+0000", "description": "Currently the webgraph jobs are called on the command line by calling main methods on their classes.  I propose to upgrade the bin/nutch shell script to allow calling these jobs as well.  This would include the webgraphdb, linkrank, scoreupdater, and nodedumper jobs.", "comments": ["+1 to adding these to the script. The names are cryptic, though ... this would call for a clear documentation in the script itself, and in appropriate places on the wiki.", "- pushing this out per http://bit.ly/c7tBv9", "Committed via NUTCH-1049."], "tasks": {"summary": "Add WebGraph classes to the bin/nutch script", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add WebGraph classes to the bin/nutch script"}, {"question": "What is the main context?", "answer": "Currently the webgraph jobs are called on the command line by calling main methods on their classes.  I propose to upgrade the bin/nutch shell script to allow calling these jobs as well.  This would i"}]}}
{"issue_id": "NUTCH-772", "project": "NUTCH", "title": "Upgrade Nutch to use Lucene 2.9.1", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2009-11-25T12:36:15.645+0000", "updated": "2009-11-28T14:08:02.709+0000", "description": "Upgrade Nutch to the latest Lucene release.", "comments": ["Patch to commit shortly, if no objections.", "Fixed in rev. 884277.", "Integrated in Nutch-trunk #995 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/995/])\n     Upgrade Nutch to use Lucene 2.9.1.\n"], "tasks": {"summary": "Upgrade Nutch to use Lucene 2.9.1", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade Nutch to use Lucene 2.9.1"}, {"question": "What is the main context?", "answer": "Upgrade Nutch to the latest Lucene release."}]}}
{"issue_id": "NUTCH-773", "project": "NUTCH", "title": "some minor bugs in AbstractFetchSchedule.java", "status": "Closed", "priority": "Minor", "reporter": "Reinhard Pötz", "assignee": "Andrzej Bialecki", "created": "2009-11-25T14:15:24.799+0000", "updated": "2009-11-28T14:08:02.967+0000", "description": "fixes some minor trivial bugs in AbstractFetchSchedule.java", "comments": ["That was a nasty bug - fixed in rev. 884198. Thanks!", "Integrated in Nutch-trunk #995 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/995/])\n     Some minor bugs in AbstractFetchSchedule.\n"], "tasks": {"summary": "some minor bugs in AbstractFetchSchedule.java", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "some minor bugs in AbstractFetchSchedule.java"}, {"question": "What is the main context?", "answer": "fixes some minor trivial bugs in AbstractFetchSchedule.java"}]}}
{"issue_id": "NUTCH-774", "project": "NUTCH", "title": "Retry interval in crawl date is set to 0", "status": "Closed", "priority": "Major", "reporter": "Reinhard Pötz", "assignee": "Chris A. Mattmann", "created": "2009-12-02T12:04:55.356+0000", "updated": "2011-04-01T15:07:22.034+0000", "description": "When i fetch and parse a feed with the feed plugin,\nhttp://www.wachauclimbing.net/home/impressum-disclaimer/feed/\nanother crawl date is generated\nhttp://www.wachauclimbing.net/home/impressum-disclaimer/comment-page-1/\n\nafter fetching a second round\nthe dump in the crawl db still shows a retry interval with value 0.\n\nhttp://www.wachauclimbing.net/home/impressum-disclaimer/comment-page-1/ Version: 7\nStatus: 2 (db_fetched)\nFetch time: Wed Dec 02 12:48:22 CET 2009\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 0 seconds (0 days)\nScore: 1.0833334\nSignature: db9ab2193924cd2d0b53113a500ca604\nMetadata: _pst_: success(1), lastModified=0\n\na check should be done in DefaultFetchSchedule (or AbstractFetchSchedule) in the\nmethod \nsetFetchSchedule\n\n", "comments": ["fixes also a minor typo in AbstractFetchSchedule.java", "corrected also wrong api documentation in AdaptiveFetchSchedule.java\ndefault value of MIN_INTERVAL is 1 minute, not 1 second.", "- pushing this out per http://bit.ly/c7tBv9", "> pushing this out per http://bit.ly/c7tBv9\n\nI can't see that these patches have been applied and committed to the trunk. So I don't think this is included in release 1.1\n\nSounds like this one is still open and is a simple fix. ", "Yep, it's still open Alex. I meant pushing out as in, unscheduling for a particular Fix Version since it was previously scheduled for 1.1, but didn't make it into the release...", "- fix applied to trunk in r964165 and backported to 1.2-branch in r964166. Thanks Reinhard, and to ab for review...", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Retry interval in crawl date is set to 0", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Retry interval in crawl date is set to 0"}, {"question": "What is the main context?", "answer": "When i fetch and parse a feed with the feed plugin,\nhttp://www.wachauclimbing.net/home/impressum-disclaimer/feed/\nanother crawl date is generated\nhttp://www.wachauclimbing.net/home/impressum-disclaime"}]}}
{"issue_id": "NUTCH-775", "project": "NUTCH", "title": "Enhance Searcher interface", "status": "Closed", "priority": "Major", "reporter": "Sami Siren", "assignee": "Sami Siren", "created": "2009-12-16T07:04:03.884+0000", "updated": "2011-06-08T21:34:22.200+0000", "description": "Current Searcher interface is too limited for many purposes:\n\nHits search(Query query, int numHits, String dedupField, String sortField,\n      boolean reverse) throws IOException;\n\nIt would be nice that we had an interface that allowed adding different features without changing the interface. I am proposing that we deprecate the current search method and introduce something like:\n\nHits search(Query query, Metadata context) throws IOException;\n\nAlso at the same time we should enhance the QueryFilter interface to look something like:\n\nBooleanQuery filter(Query input, BooleanQuery translation, Metadata context)\n    throws QueryException;\n\nI would like to hear your comments before proceeding with a patch.", "comments": ["+1. I would suggest creating a subclass of Metadata, where we can guarantee the presence of some required parameters, e.g.:\n\n{code}\npublic class SearchContext extends Metadata {\n  protected int numHits;\n  protected String sortField;\n  protected String dedupField;\n  ...\n  // setters and getters for the above\n}\n{code}\n\nand change the QueryFilter interface to use SearchContext too.", "I ended up changing the Query API instead since the changes were smaller from API perspective that way.", "If there are no objections I'll commit the proposed patch within few days.", "IMHO this could go as it is ... one suggestion though: this Query/QueryContext now resembles SolrQuery/SolrParams. Perhaps we could rename QueryContext to QueryParams?", "{quote}IMHO this could go as it is ... one suggestion though: this Query/QueryContext now resembles SolrQuery/SolrParams. Perhaps we could rename QueryContext to QueryParams?\n{quote}\nThat sounds reasonable, I will change the name before committing. Also I forgot to change web gui to use the new api, will do that also.", "I committed this", "Integrated in Nutch-trunk #1058 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1058/])\n     Enhance Searcher interface\n"], "tasks": {"summary": "Enhance Searcher interface", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Enhance Searcher interface"}, {"question": "What is the main context?", "answer": "Current Searcher interface is too limited for many purposes:\n\nHits search(Query query, int numHits, String dedupField, String sortField,\n      boolean reverse) throws IOException;\n\nIt would be nice th"}]}}
{"issue_id": "NUTCH-776", "project": "NUTCH", "title": "Configurable queue depth", "status": "Closed", "priority": "Minor", "reporter": "MilleBii", "assignee": null, "created": "2009-12-17T08:05:54.232+0000", "updated": "2011-11-15T11:48:54.113+0000", "description": "I propose that we create a configurable item for the queuedepth in Fetcher.java instead of the hard-coded value of 50.\n\nkey name : fetcher.queues.depth\n\nDefault value : remains 50 (of course)", "comments": ["Did you notice any improvement in the fetch rate after I suggested on the mailing list to use a value larger than 50? Does the memory consumption remain reasonable?  ", "Moving this issue post 1.1\nNeeds a patch file, some description of the param in nutch-default.xml and more importantly some experimentation to see how it impacts the performance of the fetching", "Just noticed this open issue. It's been duplicated by Julien in the linked issue and resolved in Nutch 1.4.\n"], "tasks": {"summary": "Configurable queue depth", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Configurable queue depth"}, {"question": "What is the main context?", "answer": "I propose that we create a configurable item for the queuedepth in Fetcher.java instead of the hard-coded value of 50.\n\nkey name : fetcher.queues.depth\n\nDefault value : remains 50 (of course)"}]}}
{"issue_id": "NUTCH-777", "project": "NUTCH", "title": "Upgrading to jetty6 broke unit tests", "status": "Closed", "priority": "Major", "reporter": "Chris A. Mattmann", "assignee": "Chris A. Mattmann", "created": "2009-12-18T18:29:50.988+0000", "updated": "2013-05-22T03:53:33.762+0000", "description": "It seems that somewhere down the line, there was an upgrade to jetty6, which broke unit tests, specifically TestFetcher and CrawlDBTestUtil. ", "comments": ["Here is what I was getting with the latest Nutch trunk:\n\n{noformat}\ncompile:\n\njob:\n      [jar] Building jar: /Users/mattmann/src/nutch/build/nutch-1.0.job\n\ncompile-core-test:\n    [javac] Compiling 43 source files to /Users/mattmann/src/nutch/build/test/classes\n    [javac] /Users/mattmann/src/nutch/src/test/org/apache/nutch/crawl/CrawlDBTestUtil.java:33: package org.mortbay.http does not exist\n    [javac] import org.mortbay.http.HttpContext;\n    [javac]                        ^\n    [javac] /Users/mattmann/src/nutch/src/test/org/apache/nutch/crawl/CrawlDBTestUtil.java:34: package org.mortbay.http does not exist\n    [javac] import org.mortbay.http.SocketListener;\n    [javac]                        ^\n    [javac] /Users/mattmann/src/nutch/src/test/org/apache/nutch/crawl/CrawlDBTestUtil.java:35: package org.mortbay.http.handler does not exist\n    [javac] import org.mortbay.http.handler.ResourceHandler;\n    [javac]                                ^\n    [javac] /Users/mattmann/src/nutch/src/test/org/apache/nutch/crawl/CrawlDBTestUtil.java:134: cannot find symbol\n    [javac] symbol  : class SocketListener\n    [javac] location: class org.apache.nutch.crawl.CrawlDBTestUtil\n    [javac]     SocketListener listener = new SocketListener();\n    [javac]     ^\n    [javac] /Users/mattmann/src/nutch/src/test/org/apache/nutch/crawl/CrawlDBTestUtil.java:134: cannot find symbol\n    [javac] symbol  : class SocketListener\n    [javac] location: class org.apache.nutch.crawl.CrawlDBTestUtil\n    [javac]     SocketListener listener = new SocketListener();\n    [javac]                                   ^\n    [javac] /Users/mattmann/src/nutch/src/test/org/apache/nutch/crawl/CrawlDBTestUtil.java:138: cannot find symbol\n    [javac] symbol  : class HttpContext\n    [javac] location: class org.apache.nutch.crawl.CrawlDBTestUtil\n    [javac]     HttpContext staticContext = new HttpContext();\n    [javac]     ^\n..snip...\n    [javac] /Users/mattmann/src/nutch/src/test/org/apache/nutch/fetcher/TestFetcher.java:167: cannot find symbol\n    [javac] symbol  : method getListeners()\n    [javac] location: class org.mortbay.jetty.Server\n    [javac]     urls.add(\"http://127.0.0.1:\" + server.getListeners()[0].getPort() + \"/\" + page);\n    [javac]                                          ^\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: Some input files use unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n    [javac] 9 errors\n\nBUILD FAILED\n/Users/mattmann/src/nutch/build.xml:229: Compile failed; see the compiler error output for details.\n\nTotal time: 37 seconds\n{noformat}", "I found this page, which shows the mapping from Jetty5 (which the Nutch test code used to depend on), to Jetty6:\n\nhttp://docs.codehaus.org/display/JETTY/Porting+to+jetty6", "Okay with the changes I'm about to commit, we have:\n\n{noformat}\ncopy-generated-lib:\n\ntest:\n     [echo] Testing plugin: urlnormalizer-regex\n    [junit] Running org.apache.nutch.net.urlnormalizer.regex.TestRegexURLNormalizer\n    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.286 sec\n    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 6.802 sec\n\ntest:\n\nBUILD SUCCESSFUL\nTotal time: 5 minutes 52 seconds\n[chipotle:~/src/nutch] mattmann% \n{noformat}\n\nYay!", "- fixed in r892350", "Integrated in Nutch-trunk #1015 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1015/])\n    - fix for  Upgrading to jetty6 broke unit tests\n"], "tasks": {"summary": "Upgrading to jetty6 broke unit tests", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrading to jetty6 broke unit tests"}, {"question": "What is the main context?", "answer": "It seems that somewhere down the line, there was an upgrade to jetty6, which broke unit tests, specifically TestFetcher and CrawlDBTestUtil. "}]}}
{"issue_id": "NUTCH-778", "project": "NUTCH", "title": "Running Nutch On linux having whoami exception?", "status": "Closed", "priority": "Major", "reporter": "Prakash Panjwani", "assignee": null, "created": "2010-01-09T11:16:39.781+0000", "updated": "2011-04-13T23:48:08.178+0000", "description": "I want to run nutch on the linux kernel,I have loged in as a root user, I have setted all the environment variable and nutch file setting. I have created a url.txt file which content the url to crawl, When i am trying to run nutch using following command\n\nbin/nutch crawl urls -dir pra\n\nit generates following exception.\n\ncrawl started in: pra\nrootUrlDir = urls\nthreads = 10\ndepth = 5\nInjector: starting\nInjector: crawlDb: pra/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nException in thread \"main\" java.io.IOException: Failed to get the current user's information.\n        at org.apache.hadoop.mapred.JobClient.getUGI(JobClient.java:717)\n        at org.apache.hadoop.mapred.JobClient.configureCommandLineOptions(JobClient.java:592)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:788)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1142)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:160)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:113)\nCaused by: javax.security.auth.login.LoginException: Login failed: Cannot run program \"whoami\": java.io.IOException: error=12, Cannot allocate memory\n        at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:250)\n        at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:275)\n        at org.apache.hadoop.mapred.JobClient.getUGI(JobClient.java:715)\n        ... 5 more\n\nServer has enough space to run any java application.I have attached the statics..\n\n\n total       used       free  \nMem:        524320     194632     329688 \n-/+ buffers/cache:     194632     329688\nSwap:      2475680          0    2475680\nTotal:     3000000     194632    2805368\n\nIs it sufficient memory space for nutch? Please some one help me ,I am new with linux kernel and nutch. \nThanks in Advance.", "comments": ["This is likely to be a problem with the Hadoop configuration or machine setup. it is not a Nutch issue as such so I'll mark this as invalid.", "Closing all resolved issues with a non-fixed status."], "tasks": {"summary": "Running Nutch On linux having whoami exception?", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Running Nutch On linux having whoami exception?"}, {"question": "What is the main context?", "answer": "I want to run nutch on the linux kernel,I have loged in as a root user, I have setted all the environment variable and nutch file setting. I have created a url.txt file which content the url to crawl,"}]}}
{"issue_id": "NUTCH-779", "project": "NUTCH", "title": "Mechanism for passing metadata from parse to crawldb", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-01-18T16:53:59.840+0000", "updated": "2013-05-22T03:53:29.551+0000", "description": "The patch attached allows to pass parse metadata to the corresponding entry of the crawldb.  \nComments are welcome", "comments": ["You can already achieve this with ScoringFilters, although it requires using three methods instead ... I would also rename the status to \"parse_meta\", it's less cryptic this way. The property needs some documentation in nutch-default.xml plus a sensible default.", "> The property needs some documentation in nutch-default.xml plus a sensible default. \n\nSure - just wanted the general approach to be checked before doing the tedious bits. Do you think it makes sense to do things the way I suggested or would you use the ScoringFilters instead?\n", "Personally I would use ScoringFilters because I'm familiar with the API, but the approach that you propose is certainly more user friendly especially for novice users.", "Improved version of the patch. Followed AB's recommendations and renamed  STATUS_PARSE_META + added description for param 'db.parsemeta.to.crawldb' in nutch-default.xml + fixed issue with IndexerMapReduce", "Could anyone please review this issue? I would like to commit it in time for the 1.1 release", "CrawlDbReducer, the cramped line {{if (metaFromParse!=null){}} needs some whitespace fixing.\n\nOther than that, +1.", "Committed revision 929038.\n\nThanks Andrzej for your feedback", "Integrated in Nutch-trunk #1112 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1112/])\n     Mechanism for passing metadata from parse to crawldb\n"], "tasks": {"summary": "Mechanism for passing metadata from parse to crawldb", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Mechanism for passing metadata from parse to crawldb"}, {"question": "What is the main context?", "answer": "The patch attached allows to pass parse metadata to the corresponding entry of the crawldb.  \nComments are welcome"}]}}
{"issue_id": "NUTCH-78", "project": "NUTCH", "title": "German texts on website", "status": "Closed", "priority": "Minor", "reporter": "Matthias Jaekle", "assignee": null, "created": "2005-08-06T01:43:13.000+0000", "updated": "2011-06-08T21:34:05.030+0000", "description": "The German properties-files with the texts to present on the websites were incomplete, or with wrong spellings.\nPlease find attached the corrected files.\n", "comments": ["anchors_de.properties, cached_de.properties, explain_de.properties, search_de.properties, text_de.properties", "Commited", "Commited. Thanks.\n"], "tasks": {"summary": "German texts on website", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "German texts on website"}, {"question": "What is the main context?", "answer": "The German properties-files with the texts to present on the websites were incomplete, or with wrong spellings.\nPlease find attached the corrected files.\n"}]}}
{"issue_id": "NUTCH-780", "project": "NUTCH", "title": "Nutch crawler did not read configuration files", "status": "Closed", "priority": "Major", "reporter": "Vu Hoang", "assignee": null, "created": "2010-01-21T04:00:59.211+0000", "updated": "2010-07-14T18:59:10.622+0000", "description": "Nutch searcher can read properties at the constructor ...\n\n{code:java|title=NutchSearcher.java|borderStyle=solid}\nNutchBean bean = new NutchBean(getFilesystem().getConf(), fs);\n... // put search engine code here\n{code}\n\n... but Nutch crawler is not, it only reads data from arguments.\n{code:java|title=NutchCrawler.java|borderStyle=solid}\nStringBuilder builder = new StringBuilder();\nbuilder.append(domainlist + SPACE);\nbuilder.append(ARGUMENT_CRAWL_DIR);\nbuilder.append(domainlist + SUBFIX_CRAWLED + SPACE);\nbuilder.append(ARGUMENT_CRAWL_THREADS);\nbuilder.append(threads + SPACE);\nbuilder.append(ARGUMENT_CRAWL_DEPTH);\nbuilder.append(depth + SPACE);\nbuilder.append(ARGUMENT_CRAWL_TOPN);\nbuilder.append(topN + SPACE);\nCrawl.main(builder.toString().split(SPACE));\n{code}", "comments": ["add method\n{code:java|title=org/apache/nutch/crawl/Crawl.java|borderStyle=solid}\npublic static Configuration overwrite(Configuration nutchConfig)\n{\n\t  Configuration crawlConfig = NutchConfiguration.createCrawlConfiguration();\n\t  Iterator<Entry<String, String>> entries = nutchConfig.iterator();\n\t  while (entries.hasNext())\n\t  {\n\t\t  Entry<String, String> entry = (Entry<String, String>) entries.next();\n\t\t  crawlConfig.set(entry.getKey(), entry.getValue());\n\t  }\n\t  \n\t  return crawlConfig;\n}\n{code}\n\nadd lines below into class org.apache.nutch.crawl.Crawl\n{code:java|title=org/apache/nutch/crawl/Crawl.java|borderStyle=solid}\npublic static Configuration nutchConfig = null;\npublic static void setNutchConfig(Configuration config) { nutchConfig = config; }\n{code}\n\nand re-configure nutch configuration inside of method main as below\n{code:java|title=org/apache/nutch/crawl/Crawl.java|borderStyle=solid}\nConfiguration conf = null;\nif (nutchConfig != null) conf = overwrite(nutchConfig);\nelse conf = NutchConfiguration.createCrawlConfiguration();\n{code}\n\nI recommend that solution :)", "Is the purpose of this issue to make Crawl.java usable via strongly-typedAPI instead of the generic main, e.g. something like this:\n\n{code}\npublic class Crawl extends Configured {\n \n public int crawl(Path output, Path seedDir, int threads, int numCycles, int topN, ...) {\n    ...\n  }\n}\n{code}", "Thanks for your solution, Andrzej", "Sounds like this issue is resolved, no? If not, feel free to file a new one with more explicit information...", "this issue was resolved"], "tasks": {"summary": "Nutch crawler did not read configuration files", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Nutch crawler did not read configuration files"}, {"question": "What is the main context?", "answer": "Nutch searcher can read properties at the constructor ...\n\n{code:java|title=NutchSearcher.java|borderStyle=solid}\nNutchBean bean = new NutchBean(getFilesystem().getConf(), fs);\n... // put search engin"}]}}
{"issue_id": "NUTCH-781", "project": "NUTCH", "title": "Update Tika to v0.6  for the MimeType detection", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-02-01T09:23:38.860+0000", "updated": "2010-02-03T06:00:06.188+0000", "description": "[from annoucement]\n\nApache Tika, a subproject of Apache Lucene, is a toolkit for detecting and\nextracting metadata and structured text content from various documents using\nexisting parser libraries.\n\nApache Tika 0.6 contains a number of improvements and bug fixes. Details can\nbe found in the changes file:\n\nhttp://www.apache.org/dist/lucene/tika/CHANGES-0.6.txt\n", "comments": ["Committed revision 905228", "did you forgot to update conf/tika-mimetypes.xml ?\n\nRelated question: do we actually need our own version on the tika config anymore? I saw there were some old issues that were fixed in the custom version but i would quess those changes, if important, have already made their way into Tika?\n\n", "Integrated in Nutch-trunk #1058 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1058/])\n    : upgrade tika to version 0.6\n: upgrade tika to version 0.6\n", "> did you forgot to update conf/tika-mimetypes.xml ?\nindeed - well spotted, thanks\n\n> Related question: do we actually need our own version on the tika config anymore? I saw there were some old issues that were fixed in the custom version but i would quess those changes, if important, have already made their way into Tika?\nthe version we had was the same as the one provided by Tika 0.4 so I suppose we could safely rely on theTika defaults. MimeUtil currently requires needs tika-mimetypes.xml to be in the available in the classpath but we could modify that so that it uses the default version from the tika jar if nothing can be found in conf. Let's put that in a separate JIRA issue if we really want it, in the meantime I'll commit the v 0.6 of tika-mimetypes.xml\n\nJ.\n", "{quote}\nthe version we had was the same as the one provided by Tika 0.4 so I suppose we could safely rely on theTika defaults. MimeUtil currently requires needs tika-mimetypes.xml to be in the available in the classpath but we could modify that so that it uses the default version from the tika jar if nothing can be found in conf. Let's put that in a separate JIRA issue if we really want it, in the meantime I'll commit the v 0.6 of tika-mimetypes.xml\n{quote}\n\nok. thanks.", "Integrated in Nutch-trunk #1059 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1059/])\n     : updated tika-mimetypes.xml\n"], "tasks": {"summary": "Update Tika to v0.6  for the MimeType detection", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Update Tika to v0.6  for the MimeType detection"}, {"question": "What is the main context?", "answer": "[from annoucement]\n\nApache Tika, a subproject of Apache Lucene, is a toolkit for detecting and\nextracting metadata and structured text content from various documents using\nexisting parser libraries.\n\n"}]}}
{"issue_id": "NUTCH-782", "project": "NUTCH", "title": "Ability to order htmlparsefilters", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-02-01T11:41:53.497+0000", "updated": "2010-03-02T04:09:33.659+0000", "description": "Patch which adds a new parameter 'htmlparsefilter.order' which specifies the order in which HTMLParse filters are applied. HTMLParse filter ordering MAY have an impact on end result, as some filters could rely on the metadata generated by a previous filter.\n\n", "comments": ["Committed revision 917557", "Integrated in Nutch-trunk #1083 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1083/])\n    : Ability to order htmlparsefilters\n"], "tasks": {"summary": "Ability to order htmlparsefilters", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Ability to order htmlparsefilters"}, {"question": "What is the main context?", "answer": "Patch which adds a new parameter 'htmlparsefilter.order' which specifies the order in which HTMLParse filters are applied. HTMLParse filter ordering MAY have an impact on end result, as some filters c"}]}}
{"issue_id": "NUTCH-783", "project": "NUTCH", "title": "IndexingFiltersChecker Utility", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Markus Jelsma", "created": "2010-02-01T12:01:38.938+0000", "updated": "2011-07-11T10:56:04.152+0000", "description": "This patch contains a new utility which allows to check the configuration of the indexing filters. The IndexingFiltersChecker reads and parses a URL and run the indexers on it. Displays the fields obtained and the first\n 100 characters of their value.\n\nCan be used e.g. ./nutch org.apache.nutch.indexer.IndexingFiltersChecker http://www.lemonde.fr/\n\n", "comments": ["Removed tag 1.1\nWill rename to IndexingPluginsChecker later", "What's this? Shouldn't it be closed?", "Why should it be closed? As said in the description and later comments it is used to test indexing plugins, so it is not directly bound to Lucene or SOLR  ", "You're right. Shouldn't it be marked for a version then?", "Marked for 2.0 - patch might need adapting", "Yes, this code is not compatible with Nutch API in 1.4\n\n{code}\n+      List<String> values = doc.getFieldValues(fname);\n+      if (values != null) {\n+        for (String value : values){\n+          int minText = Math.min(100, value.length());\n+          System.out.println(fname + \" :\\t\" + value.substring(0, minText));\n+        }\n+      }\n{code}\n\nchanged to\n\n{code}\n      List<Object> values = Arrays.asList(doc.getFieldValue(fname));\n      if (values != null) {\n        for (Object value : values) {\n          String str = value.toString();\n          int minText = Math.min(100, str.length());\n          System.out.println(fname + \" :\\t\" + str.substring(0, minText));\n        }\n      }\n{code}\n\n\nIt works now. I think it's nice to have in 1.4 and 2.0. ", "Why not. Let's rename it to IndexingFiltersChecker. Markus to you want to look after this one or shall I do it?", "Alright! I can include the changes in a new patch and add it as a new command. Also agreed on renaming the guy.", "Committed for 1.4 in rev 1145117. Cheers for Julien for this nice utility.", "Changed description to reflect name change.", "Thanks for committing it Markus"], "tasks": {"summary": "IndexingFiltersChecker Utility", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "IndexingFiltersChecker Utility"}, {"question": "What is the main context?", "answer": "This patch contains a new utility which allows to check the configuration of the indexing filters. The IndexingFiltersChecker reads and parses a URL and run the indexers on it. Displays the fields obt"}]}}
{"issue_id": "NUTCH-784", "project": "NUTCH", "title": "CrawlDBScanner ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-02-01T14:32:31.813+0000", "updated": "2010-03-30T04:15:04.747+0000", "description": "The patch file contains a utility which dumps all the entries matching a regular expression on their URL. The dump mechanism of the crawldb reader is not  very useful on large crawldbs as the ouput can be extremely large and the -url  function can't help if we don't know what url we want to have a look at.\n\nThe CrawlDBScanner can either generate a text representation of the CrawlDatum-s or binary objects which can then be used as a new CrawlDB. \n\nUsage: CrawlDBScanner <crawldb> <output> <regex> [-s <status>] <-text>\n\nregex: regular expression on the crawldb key\n-s status : constraint on the status of the crawldb entries e.g. db_fetched, db_unfetched\n-text : if this parameter is used, the output will be of TextOutputFormat; otherwise it generates a 'normal' crawldb with the MapFileOutputFormat\n\nfor instance the command below : \n./nutch com.ant.CrawlDBScanner crawl/crawldb /tmp/amazon-dump .+amazon.com.* -s db_fetched -text\n\nwill generate a text file /tmp/amazon-dump containing all the entries of the crawldb matching the regexp  .+amazon.com.* and having a status of db_fetched\n\n\n", "comments": ["Committed revision 928746", "This should have been reviewed first - I don't question the usefulness of this class, but I think that this should have been added as an option to CrawlDbReader. As it is now we get a new tool with a cryptic name that performs a function that is a variant of another existing tool...", "Integrated in Nutch-trunk #1111 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1111/])\n     : CrawlDBScanner\n"], "tasks": {"summary": "CrawlDBScanner ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "CrawlDBScanner "}, {"question": "What is the main context?", "answer": "The patch file contains a utility which dumps all the entries matching a regular expression on their URL. The dump mechanism of the crawldb reader is not  very useful on large crawldbs as the ouput ca"}]}}
{"issue_id": "NUTCH-785", "project": "NUTCH", "title": "Fetcher : copy metadata from origin URL when redirecting + call scfilters.initialScore on newly created URL", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-02-01T16:51:42.398+0000", "updated": "2010-03-30T08:36:13.307+0000", "description": "When following the redirections, the Fetcher does not copy the metadata from the original URL to the new one or calls the method scfilters.initialScore", "comments": ["Could anyone please review this issue? I would like to commit it in time for the 1.1 release", "+1. The scoring api should allow us to set this metadata in one call, but changing the API now would be problematic.", "Committed revision 929039\n\nThanks Andrzej for reviewing it"], "tasks": {"summary": "Fetcher : copy metadata from origin URL when redirecting + call scfilters.initialScore on newly created URL", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fetcher : copy metadata from origin URL when redirecting + call scfilters.initialScore on newly created URL"}, {"question": "What is the main context?", "answer": "When following the redirections, the Fetcher does not copy the metadata from the original URL to the new one or calls the method scfilters.initialScore"}]}}
{"issue_id": "NUTCH-786", "project": "NUTCH", "title": "Better list of suffix domains", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-02-05T11:49:27.363+0000", "updated": "2010-02-05T14:05:39.251+0000", "description": "Small improvement to the content of domain-suffixes.xml : added compound TLD for .ar, .co, .id, .il, .mx, .nz and .za", "comments": ["Small improvement to the content of domain-suffixes.xml : added compound TLD for .ar, .co, .id, .il, .mx, .nz and .za", "Committed revision 906907", "Is this something that should also be applied to crawler-commons? I believe Ian had added support for finding \"Effective TLDs\" and that this support included an \"effective_tld_names.dat\" file.\n"], "tasks": {"summary": "Better list of suffix domains", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Better list of suffix domains"}, {"question": "What is the main context?", "answer": "Small improvement to the content of domain-suffixes.xml : added compound TLD for .ar, .co, .id, .il, .mx, .nz and .za"}]}}
{"issue_id": "NUTCH-787", "project": "NUTCH", "title": "Upgrade Lucene to 3.0.1.", "status": "Closed", "priority": "Trivial", "reporter": "Dawid Weiss", "assignee": "Andrzej Bialecki", "created": "2010-02-05T12:09:02.144+0000", "updated": "2010-03-20T04:11:17.216+0000", "description": null, "comments": ["Just did an initial check -- this should be doable, although will result in a sizeable patch due to API changes and removed deprecations. I think it still makes sense to try and push the 3.0 version of Lucene into Nutch, so I will keep working on this and seek help in reviewing the patch (and incompatible changes) once it's ready.", "Text-patch of changes porting the code to Lucene 3.0.0.", "Definitely not an easy thing to do. I need to finish for today, the code compiles, here's a brief summary of changes:\n\n- modified all filters and streams to use token attributes instead of raw Tokens. In many places I tried to be least intrusive so that the patch can be easily reviewed and accepted; improvements resulting from the new API can follow,\n\n- replaced deprecated constants to their new equivalents (UN_TOKENIZED, etc),\n\n- there are no compressed fields any more, so this stuff is commented out.\n\nIf I may ask as many people with Lucene/Nutch knowledge to go through the patch and point out potential problems, it would be great. At the moment one core test fails for me -- TestIndexSorter. I don't know if the difference in boosts is something that is a result of Lucene changes or my bug introduced somewhere along the way. \n\n", "The failing test in TestIndexSorter is caused by the change of implementation inside Lucene. In Lucene 2.9, SegmentMerger calls IndexReader#document(int, FieldSelector), but in 3.0 this has been changed to a call to document(int):\n\n        Document doc = reader.document(docCount);\n\nNow, IndexSorter in Nutch overrides both methods and delegates to the superclass (IndexReader) with mapping from old ids to new ids, but IndexReader re-delegates back to the overriden method, so IDs are effectively remapped back to original values.\n", "This patch moves Nutch from Lucene 2.9.1 to Lucene 3.0.0. All tests pass. The patch does not contain binary files (Lucene JARs), these should be applied manually.\n\nD       src/plugin/summary-lucene/lib/lucene-highlighter-2.9.1.jar\nA       src/plugin/summary-lucene/lib/lucene-highlighter-3.0.0.jar\nD       src/plugin/lib-lucene-analyzers/lib/lucene-analyzers-2.9.1.jar\nA       src/plugin/lib-lucene-analyzers/lib/lucene-analyzers-3.0.0.jar\nD       lib/lucene-misc-2.9.1.jar\nA       lib/lucene-core-3.0.0.jar\nD       lib/lucene-core-2.9.1.jar\nA       lib/lucene-misc-3.0.0.jar\n", "O.K. I think this is ready for review/ testing and integration. All built-in tests pass, it would be good if people could test it against their indexes.", "Lucene 3.0.1 is out now .. I'll test this patch with 3.0.1 artifacts and will report.", "I'll be happy to help if I can. I admit I only ran the build tests -- some empirical crawls and other types of jobs would be more then desirable, but I don't have the infrastructure to do it.", "Using Lucene 3.0.1 artifacts I verified that your patch passes all tests and produces correct searchable indexes. I'll commit this shortly.", "We're shooting at 3.0.1 now.", "Thanks Andrzej.", "Committed. Thanks Dawid!", "Integrated in Nutch-trunk #1101 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1101/])\n     Upgrade to Lucene 3.0.1.\n"], "tasks": {"summary": "Upgrade Lucene to 3.0.1.", "classification": "task", "qa_pairs": []}}
{"issue_id": "NUTCH-788", "project": "NUTCH", "title": "search.jsp typo causing searches to fail", "status": "Closed", "priority": "Major", "reporter": "Sammy Yu", "assignee": "Sami Siren", "created": "2010-02-11T05:40:00.902+0000", "updated": "2013-05-22T03:53:19.623+0000", "description": "Call to initialize the servlet parameter is missing parentheses.\n", "comments": ["Thanks Sammy for the fix, I did not realize you had spotted this too. It's now fixed in trunk.", "Thanks Sami!"], "tasks": {"summary": "search.jsp typo causing searches to fail", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "search.jsp typo causing searches to fail"}, {"question": "What is the main context?", "answer": "Call to initialize the servlet parameter is missing parentheses.\n"}]}}
{"issue_id": "NUTCH-789", "project": "NUTCH", "title": "Improvements to Tika parser", "status": "Closed", "priority": "Minor", "reporter": "Chris A. Mattmann", "assignee": "Chris A. Mattmann", "created": "2010-02-12T06:53:37.109+0000", "updated": "2013-05-22T03:54:52.138+0000", "description": "As reported by Sami in NUTCH-766, Sami has a few improvements he made to the Tika parser. We'll track that progress here.", "comments": ["- updates contributed by Sami. I'll generate a diff and then re-attach.", "It would be really useful to include the improvements in the functionality since that way almost all (-flash ?) parsers would be covered.", "Shall we postpone the work on this issue to after 1.1?", "There are no diffs, so it's difficult to figure out what's changed ... I think that Tika will soon release v. 0.7 which may also impact this patch if we decide to upgrade before our release. I asked the Tika guys about their release, let's wait a couple days more.", "Folks, I'm going to put together an RC for Tika 0.7 and take care of JIRA now. Once I do that, we can try and close out this issue for 1.1. I should be able to do this before the 48 hr deadline I threw up for Nutch 1.1...", "Hey Julien -- okey dok, Tika 0.7 has been released. Feel free to upgrade, and close this one out...after that, I'll cut the Nutch 1.1 RC.\n\nThanks!\n\nCheers,\nChris\n", "Will upgrade as soon as 0.7 is available from http://repo1.maven.org/maven2/org/apache/tika/ - which is not the case yet.\nI will leave this issue open but unmark it as 1.1", "Hey Julien, Tika 0.7 is available from Maven central:\n\nhttp://repo1.maven.org/maven2/org/apache/tika/tika-parsers/\n\nCheers,\nChris\n", "Have created a separate issue for the upgrade of Tika 0.7 and moved this one out of 1.1", "- it's been over 2 years since this issue was contributed. I'm marking it as closed. If we want the improvements, separate issues can be opened up."], "tasks": {"summary": "Improvements to Tika parser", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Improvements to Tika parser"}, {"question": "What is the main context?", "answer": "As reported by Sami in NUTCH-766, Sami has a few improvements he made to the Tika parser. We'll track that progress here."}]}}
{"issue_id": "NUTCH-79", "project": "NUTCH", "title": "Fault tolerant searching.", "status": "Closed", "priority": "Major", "reporter": "Piotr Kosiorowski", "assignee": null, "created": "2005-08-09T01:40:57.000+0000", "updated": "2011-06-08T21:34:05.249+0000", "description": "I have finally managed to prepare first version of fault tolerant searching I have promised long time ago. \nIt reads server configuration from search-groups.txt file (in startup directory or directory specified by searcher.dir) if no search-servers.txt file is present. If search-servers.txt  is presentit would be read and handled as previously.\n---------------------------------------------------\nFormat of search-groups.txt:\n* <pre>\n *  search.group.count=[int] \n *  search.group.name.[i]=[string] (for i=0 to count-1)\n *  \n *  For each name: \n *  [name].part.count=[int] partitionCount \n *  [name].part.[i].host=[string] (for i=0 to partitionCount-1)\n *  [name].part.[i].port=int (for i=0 to partitionCount-1)\n *  \n *  Example: \n *  search.group.count=2 \n *  search.group.name.0=master\n *  search.group.name.1=backup\n *  \n *  master.part.count=2 \n *  master.part.0.host=host1 \n *  master.part.0.port=7777\n *  master.part.1.host=host2 \n *  master.part.1.port=7777\n *  \n *  backup.part.count=2 \n *  backup.part.0.host=host3 \n *  backup.part.0.port=7777\n *  backup.part.1.host=host4 \n *  backup.part.1.port=7777\n * </pre>.\n------------------------------------------------\n\nIf more than one search group is defined in configuration file requests are distributed among groups in round-robin fashion. If one of the servers from the group fails to respond the whole group is treated as inactive and removed from the pool used to distributed requests. There is a separate recovery thread that every \"searcher.recovery.delay\" seconds (default 60) tries to check if inactive became alive and if so adds it back to the pool of active groups.\n\n\n", "comments": ["This patch contains first version of the code. ", "Current code has two issues:\n1) marked with TODO comment in two places\n     public String[] getSegmentNames()  - on NutchBean is not used anywhere\n     The same method on DistributedSearch.Client  is not an implementation of any interface.\n     Probably there is no need for NutchBean to implement this method as it is not used in Nutch source \n     or DistributedSearch.Client should implement DistributedSearch.Protocol so this method would be acessible via   interface and in this way DistributedSearch.Client.getSegemntNames() can be substitued by ClientManager.getSegmentNames() (and ClientManager would have to implement DistributedSearch.Protocol).\n\n2) NutchBean.close method so one can terminate all related threads - especially useful in JUnit tests.\n\nLooking back I think I should have added this two things together with the rest but for some reason I wanted to get confirmation on them - before introducing the change.\n\nI also want to perform some stress tests before commiting - such tests were done before refactoring the code into org.apache.nutch packages so the second pass would be good.\n\n\n", "Piotr,\n\nAny update on this? Have you been able to run with this or still working out the kinks?", "I think it should work without changes I suggested in previous comment - they would be simply useful additions.\nI was not using it for quite a while so I would get back to it to make sure it works with latest code (I hope sooner than later) - but no promises at the moment", "Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Fault tolerant searching.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fault tolerant searching."}, {"question": "What is the main context?", "answer": "I have finally managed to prepare first version of fault tolerant searching I have promised long time ago. \nIt reads server configuration from search-groups.txt file (in startup directory or directory"}]}}
{"issue_id": "NUTCH-790", "project": "NUTCH", "title": "Some external javadoc links are broken", "status": "Closed", "priority": "Trivial", "reporter": "Sami Siren", "assignee": "Sami Siren", "created": "2010-02-14T16:41:09.182+0000", "updated": "2013-05-22T03:54:50.860+0000", "description": "Nutch javadoc links for lucene and hadoop are broken.", "comments": ["proposed patch, fixes links for lucene and hadoop, also updates j2se link to version 1.6", "+1 to commit this. Thanks, Sami!", "committed", "Integrated in Nutch-trunk #1069 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1069/])\n     Some external javadoc links are broken\n", "Hello,\n\nThe links to in http://wiki.apache.org/nutch/AboutPlugins to \nOnlineClusterer (...)\nIndexingFilter (...)\nOntology\nand so on\nthat should link to \n´http://nutch.apache.org/apidocs/org/apache/nutch/clustering/OnlineClusterer.html are all broken.\n\nI would take the time to update the wiki with the correct links if I could found them, Can you help?"], "tasks": {"summary": "Some external javadoc links are broken", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Some external javadoc links are broken"}, {"question": "What is the main context?", "answer": "Nutch javadoc links for lucene and hadoop are broken."}]}}
{"issue_id": "NUTCH-791", "project": "NUTCH", "title": "External links for published javadocs are partially broken", "status": "Closed", "priority": "Major", "reporter": "Sami Siren", "assignee": null, "created": "2010-02-14T16:49:23.979+0000", "updated": "2011-04-13T23:48:07.849+0000", "description": "Lucene and Hadoop links point to non existing urls. For some versions of apidocs the links are just broken and for some they do not exist at all. Basically what is required is that the javadocs are generated again with proper urls for external packages.", "comments": ["Duplicates 790?", "Closing all resolved issues with a non-fixed status."], "tasks": {"summary": "External links for published javadocs are partially broken", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "External links for published javadocs are partially broken"}, {"question": "What is the main context?", "answer": "Lucene and Hadoop links point to non existing urls. For some versions of apidocs the links are just broken and for some they do not exist at all. Basically what is required is that the javadocs are ge"}]}}
{"issue_id": "NUTCH-792", "project": "NUTCH", "title": "Nutch version still contains 1.0", "status": "Closed", "priority": "Major", "reporter": "Sami Siren", "assignee": "Sami Siren", "created": "2010-02-14T17:09:57.258+0000", "updated": "2013-05-22T03:53:26.985+0000", "description": "Should be 1.1-dev now in trunk.", "comments": ["pump version to 1.1-dev", "committed", "Integrated in Nutch-trunk #1069 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1069/])\n     update version\n"], "tasks": {"summary": "Nutch version still contains 1.0", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Nutch version still contains 1.0"}, {"question": "What is the main context?", "answer": "Should be 1.1-dev now in trunk."}]}}
{"issue_id": "NUTCH-793", "project": "NUTCH", "title": "search.jsp compile errors", "status": "Closed", "priority": "Major", "reporter": "Sami Siren", "assignee": "Sami Siren", "created": "2010-02-15T08:09:00.534+0000", "updated": "2013-05-22T03:53:29.178+0000", "description": "Related to the searcher interface changes recently committed I broke search.jsp which does not currently compile.", "comments": ["committed a fix", "Integrated in Nutch-trunk #1071 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1071/])\n    "], "tasks": {"summary": "search.jsp compile errors", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "search.jsp compile errors"}, {"question": "What is the main context?", "answer": "Related to the searcher interface changes recently committed I broke search.jsp which does not currently compile."}]}}
{"issue_id": "NUTCH-794", "project": "NUTCH", "title": "Language Identification must use check the parse metadata for language values ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-02-16T09:30:29.597+0000", "updated": "2013-05-22T03:53:27.300+0000", "description": "The following HTML document : \n\n<html lang=\"fi\"><head>document 1 title</head><body>jotain suomeksi</body></html>\n\nis rendered as the following xhtml by Tika : \n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?><html xmlns=\"http://www.w3.org/1999/xhtml\"><head><title/></head><body>document 1 titlejotain suomeksi</body></html>\n\nwith the lang attribute getting lost.  The lang is not stored in the metadata either.\n\nI will open an issue on Tika and modify TestHTMLLanguageParser so that the tests don't break anymore ", "comments": ["Apart from the html attribute being lost (see above) there is also an issue with  the fact that Tika does not put the lang attributes in its XHTML representation but stores that in the metadata instead. \nI will shortly release a patch to address that in the class HTMLLanguageParser", "Committed patch in revision 910454\n\nWaiting for issue to be fixed in Tika before closing this issue", "Integrated in Nutch-trunk #1071 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1071/])\n     : Language Identification must use check the parse metadata for language values\n", "@julien -- I think this issue has been fixed in Tika right? If not, feel free to reopen, or better yet, re-file the issue against a post 1.1 Nutch release. Thanks!", "The issue has not been fixed in Tika. Will refile post 1.1 as you suggested. Can we update to Tika 0.7 before finalising 1.1?", "Hey Julien, yepper, I posted an RC of Tika 0.7, see: http://bit.ly/c7FZRc. If the VOTE passes on that in say the next 72 hours, I will push out a Tika 0.7 release to the mirrors. If everyone is OK with that, we can release Nutch 1.1 after...thoughts?"], "tasks": {"summary": "Language Identification must use check the parse metadata for language values ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Language Identification must use check the parse metadata for language values "}, {"question": "What is the main context?", "answer": "The following HTML document : \n\n<html lang=\"fi\"><head>document 1 title</head><body>jotain suomeksi</body></html>\n\nis rendered as the following xhtml by Tika : \n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?><"}]}}
{"issue_id": "NUTCH-795", "project": "NUTCH", "title": "Add ability to maintain nofollow attribute in linkdb", "status": "Open", "priority": "Major", "reporter": "Sammy Yu", "assignee": null, "created": "2010-02-18T19:46:42.480+0000", "updated": "2025-07-09T20:25:54.860+0000", "description": null, "comments": ["Dependent on NUTCH-693.", "Please see my comment to that issue. Or is there some other use case that you have in mind?"], "tasks": {"summary": "Add ability to maintain nofollow attribute in linkdb", "classification": "task", "qa_pairs": []}}
{"issue_id": "NUTCH-796", "project": "NUTCH", "title": "Zero results problems difficult to troubleshoot due to lack of logging", "status": "Closed", "priority": "Major", "reporter": "Jesse Hires", "assignee": "Andrzej Bialecki", "created": "2010-02-20T00:34:56.723+0000", "updated": "2011-06-08T21:34:23.630+0000", "description": "There are a few places where search can fail in a distributed environment, but when configuration is not quite right, there are no indications of errors or logging.\nIncreased logging of failures would help troubleshoot such problems, as well as lower the \"I get 0 results, why?\" questions that come across the mailing lists. \n\nAreas where logging would be helpful:\nsearch app cannot locate search-servers.txt\nsearch app cannot find searcher node listed in search-server.txt\nsearch app cannot connect to port on searcher specified in search-server.txt\nsearcher (bin/nutch server...) cannot find index\nsearcher cannot find segments\nAccess denied in any of the above scenarios.\n\nThere are probably more that would be helpful, but I am not yet familiar to know all the points of possible failure between the webpage and a search node.", "comments": ["I propose this patch. If there are no objections I'll commit it shortly.", "Patch applied in rev. 924945. Thanks for reporting it.", "Integrated in Nutch-trunk #1100 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1100/])\n      Zero results problems difficult to troubleshoot due to lack of logging.\n"], "tasks": {"summary": "Zero results problems difficult to troubleshoot due to lack of logging", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Zero results problems difficult to troubleshoot due to lack of logging"}, {"question": "What is the main context?", "answer": "There are a few places where search can fail in a distributed environment, but when configuration is not quite right, there are no indications of errors or logging.\nIncreased logging of failures would"}]}}
{"issue_id": "NUTCH-797", "project": "NUTCH", "title": "URL not properly constructed when link target begins with a \"?\"", "status": "Closed", "priority": "Minor", "reporter": "Robert Hohman", "assignee": "Sebastian Nagel", "created": "2010-02-25T20:47:52.136+0000", "updated": "2014-05-01T06:23:56.868+0000", "description": "This is my first bug and patch on nutch, so apologies if I have not provided enough detail.\n\nIn crawling the page at http://careers3.accenture.com/Careers/ASPX/Search.aspx?co=0&sk=0 there are links in the page that look like this:\n\n<a href=\"?co=0&sk=0&p=2&pi=1\">2</a></td><td><a href=\"?co=0&sk=0&p=3&pi=1\">3</a>\n\nin org.apache.nutch.parse.tika.DOMContentUtils rev 916362 (trunk), as getOutlinks looks for links, it comes across this link, and constucts a new url with a base URL class built from \"http://careers3.accenture.com/Careers/ASPX/Search.aspx?co=0&sk=0\", and a target of \"?co=0&sk=0&p=2&pi=1\"\n\nThe URL class, per RFC 3986 at http://labs.apache.org/webarch/uri/rfc/rfc3986.html#relative-merge, defines how to merge these two, and per the RFC, the URL class merges these to: http://careers3.accenture.com/Careers/ASPX/?co=0&sk=0&p=2&pi=1\n\nbecause the RFC explicitly states that the rightmost url segment (the Search.aspx in this case) should be ripped off before combining.\n\nWhile this is compliant with the RFC, it means the URLs which are created for the next round of fetching are incorrect.  Modern browsers seem to handle this case (I checked IE8 and Firefox 3.5), so I'm guessing this is an obscure exception or handling of what is a poorly formed url on accenture's part.\n\nI have fixed this by modifying DOMContentUtils to look for the case where a ? begins the target, and then pulling the rightmost component out of the base and inserting it into the target before the ?, so the target in this example becomes:\nSearch.aspx?co=0&sk=0&p=2&pi=1\n\nThe URL class then properly constructs the new url as:\nhttp://careers3.accenture.com/Careers/ASPX/Search.aspx?co=0&sk=0&p=2&pi=1\n\nIf it is agreed that this solution works, I believe the other html parsers in nutch would need to be modified in a similar way.\n\nCan I get feedback on this proposed solution?  Specifically I'm worried about unforeseen side effects.\n\nMuch thanks\n\nHere is the patch info:\nIndex: src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/DOMContentUtils.java\n===================================================================\n--- src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/DOMContentUtils.java\t(revision 916362)\n+++ src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/DOMContentUtils.java\t(working copy)\n@@ -299,6 +299,50 @@\n     return false;\n   }\n   \n+  private URL fixURL(URL base, String target) throws MalformedURLException\n+  {\n+\t  // handle params that are embedded into the base url - move them to target\n+\t  // so URL class constructs the new url class properly\n+\t  if  (base.toString().indexOf(';') > 0)  \n+          return fixEmbeddedParams(base, target);\n+\t  \n+\t  // handle the case that there is a target that is a pure query.\n+\t  // Strictly speaking this is a violation of RFC 2396 section 5.2.2 on how to assemble\n+\t  // URLs but I've seen this in numerous places, for example at\n+\t  // http://careers3.accenture.com/Careers/ASPX/Search.aspx?co=0&sk=0\n+\t  // It has urls in the page of the form href=\"?co=0&sk=0&pg=1\", and by default\n+\t  // URL constructs the base+target combo as \n+\t  // http://careers3.accenture.com/Careers/ASPX/?co=0&sk=0&pg=1, incorrectly\n+\t  // dropping the Search.aspx target\n+\t  //\n+\t  // Browsers handle these just fine, they must have an exception similar to this\n+\t  if (target.startsWith(\"?\"))\n+\t  {\n+\t\t  return fixPureQueryTargets(base, target);\n+\t  }\n+\t  \n+\t  return new URL(base, target);\n+  }\n+  \n+  private URL fixPureQueryTargets(URL base, String target) throws MalformedURLException\n+  {\n+\tif (!target.startsWith(\"?\"))\n+\t\treturn new URL(base, target);\n+\n+\tString basePath = base.getPath();\n+\tString baseRightMost=\"\";\n+\tint baseRightMostIdx = basePath.lastIndexOf(\"/\");\n+\tif (baseRightMostIdx != -1)\n+\t{\n+\t\tbaseRightMost = basePath.substring(baseRightMostIdx+1);\n+\t}\n+\t\n+\tif (target.startsWith(\"?\"))\n+\t\ttarget = baseRightMost+target;\n+\t\n+\treturn new URL(base, target);\n+  }\n+\n   /**\n    * Handles cases where the url param information is encoded into the base\n    * url as opposed to the target.\n@@ -400,8 +444,7 @@\n             if (target != null && !noFollow && !post)\n               try {\n                 \n-                URL url = (base.toString().indexOf(';') > 0) ? \n-                  fixEmbeddedParams(base, target) :  new URL(base, target);\n+                URL url = fixURL(base, target);\n                 outlinks.add(new Outlink(url.toString(),\n                                          linkText.toString().trim()));\n               } catch (MalformedURLException e) {\n", "comments": ["Thanks for reporting this, and providing a patch. An updated revision of the standard, RFC3986 section 5.4.1 example 7 follows the same reasoning. I'll fix this shortly.", "Hm, actually the picture is more complicated than I thought - if we apply both methods (fixEmbeddedParams and fixPureQueryTargets) then some of the test cases from RFC fail. However, all tests succeed if we only apply the fixPureQueryTargets !\n\nLooking at the origin of the fixEmbeddedParams method (NUTCH-436) something must been fixed in java.net.URL, because the test case mentioned in that issue now passes if we apply only fixPureQueryTargets. The same case with test cases in a near-duplicate issue NUTCH-566.\n\nConsequently I'm going to remove fixEmbeddedParams. I added all tests from RFC3986 section 5.4.1, and they all pass now. I'll attach an updated patch shortly.", "Updated patch with some refactoring and unit tests. If no objections I'll commit this shortly.", "I thought this same issue (relative URL with leading '?') had been fixed in Tika. Or at least I reported it, and I thought Jukka rolled in code that would handle it. See [TIKA-287], and the comment about \"Note that special care must be taken to work around a known bug in the Java URL() class, when the relative URL is a query string and the base URL doesn't end with a '/'.\"\n\nOr is this the case of Nutch needing to implement similar link extraction support?", "Unfortunately the way your fix was applied there is not reusable (private method in HtmlParser... ugh :( ). So for the time being I think we'll go with our utility class ... which we should really move to the crawler-commons anyway!", "Agreed re crawler-commons...feels like there's a beefy chunk of URL handling code that should go there.", "Makes sense, thanks for looking at this guys", "Wouldn't it be easier for Nutch to pass the base URL as the CONTENT_LOCATION metadata to the Tika parser? Then Tika would automatically apply these fixes, as discussed in TIKA-287.", "A few issues with this:\n\n* does this mean that the fixes would be applied to links found in other content types as well, not just html (the fixup code in TIKA-287 is located in HtmlParser)?\n\n* we need this also in other places, e.g. in the redirection handling code (both meta-refresh, javascript location.href and protocol-level redirect)\n\n* for a while we still need this in the parse-html plugin that does not use Tika.", "I guess we need to apply the same logic also to other Tika parsers that may deal with relative URLs.\n\nSince we in any case need this functionality in Tika, would it be useful for Nutch if it was made available as a public utility class or method in tika-core? It would be great if we could avoid duplicating the code in different projects.", "That's one option, at least until the crawler-commons produces any artifacts ... Eventually I think that this code and other related code (e.g. deciding which URL is canonical in presence of redirects, url normalization and filtering) should end up in the crawler-commons.", "If there are no futher comments I'm going to commit the current patch with a TODO to revisit this code if/when it's refactored to an external dependency.", "Back on radar: has this ever been committed at all?", "Hi markus  - I am not sure if the committers committed it. I thought they were going to. \n\nWe have moved off of nutch and so I am a little out of touch with what the latest is. \n\nIf you hve any other questions let me know. \n\n\n\n\n", "We'll look in to it. Thanks for reporting.", "Hhmm, I wonder what the scenario with this is? Andrzej (or any other Tika commiters who might be watching) can you comment on whether this has been fixed in the tika 0.10? If this is the case, and we upgrade to Tika 0.10 as per NUTCH-1154 I thin this issue can be closed and we will be one step closer to getting 1.4 out the door. Alternatively, if this is not the case can someone similarly comment on how we can take this forward. It seems that most of the hard work has already been done!  ", "The fixup code in Tika is still a private method in HtmlParser, so in this case the upgrade to Tika 0.10 won't help, we still have to apply the above patch.\n\nI'll commit this shortly.", "Committed in rev. 1181747 to trunk. Nutchgora needs more work, so I'm leaving this open.", "Andrzej, it looks like the fix for NUTCH-1115 is gone since this commit.", "Uhh, sorry - I'll fix this in a moment.", "I'm puzzled by the algorithm in fixEmbeddedParams (which was refactored into URLUtil), and I don't understand how it was ever supposed to work. If I enable this method then most of the test URLs in TestURLUtil fail, because they are not resolved according to the RFC.\n\nIn your example in NUTCH-1115, what was the expected result of resolving the base url \"http://www.funkybabes.nl/;ROOOWAN/fotoboek\" and e.g. a target of \"forumregels\" ?\n\n* http://www.funkybabes.nl/forumregels\n* http://www.funkybabes.nl/;ROOOWAN/forumregels\n* http://www.funkybabes.nl/forumregels;ROOOWAN\n* none of the above ;)", "I would expect http://www.funkybabes.nl/forumregels but iirc you would get http://www.funkybabes.nl/;ROOOWAN/forumregels. Such a structure would get out of hand quickly. That's why i included the fix to disable the fixEmbeddedParams method. NUTCH-436 is the issue when fixEmbeddedParams was introduced. I haven't seen real life cases where this is used at all.", "Well, I would expect http://www.funkybabes.nl/forumregels;ROOOWAN ... i.e. the embedded params from the base url would be transferred to the resolved url. FWIW, the RFC defines that a slash \"/\" is a valid sub-separator in the params sections, so it could be even argued that the value of the param in this case is \"ROOOWAN/fotoboek\".\n\nHow about modifying the meaning of this option, so that it simply removes any embedded params from the base and discards them completely? This would satisfy the requirements of NUTCH-1115 and at least the use case would be clear - this option is for cleaning unlikely and probably buggy URLs.", "Mmm, i think you are correct. It's bit confusing indeed. If its modified to simply remove them it should definately fix NUTCH-1115. There are quite a few buggy URL's that wouldn't enter the DB with them removed.\n\nI'll be happy to test any patches to the current trunk (the one without NUTCH-1115 ;) )", "Tentative patch, which changes the meaning of \"fixEmbeddedParams\" to \"removeEmbeddedParams\".", "Hm, seems the parser.fix.embeddedparams switch doesn't have any effect anymore. I'll get the same output without embedded params. At least no embedded params but i cannot be enabled either right now.", "That's unexpected :) I checked the patch and I can't see where the bug could be ... Did you make sure that your config is correct, and that the job actually sees the right value of this property in the config (check the job.xml via JobTracker)? TestDOMContentUtils indicates that it should work, so we need to make sure that the flag has correct value.", "This test was on a local instance. I tried both values for parser.fix.embeddedparams with:\n$ bin/nutch parsechecker http://www.funkybabes.nl/;ROOOWAN/fotoboek\n\n\nIs this how it should be implemented? I'm not sure. Embedded params are a bit puzzling :)", "Integrated in Nutch-trunk #1631 (See [https://builds.apache.org/job/Nutch-trunk/1631/])\n    NUTCH-797 Fix parse-tika and parse-html to use relative URL resolution per RFC-3986.\n\nab : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1181747\nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/src/java/org/apache/nutch/util/URLUtil.java\n* /nutch/trunk/src/plugin/parse-html/ivy.xml\n* /nutch/trunk/src/plugin/parse-html/plugin.xml\n* /nutch/trunk/src/plugin/parse-html/src/java/org/apache/nutch/parse/html/DOMContentUtils.java\n* /nutch/trunk/src/plugin/parse-html/src/test/org/apache/nutch/parse/html/TestDOMContentUtils.java\n* /nutch/trunk/src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/DOMContentUtils.java\n* /nutch/trunk/src/test/org/apache/nutch/util/TestURLUtil.java\n", "Integrated in nutch-trunk-maven #3 (See [https://builds.apache.org/job/nutch-trunk-maven/3/])\n    NUTCH-797 Fix parse-tika and parse-html to use relative URL resolution per RFC-3986.\n\nab : http://svn.apache.org/viewvc/nutch/trunk/viewvc/?view=rev&root=&revision=1181747\nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/src/java/org/apache/nutch/util/URLUtil.java\n* /nutch/trunk/src/plugin/parse-html/ivy.xml\n* /nutch/trunk/src/plugin/parse-html/plugin.xml\n* /nutch/trunk/src/plugin/parse-html/src/java/org/apache/nutch/parse/html/DOMContentUtils.java\n* /nutch/trunk/src/plugin/parse-html/src/test/org/apache/nutch/parse/html/TestDOMContentUtils.java\n* /nutch/trunk/src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/DOMContentUtils.java\n* /nutch/trunk/src/test/org/apache/nutch/util/TestURLUtil.java\n", "This has been committed but the issue is still open and marked as unresolved. I've just spent around 30 mins looking through the three open issues closely surrounding this problem area with constructing outlinks beginning with ?'s. I think that we need to have a close look to try and sort the three issues out.  ", "Set and Classify", "Tested using parsechecker (cf. NUTCH-1743) with attached sample document:\n* fixed for trunk and parse-tika\n* still open for parse-html in 2.x\n\nSame applies to NUTCH-566 and NUTCH-952.", "Patch for 2.x:\n- port URLUtil.resolveURL() from 1.x (including unit test)\n- removed fixEmbeddedParams(): it's still in 1.x but unused (NUTCH-797 removed/deactivated NUTCH-1115)", "Hi [~jnioche], is there anything left (except patching 2.x)? It's fixed for 1.x since long.", "No idea Seb. Have just assigned this one from Andrzej to myself but haven't looked at it in details", "Ok, then I'll take over to patch 2.x and resolve this issue.", "Changed title: it's also a problem of parse-html, recent Tika versions are not affected.", "Simplified patch for 2.x, without changes to fixEmbeddedParams: it would break unit tests, and should also fixed in parse-tika. Opened NUTCH-1767 to address links containing \"params\".", "Thanks for taking care of this [~wastl-nagel]. Looks good +1 to commit", "Committed to 2.x, r1590796.", "SUCCESS: Integrated in Nutch-nutchgora #1006 (See [https://builds.apache.org/job/Nutch-nutchgora/1006/])\nNUTCH-797 URL not properly constructed when link target begins with a \"?\" (snagel: http://svn.apache.org/viewvc/nutch/branches/2.x/?view=rev&rev=1590796)\n* /nutch/branches/2.x/CHANGES.txt\n* /nutch/branches/2.x/src/java/org/apache/nutch/util/URLUtil.java\n* /nutch/branches/2.x/src/plugin/parse-html/src/java/org/apache/nutch/parse/html/DOMContentUtils.java\n* /nutch/branches/2.x/src/test/org/apache/nutch/util/TestURLUtil.java\n"], "tasks": {"summary": "URL not properly constructed when link target begins with a \"?\"", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "URL not properly constructed when link target begins with a \"?\""}, {"question": "What is the main context?", "answer": "This is my first bug and patch on nutch, so apologies if I have not provided enough detail.\n\nIn crawling the page at http://careers3.accenture.com/Careers/ASPX/Search.aspx?co=0&sk=0 there are links in"}]}}
{"issue_id": "NUTCH-798", "project": "NUTCH", "title": "Upgrade to SOLR1.4", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2010-02-26T15:23:27.581+0000", "updated": "2013-05-22T03:53:20.441+0000", "description": "in particular SOLR1.4 has a StreamingUpdateSolrServer which would simplify the way we buffer the docs before sending them to the SOLR instance ", "comments": ["+1", "+1, preferably before the 1.1 freeze so that we  can test it.", "Updated SOLRJ's dependencies at the same time : \n\nDeleting       lib/apache-solr-common-1.3.0.jar\nAdding  (bin)  lib/apache-solr-core-1.4.0.jar\nDeleting       lib/apache-solr-solrj-1.3.0.jar\nAdding  (bin)  lib/apache-solr-solrj-1.4.0.jar\nDeleting       lib/commons-httpclient-3.0.1.jar\nAdding  (bin)  lib/commons-httpclient-3.1.jar\nAdding  (bin)  lib/commons-io-1.4.jar\nAdding  (bin)  lib/geronimo-stax-api_1.0_spec-1.0.1.jar\nAdding  (bin)  lib/jcl-over-slf4j-1.5.5.jar\nDeleting       lib/slf4j-api-1.4.3.jar\nAdding  (bin)  lib/slf4j-api-1.5.5.jar\nAdding  (bin)  lib/wstx-asl-3.2.7.jar\n\nCommitted revision 921831", "Integrated in Nutch-trunk #1093 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1093/])\n     : Upgrade to SOLR1.4 and its dependencies\n"], "tasks": {"summary": "Upgrade to SOLR1.4", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade to SOLR1.4"}, {"question": "What is the main context?", "answer": "in particular SOLR1.4 has a StreamingUpdateSolrServer which would simplify the way we buffer the docs before sending them to the SOLR instance "}]}}
{"issue_id": "NUTCH-799", "project": "NUTCH", "title": "SOLRIndexer to commit once all reducers have finished", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-03-01T15:03:01.178+0000", "updated": "2010-03-06T04:09:15.139+0000", "description": "What about doing only one SOLR commit after the MR job has finished in SOLRIndexer instead of doing that at the end of every Reducer? \nI ran into timeout exceptions in some of my reducers and I suspect that this was due to the fact that other reducers had already finished and called commit. ", "comments": ["I think it's ok to do it this way - the commit per reducer may be actually harmful if commit succeeds but the task is killed for any reason and re-ran.\n\nNote: the patch has some formatting errors.", "Thanks for your feedback Andrzej\n\nCommitted revision 919358.", "Integrated in Nutch-trunk #1087 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1087/])\n     SOLRIndexer to commit once all reducers have finished\n"], "tasks": {"summary": "SOLRIndexer to commit once all reducers have finished", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "SOLRIndexer to commit once all reducers have finished"}, {"question": "What is the main context?", "answer": "What about doing only one SOLR commit after the MR job has finished in SOLRIndexer instead of doing that at the end of every Reducer? \nI ran into timeout exceptions in some of my reducers and I suspec"}]}}
{"issue_id": "NUTCH-8", "project": "NUTCH", "title": "fetcher.retry.max no longer in use", "status": "Closed", "priority": "Minor", "reporter": "Kelvin Tan", "assignee": null, "created": "2005-03-12T07:27:49.000+0000", "updated": "2005-03-29T03:48:16.000+0000", "description": "In current Subversion code, the configuration entry fetcher.retry.max is no longer being used. It should either be removed from nutch-default.xml, or the functionality (re-)implemented in Fetcher.", "comments": ["A patch.\nValue is now used in updatedatabasetool.\nHTH, \nStefan", "This was a feature of an obsolete version of the fetcher.  Therre is a related feature of the updatedb tool that was not configurable.  So I removed this parameter and replaced it with a configurable parameter for the updatedb tool.\n\nFixed in revision 157453.\n\nhttp://svn.apache.org/viewcvs.cgi?rev=157453&view=rev", "As mentioned Fixed in revision 157453 by Doug. "], "tasks": {"summary": "fetcher.retry.max no longer in use", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "fetcher.retry.max no longer in use"}, {"question": "What is the main context?", "answer": "In current Subversion code, the configuration entry fetcher.retry.max is no longer being used. It should either be removed from nutch-default.xml, or the functionality (re-)implemented in Fetcher."}]}}
{"issue_id": "NUTCH-80", "project": "NUTCH", "title": "Web UI only works when project deployed in root", "status": "Closed", "priority": "Major", "reporter": "AJ Banck", "assignee": null, "created": "2005-08-15T20:26:23.000+0000", "updated": "2005-08-16T03:58:58.000+0000", "description": "", "comments": ["Please close this case. Created by mistake.", "Closed according to reporter comment."], "tasks": {"summary": "Web UI only works when project deployed in root", "classification": "task", "qa_pairs": []}}
{"issue_id": "NUTCH-800", "project": "NUTCH", "title": "Generator builds a URL list that is not encoded", "status": "Open", "priority": "Major", "reporter": "Jesse Campbell", "assignee": null, "created": "2010-03-05T22:00:18.631+0000", "updated": "2025-07-09T20:25:40.899+0000", "description": "The URL string that is grabbed by the generator when creating the fetch list does not get encoded, could potentially allow unsafe excecution, and breaks reading improperly encoded URLs from the scraped pages.\nSince we a) cannot guarantee that any site we scrape is not malitious, and b) likely do not have control over all content providers, we are currently forced to use a regex normalizer to perform the same function as a built-in java class (it would be unsafe to leave alone)\n\nA quick solution would be to update Generator.java to utilize the java.net.URLEncoder static class:\n\nline 187: \nold: String urlString = url.toString();\nnew: String urlString = URLEncoder.encode(url.toString(),\"UTF-8\");\n\nline 192:\nold: u = new URL(url.toString());\nnew: u = new URL(urlString);\n\n\nThe use of URLEncoder.encode could also be at the updatedb stage.", "comments": ["I'm puzzled by your problem description. Is Nutch affected by a potentially malicious URL data? URL form encoding is just a transport encoding, it doesn't make URL inherently safe (or unsafe).", "Well as it is right now, badly encoded urls will cause the crawler to break (with exceptions)\nThis tells me that it is not parsing the url string properly, which makes me question the possibility that there *could* be code injection...\nWhere I work, we try to be defensive... anything that comes from an outside source (in this case URLs either entered by the user in a text file or scraped from a website) should be encoded so that code injection isn't possible, or is at least harder.\nI realize we're running java and not JS, so it would not be quite as simple as dropping in an Alert() command...\n\nI also want it fixed because I don't really like the idea of using a regex normalizer to fix URLs with spaces in them... regex also is known to have multiple vulnerabilities in all languages."], "tasks": {"summary": "Generator builds a URL list that is not encoded", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Generator builds a URL list that is not encoded"}, {"question": "What is the main context?", "answer": "The URL string that is grabbed by the generator when creating the fetch list does not get encoded, could potentially allow unsafe excecution, and breaks reading improperly encoded URLs from the scrape"}]}}
{"issue_id": "NUTCH-801", "project": "NUTCH", "title": "Remove RTF and MP3 parse plugins", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2010-03-10T14:45:35.994+0000", "updated": "2013-05-22T03:53:25.303+0000", "description": "*Parse-rtf* and *parse-mp3* are not built by default  due to licensing issues. Since we now have *parse-tika* to handle these formats I would be in favour of removing these 2 plugins altogether to keep things nice and simple. The other plugins will probably be phased out only after the release of 1.1  when parse-tika will have been tested a lot more.\n\nAny reasons not to?\n\nJulien\n\n", "comments": ["+1 on this from me, Julien. Sounds good.", "Definitely +1, the only reason they lingered so long was the lack of a suitable replacement.", "Committed revision 921840.\n", "Integrated in Nutch-trunk #1093 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1093/])\n     Remove RTF and MP3 parse plugins\n"], "tasks": {"summary": "Remove RTF and MP3 parse plugins", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove RTF and MP3 parse plugins"}, {"question": "What is the main context?", "answer": "*Parse-rtf* and *parse-mp3* are not built by default  due to licensing issues. Since we now have *parse-tika* to handle these formats I would be in favour of removing these 2 plugins altogether to kee"}]}}
{"issue_id": "NUTCH-802", "project": "NUTCH", "title": "Problems managing outlinks with large url length", "status": "Closed", "priority": "Major", "reporter": "Pablo Aragón", "assignee": "Andrzej Bialecki", "created": "2010-03-18T10:39:37.106+0000", "updated": "2013-05-22T03:53:17.925+0000", "description": "Nutch can get idle during the collection of outlinks if  the URL address of the outlink is too large.\n\nThe maximum sizes of an URL for the main web servers are:\n\n    * Apache: 4,000 bytes\n    * Microsoft Internet Information Server (IIS): 16, 384 bytes\n    * Perl HTTP::Daemon: 8.000 bytes\n\nURL adress sizes bigger than 4000 bytes are problematic, so the limit should be set in the nutch-default.xml configuration file.\n\nI attached a patch", "comments": ["Submitting a patch is not \"fixing\", it's fixed when the patch is accepted and applied.", "We already have a general way to control this and other aspects of URL-s as such, namely with URLFilters. I agree that this functionality could be useful, but in a form of a URLFilter (or adding this control to e.g. urlfilter-basic or urlfilter-validator).", "What are we going to do with this? Mark as won't fix? I also prefer regex as the solution.", "From recent user list correspondence it would appear that the community are comfortable working with urlfilter's as well.\n\nI agree Markus", "+1 for marking as won't fix. No-one seems to have touched this in ages. If someone wishes to address it in the future they can open a new issue with the more appropriate solution.", "Agree with Markus and Lewis. Hence marking this one as wont fix. If someone wishes to address it in the future they can open a new issue with the more appropriate solution."], "tasks": {"summary": "Problems managing outlinks with large url length", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Problems managing outlinks with large url length"}, {"question": "What is the main context?", "answer": "Nutch can get idle during the collection of outlinks if  the URL address of the outlink is too large.\n\nThe maximum sizes of an URL for the main web servers are:\n\n    * Apache: 4,000 bytes\n    * Micros"}]}}
{"issue_id": "NUTCH-803", "project": "NUTCH", "title": "Upgrade Hadoop to 0.20.2", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-03-19T11:40:46.299+0000", "updated": "2010-03-20T04:11:17.363+0000", "description": "Per subject. We are currently using 0.20.1, so there are no API changes.", "comments": ["All tests pass - committed.", "Integrated in Nutch-trunk #1101 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1101/])\n     Upgrade to Hadoop 0.20.2.\n"], "tasks": {"summary": "Upgrade Hadoop to 0.20.2", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade Hadoop to 0.20.2"}, {"question": "What is the main context?", "answer": "Per subject. We are currently using 0.20.1, so there are no API changes."}]}}
{"issue_id": "NUTCH-804", "project": "NUTCH", "title": "CrawlDatum.statNames can be modified", "status": "Open", "priority": "Minor", "reporter": "Mike Baranczak", "assignee": null, "created": "2010-03-25T21:06:42.941+0000", "updated": "2025-07-09T20:25:52.401+0000", "description": "public static final HashMap<Byte, String> statNames\n\nIt's possible to modify the contents of this hash map from anywhere in the application, which could cause problems in unrelated places. Unless I'm missing something, there's no good reason to modify this map after it's initialized. So, it should either not be declared public, or be made read-only.", "comments": [], "tasks": {"summary": "CrawlDatum.statNames can be modified", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "CrawlDatum.statNames can be modified"}, {"question": "What is the main context?", "answer": "public static final HashMap<Byte, String> statNames\n\nIt's possible to modify the contents of this hash map from anywhere in the application, which could cause problems in unrelated places. Unless I'm "}]}}
{"issue_id": "NUTCH-805", "project": "NUTCH", "title": "Unable to resolve the url-blah-blah, skipping", "status": "Closed", "priority": "Major", "reporter": "P Kaustubh", "assignee": null, "created": "2010-03-26T11:20:29.899+0000", "updated": "2011-04-13T23:48:07.700+0000", "description": "I configured the nutch-0.9 as well as nutch-1.0 to crawl intranet website. The machine access the internet/intranet using proxy i had made this setup in nutch-default.xml\n\neverything works well untill i run script, when fetcher tries to access the urls from seed gives error as \n\nunable to resolve www.urladdres.com , skipping\nQueueFeeder finished: total 1 records.\n-finishing thread FetcherThread, activeThreads=0\n-finishing thread FetcherThread, activeThreads=0\n-finishing thread FetcherThread, activeThreads=0\n-finishing thread FetcherThread, activeThreads=0\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: done\n", "comments": ["Is this actually a bug? Or just a support request?\n\nI think this issue should be closed and if necessary support can be given through the mailing list. ", "- the amount of detail required to track the true issue down here begets assistance on the mailing list, and this isn't a bug per se. Please ask your question on user@nutch.apache.org and look for help there.", "Closing all resolved issues with a non-fixed status."], "tasks": {"summary": "Unable to resolve the url-blah-blah, skipping", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Unable to resolve the url-blah-blah, skipping"}, {"question": "What is the main context?", "answer": "I configured the nutch-0.9 as well as nutch-1.0 to crawl intranet website. The machine access the internet/intranet using proxy i had made this setup in nutch-default.xml\n\neverything works well untill"}]}}
{"issue_id": "NUTCH-806", "project": "NUTCH", "title": "Merge CrawlDBScanner with CrawlDBReader", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-03-29T13:46:57.976+0000", "updated": "2024-03-13T14:51:40.176+0000", "description": "The CrawlDBScanner [NUTCH-784] should be merged with the CrawlDBReader. Will do that after the 1.1 release ", "comments": ["Markus had already added the functionalities from the scanner to the reader. I've removed the CrawlDBScanner as it is now useless.\n\nTrunk : Committed revision 1508065.\n\n", "Bulk progression of all “Resolved” issues to “Closed”. Performed my lewismc on 2024-03-13."], "tasks": {"summary": "Merge CrawlDBScanner with CrawlDBReader", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Merge CrawlDBScanner with CrawlDBReader"}, {"question": "What is the main context?", "answer": "The CrawlDBScanner [NUTCH-784] should be merged with the CrawlDBReader. Will do that after the 1.1 release "}]}}
{"issue_id": "NUTCH-807", "project": "NUTCH", "title": "JSParseFilter produces malformed URL", "status": "Closed", "priority": "Major", "reporter": "Minyao Zhu", "assignee": null, "created": "2010-04-02T07:31:51.911+0000", "updated": "2012-12-09T07:39:30.779+0000", "description": "This is found when crawling site: http://zhidao.baidu.com/    ( a Chinese language site )\n\nIt appears this page contains javascripts which confused JSParseFilter, which produced URL like this:\n\nhttp://zhidao.baidu.com/){if(A===46){baidu.hide(\n\nNot sure the impact/scope of this issue in general.  The observation for this specific site is, much less pages got crawled.\n\nThanks.", "comments": ["Closing old issues. The JSParseFilter is known to generate noisy URLS and is not used by default anymore. This won't get fixed"], "tasks": {"summary": "JSParseFilter produces malformed URL", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "JSParseFilter produces malformed URL"}, {"question": "What is the main context?", "answer": "This is found when crawling site: http://zhidao.baidu.com/    ( a Chinese language site )\n\nIt appears this page contains javascripts which confused JSParseFilter, which produced URL like this:\n\nhttp:/"}]}}
{"issue_id": "NUTCH-808", "project": "NUTCH", "title": "Evaluate ORM Frameworks which support non-relational column-oriented datastores and RDBMs ", "status": "Closed", "priority": "Major", "reporter": "Enis Soztutar", "assignee": "Enis Soztutar", "created": "2010-04-02T12:02:10.154+0000", "updated": "2010-04-26T17:17:42.834+0000", "description": "We have an ORM layer in the NutchBase branch, which uses Avro Specific Compiler to compile class definitions given in JSON. Before moving on with this, we might benefit from evaluating other frameworks, whether they suit our needs. \n\nWe want at least the following capabilities:\n- Using POJOs \n- Able to persist objects to at least HBase, Cassandra, and RDBMs \n- Able to efficiently serialize objects as task outputs from Hadoop jobs\n- Allow native queries, along with standard queries \n\n\n\n\nAny comments, suggestions for other frameworks are welcome.", "comments": ["A candidate framework is DataNucleus. It has the following benefits. \n\n- Apache 2 license. \n- JDO support \n- HBase, RDBMS, XML persistance. \n\nI will further investigate whether we can integrate Hadoop writables/Avro serialization so that objects can be passed from Mapred. \n", "So, this is the results so far : \n\nDataNucleus was previously known as JPOX and it was the reference implementation for Java Data objects (JDO). JDO is a java standard for persistence. A similar specification, named JPA is also a persistence standard, which is forked from EJB 3. However, JPA is designed for RDBMs only, so it will not be useful for us (http://www.datanucleus.org/products/accessplatform/persistence_api.html). \n\nIn JDO, the first step is to define the domain objects as POJOs. Then, the persistance metadata is specified either using annotations, XML or both. Then a byte code enhancer uses instrumentation to add required methods to the classes defined as @PersistanceCapable. The database tables can be generated by hand, automatically by datanucleus, or by using a tool (SchemaTool). \nThe persistence layer uses standard JDO syntax, which is similar to JDBC. The objects can be queried using JPQL. \n\nI have run a small test to persist objects of WebTableRow class (from NutchBase branch) to both MySQL and HBase. Although it took me a fair bit of time to set-up, I was able to persist objects to both. \n\nHowever, although it is possible to map complex fields (like lists, maps, arrays, etc) to RDBMs using different strategies (such as serializing directly, using Joins, using Foreign Keys), I was not able to find a way to leverage HBase data model. For example, we want to be able to map lists and maps to columns in column families. Without such functionality using column oriented stores does not bring any advantage. \n\nFor the byte[] serialization for MapReduce, we can either implement a new datastore for datanucleus, which also implements Hadoop's Serialization, or use Avro to generate Java classes to be feed into JPOX enhancer, or else manually implement Writable. \n\nTo sum up, datanucleus brings the following advantages :\n- out of the box RDBMs support \n- XML or annotation metadata\n- JDO is a Java standard \n- standard query interface\n- JSON support\n\nThe disadvantages to use DataNucleus would be:\n- JDO is rather complex, Implementing a datastore is not very trivial\n- We need write patches to datanucleus to flexibly map complex fields to leverage HBase's data model\n- We have no control on the source code\n- no native Hbase support (for example using filters, etc)\n\nOn the other hand, current implementation is \n- tested on production, \n- can leverage HBase data model, \n- can be modified to work with Avro serialization directly, \n- cassandra support could be added with little effort\n- can support multiple languages (in the future)\n\nI believe that having SQLite, MySQL and HBase support is critical for Nutch 2.0, for out-of-the-box use, ease of deployment and real-scale computing respectively. But obviously we cannot use DataNucleus out of the box either. \n\n\nORM is inherently a hard problem. I propose we go ahead and make the changes to DataNucleus to see if it is feasible, and continue with it if it suits our needs. Of course, having a custom framework will also be great, so any feedback would be more than welcome. ", "Hi Enis,\n\n{quote}\nOn the other hand, current implementation is ...\n{quote}\n\nWhat do you mean by current implementation? NutchBase?\n\nMy gut feeling would be to write a custom framework instead of relying on DataNucleus and use AVRO if possible. I really think that HBase support is urgently needed but am less convinced that we need MySQL in the very short term. \n\nI know that Cascading have various Tape/Sink implementations including JDBC, HBase  but also SimpleDB. Maybe it would be worth having a look at how they do it?", "bq. What do you mean by current implementation? NutchBase?\nIndeed. In package o.a.n.storage deals with ORM (though not all classes)\n\nbq. I know that Cascading have various Tape/Sink implementations including JDBC, HBase but also SimpleDB. Maybe it would be worth having a look at how they do it?\nThe way cascading does this is to convert Tuples (cascading data structure) to HBase/JDBC records. The schema for HBase/JDBC is given as a metadata. Since they deal with only tuple -> table row, it is not that difficult. But again, cascading does not allow for mapping lists to columns, etc. \n\nbq. My gut feeling would be to write a custom framework instead of relying on DataNucleus and use AVRO if possible. I really think that HBase support is urgently needed but am less convinced that we need MySQL in the very short term. \nYeah, the more I think about it, the more I come to terms with custom implementation. However, I think we might benefit a lot from the ideas from JDO in the long term. Also, JDBC implementation may not be relevant for large scale deployments, but it will be a very nice side effect of the ORM layer, which will allow easy deployment, which in turn will hopefully bring more users. ", "We have decided to go on with implementing an ORM layer as per the discussion on NUTCH-811. Closing this issue. "], "tasks": {"summary": "Evaluate ORM Frameworks which support non-relational column-oriented datastores and RDBMs ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Evaluate ORM Frameworks which support non-relational column-oriented datastores and RDBMs "}, {"question": "What is the main context?", "answer": "We have an ORM layer in the NutchBase branch, which uses Avro Specific Compiler to compile class definitions given in JSON. Before moving on with this, we might benefit from evaluating other framework"}]}}
{"issue_id": "NUTCH-809", "project": "NUTCH", "title": "Parse-metatags plugin", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-04-02T14:15:00.577+0000", "updated": "2013-05-02T02:30:52.308+0000", "description": "h2. Parse-metatags plugin\n\nThe parse-metatags plugin consists of a HTMLParserFilter which takes as parameter a list of metatag names with '*' as default value. The values are separated by ';'.\n\nIn order to extract the values of the metatags description and keywords, you must specify in nutch-site.xml\n\n{code:xml}\n<property>\n  <name>metatags.names</name>\n  <value>description;keywords</value>\n</property>\n{code}\n\nThe MetatagIndexer uses the output of the parsing above to create two fields 'keywords' and 'description'. Note that keywords is multivalued.\n\nThe query-basic plugin is used to include these fields in the search e.g. in nutch-site.xml\n\n{code:xml}\n<property>\n  <name>query.basic.description.boost</name>\n  <value>2.0</value>\n</property>\n\n<property>\n  <name>query.basic.keywords.boost</name>\n  <value>2.0</value>\n</property>\n{code}\n\n\nThis code has been developed by DigitalPebble Ltd and offered to the community by ANT.com\n\n", "comments": ["Modified version of the plugin which is compatible with parse-tika", "Why don't we include this plugin?", "It's been a long time and I'd forgotten about this one :-)\n\nObviously we don't need the QueryFilter anymore. Am not entirely happy with the indexing part of it though as we handle only 2 values (description and keywords) whereas the parsing step is open to any values specified by the users.\n\nWe also have the urlmeta plugin which allows to track md specified in the seed lists and index them. The name of this plugin should be improved BTW\n\n(thinking aloud) why don't we have a generic indexing implementation which could index any metadata specified by the user be it from the crawldb or the parse metadata? The parse-metatags plugin would then only deal with the parsing step and leave the indexing to this indexer, which could also be used by the existing urlmeta (which would then only help with the transfer of the MD from a root page to its outlinks).\n\nWe can also leave things as they are and just rename urlmeta into something like seed-metadata-propagation (or anything better) and keep the possibility to do specific things in the indexing part of the metadata like for instance splitting the keywords into multiple fields.\n\n ", "Hm, sounds a bit like the index-extra plugin. I haven't used neither (nor this one) so i can't really tell which we'd dump and which we'd include.", "I did not know about index-extra at all, have linked it to this issue.\n", "I updated the plugin to work under Nutch 1.3 (see attached patch NUTCH-809_metatags_1.3.patch). Documentation of usage is in the readme.txt of the plugin (it's called index-metatags).", "This is great Elisabeth, thank you. Marked for possible inclusion in 1.4 (and of course nutchgora :0))", "-1 for committing patch 809_metatags as-is. The package names should be org.apache + the patch should modify schema.xml\n\nIdeally we'd merge it with NUTCH-422 + NUTCH-1005 but also url-meta instead of having various plugins doing very similar things. However this could be done in 1.5 and we could try and include this in 1.4\n\nThanks\n", "- push ", "Is this available as a packaged plugin?  If so, how does one go about installing it?  Thanks!", "I attached 'metatags-plugin+tutorial.zip' which contains the bundled plugin and some documentation on how to install and use the metatags plugin. \n\nGuys, I wasn't sure if the documenation of the plugin should be included in the wiki since it's not actually part of Nutch yet, so please let me know if I should put it up on the wiki (and where:).", "Hi Elisabeth although I haven't had time to look through your zip yet a big thank you must be aimed your way. If you have time and are willing please create a new page on the Nutch wiki under plugin central. As you can see this issue is closely linked to some others of similar nature so it may/may not change in the future, however community driven documentation is exactly what we are after and it is greatly welcomed.\n\nPlease contact me off list or @ dev@ with your wiki username and I will add you to a the wiki contributers page.\n\nThank you \n\n[1] http://wiki.apache.org/nutch/PluginCentral ", "Documentation available on: http://wiki.apache.org/nutch/IndexMetatags", "Hi Elisabeth - Metatags plugin is great.\nBut, it does not work with Nutch 1.4 (followed the documentation)\nDoes it only work with Nutch 1.3?\nThanks!", "I haven't tested the plugin in 1.4 myself, but I think a few guys on the mailing list already used it with 1.4. ", "Patch for Nutch-809 against trunk. Delegates the indexing to index-metatags", "Trunk : Committed revision 1303371.\n\nNot activated by default. See nutch-default.xml for details. \n\nTODO update the WIKI, port to the gora branch add fields to SOLR and activate it by default (any volunteers?)", "Integrated in nutch-trunk-maven #206 (See [https://builds.apache.org/job/nutch-trunk-maven/206/])\n    NUTCH-809 Parse-metatags plugin (jnioche) (Revision 1303371)\n\n     Result = SUCCESS\njnioche : \nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/conf/nutch-default.xml\n* /nutch/trunk/src/plugin/build.xml\n* /nutch/trunk/src/plugin/parse-metatags\n* /nutch/trunk/src/plugin/parse-metatags/README.txt\n* /nutch/trunk/src/plugin/parse-metatags/build.xml\n* /nutch/trunk/src/plugin/parse-metatags/ivy.xml\n* /nutch/trunk/src/plugin/parse-metatags/plugin.xml\n* /nutch/trunk/src/plugin/parse-metatags/sample\n* /nutch/trunk/src/plugin/parse-metatags/sample/testMetatags.html\n* /nutch/trunk/src/plugin/parse-metatags/src\n* /nutch/trunk/src/plugin/parse-metatags/src/java\n* /nutch/trunk/src/plugin/parse-metatags/src/java/org\n* /nutch/trunk/src/plugin/parse-metatags/src/java/org/apache\n* /nutch/trunk/src/plugin/parse-metatags/src/java/org/apache/nutch\n* /nutch/trunk/src/plugin/parse-metatags/src/java/org/apache/nutch/parse\n* /nutch/trunk/src/plugin/parse-metatags/src/java/org/apache/nutch/parse/MetaTagsParser.java\n* /nutch/trunk/src/plugin/parse-metatags/src/test\n* /nutch/trunk/src/plugin/parse-metatags/src/test/org\n* /nutch/trunk/src/plugin/parse-metatags/src/test/org/apache\n* /nutch/trunk/src/plugin/parse-metatags/src/test/org/apache/nutch\n* /nutch/trunk/src/plugin/parse-metatags/src/test/org/apache/nutch/parse\n* /nutch/trunk/src/plugin/parse-metatags/src/test/org/apache/nutch/parse/html\n* /nutch/trunk/src/plugin/parse-metatags/src/test/org/apache/nutch/parse/html/TestMetatagParser.java\n", "Hi Julien,\n\nCan you confirm what you would like to see added to the wiki?, I will try my best to get this added, are you referring to the [0]? Also I thought the best thing to do regarding porting to Nutchgora is just to add it to the ever growing NUTCH-1104 list, so I have done so. If and when this is required over there someone can duly oblige :)\nRegarding adding fields to Solr I assume you mean schema and solr-mapping.xml?\nFinally can you expand on 'activate by default', what exactly is it that not activated by default? I read your README.txt but I can see any mention of it in there.   \nThanks\n\nOh and great patch, this is one which as we know is very much appreciated by everyone. \n[0] http://wiki.apache.org/nutch/IndexStructure", "Integrated in Nutch-trunk #1794 (See [https://builds.apache.org/job/Nutch-trunk/1794/])\n    NUTCH-809 Parse-metatags plugin (jnioche) (Revision 1303371)\n\n     Result = SUCCESS\njnioche : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1303371\nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/conf/nutch-default.xml\n* /nutch/trunk/src/plugin/build.xml\n* /nutch/trunk/src/plugin/parse-metatags\n* /nutch/trunk/src/plugin/parse-metatags/README.txt\n* /nutch/trunk/src/plugin/parse-metatags/build.xml\n* /nutch/trunk/src/plugin/parse-metatags/ivy.xml\n* /nutch/trunk/src/plugin/parse-metatags/plugin.xml\n* /nutch/trunk/src/plugin/parse-metatags/sample\n* /nutch/trunk/src/plugin/parse-metatags/sample/testMetatags.html\n* /nutch/trunk/src/plugin/parse-metatags/src\n* /nutch/trunk/src/plugin/parse-metatags/src/java\n* /nutch/trunk/src/plugin/parse-metatags/src/java/org\n* /nutch/trunk/src/plugin/parse-metatags/src/java/org/apache\n* /nutch/trunk/src/plugin/parse-metatags/src/java/org/apache/nutch\n* /nutch/trunk/src/plugin/parse-metatags/src/java/org/apache/nutch/parse\n* /nutch/trunk/src/plugin/parse-metatags/src/java/org/apache/nutch/parse/MetaTagsParser.java\n* /nutch/trunk/src/plugin/parse-metatags/src/test\n* /nutch/trunk/src/plugin/parse-metatags/src/test/org\n* /nutch/trunk/src/plugin/parse-metatags/src/test/org/apache\n* /nutch/trunk/src/plugin/parse-metatags/src/test/org/apache/nutch\n* /nutch/trunk/src/plugin/parse-metatags/src/test/org/apache/nutch/parse\n* /nutch/trunk/src/plugin/parse-metatags/src/test/org/apache/nutch/parse/html\n* /nutch/trunk/src/plugin/parse-metatags/src/test/org/apache/nutch/parse/html/TestMetatagParser.java\n", "Hi Lewis\n\nbq. Can you confirm what you would like to see added to the wiki?, I will try my best to get this added, are you referring to the [0]? \n\nNope. I meant replacing the wiki page written by Elizabeth with instructions on what to do to get the metatags parsed and indexed. What I committed relies on another plugin for indexing metadata whereas the old one had its own indexer etc...\n\nbq. Also I thought the best thing to do regarding porting to Nutchgora is just to add it to the ever growing NUTCH-1104 list, so I have done so. If and when this is required over there someone can duly oblige\n\ngood thinking\n\nbq. Regarding adding fields to Solr I assume you mean schema and solr-mapping.xml?\n\nyes, this will be needed if we want this to be on by default which I think is a good idea\n\nbq. Finally can you expand on 'activate by default', what exactly is it that not activated by default? I read your README.txt but I can see any mention of it in there.\n\nPlugins have to be listed in plugin.includes in order to be used. Thinking about it it would be good to declare a dependency to index-metatags so that the later is activated automatically (assuming plugin.auto-activation = true) \n\nThanks\n\nJulien\n", "20120304-push-1.6", "Updated instructions on WIKI http://wiki.apache.org/nutch/IndexMetatags\n", "Hello,\n\nI have been working with the plugin for some time and really works for everything (approx. 100 metadata fields) I need to extract from a set of webpages. I am mapping these fields to solr and only have a problem when it comes to fields which I want to convert to a format other than string. I have several date fields which are formatted as yyyy-mm-dd and no matter which way I try I do not get it to end up as solr date field as this requires the data in the format yyyy-mm-ddThh:mm:ssZ. Simply declaring the field as date in the schema results in an error. I have no control over the format in which the dates are stored in the webpages and nothing I tried in solr works, so my only remaining guess is that I need to look into changing the format within nutch. Any hint how to do that?\n\nThanks!\n\nKristof", "Kristof, please use the mailing list instead. Thanks", "Julien, maybe another way round will work. I have been trying to find the source file for MetaTagsIndexer.class to adjust it, but I can only seem to find MetaTagsParser.java. Would be great if anyone could send me the MetaTagsIndexer,java file. Thanks! If that does not work, I will try the mailing list.", "I finally decompiled the binary of MetaTagsIndexer.class and extended it so that I can now configure in nutch-site.xml which metatag I want to convert to date format. Works fine with mapping it to Solr date fields. As I am new to JIRA I do know how to share this. Let me know if would be helpful and if so how I can share it here.", "Please open a new issue and describe your improvement. You can then attach your patch to the newly created ticket. \n\nthanks"], "tasks": {"summary": "Parse-metatags plugin", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Parse-metatags plugin"}, {"question": "What is the main context?", "answer": "h2. Parse-metatags plugin\n\nThe parse-metatags plugin consists of a HTMLParserFilter which takes as parameter a list of metatag names with '*' as default value. The values are separated by ';'.\n\nIn ord"}]}}
{"issue_id": "NUTCH-81", "project": "NUTCH", "title": "Webapp only works when deployed in root", "status": "Closed", "priority": "Major", "reporter": "AJ Banck", "assignee": "Sami Siren", "created": "2005-08-15T20:31:21.000+0000", "updated": "2006-10-24T16:14:21.000+0000", "description": "Index.jsp does a redirect (not forward) to the language folder.\nThe links in the html however are relative to the language folder, not the application root.\n\nNot sure what the desired behavoir is, change the html (where is it generated?) or the redirect.", "comments": ["Possible solution for working from a non-root application. This by doing a redirect with the language path. This changes the URL in the browser. If this is not desired all xml files have to be edited to work from the root of the webapp, not relative to the language folder.", "Description is inaccurate, the current index.jsp does a forward not a redirect. If doing forward is needed the xsl or xml must be changed.", "After changing the $NUTCH_WEBAPPDIR/index.jsp as described (substitute the jsp:forward with response.sendRedirect()), nutch can be deployed in any tomcat-webapp-context. Most of the links used are relative, so all the images are found. \n\nJust on point: in the search.jsp it must be\n\n   <jsp:include page=\"<%=  \"/include/footer.html\"%>\"/>\n\ninstead of \n\n  <jsp:include page=\"<%=  \"../include/footer.html\"%>\"/>\n                                                ^^^^\n\nthere was a copy and paste to much :-)\n\nThe fix is trivial - and well known. It would make the initual setup for beginners a little bit more comfortable. So I strongly suggest to commit it.  \n\nBTW: in the default template is a link to \n\n                         http://www.nutch.org/faq.html\n\nthis gots redirected to :\n\n                        http://lucene.apache.org/nutch/faq.html\n\ncan someone in charge of nutch.org change this redirect to:\n\n                         http://wiki.apache.org/nutch/FAQ\n\n?", "this is now applied, thanks", "with the move from sf to apache the old faq isn' accessable any more. This patch changes the link from http://www.nutch.org/faq.html to\nhttp://wiki.apache.org/nutch/FAQ", "closing issues for released versions"], "tasks": {"summary": "Webapp only works when deployed in root", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Webapp only works when deployed in root"}, {"question": "What is the main context?", "answer": "Index.jsp does a redirect (not forward) to the language folder.\nThe links in the html however are relative to the language folder, not the application root.\n\nNot sure what the desired behavoir is, cha"}]}}
{"issue_id": "NUTCH-810", "project": "NUTCH", "title": "Upgrade to Tika 0.7", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-04-06T08:57:49.114+0000", "updated": "2010-04-07T04:24:35.581+0000", "description": "Upgrading to Tika 0.7 before 1.1 release\n\nThe TikaConfig mechanism has changed and does not rely on a default XML config file anymore. Am working on it.", "comments": ["Committed in rev 931098.\n\nhttp://issues.apache.org/jira/browse/TIKA-317 changed the way the TikaConfig is created as it does not rely on a  tika-config.xml file any longer. Our custom TikaConfig has been modified to reflect these changes.\n\nThis was the last remaining issue marked for 1.1 \n\n", "Integrated in Nutch-trunk #1116 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1116/])\n     Upgraded to Tika 0.7\n"], "tasks": {"summary": "Upgrade to Tika 0.7", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade to Tika 0.7"}, {"question": "What is the main context?", "answer": "Upgrading to Tika 0.7 before 1.1 release\n\nThe TikaConfig mechanism has changed and does not rely on a default XML config file anymore. Am working on it."}]}}
{"issue_id": "NUTCH-811", "project": "NUTCH", "title": "Develop an ORM framework ", "status": "Closed", "priority": "Major", "reporter": "Enis Soztutar", "assignee": "Enis Soztutar", "created": "2010-04-13T18:34:35.833+0000", "updated": "2010-08-11T21:00:52.688+0000", "description": "By Nutch-808, it is clear that we need an ORM layer on top of the datastore, so that different backends can be used to store data. \n\nThis issue will track the development of the ORM layer. Initially full support for HBase is planned, with RDBM, Hadoop MapFile and Cassandra support scheduled for later. ", "comments": ["Actually, we plan to develop the code for this layer in another project because, \n - ORM layer is orthogonal to Nutch code, so it does not belong there\n - Extracting the code will be much harder later\n - If developed well, this code will be useful to other projects (interesting is that there is no API to support both HBase and Cassandra)\n - Code will be much more clean \n - Nutch can use the artifacts from this project\n\nNevertheless, we plan to piggyback on the Nutch community to support the initial development, review and exposure. I will update this issue as the code develops, and will kindly ask for reviews. In the long term, we can move the project to Apache Sandbox, or as a Hadoop/Nutch sub project (once Nutch becomes TLP). \n\nA design document and initial code will be available shortly. ", "I have further developed the code, which was once part of NutchBase for handling object to hbase mapping into a new project as per the above discussion. \nThe project is named Gora, and it is hosted at GitHub. \n\nThe project is hosted at\nhttp://github.com/enis/gora\n\nA short design document is at http://wiki.github.com/enis/gora/design, and a quick start guide is at http://wiki.github.com/enis/gora/quick-start.\n\nYou can check out the code using\n$ git clone git://github.com/enis/gora.git\n\nWhat it means for Nutch?\nGora started as a part of Dogacan's NutchBase implementation, but the goals for the project are clearly different. However, Gora is primarily developed to handle Nutch's use cases. Specifically, Gora will handle the HBase integration layer for nutchbase, and later a Hadoop Mapfile or TFile based persistency will be developed. \n\nIn the short term, we plan to use Gora's artifacts as a library in Nutch. Either me or Dogacan will switch the current NutchBase branch to using Gora shortly. \n\n\nGora is still in very early stages and needs your support. We would be more than happy if the Nutch community could share comments, feedbacks, use cases and feature requests, or even patches. I suppose we can use this issue or the mailing list for this task. \n", "Will development for gora be tracked under this or any nutch ticket?", "Hi Piet,\nThe code for Gora will reside in GitHub for now, since Nutch and Gora are pretty orthogonal. But as stated before, Nutch is the first user of Gora, and Gora does not yet have a separate community so I intend to always keep nutch community updated (via this issue and nutch-dev mailing list), and hope for feedback from the Nutch community.\n\nMoreover, NutchBase has already been ported to using Gora, so at some point, Gora should be reviewed and accepted as a dependency for Nutch.", "Any objections to closing this issue?", "+1, close it out...", "Nutch now uses GORA as an ORM layer.\n\nClosing this issue as fixed."], "tasks": {"summary": "Develop an ORM framework ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Develop an ORM framework "}, {"question": "What is the main context?", "answer": "By Nutch-808, it is clear that we need an ORM layer on top of the datastore, so that different backends can be used to store data. \n\nThis issue will track the development of the ORM layer. Initially f"}]}}
{"issue_id": "NUTCH-812", "project": "NUTCH", "title": "Crawl.java incorrectly uses the Generator API resulting in NPE", "status": "Closed", "priority": "Critical", "reporter": "Andrzej Bialecki", "assignee": "Chris A. Mattmann", "created": "2010-04-17T00:17:21.101+0000", "updated": "2013-05-22T03:53:37.352+0000", "description": "As reported by Phil Barnett on nutch-user:\n\n{quote}\nThe Fix.\n\nIn line 131 of Crawl.java\n\nGenerate no longer returns segments like it used to. Now it returns segs.\n\nline 131 needs to read\n\n If (segs == null)\n\n Instead of the current\n\nIf (segments == null)\n\nAfter that change and a recompile, crawl is working just fine.\n{quote}", "comments": ["- fixed in r935453. Thanks, Phil and Andrzej!", "Integrated in Nutch-trunk #1129 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1129/])\n    - fix for NUTCH-812 Crawl.java incorrectly uses the Generator API resulting in NPE\n"], "tasks": {"summary": "Crawl.java incorrectly uses the Generator API resulting in NPE", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Crawl.java incorrectly uses the Generator API resulting in NPE"}, {"question": "What is the main context?", "answer": "As reported by Phil Barnett on nutch-user:\n\n{quote}\nThe Fix.\n\nIn line 131 of Crawl.java\n\nGenerate no longer returns segments like it used to. Now it returns segs.\n\nline 131 needs to read\n\n If (segs =="}]}}
{"issue_id": "NUTCH-813", "project": "NUTCH", "title": "Repetitive crawl 403 status page", "status": "Closed", "priority": "Minor", "reporter": "Tien Nguyen Manh", "assignee": null, "created": "2010-04-18T04:08:29.269+0000", "updated": "2013-05-22T03:54:51.921+0000", "description": "When we crawl a page the return a 403 status. It will be crawl repetitively each days with default schedule.\nEven when we restrict by paramter db.fetch.retry.max\n", "comments": ["The described problem is identical to that of NUTCH-578. The provided patch (call setPageGoneSchedule when retry counter hits db.fetch.retry.max) is included in all patches of NUTCH-578."], "tasks": {"summary": "Repetitive crawl 403 status page", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Repetitive crawl 403 status page"}, {"question": "What is the main context?", "answer": "When we crawl a page the return a 403 status. It will be crawl repetitively each days with default schedule.\nEven when we restrict by paramter db.fetch.retry.max\n"}]}}
{"issue_id": "NUTCH-814", "project": "NUTCH", "title": "SegmentMerger bug", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Andrzej Bialecki", "created": "2010-04-27T10:45:47.488+0000", "updated": "2010-04-27T15:24:05.982+0000", "description": "Dennis reported:\n\n{quote}\nIn the SegmentMerger.java file about line 150 we have this:\n\n       final SequenceFile.Reader reader =\n         new SequenceFile.Reader(FileSystem.get(job), fSplit.getPath(),\njob);\n\nThen about line 166 in the record reader we have this:\n\nboolean res = reader.next(key, w);\n\nIf I am reading that right, that would mean that the map tap would loop\nover all records for a given file and not just a given split.\n{quote}\nRight, this should instead use SequenceFileRecordReader that already has the logic to handle splits. Patch coming shortly - thanks for spotting this! This could be the reason for \"out of disk space\" errors that many users reported.", "comments": ["Patch fixing the issue, and a unit test. I will commit this shortly.", "Hey Andrzej,\n\nAfter you commit this, should I cut a new RC (rc #3)?\n\nCheers,\nChris", "This should be attributed to Rob Bradshaw (rdb@baynote.com) upon commit, \nhe found the issue.\n\nDennis\n\n", "Committed in rev. 938511. Thanks!"], "tasks": {"summary": "SegmentMerger bug", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "SegmentMerger bug"}, {"question": "What is the main context?", "answer": "Dennis reported:\n\n{quote}\nIn the SegmentMerger.java file about line 150 we have this:\n\n       final SequenceFile.Reader reader =\n         new SequenceFile.Reader(FileSystem.get(job), fSplit.getPath(),"}]}}
{"issue_id": "NUTCH-815", "project": "NUTCH", "title": "Invalid blank line before If-Modified-Since HTTP header", "status": "Closed", "priority": "Major", "reporter": "Pascal Dimassimo", "assignee": "Andrzej Bialecki", "created": "2010-04-27T16:02:17.373+0000", "updated": "2010-04-27T18:07:56.695+0000", "description": "If there is a Modified time stored in the crawldb for a link, the class org.apache.nutch.protocol.http.HttpResponse will use it as the value for the If-Modified-Since header. \n\nLine 131:\nreqStr.append(\"\\r\\n\");\nif (datum.getModifiedTime() > 0) {\n        reqStr.append(\"If-Modified-Since: \" + HttpDateFormat.toString(datum.getModifiedTime()));\n        reqStr.append(\"\\r\\n\");\n}\n\nThe problem is that an extra blank line is insert before this header. This make the header invalid:\n----------------------------------------------------------------------------------\nGET /tinysite/second.html HTTP/1.0\nHost: localhost:8080\nAccept-Encoding: x-gzip, gzip, deflate\nUser-Agent: nutch/Nutch-1.0\nAccept-Language: en-us,en-gb,en;q=0.7,*;q=0.3\n\nIf-Modified-Since: Tue, 27 Apr 2010 13:51:50 GMT\n----------------------------------------------------------------------------------\n\nI'm using the AdaptiveFetchSchedule to set the Modified time in the crawldb. \n\nI've made a test by moving the line 131 after the if block and it works. I think this is where that line should go.", "comments": ["Good catch. I'll fix it shortly.", "Fixed in rev. 938586. Thanks!"], "tasks": {"summary": "Invalid blank line before If-Modified-Since HTTP header", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Invalid blank line before If-Modified-Since HTTP header"}, {"question": "What is the main context?", "answer": "If there is a Modified time stored in the crawldb for a link, the class org.apache.nutch.protocol.http.HttpResponse will use it as the value for the If-Modified-Since header. \n\nLine 131:\nreqStr.append"}]}}
{"issue_id": "NUTCH-816", "project": "NUTCH", "title": "Add zip target to build.xml", "status": "Closed", "priority": "Major", "reporter": "Chris A. Mattmann", "assignee": "Chris A. Mattmann", "created": "2010-04-27T19:53:32.276+0000", "updated": "2013-05-22T03:53:24.676+0000", "description": "Just like we have an ant tar target (pun intended) we should have an ant zip target. I'd like to have this ready for the release and future releases.", "comments": ["- fixed in r942427"], "tasks": {"summary": "Add zip target to build.xml", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add zip target to build.xml"}, {"question": "What is the main context?", "answer": "Just like we have an ant tar target (pun intended) we should have an ant zip target. I'd like to have this ready for the release and future releases."}]}}
{"issue_id": "NUTCH-817", "project": "NUTCH", "title": "parse-(html)does follow links of full html page, parse-(tika) does follow any links and stops at level 1", "status": "Closed", "priority": "Major", "reporter": "matthew a. grisius", "assignee": "Julien Nioche", "created": "2010-05-02T02:58:00.264+0000", "updated": "2021-01-28T14:03:58.917+0000", "description": "submitted per Julien Nioche. I did not see where to attach a file so I pasted it here. btw: Tika command line returns empty html body for this file.\n\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Frameset//EN\" \"http://www.w3.org/TR/html4/frameset.dtd\">\n\n<!--NewPage-->\n\n<HTML>\n\n<HEAD>\n\n<!-- Generated by javadoc on Fri Mar 28 17:23:42 EDT 2008-->\n\n<TITLE>\n\nMatrix Application Development Kit\n\n</TITLE>\n\n<SCRIPT type=\"text/javascript\">\n\n    targetPage = \"\" + window.location.search;\n\n    if (targetPage != \"\" && targetPage != \"undefined\")\n\n       targetPage = targetPage.substring(1);\n\n    function loadFrames() {\n\n        if (targetPage != \"\" && targetPage != \"undefined\")\n\n             top.classFrame.location = top.targetPage;\n\n    }\n\n</SCRIPT>\n\n<NOSCRIPT>\n\n</NOSCRIPT>\n\n</HEAD>\n\n<FRAMESET cols=\"20%,80%\" title=\"\" onLoad=\"top.loadFrames()\">\n\n<FRAMESET rows=\"30%,70%\" title=\"\" onLoad=\"top.loadFrames()\">\n\n<FRAME src=\"overview-frame.html\" name=\"packageListFrame\" title=\"All Packages\">\n\n<FRAME src=\"allclasses-frame.html\" name=\"packageFrame\" title=\"All classes and interfaces (except non-static nested types)\">\n\n</FRAMESET>\n\n<FRAME src=\"overview-summary.html\" name=\"classFrame\" title=\"Package, class and interface descriptions\" scrolling=\"yes\">\n\n<NOFRAMES>\n\n<H2>\n\nFrame Alert</H2>\n\n\n\n<P>\n\nThis document is designed to be viewed using the frames feature. If you see this message, you are using a non-frame-capable web client.\n\n<BR>\n\nLink to<A HREF=\"overview-summary.html\">Non-frame version.</A>\n\n</NOFRAMES>\n\n</FRAMESET>\n\n</HTML>\n", "comments": ["parse-html parses links in frameset, parse-tika does not provide and links to follow.\n\nI now see where to attach file after I submitted issue . . .", "As explained on the mailing list this is related to [https://issues.apache.org/jira/browse/TIKA-379]. Since the HTML attributes are not rendered in the XHTML representation  from Tika the parser cannot detect the links in Frame.\n\nSee [https://issues.apache.org/jira/browse/NUTCH-794] for a related issue.\n\nI will submit an improved patch to Tika for the handling of HTML attributes, in the meantime you can revert to the 'old' HTML parser by specifying parse-html in plugin.includes and make sure that \n\n{code:xml}\n\t<mimeType name=\"text/html\">\n\t\t<plugin id=\"parse-html\" />\n\t</mimeType>\n{code}\n\nis enabled in parse-plugins.xml", "Fixed ages ago (1.3?) when upgraded to Tika 0.8"], "tasks": {"summary": "parse-(html)does follow links of full html page, parse-(tika) does follow any links and stops at level 1", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "parse-(html)does follow links of full html page, parse-(tika) does follow any links and stops at level 1"}, {"question": "What is the main context?", "answer": "submitted per Julien Nioche. I did not see where to attach a file so I pasted it here. btw: Tika command line returns empty html body for this file.\n\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Frame"}]}}
{"issue_id": "NUTCH-818", "project": "NUTCH", "title": "Parse-tika uses minorCodes instead of majorCodes in ParseStatus", "status": "Closed", "priority": "Blocker", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-05-11T14:26:52.682+0000", "updated": "2010-05-11T14:39:27.404+0000", "description": "Parse-tika uses minorCodes instead of majorCodes in ParseStatus which results in a IAOOB Exception in ParseSegment as the values are outside the range of majorCodes.\nThis happens for instance when no parser implementation can be found for a given mimetype.", "comments": ["Will commit shortly", "Committed revision 943128"], "tasks": {"summary": "Parse-tika uses minorCodes instead of majorCodes in ParseStatus", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Parse-tika uses minorCodes instead of majorCodes in ParseStatus"}, {"question": "What is the main context?", "answer": "Parse-tika uses minorCodes instead of majorCodes in ParseStatus which results in a IAOOB Exception in ParseSegment as the values are outside the range of majorCodes.\nThis happens for instance when no "}]}}
{"issue_id": "NUTCH-819", "project": "NUTCH", "title": "Included Solr schema.xml and solrindex-mapping.xml don't play together", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-05-11T14:54:06.424+0000", "updated": "2010-05-11T15:05:50.288+0000", "description": "conf/solrindex-mapping.xml already fills in \"id\" field, but conf/schema.xml uses copyField to create \"id\" from \"url\". This results in multiple values of the \"id\" field which is not allowed, because id is defined as uniqueKey in the schema.", "comments": ["Fix commited in rev. 943136."], "tasks": {"summary": "Included Solr schema.xml and solrindex-mapping.xml don't play together", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Included Solr schema.xml and solrindex-mapping.xml don't play together"}, {"question": "What is the main context?", "answer": "conf/solrindex-mapping.xml already fills in \"id\" field, but conf/schema.xml uses copyField to create \"id\" from \"url\". This results in multiple values of the \"id\" field which is not allowed, because id"}]}}
{"issue_id": "NUTCH-82", "project": "NUTCH", "title": "Nutch Commands should run on Windows without external tools", "status": "Closed", "priority": "Major", "reporter": "AJ Banck", "assignee": "Andrzej Bialecki", "created": "2005-08-16T03:21:31.000+0000", "updated": "2009-04-10T12:29:05.153+0000", "description": "Currently there is only a shellscript to run the Nutch commands. This should be platform independant.\nBest would be Ant tools, or scripts generated by a template tool to avoid replication.", "comments": ["nutch.bat to be placed in bin folder.\nThis allows running all Nutch commandline tools from Windows. Tested for Windows 2000 and XP.", "Update, remove obsolete comment", "CONF folder should be before nutch.jar\n\nset NUTCH_CLASSPATH=%NUTCH_HOME%;%NUTCH_HOME%/conf;%NUTCH_HOME%/nutch.jar\n", "Perl version of the control script, meant to work on both Windows and Unix like operating systems.  Has been tested on Windows 2000/XP/2003 Server, and OpenBSD", "I do not think we should have multiple versions of the command line tools, since that complicates maintenance.  A windows batch file is not portable, and is thus not a good candidate to replace the bash versions.  I also don't see that requiring perl is any better than requiring cygwin on windows, and I suspect even with Perl we'd probably require cygwin.  So, unless someone objects, I will close this issue.", "No offense, but you must not use windows.  While perl does not come with windows (just as it does not really come with many linux distributions), it is easily available for every system I have ever used - and available in binary form.  For instance, in windows, there is the free perl implimentation from ActiveWorks (ActivePerl).  Really, if you want full cross platform support, why is our command line tool not Java, considering that is a requirement to run the software anyway?  As for the whole \"just install cygwin\" thing, cygwin requires specific directory structures, and is not a quick, nor easy, download.", "I do in fact sometimes develop Nutch on windows.\n\nI would be happy if someone supplied a Java replacement for the command line tools.  That would indeed remove a dependency.  But I still don't see how requiring Perl simplifies things.  I'm also not much of a Perl programmer.  Are you willing to maintain translations of all of the scripts in nutch's bin directory as Perl?\n\nNote that the mapred branch has more scripts.  Also note that the mapred branch relies on the 'df' program to portably access the amount of free space on a volume.  Is there a portable Perl alternative?\n\nhttp://svn.apache.org/viewcvs.cgi/lucene/nutch/branches/mapred/bin/?rev=326780\n", "I think the problem here is that a bash script, by definition is not portable.  Yes, you can *emulate* a *sh shell on windows, using cygwin, and build any required tools (df, etc - it does not exist in ALL shell implimentations) - but it is still *emulation*.  In general, anything you can do in bash, you can do in perl, but it is not exactly simple - and sometimes requires perl modules, which I would agree is not a good thing.  I would suggest that someone (I will even look into it) write a java launching tool.  I see providing only a bash control script as restricting the use of nutch to people running *nix systems. \n\nThe only problem with java is that it needs to be compiled, and for something like a control script it is nice to be able to *simply* modify it.  Just a guess here, but doesn't java have a script interpreter built into it?", "I personally disagree Perl is a better alternative to Cygwin... Most people familiar with Unix/ Windows development will have no problems modifying a bash script, whereas a Perl script... hmm.. Perl is perl :) \n\nAs for a pure Java solution, I agree this would be handy. However, Java is quite a pain to invoke, especially with multiple JVM switches such as -Xmx... So you'd probably have to fall back to a 'boot' script anyway at some point. The only pure Java thing that comes to my mind is using ANT to spawn a JVM and then write commons-cli equivalents of command line tools... but this, as much as I hate to have platform-dependent scripts, seems like an overkill compared to the bash solution.", "Ant and Tomcat supply both Unix shell scripts and Windows batch files.  Neither uses Perl.  I am hesitant to go this two-implementation route, as Nutch's scripting requirements (especially with MapReduce) are greater than Ant or Tomcat.  Nutch's new scripts manage daemons on remote servers with ssh and rsync, supplied via cygwin on Windows.\n", "Another \"pure Java\" solution is to rewrite the \"nutch\" bash script in BeanShell (http://www.beanshell.org).\n\nI just took a quick (~1 hr) stab at this. The syntax seems quite agreeable, with many builtin versions of standard unix commands (cd(), cat(), etc). However, I quickly hit two barriers:\n\n1) Reading environment variables. System.getenv() works on 1.5, but is nonfunctional on Java 1.3 and The only workaround on 1.4 is what ant does: run a native command, read the\noutput, and set system properties.\n\n2) Setting -Xmx et al. My sense is that it's simply not possible.\n\nOther than these issues, it would be quite easy to rewrite all of the usage/command/path-building logic into a beanshell script. Then there could be two *small* scripts (bash & .bat) to handle the stuff that can't be done in Java, and one beanshell script for the rest. Does that seem useful? \n\nFYI, the core beanshell interpreter is ~143k.", "Hadoop depends on a number of Unix-like utilities, so for now using Cygwin is a must, and since Cygwin comes with shell I see little benefit in rewriting the scripts."], "tasks": {"summary": "Nutch Commands should run on Windows without external tools", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Nutch Commands should run on Windows without external tools"}, {"question": "What is the main context?", "answer": "Currently there is only a shellscript to run the Nutch commands. This should be platform independant.\nBest would be Ant tools, or scripts generated by a template tool to avoid replication."}]}}
{"issue_id": "NUTCH-820", "project": "NUTCH", "title": "Infinite loop when hitspersite is set", "status": "Closed", "priority": "Major", "reporter": "Xiao Yang", "assignee": null, "created": "2010-05-13T09:00:57.817+0000", "updated": "2011-06-08T21:34:24.362+0000", "description": "NutchBean will re-search over and over, when the page number become large and the excluded sites exceed MAX_PROHIBITED_TERMS.", "comments": ["Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Infinite loop when hitspersite is set", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Infinite loop when hitspersite is set"}, {"question": "What is the main context?", "answer": "NutchBean will re-search over and over, when the page number become large and the excluded sites exceed MAX_PROHIBITED_TERMS."}]}}
{"issue_id": "NUTCH-821", "project": "NUTCH", "title": "Use ivy in nutch builds", "status": "Closed", "priority": "Major", "reporter": "Enis Soztutar", "assignee": "Enis Soztutar", "created": "2010-05-13T18:05:30.957+0000", "updated": "2010-07-07T09:26:06.182+0000", "description": "Ivy is the de-facto dependency management tool used in conjunction with Ant. It would be nice if we switch to using Ivy in Nutch builds. \n\nMaven is also an alternative, but I think Nutch will benefit more with an Ant+Ivy architecture. ", "comments": ["+1 for Ivy.", "Are the Nutch artifacts published somewhere in a repository available for Maven or Ant+Ivy users?", "If your aim is publishing Nutch artifacts in a repository which is compatible with Maven as well as Ivy, you might consider using Maven Ant Tasks (http://maven.apache.org/ant-tasks/) (which IMHO has less pains on the publishing side).", "Ciao Paolo, the aim is not to publish Nutch artifacts but mainly to manage the dependencies. A simple and straightforward combination of Ant and Ivy will do fine for that.", "Is there any plan to publish Nutch artifacts in the future?", "I'm a Maven fan but haven't had time to work up a Nutch Maven build as of yet. :(", "I believe publishing nutch artifacts to maven repo is another issue. We can open a new issue if there is enough demand. I believe, ivy can generate the necessary pom's for publishing to maven. \n\n", "I'm attaching a patch which introduces ivy to nutch builds. It is a patch against current nutchbase code base hosted at github. \nWe can adopt this patch to current nutch trunk, but I see little point in doing so since the two code bases will merge eventually. \n\nThe way ivy patch works is as follows : \nThere are two configurations default and test. Test dependencies are managed at test configuration. \nlibraries not managed by ivy are still at their old locations ( for example \"nutch/lib/\" or \"nutch/src/plugin/clustering-carrot/lib/\") \nthe root ivy file is located at nutch/ivy/ivy.xml \nEach plugin has it's own ivy file, which is located at nutch/src/plugin/<plugin>/ivy.xml\nroot dependencies are downloaded to build/lib\nplugin dependencies are downloaded to build/plugins/<plugin>/\n\nI have upgraded some of the libraries in the process, where only a the release number changed only in minor number (for example lucene-core-2.4.0 -> 2.4.1 )\n\n", "Same \"pain\" you are experiencing with the Nutch dependencies which are not in the Maven repository will be experienced by Nutch users which might want to depend/reuse Nutch code in their project. \n\nBut, I do agree... publishing Nutch artifacts is another issue.", "Opened issue NUTCH-825, for publishing the artifacts. ", "Adds IVY support for dependencies\n\nThe lib/. dir is maintained and will be used to store dependencies which are not accessible via Ivy (e.g. GORA). The libs managed by Ivy are put in the directory build/lib. \n\nThis patch also differentiates the _build_ path from the _dist_ path.\n\n", "I think this patch refers to some parts that were already removed in NUTCH-837 ...\n\nAlso, it would be nice to have a target that sets up an Eclipse project - after this patch is applied the lib/ is nearly empty and you need to run build at least once to bring dependencies - this may be confusing.", "{QUOTE}\nI think this patch refers to some parts that were already removed in NUTCH-837 ...\n{QUOTE}\n\nI applied NUTCH-837 before but indeed it does remove references to parts deleted in NUTCH-837. Maybe I should have done it in a separate issue. \n\n{QUOTE}\nAlso, it would be nice to have a target that sets up an Eclipse project - after this patch is applied the lib/ is nearly empty and you need to run build at least once to bring dependencies - this may be confusing.\n{QUOTE}\n\nThe jars are put in the build/lib directory so this assumes that the project has been built in order to get the dependencies. I think there are resources in Eclipse for dealing with Ivy configurations. If anyone has any pointers they will be most welcome  ", "I found [http://ant.apache.org/ivy/ivyde/] which allows to manage Ivy dependencies in Eclipse. \nI had to rewrite ivy/ivy.xml to make the version numbers explicit as IvyDE was not able to load the properties in ivy/library.properties but it worked fine after that. The beauty of it is that we don't rely on the content of build/lib at all", "Guys,\n\nWhy have any libs in the lib dir at all? If we need to get the jars uploaded to Maven Central, we can do a one time upload, via the process here:\n\nhttp://maven.apache.org/guides/mini/guide-central-repository-upload.html\n\nWhich jars do we need to get into Central? I can take the lead on it as I've got some work to do there for Tika anyways.\n\nCheers,\nChris\n", "+1 to Chris. In fact, I would ask to piggyback Gora on this process for now and pushing Gora jars into this repository as well :)", "+1 for maven, also having HBase in there would be great (=", "@Chris : isn't this restricted to the jars *we* produce? I agree with Dogacan that this would be the right way to access the Gora jars \n\nSome of the plugins have their own lib directory as well as ivy dependencies. The plugins currently have unpublished dependencies on : automaton.jar, javaswf.jar, common-feedparser-0.6-fork.jar. Given that some of these plugins will be removed at some stage (feed & parse-swf moving to Tika) it is probably not worth bothering and we can keep them in the plugin libs.\n\nAs for the main lib/ directory it currently contains only the native jars for Hadoop but the Gora related jars would have to go there as well unless we put them into Central as you suggested. We should probably discuss how to deal with Hadoop related resources (native jars, conf objects, scripts in bin) in a separate JIRA and whether or not we should keep them at all.\n\nCould you guys review the patch I sent yesterday? Since we'd decide what to do with the Hadoop native jars later and do not need the gora jars just yet it can still be applied as it is\n\n\n\n", "Hi Julien:\n\nI reviewed your patch, and am +1 for you to commit it. That said, I wanted to clarify my proposal. My proposal is that we get rid of the $NUTCH/lib directory in its entirety, I wasn't so much as commenting on the plugin jars, though now that you bring it up, maybe we could integrate ivy in them as well. I don't think it would be too super hard to get the jars into Maven central. I'm reading up on the Sonatype Forge right now and might be able to get this working.\n\nAs for Hadoop, is there any requirement that we manage the jar lib for Hadoop in Nutch? Couldn't we simply pull the jar down dynamically via ivy or via some magic in build.xml? I think if we could then we could simply remove $NUTCH/lib whic was my original intention.\n\nIn the meanwhile, I'll create another issue to track all this, but you have my +1 to commit your patch and mark this issue as resolved. I'll link the new issue I create back to this one to indicate their relationship to one another.\n\nCheers,\nChris\n", "+1 for this patch for now - all good comments, there's plenty of improvements we can make, so let's line them up as separate issues.", "Committed revision 961306 and 961318\n\nSlightly improved management of plugins since the patch to make them use the common ivysettings + removed ivy/library.properties to get IvyDE to work properly\n\nThanks for the comments and review of the patch"], "tasks": {"summary": "Use ivy in nutch builds", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Use ivy in nutch builds"}, {"question": "What is the main context?", "answer": "Ivy is the de-facto dependency management tool used in conjunction with Ant. It would be nice if we switch to using Ivy in Nutch builds. \n\nMaven is also an alternative, but I think Nutch will benefit "}]}}
{"issue_id": "NUTCH-822", "project": "NUTCH", "title": "DOAP file still refers to Lucene", "status": "Closed", "priority": "Minor", "reporter": "Sebb", "assignee": "Andrzej Bialecki", "created": "2010-05-13T22:29:55.459+0000", "updated": "2013-05-22T03:54:50.745+0000", "description": "The Nutch DOAP file still refers to Lucene, so Nutch is still listed as a Lucene sub-project in the http://projects.apache.org/ listings", "comments": ["I just updated the file. We need to wait some time until that page is regenerated. I'm leaving this issue open for now - the downloads are still located under lucene/nutch.", "The PMC is now correct on the projects page, thanks.", "I too can't see anything wrong any more. Can this issue be closed?", "- this was fixed by ab in r944193"], "tasks": {"summary": "DOAP file still refers to Lucene", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "DOAP file still refers to Lucene"}, {"question": "What is the main context?", "answer": "The Nutch DOAP file still refers to Lucene, so Nutch is still listed as a Lucene sub-project in the http://projects.apache.org/ listings"}]}}
{"issue_id": "NUTCH-823", "project": "NUTCH", "title": "Download page should not have pointer to nightly builds", "status": "Closed", "priority": "Minor", "reporter": "Sebb", "assignee": "Chris A. Mattmann", "created": "2010-05-14T15:37:00.391+0000", "updated": "2010-08-08T12:29:29.698+0000", "description": "The download page\n\nhttp://www.apache.org/dist/lucene/nutch/\n\nhas a pointer to nightly builds. These are not supposed to be advertised to the general public, see:\n\nhttp://www.apache.org/dev/release.html#what\n\n\"Do not include any links on the project website that might encourage non-developers to download and use nightly builds, snapshots, release candidates, or any other similar package. \"", "comments": ["- since the site was updated, this issue is fixed.", "The old directory under lucene is now empty, however:\n\nhttp://www.apache.org/dist/nutch/\n\nstill contains a link to nightly builds, so the problem has not been resolved - just moved.", "Hi Sebb:\n\nOK, took care of it this time, looks like for good. Here are the contents of the Nutch HEADER.html file:\n\n{noformat}\n# more HEADER.html \n<h1><a href=\"http://nutch.apache.org/\">Nutch</a> Releases</h1>\n\n<p>Please make sure you're downloading from <a\nhref=\"http://www.apache.org/dyn/closer.cgi/nutch/\">a nearby\nmirror site</a>, not from www.apache.org.</p>\n{noformat}\n\nThe mirrors should pick up the updated HEADER.html shortly...\n\nCheers,\nChris", "Thanks"], "tasks": {"summary": "Download page should not have pointer to nightly builds", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Download page should not have pointer to nightly builds"}, {"question": "What is the main context?", "answer": "The download page\n\nhttp://www.apache.org/dist/lucene/nutch/\n\nhas a pointer to nightly builds. These are not supposed to be advertised to the general public, see:\n\nhttp://www.apache.org/dev/release.htm"}]}}
{"issue_id": "NUTCH-824", "project": "NUTCH", "title": "Crawling - File Error 404 when fetching file with an hexadecimal character in the file name.", "status": "Closed", "priority": "Major", "reporter": "Michela Becchi", "assignee": "Julien Nioche", "created": "2010-05-20T20:26:56.183+0000", "updated": "2011-06-25T12:53:50.132+0000", "description": "Hello,\n\nI am performing a local file system crawling.\nMy problem is the following: all files that contain some hexadecimal characters in the name do not get crawled.\n\nFor example, I will see the following error:\n\nfetching file:/nutch-1.0/wikidump/wiki-en/en/articles/a/2E/m/A.M._%28album%29_8a09.html\norg.apache.nutch.protocol.file.FileError: File Error: 404\n        at org.apache.nutch.protocol.file.File.getProtocolOutput(File.java:92)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:535)\nfetch of file:/nutch-1.0/wikidump/wiki-en/en/articles/a/2E/m/A.M._%28album%29_8a09.html failed with: org.apache.nutch.protocol.file.FileError: File Error: 404\n\nI am using nutch-1.0.\n\nAmong other standard settings, I configured nutch-site.conf as follows:\n\n<property>\n  <name>plugin.includes</name>\n  <value>protocol-file|protocol-http|urlfilter-regex|parse-(text|html|js|pdf)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)</value>\n  <description>Regular expression naming plugin directory names to\n  include.  Any plugin not matching this expression is excluded.\n  In any case you need at least include the nutch-extensionpoints plugin. By\n  default Nutch includes crawling just HTML and plain text via HTTP,\n  and basic indexing and search plugins. In order to use HTTPS please enable\n  protocol-httpclient, but be aware of possible intermittent problems with the\n  underlying commons-httpclient library.\n  </description>\n</property>\n\n<property>\n  <name>file.content.limit</name>\n  <value>-1</value>\n</property>\n\nMoreover, crawl-urlfilter.txt   looks like:\n\n# skip http:, ftp:, & mailto: urls\n-^(http|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe|jpeg|JPEG|bmp|BMP)$\n\n# skip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n\n# skip URLs with slash-delimited segment that repeats 3+ times, to break loops\n-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/\n\n# accept hosts in MY.DOMAIN.NAME\n#+^http://([a-z0-9]*\\.)*MY.DOMAIN.NAME/\n\n# accept everything else\n+.*\n~    \n\n---\n\nThanks,\n\nMichela\n", "comments": ["Changed prio to major. \n\nThe problem occurs when the files are not hexa encoded e.g. A.M._(album)_8a09.html\n\n", "Hi,\n\nI fixed (or, at least, circumvented) this by modifying the org/apache.nutch.protocol.file.FileResponse class belonging to the protocol-file plugin.\n\nIn particular, at line 120, I added\n\n120     String path = \"\".equals(url.getPath()) ? \"/\" : url.getPath();\n121     +String decoded_path = path;  //@Michela \n122 \n123     +try {\n124     +    decoded_path=java.net.URLDecoder.decode(path,\"UTF-8\");\n125     +}catch(Exception ex){\n126     +}\n\nThen, rather than\n\n- java.io.File f = new java.io.File(path);\n\nI have\n\n+ java.io.File f = new java.io.File(decoded_path);\n\nThanks,\n\nMichela", "Hi,\n\nI have just downloaded the nutch-1.2 release and it seems to me that the fix above is not included in the protocol-file plugin.\n\nI am new to the ways of the Jira, but I interpret the information above as saying that the fix should have been included already from the 1.0 release. Have I misunderstood something?\n\nThanks,\nNikolaj", "You're correct, no patch has been submitted and it's not in the source, it shouldn't been set to resolved. Can someone submit a patch for trunk and 1.3. The code is similar to the 1.0 version but is a bit different between trunk and 1.2!", "Have reactivated the tests for protocol-file in 1.3 and reorganised the test documents to follow other plugins i.e. test docs in sample dir\nProtocol-file now decodes the input URLs with UTF-8\n\ntrunk : Committed revision 1056394\n1.3 : Committed revision 1056401\n\nThanks Michela", "Bulk close of resolved issues for 1.3."], "tasks": {"summary": "Crawling - File Error 404 when fetching file with an hexadecimal character in the file name.", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Crawling - File Error 404 when fetching file with an hexadecimal character in the file name."}, {"question": "What is the main context?", "answer": "Hello,\n\nI am performing a local file system crawling.\nMy problem is the following: all files that contain some hexadecimal characters in the name do not get crawled.\n\nFor example, I will see the follo"}]}}
{"issue_id": "NUTCH-825", "project": "NUTCH", "title": "Publish nutch artifacts to central maven repository", "status": "Closed", "priority": "Major", "reporter": "Enis Soztutar", "assignee": "Chris A. Mattmann", "created": "2010-05-21T09:09:41.641+0000", "updated": "2018-08-30T12:27:41.986+0000", "description": "As per the discussion at NUTCH-821, publishing nutch artifacts to maven will be nice. NUTCH-821 already introduces dependency management with ivy. As for the remaining, ant task for generating pom files should be developed, and artifacts should be published to maven repo by a committer after a release. ", "comments": ["Publishing the Nutch artifacts is beneficial not only to Maven users, but also to Ivy ones as well as all the others tools which are compatible with the de-facto Maven repository layout conventions.\n\nOther Apache projects are using Ant+Ivy and, at the same time, publishing artifacts (or aiming to do it) to the Maven central repository.\n\nPerhaps, it would be worth considering the pros/cons of Ant+Ivy vs. Ant+Maven respect to \"publishing\" (i.e. beneficial to Nutch users) in addition to the dependency management (i.e. beneficial to Nutch developer).\n\nIMHO, as I have said, in NUTCH-821... for publishing, Ant+Maven might be a simpler solution.", "- I'll try and get this fixed for 2.0", "Once INFRA-3098 is taken care of, I can proceed with staging the Nutch artifacts to repository.apache.org for Maven Central sync.", "- fix committed to trunk in r1028294. The fix includes the following ant targets:\n\n{code}release{code}\nBuilds the -javadoc.jar, -sources.jar and artfiact.jar to deploy to Apache Nexus.\n\n{code}deploy{code}\nDeploys the 3 release artifacts by signing them, checkusmming them, and then pushing them to the Apache Nexus staging repository. After that, the release manager should follow the Nexus process to close the staging repository, and then to release it if everything looks well.\n\nThese changes require the release manager to do the following things:\n\n1. create an apache-release profile in their $HOME/.m2/settings.xml file. It should contain the following information:\n\n{code}\n<settings>\n   <servers>\n    <!-- To publish a snapshot of some part of Maven -->\n    <server>\n      <id>apache.snapshots.https</id>\n      <username>username</username>\n      <password>password</password>\n    </server>\n    <!-- To publish a website of some part of Maven -->\n    <server>\n      <id>apache.website</id>\n      <username>username</username>\n      <filePermissions>664</filePermissions>\n      <directoryPermissions>775</directoryPermissions>\n    </server>\n    <!-- To stage a release of some part of Maven -->\n    <server>\n      <id>apache.releases.https</id>\n      <username>username</username>\n      <password>password</password>\n    </server>\n  ...\n  </servers>\n  <profiles>\n      <profile>\n        <id>apache-release</id>\n        <properties>\n          <gpg.passphrase>passphrase</gpg.passphrase>\n        </properties>\n      </profile>\n  ..\n </profiles>\n</settings>\n{code}\n\n2. Install the Maven Ant tasks plugin in your $ANT_HOME/lib directory. You can get the plugin here: http://maven.apache.org/ant-tasks/index.html\n\nNote, publishing to Apache Nexus doesn't mean you are automatically publishing to Maven Central. Maven Central syncs from Nexus (repository.apache.org) on some regular interval so Central will pick up the syncs, but at a later date/time usually.\n\nAdditionally: I didn't change the build to use Maven, so those Ant guys out there can sleep soundly :) I needed to add the pom.xml and Maven ant tasks to integrate Maven's easy publishing ability to Maven Central into our Ant build, using the Sonatype guide here: https://docs.sonatype.org/display/Repository/Sonatype+OSS+Maven+Repository+Usage+Guide\n", "Right now, you can see the Nutch 2.0-dev jars published here:\n\nhttps://repository.apache.org/content/groups/public/org/apache/nutch/nutch/2.0-dev/\n\nThey should be sync'ed to Central shortly. ", "I'm also in the process of integrating Nutch into a personal Maven based project. I've been looking at the repository and I see that only Nutch 2.0 has Maven support, so I've been trying to use that (unsuccessfully). \nThe issue seems to be the gora-core and gora-sql dependencies, which I cannot find anywhere. I've looked in the Apache Nexus instance (including Apache snapshots) and still nothing. \nAlso, I'm using both Maven 2.2.1 as well as Maven 3.0.1 to build. The errors are different (Maven 3.3.1 fails faster). \nI understand that this is the trunk and things are not meant to be stable, but this seems to be more than just stability, as I am effectively unable to either build it. \nOf course, I may be missing something. \nAny feedback is appreciated. \n\nI will attach the Maven output (although it's pretty standard): \n\n\n\n[INFO] ------------------------------------------------------------------------\n[ERROR] BUILD ERROR\n[INFO] ------------------------------------------------------------------------\n[INFO] Failed to resolve artifact.\n\nMissing:\n----------\n1) org.gora:gora-core:jar:0.1\n\n  Try downloading the file manually from the project website.\n\n  Then, install it using the command: \n      mvn install:install-file -DgroupId=org.gora -DartifactId=gora-core -Dversion=0.1 -Dpackaging=jar -Dfile=/path/to/file\n\n  Alternatively, if you host your own repository you can deploy the file there: \n      mvn deploy:deploy-file -DgroupId=org.gora -DartifactId=gora-core -Dversion=0.1 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n\n  Path to dependency: \n  \t1) org.apache.nutch:nutch:jar:2.0-dev\n  \t2) org.gora:gora-core:jar:0.1\n\n2) org.gora:gora-sql:jar:0.1\n\n  Try downloading the file manually from the project website.\n\n  Then, install it using the command: \n      mvn install:install-file -DgroupId=org.gora -DartifactId=gora-sql -Dversion=0.1 -Dpackaging=jar -Dfile=/path/to/file\n\n  Alternatively, if you host your own repository you can deploy the file there: \n      mvn deploy:deploy-file -DgroupId=org.gora -DartifactId=gora-sql -Dversion=0.1 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n\n  Path to dependency: \n  \t1) org.apache.nutch:nutch:jar:2.0-dev\n  \t2) org.gora:gora-sql:jar:0.1\n\n----------\n2 required artifacts are missing.\n\nfor artifact: \n  org.apache.nutch:nutch:jar:2.0-dev\n\nfrom the specified remote repositories:\n  central (http://repo1.maven.org/maven2)\n\n\n\n\n\nThanks. \nEugen. \n\n", "Hi Eugen,\n\nThe Nutch portion of this is effectively done. We need to get Gora published to Central, but that will involve making a release of Gora. Note the JAR I published was really a test (hence the -dev to note in development). But, the mechanism is in place to push Nutch to Central. \n\nWe'll work the Gora portions as well...\n\nCheers,\nChris\n", "What about Nutch plugins?"], "tasks": {"summary": "Publish nutch artifacts to central maven repository", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Publish nutch artifacts to central maven repository"}, {"question": "What is the main context?", "answer": "As per the discussion at NUTCH-821, publishing nutch artifacts to maven will be nice. NUTCH-821 already introduces dependency management with ivy. As for the remaining, ant task for generating pom fil"}]}}
{"issue_id": "NUTCH-826", "project": "NUTCH", "title": "Mailing list is broken.", "status": "Closed", "priority": "Blocker", "reporter": "John Sherwood", "assignee": "Julien Nioche", "created": "2010-05-24T06:29:11.544+0000", "updated": "2011-04-01T15:07:19.732+0000", "description": "All of the following addresses are failing:\n\nnutch-user@nutch.apache.org\nnutch-user-subscribe@nutch.apache.org\nnutch-user-subscribe@lucene.apache.org\n\nFor the last one, the mailer daemon said \n\"This mailing list has moved to user at nutch.apache.org.\"\n\nBelow is the message I tried to send:\n\nHi people,\n\nI've been banging my head against this problem for two days now.\nSimply, I want to add a field with the value of a given meta tag.\n\nI've been trying the parse-xml plugin, but that seems that it doesn't\nwork with version 1.0.  I've tried the code at\nhttp://sujitpal.blogspot.com/2009/07/nutch-getting-my-feet-wet.html\nand it hasn't worked.  I don't even know why.  I don't even know if my\nplugin is being used... or even looked for!  Nutch seems to have a\ninfuriating \"Fail silently\" policy for plugins.  I put a\nSystem.exit(1) in my filters just to see if my code is even being\nencountered.  It has not in spite of my config telling it to.\n\nHere's my config:\nnutch-site.xml\n...\n<property>\n <name>plugin.includes</name>\n <value>protocol-http|urlfilter-regex|parse-html|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|metadata</value>\n</property>\n...\n\nparse-plugins.xml\n...\n<mimeType name=\"application/xhtml+xml\">\n   <plugin id=\"parse-html\" />\n   <plugin id=\"metadata\" />\n</mimeType>\n\n\n<mimeType name=\"text/html\">\n      <plugin id=\"parse-html\" />\n      <plugin id=\"metadata\" />\n</mimeType>\n\n<mimeType name=\"text/sgml\">\n      <plugin id=\"parse-html\" />\n      <plugin id=\"metadata\" />\n</mimeType>\n\n<mimeType name=\"text/xml\">\n         <plugin id=\"parse-html\" />\n         <plugin id=\"parse-rss\" />\n        <plugin id=\"metadata\" />\n        <plugin id=\"feed\" />\n</mimeType>\n...\n<alias name=\"metadata\"\nextension-id=\"com.example.website.nutch.parsing.MetaTagExtractorParseFilter\"\n/>\n...\n\nI've also copied the plugin.xml and jar from my build/metadata to the\nplugins root dir.\n\nNonetheless, Nutch runs and puts data in solr for me.  Afaik, Nutch is\ncompletely unaware of my plugin despite my config options.  Is the\nsome other place I need to tell Nutch to use my plugin?  Is there some\nother approach to do this without having to write a plugin?  This does\nseem like a lot of work to simply get a meta tag into a field.  Any\nhelp would be appreciated.\n\nSincerely,\n\nJohn Sherwood", "comments": ["Nutch has recently become a TLP and some of the info on the website needs updating.\n\nTo subscribe to the list, send a message to:\n  <user-subscribe@nutch.apache.org>\n\nTo remove your address from the list, send a message to:\n  <user-unsubscribe@nutch.apache.org>\n\nSend mail to the following for info and FAQ for this list:\n  <user-info@nutch.apache.org>\n  <user-faq@nutch.apache.org>\n\nPS : this is hardly a blocker ", "Committed revision 947569.\n\nThe changes should be visible on the website within a few hours", "Integrated in Nutch-trunk #1163 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1163/])\n    NUTCH-826 : update mailing list and version control pages on wesite after move to TLP\n", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Mailing list is broken.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Mailing list is broken."}, {"question": "What is the main context?", "answer": "All of the following addresses are failing:\n\nnutch-user@nutch.apache.org\nnutch-user-subscribe@nutch.apache.org\nnutch-user-subscribe@lucene.apache.org\n\nFor the last one, the mailer daemon said \n\"This m"}]}}
{"issue_id": "NUTCH-827", "project": "NUTCH", "title": "HTTP POST Authentication", "status": "Closed", "priority": "Minor", "reporter": "Jasper van Veghel", "assignee": "Lewis John McGibbney", "created": "2010-05-27T12:08:10.191+0000", "updated": "2024-03-13T14:51:05.069+0000", "description": "I've created a patch against the trunk which adds support for very rudimentary POST-based authentication support. It takes a link from nutch-site.xml with a site to POST to and its respective parameters (username, password, etc.). It then checks upon every request whether any cookies have been initialized, and if none have, it fetches them from the given link.\n\nThis isn't perfect but Works For Me (TM) as I generally only need to retrieve results from a single domain and so have no cookie overlap (i.e. if the domain cookies expire, all cookies disappear from the HttpClient and I can simply re-fetch them). A natural improvement would be to be able to specify one particular cookie to check the expiration-date against. If anyone is interested in this beside me I'd be glad to put some more effort into making this more universally applicable.", "comments": ["Could you possible say exactly where the username and password need to go? I presently have these in runtime/local/conf/httpclient-auth.xml, but this doesn't seem to work. Also, what url needs to go in the nutch-site.xml file? ", "20120304-push-1.6", "Jasper, if you're still around, can you please answer Ian's question? I have a similar problem figuring out how exactly I can provide the username and password.\n\nThank you!", "Hey guys,\n\nThis has been some time back, but take a look at the patch:\n\nnutch-default.xml ..\n\n<name>http.cookie.login.page</name>\n<description>URL of the login page to derive the cookies from. Cookies will be stored upon initialization and re-initialized upon expiration. Any URL request attributes will be [..] POSTed to the page. [..]</description>\n\nApologies for the poor grammar in the original. ;-) Basically:\n\n- Whenever protocol-httpclient performs an HTTP request, it will first check if there are cookies stored in the cookie jar.\n- If there are cookies in the cookie jar AND none of the cookies have expired, it will do nothing.\n- If there are no cookies in the cookie jar OR at least one of the cookies has expired, it will ..\n- POST the URL / parameters provided in \"http.cookie.login.page\" property\n- In the process of which, the cookie jar should get filled with the cookies you need to perform subsequent (authenticated) requests\n\nThe \"http.cookie.login.page\" property could contain something like \"http://abc/def?username=foo&password=bar\" .. the 'username' and 'password' properties will them be POSTed to 'http://abc/def', which should result in cookies being added to the cookie jar which is used for each subsequent request.\n\nThis isn't exactly a fool-proof solution (what if other requests generate expired cookies? what if the login fails? etc.), but for the project for which I wrote the patch, it suited our needs. Hope it helps!", "Hi Jasper,\n\nThanks a lot for the explanation! I applied the patch and compiled Nutch just fine, but can't confirm that it is working. Can you point to a website that this patch worked to pass the form auth at? I need to verify that it is working for me, but can't at the moment.\n\n\nThanks in advance,\nMax", "Hi Max,\n\nI'm sorry, but I don't really use the patch anymore, so I wouldn't be able to tell you. We used it in conjunction with an internal SAP system that we needed to spider, so that's not a public source you could try it against. Why not write up your own quick script which lets you POST some data, sets a cookie, and then returns some specific piece of data only when that cookie is set?\n\nGood luck!\n\nJasper", "Hi Jasper,\n\nThank you for the tip! I'll have to go that way then.\n\n\nBest regards,\nMax", "Hi Jasper,\n\nI've set up a script that does just that (receives POSTed data, sets a cookie, returns some data if the cookie is set), but now I have this error in my log:\n\n2012-10-01 13:11:24,557 ERROR httpclient.Http - Cookie-based authentication failed; cookies will not be present for this request but an attempt to retrieve them will be made for the next one.\n2012-10-01 13:11:24,682 ERROR httpclient.Http - Unable to retrieve login page; code = 200\n\nThe second line with response code 200 is what I don't understand. I'd appreciate any tips you could give in this regard.\n\n\nThanks,\nMax", "Looks like a pretty sloppy mistake in the patch .. ;-)\n\n{code}\n+      if (code == 200 && Http.LOG.isTraceEnabled()) {\n+        Http.LOG.trace(\"url: \" + url +\n+            \"; status code: \" + code +\n+            \"; cookies received: \" + Http.getClient().getState().getCookies().length);\n+      } else {\n+          Http.LOG.error(\"Unable to retrieve login page; code = \" + code);\n+      }\n{code}\n\nChange that to something like ..\n\n{code}\n+      if (code == 200 && Http.LOG.isTraceEnabled()) {\n+        Http.LOG.trace(\"url: \" + url +\n+            \"; status code: \" + code +\n+            \"; cookies received: \" + Http.getClient().getState().getCookies().length);\n+      } else if (code != 200) {\n+          Http.LOG.error(\"Unable to retrieve login page; code = \" + code);\n+      }\n{code}\n\nAnd also change this ..\n\n{code}\n+          LOG.error(\"Cookie-based authentication failed; cookies will not be present for this request but an attempt to retrieve them will be made for the next one.\");\n{code}\n\nTo something like this ..\n\n{code}\n+          LOG.error(\"Cookie-based authentication failed; cookies will not be present for this request but an attempt to retrieve them will be made for the next one.\", e);\n{code}\n\nTo see where the Exception is coming from. All it does after that LOG.error() is release the connection. So it shouldn't be throwing an Exception.", "Now I get the following error:\n\n2012-10-01 14:40:54,996 ERROR httpclient.Http - Cookie-based authentication failed; cookies will not be present for this request but an attempt to retrieve them will be made for the next one.\njava.lang.IllegalArgumentException: Entity enclosing requests cannot be redirected without user intervention\n\tat org.apache.commons.httpclient.methods.EntityEnclosingMethod.setFollowRedirects(EntityEnclosingMethod.java:225)\n\tat org.apache.nutch.protocol.httpclient.HttpCookieAuthentication.<init>(HttpCookieAuthentication.java:73)\n\tat org.apache.nutch.protocol.httpclient.Http.resolveCookieCredentials(Http.java:402)\n\tat org.apache.nutch.protocol.httpclient.Http.resolveCredentials(Http.java:387)\n\tat org.apache.nutch.protocol.httpclient.Http.getResponse(Http.java:152)\n\tat org.apache.nutch.protocol.http.api.RobotRulesParser.getRobotRulesSet(RobotRulesParser.java:440)\n\tat org.apache.nutch.protocol.http.api.RobotRulesParser.getRobotRulesSet(RobotRulesParser.java:425)\n\tat org.apache.nutch.protocol.http.api.HttpBase.getRobotRules(HttpBase.java:403)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:668)\n\nSorry to bug you about this...\n\n\nThanks for your time!\nMax", "That exception looks familiar — I think that we ended up solving that simply by removing ..\n\n{code}\n+    method.setFollowRedirects(followRedirects);\n{code}\n\nAs redirects are not supported for POST-requests.", "Hi Jasper,\n\nThanks, removing that line fixed the exception problem.\nAt the moment, the log file doesn't have any errors related to HTTPclient plugin or authentication process. However, my tests show that the cookie can't be read by the test auth page I've set up.\n\nIs there an easy way to verify if the cookie was created by Nutch and stored as intended?\n\n\nThanks,\nMax", "{code}\n+        Http.LOG.trace(\"url: \" + url +\n+            \"; status code: \" + code +\n+            \"; cookies received: \" + Http.getClient().getState().getCookies().length);\n{code}\n\nIf you turn on TRACE logging, you should see messages like that.", "Thank you, Jasper. I did just that and now I see that cookies are received (at least in some cases).\n\nDo you know of any reason why I still wouldn't be able to retrieve pages that require authentication (even though I see the cookies stored)? Does it have to do with those pages returning status code \"200\"?\n\nThanks for the help!", "I was assigned a task to use nutch2 to crawla web site which uses form-based authentication.\nBased on Jasper's code, I made some improvement to make it work. Please view the patch: http-client-form-authtication.patch.\n\nTo use it, first we try to figure it out how to use http client to do form based login successfully, We can use Chrome Devtools to get the login formId, username and password fields, get the exact post request; we may remove some form fields, or add some headers.\n{code:borderStyle=solid}\nprivate static void authTestAspWebApp() throws Exception, IOException {\n  HttpFormAuthConfigurer authConfigurer = new HttpFormAuthConfigurer();\n  authConfigurer.setLoginUrl(\"http://localhost:44444/Account/Login.aspx\")\n    .setLoginFormId(\"ctl01\").setLoginRedirect(true);\n  Map<String, String> loginPostData = new HashMap<String, String>();\n  loginPostData.put(\"ctl00$MainContent$LoginUser$UserName\", \"admin\");\n  loginPostData.put(\"ctl00$MainContent$LoginUser$Password\", \"admin123\");\n  authConfigurer.setLoginPostData(loginPostData);\n \n  Set<String> removedFormFields = new HashSet<String>();\n  removedFormFields.add(\"ctl00$MainContent$LoginUser$RememberMe\");\n  authConfigurer.setRemovedFormFields(removedFormFields);\n \n  HttpFormAuthentication example = new HttpFormAuthentication(\n    authConfigurer);\n  example.login();\n  String result = example\n    .httpGetPageContent(\"http://localhost:44444/secret/needlogin.aspx\");\n  System.out.println(result);\n }\n{code}\n\nAfter make the test code work, we define form authentication info in httpclient-auth.xml:\n{code:xml}\n<?xml version=\"1.0\"?>\n<auth-configuration>\n  <credentials authMethod=\"formAuth\" loginUrl=\"http://localhost:44444/Account/Login.aspx\" loginFormId=\"ctl01\" loginRedirect=\"true\">\n    <loginPostData>\n      <field name=\"ctl00$MainContent$LoginUser$UserName\" value=\"admin\"/>\n      <field name=\"ctl00$MainContent$LoginUser$Password\" value=\"admin123\"/>\n    </loginPostData>\n    <removedFormFields>\n      <field name=\"ctl00$MainContent$LoginUser$RememberMe\"/>\n    </removedFormFields>\n  </credentials>\n</auth-configuration>\n{code}\n\nBe sure to use protocol-httpclient plugin in nutch-site.xml: not protocol-http.\nIf you are interested, you may read:http://lifelongprogrammer.blogspot.com/2014/02/part1-using-apache-http-client-to-do-http-post-form-authentication.html", "I am working on this issue as I require form-based authentication for a current research task.", "Patch for trunk.\nThis has been tested and verified to enable access to various large Databases requiring HTTP Post authentication. I also would like to mention that setting the redirect boolean flag to true is usually always required.\nWould really appreciate if folks could try this out and comment.", "Looks promising: \n- successfully crawled one protected site\n- a second trial failed because the form element is referenced via \"name\" attribute instead of \"id\". That's obviously ok, maybe old-style/deprecated (cf.  [[1|http://www.w3schools.com/tags/att_form_name.asp]], [[2|https://developer.mozilla.org/de/docs/Web/HTML/Element/form#Attributes]]). I'll continue this trial to provide a fix/work-around.\n- log level TRACE should provide sufficient information what goes wrong when logging in\n- config file to be committed should be {{conf/httpclient-auth.xml.template}} instead of {{conf/httpclient-auth.xml}}", "[~wastl-nagel] fantastic, thanks\nbq. a second trial failed because the form element is referenced via \"name\" attribute instead of \"id\". That's obviously ok, maybe old-style/deprecated (cf. [1], [2]). I'll continue this trial to provide a fix/work-around.\nAh... possibly try id, if empty try name?\n.bq log level TRACE should provide sufficient information what goes wrong when logging in\n+1\nbq. config file to be committed should be conf/httpclient-auth.xml.template instead of conf/httpclient-auth.xml\n+1, patch coming up\nThanks for review\n\n", "Updated patch for trunk which takes on [~wastl-nagel]'s comments.\n * I've moved the additions to httpclient-auth.xml to httpclient-auth.xml.template\n * I've also added some primative checking for form 'name' if we cannot locate an 'id'\n\n{code}\n    Element loginform = doc.getElementById(authConfigurer.getLoginFormId());\n    if (loginform == null) {\n      LOGGER.debug(\"'id' attribute for form element is null, trying 'name'.\");\n      loginform = doc.select(\"form.answer[name=\"+ authConfigurer.getLoginFormId() + \"]\").first();\n      if (loginform == null) {\n        LOGGER.debug(\"'name' attribute for form element is also null.\");\n        throw new IllegalArgumentException(\"No form exists: \"\n            + authConfigurer.getLoginFormId());\n      }\n    }\n{code}\n\nThe rest seem to be OK to me and I am able to use this patch to fetch content from secure databases.", "Would be great to commit and get in to 1.10", "Hi [~lewismc], attached patch fixes two points\n* the CSS statement to select of \"form\" elements by \"name\" attribute didn't work properly\n* (should be documented) the configuration allows to set <additionalPostHeaders>, e.g.\n{noformat}\n <additionalPostHeaders>\n   <field name=\"User-Agent\"\n          value=\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:35.0) Gecko/20100101 Firefox/35.0\" />\n </additionalPostHeaders>\n{noformat}\n\nOne point is open (but we could delay it, it may take some work):\n* the form authentication is global and ignores {{<authScope>}}. So you have to restrict your crawl to the form authentication pages only. Ideally, also form authentication should be bound to a scope (one host, one URL prefix, etc.) same as HTTP authentication.\n", "Fantastic [~wastl-nagel]\nI will commit this patch and log an issue to accommodate and address your final suggestion (and an excellent one it is too!).\nThanks Seb.", "Committed @revision 1659697 in trunk\nThank you to everyone involved. All credited in CHANGES", "part 2 (new files) Committed @revision 1659701", "SUCCESS: Integrated in Nutch-trunk #2976 (See [https://builds.apache.org/job/Nutch-trunk/2976/])\nNUTCH-827 HTTP POST Authentication (lewismc: http://svn.apache.org/viewvc/nutch/trunk/?view=rev&rev=1659701)\n* /nutch/trunk/src/plugin/protocol-httpclient/src/java/org/apache/nutch/protocol/httpclient/HttpFormAuthConfigurer.java\n* /nutch/trunk/src/plugin/protocol-httpclient/src/java/org/apache/nutch/protocol/httpclient/HttpFormAuthentication.java\nNUTCH-827 HTTP POST Authentication (lewismc: http://svn.apache.org/viewvc/nutch/trunk/?view=rev&rev=1659697)\n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/conf/httpclient-auth.xml.template\n* /nutch/trunk/src/plugin/protocol-httpclient/ivy.xml\n* /nutch/trunk/src/plugin/protocol-httpclient/plugin.xml\n* /nutch/trunk/src/plugin/protocol-httpclient/src/java/org/apache/nutch/protocol/httpclient/Http.java\n", "[~lewismc], I think the new template wasn't included in the commit.", "Hi [~tpalsulich], what is meant by \"new template\"? The file \"conf/httpclient-auth.xml.template\" looks ok. In case of running Nutch from dev enviroment the file  \"conf/httpclient-auth.xml\" needs to be replaced (or merged) by the template. It's not automatically updated/overwritten (which is ok, in case there are local changes).", "Ahh. My mistake. You're right. Thanks!", "Seems this works until I got a cookies with a $Domain=\".domain.com\". The \".\" at the beginning cause the Cookie rejected:\n{code}\nWARN  httpclient.HttpMethodBase - Cookie rejected: \"$Version=0; FORMCRED=itIsL35rD+qWVYeq3Gc7l5nWKAYx3Pz/YpsjEX86ftJlta; $Path=/; $Domain=.domain.com\". Domain attribute \".domain.com\" violates RFC 2109: host minus domain may not contain any dots\n{code}\nI think i can fix it with adding a CookiePolicy setting configuration item in to the login method. Thought?\n{code:java}\npublic void login() throws Exception {\n    // make sure cookies are turned on\n    CookieHandler.setDefault(new CookieManager());\n    // And the cookies policy could be changed here...\n    String pageContent = httpGetPageContent(authConfigurer.getLoginUrl());\n    List<NameValuePair> params = getLoginFormParams(pageContent);\n    sendPost(authConfigurer.getLoginUrl(), params);\n  }\n{code}", "It turns out this cookie policy should be changed in HTTP.java\n", "Hi [~stevegy], would you mind to open a new Jira for this problem? Thanks!", "Sure. I will create a new issue for this one.", "Bulk progression of all “Resolved” issues to “Closed”. Performed my lewismc on 2024-03-13."], "tasks": {"summary": "HTTP POST Authentication", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "HTTP POST Authentication"}, {"question": "What is the main context?", "answer": "I've created a patch against the trunk which adds support for very rudimentary POST-based authentication support. It takes a link from nutch-site.xml with a site to POST to and its respective paramete"}]}}
{"issue_id": "NUTCH-828", "project": "NUTCH", "title": "Fetch Filter", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2010-06-08T05:34:20.781+0000", "updated": "2024-03-13T14:51:05.436+0000", "description": "Adds a Nutch extension point for a fetch filter.  The fetch filter allows filtering content and parse data/text after it is fetched but before it is written to segments.  The fliter can return true if content is to be written or false if it is not.  \n\nSome use cases for this filter would be topical search engines that only want to fetch/index certain types of content, for example a news or sports only search engine.  In these types of situations the only way to determine if content belongs to a particular set is to fetch the page and then analyze the content.  If the content passes, meaning belongs to the set of say sports pages, then we want to include it.  If it doesn't then we want to ignore it, never fetch that same page in the future, and ignore any urls on that page.  If content is rejected due to a fetch filter then its status is written to the CrawlDb as gone and its content is ignored and not written to segments.  This effectively stop crawling along the crawl path of that page and the urls from that page.  An example filter, fetch-safe, is provided that allows fetching content that does not contain a list of bad words.", "comments": ["Shall we postpone this after the release of 1.1? This is a new functionality and at this stage we probably just want to iron out bugs on what we currently have. Makes sense? ", "Yeah. I am not proposing to get this into 1.1.  Oh wait, I did with the affects selection.  No this can / should wait until after the 1.1 release.  Anybody that wants it before then can patch :)", "Forgot to add the nutch-default.xml changes to the old patch.  Here is a new one.", "I generally like the idea of a decision point, but I think the place where this decision is taken in this patch (Fetcher) is not right. Since you rely on the presence of ParseResult (understandably so) it seems to me that a much better place to run the filters would be inside ParseUtils.parse(content), and you could return null (or a special ParseResult) to indicate that the content is to be discarded.\n\nThis way you can both run this filtering as a part of a Fetcher in parsing mode, and as a part of ParseSegment, without duplicating the same logic. Consequently, I propose to change the name from FetchFilter to ParseFilter.", "I agree about wanting the decision not just in fetcher while parsing but also in parse segment.  Here is the problem as I see it in returning null content.\n\nSay we are wanting to create a topical search engine about sports.  We fetch pages.  Run through a fetch filter for a yes/no is this page in sports by its content.  If we null out content and from that ParseText and ParseData, we still have the CrawlDatum to deal with.  If we leave it as is, the CrawlDatum will get updated into CrawlDb as successfully fetched.  Content and Parse won't get collected because they are null.  We won't have the problem of Outlinks on that page getting queued in CrawlDb but the original URL will still be there and will be queued after an interval for repeated crawling.  Over time what we have is a large number of URLs that we know to be filtered being repeatedly crawled.\n\nThe decision point isn't just keep the content.  It is should we keep the URL and its content/parse and continue crawling down the path of the URLs outlinks or should we ignore this URL and not crawl anything it points to, break the crawl graph at this point.  Hence FetchFilter.  My solution to this was to null out content/parse and add a different CrawlDatum that essentially said the page was gone.  Ideally we should have a separate status but the gone worked as a first pass.  This gets updated back into CrawlDb and won't get recrawled at a later date  This was only possible in the Fetcher though.\n\nThoughts on how we might approach this?\n\n", "First, as you point out, we cannot ignore the page because the problem will repeat itself as we keep re-discovering it, so we have to \"poison\" it with GONE - and I think it's ok to add another status here to express that we never ever want to collect this page, because GONE gets reset periodically.\n\nIf we run Fetcher in parsing mode then we can change this status immediately, so no problem here. If we run ParseSegment then we can also update this status in a similar way as we implement the signature update, i.e. in ParseOutputFormat emit a <pageUrl,CrawlDatum> that will switch the status of this page when collected later on in CrawlDbReducer.", "Nice.  I didn't realize the signature update would do that.  I am assuming since ParseUtil doesn't interact with the CrawlDatum we are going to have to call the FetchFilters (I am ok with renaming this btw) twice, once in the fetcher and once in the ParseSegment, both dealing with their respective CrawlDatum needs?", "- bumpity to 1.2 since 1.1 is out the door", "I have implemented this filter to filter out pages not containing urdu language, but this filter kills the seed and hence I am not able to crawl the whole web.", "20120304-push-1.6", "Merely updates Dennis' original patch for 1.8-SNAPSHOT.\nAs there is some interest on the mailing list maybe some other can try this out and we can make a decision as to what happens with the issue?\nThanks", "A better approach is to operate within the parsing step, as explained by Andrzej. You can already remove the outlinks from a page in a HTMLParseFilter and change the status of a page. Moreover there has been little interest in this issue over the last few years", "Bulk progression of all “Resolved” issues to “Closed”. Performed my lewismc on 2024-03-13."], "tasks": {"summary": "Fetch Filter", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fetch Filter"}, {"question": "What is the main context?", "answer": "Adds a Nutch extension point for a fetch filter.  The fetch filter allows filtering content and parse data/text after it is fetched but before it is written to segments.  The fliter can return true if"}]}}
{"issue_id": "NUTCH-829", "project": "NUTCH", "title": "duplicate hadoop temp files", "status": "Closed", "priority": "Minor", "reporter": "Mike Baranczak", "assignee": null, "created": "2010-06-13T23:16:24.122+0000", "updated": "2013-05-22T03:54:46.292+0000", "description": "When two crawls are started at exactly the same time, I see the following error: \n{quote}\norg.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/tmp/hadoop-mike/mapred/temp/generate-temp-1276463469075 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:111)\n\tat org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:793)\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1142)\n\tat org.apache.nutch.crawl.Generator.generate(Generator.java:472)\n\tat org.apache.nutch.crawl.Generator.generate(Generator.java:409)\n        [...]\n{quote}\n\nI traced it down to this code in Generator (I'm using Nutch 1.0, but this is still in the trunk):\n\n{quote}\nPath tempDir =\n      new Path(getConf().get(\"mapred.temp.dir\", \".\") +\n               \"/generate-temp-\"+ System.currentTimeMillis());\n{quote}\n\nI admit that this is an unlikely scenario for most users, but it just so happens that I ran into it. To absolutely guarantee that the temp directory doesn't already exist, I suggest changing System.currentTimeMillis() to java.util.UUID.randomUUID().toString().", "comments": ["java.util.UUID was only introduced in Java 1.5\n\nBut I read from the FAQ that \n\n    What Java version is required to run Nutch?\n    Nutch 0.7 will run with Java 1.4 and up. Nutch 1.0 with Java 6. \n\nSo I guess that is fine. ", "Patch for Mike's issue.\nAny objections to commit. I can't locate where/if this happens in 2.x, so I didn't produce a patch.", "Hi Lewis,\nThere was one more place in Generator where this change could have been done. Adding patch for the same. I agree that this defect wont have any impact over 2.x generator.\n\nIf there are no objections, lets commit it soon.", "+1 for commit. @Tejas, please remove your changes to tika-core in ivy.xml when committing. \nThank you very much Tejas.", "Thanks Lewis for pointing that out. Committed @ revision 1476702", "Integrated in Nutch-trunk #2183 (See [https://builds.apache.org/job/Nutch-trunk/2183/])\n    NUTCH-829 duplicate hadoop temp files (Revision 1476702)\n\n     Result = SUCCESS\ntejasp : http://svn.apache.org/viewvc/nutch/trunk/?view=rev&rev=1476702\nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/src/java/org/apache/nutch/crawl/Generator.java\n"], "tasks": {"summary": "duplicate hadoop temp files", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "duplicate hadoop temp files"}, {"question": "What is the main context?", "answer": "When two crawls are started at exactly the same time, I see the following error: \n{quote}\norg.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/tmp/hadoop-mike/mapred/temp/genera"}]}}
{"issue_id": "NUTCH-83", "project": "NUTCH", "title": "Release deliverable as zip", "status": "Closed", "priority": "Major", "reporter": "AJ Banck", "assignee": null, "created": "2005-08-16T04:10:37.000+0000", "updated": "2011-04-01T14:28:13.530+0000", "description": "Like Lucene, Nutch could be delivered as a .zip file so it can be used with default tools on Windows.", "comments": ["Patch of build.xml that adds a \"package-zip\" target and call this from the dist target.", "Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Release deliverable as zip", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Release deliverable as zip"}, {"question": "What is the main context?", "answer": "Like Lucene, Nutch could be delivered as a .zip file so it can be used with default tools on Windows."}]}}
{"issue_id": "NUTCH-830", "project": "NUTCH", "title": "ScoringFilter to restrict the crawl to the hosts/domains listed in the seeds", "status": "Closed", "priority": "Minor", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-06-23T13:03:20.460+0000", "updated": "2010-08-17T08:57:26.999+0000", "description": "The DomainURLFilter allows to specify the domains to consider for a crawl. This works fine but requires to edit a list of domain / hosts manually. The patch presented here offers the same functionality but uses a different mechanism as we use a custom scoring filter to filter the outlinks. \n\n1. add a metadata to your seed list e.g. '_origin_' with as values the seed URL\ne.g. http://www.cnn.com/    _origin_=http://www.cnn.com/\n\n2. The custom scoring filter would take care of :\n    * transmitting the origin metadata to its outlinks\n    * remove from the outlinks the ones which do not have the same host / domain as the origin\n\nThe parameter _scoring.insite.mode_ allows to specify whether to restrict on the host or domain. The parameter _scoring.insite.addOriginOnInject_ allows to addition of the metadata during the injection step and reuses the URL automatically.", "comments": ["This approach has a major flaw which is that it loses the links between two authorized host or domain names (which would harm their scores). It also lets redirections from the authorized host but not the subsequent outlinks, meaning that we can get a document with some content and hence indexed even if its host name does not match the seed it was found from. This is not good or bad in itself, just a bit counter intuitive.\n\nThis was probably just an interesting example of how to use the ScoringFilter but as far as the functionality goes, using the domainFilter should be a more satisfying approach.\n"], "tasks": {"summary": "ScoringFilter to restrict the crawl to the hosts/domains listed in the seeds", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "ScoringFilter to restrict the crawl to the hosts/domains listed in the seeds"}, {"question": "What is the main context?", "answer": "The DomainURLFilter allows to specify the domains to consider for a crawl. This works fine but requires to edit a list of domain / hosts manually. The patch presented here offers the same functionalit"}]}}
{"issue_id": "NUTCH-831", "project": "NUTCH", "title": "Allow configuration of how fields crawled by Nutch are stored / indexed / tokenized", "status": "Closed", "priority": "Minor", "reporter": "Jeroen van Vianen", "assignee": "Chris A. Mattmann", "created": "2010-06-23T13:16:44.440+0000", "updated": "2013-05-22T03:54:54.348+0000", "description": "Currently, it is impossible to change the way Nutch stores / indexes / tokenizes the fields it creates while crawling and indexing URLs.\n\nI wanted to be able to *store* the content field so I could use my own Lucene code and hightlighting code to work on the stored content field. Currently, content is only tokenized.\n\nSee nutch-trunk/src/plugin/index-basic/src/java/org/apache/nutch/indexer/basic/BasicIndexer.addIndexBackendOptions(Configuration conf) for the current settings.\n\nThere's already code in Nutch to configure how fields are stored / indexed / tokenized from conf/nutch-site.xml:\n\n<property>\n  <name>lucene.field.store.content</name>\n  <value>YES</value>\n</property>\n\n(content is the name of the field)\n\nHowever, the BasicIndexer overrides these settings with its own. Attached is a patch which will make sure the above settings are only applied when none have been specified in nutch-site.xml", "comments": ["Here's the patch to LuceneWriter", "Moved to Fixed 1.2 \n1.1 having been released it is not likely to contain this fix, as for 2.0 it will delegate the indexing to SOLR and won't contain any Lucene related code", "I applied this patch to the Nutch 1.2 branch and all tests passed:\n\ntest:\n     [echo] Testing plugin: urlnormalizer-regex\n    [junit] Running org.apache.nutch.net.urlnormalizer.pass.TestPassURLNormalizer\n    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.28 sec\n    [junit] Running org.apache.nutch.net.urlnormalizer.regex.TestRegexURLNormalizer\n    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.209 sec\n\ntest:\n\nBUILD SUCCESSFUL\nTotal time: 10 minutes 50 seconds\n[chipotle:~/tmp/nutch-1.2] mattmann% \n\nI'll commit the patch there so you can have it in SVN and use it, but I'll set the fix version to nil since the movement is towards Solr in the trunk. Thanks for the contribution, regardless, Jeroen!\n\nCheers,\nChris\n", "- fixed in r958828 and applied to branch-1.2. You can always use that version Jeroen until we get the 2.0 version stable in trunk.\n\nThanks for your contribution!", "- applied to 1.2 branch", "In the future a maintenance patch like this could be applied to branch-1.2, especially since NUTCH-837 will remove this code completely from trunk.", "Hey Andrzej,\n\nExactly, I applied the patch to branch-1.2 but not to the trunk. Looks like we're building up a few patches there. If we get a few more, I will gladly spin up a 1.2 release to push it out the door...\n\nCheers,\nChris\n"], "tasks": {"summary": "Allow configuration of how fields crawled by Nutch are stored / indexed / tokenized", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Allow configuration of how fields crawled by Nutch are stored / indexed / tokenized"}, {"question": "What is the main context?", "answer": "Currently, it is impossible to change the way Nutch stores / indexes / tokenizes the fields it creates while crawling and indexing URLs.\n\nI wanted to be able to *store* the content field so I could us"}]}}
{"issue_id": "NUTCH-832", "project": "NUTCH", "title": "Website menu has lots of broken links - in particular the API docs", "status": "Closed", "priority": "Major", "reporter": "Alexander Lachlan Mclintock", "assignee": "Chris A. Mattmann", "created": "2010-06-24T11:11:53.736+0000", "updated": "2013-05-22T03:53:27.860+0000", "description": "The website seems to have lots of broken links. eg the menu on the left points to various URLs of the form \n\nhttp://nutch.apache.org/apidocs-1.0/index.html\n\nbut these don't seem to exist on the server. \n\nAlso \n\nhttp://nutch.apache.org/release/\n", "comments": ["It seems to me that the apidocs themselves are not created by the forrest process - only the top level index.html\nSo the old apidocs need to be recopied from the old lucene site.\n\nPossibly this was an rsync deploy problem?", "This patch amends some of the xdocs - mostly to correct old or broken links, but in some places to add some text.\n\n+++ src/documentation/content/xdocs/site.xml\n+++ src/documentation/content/xdocs/version_control.xml\t\n+++ src/documentation/content/xdocs/index.xml\t\n+++ src/documentation/content/xdocs/about.xml\t\n+++ src/documentation/content/xdocs/nightly.xml\t\n+++ src/documentation/content/xdocs/i18n.xml\t\n\n", "- patch applied in r958129. Thanks, Alex!\n\nhttp://svn.apache.org/viewvc/?rev=958129&view=rev", "Integrated in Nutch-trunk #1189 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1189/])\n    - fix for NUTCH-832 Website menu has lots of broken links - in particular the API docs\n"], "tasks": {"summary": "Website menu has lots of broken links - in particular the API docs", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Website menu has lots of broken links - in particular the API docs"}, {"question": "What is the main context?", "answer": "The website seems to have lots of broken links. eg the menu on the left points to various URLs of the form \n\nhttp://nutch.apache.org/apidocs-1.0/index.html\n\nbut these don't seem to exist on the server"}]}}
{"issue_id": "NUTCH-833", "project": "NUTCH", "title": "Website is still Lucene branded", "status": "Closed", "priority": "Trivial", "reporter": "Alexander Lachlan Mclintock", "assignee": "Chris A. Mattmann", "created": "2010-06-24T11:18:02.949+0000", "updated": "2013-05-22T03:54:48.942+0000", "description": "The Nutch website still has a lot of Lucene branding and links which are confusing. eg the breadcrumbs\n\nApache > Lucene > Nutch  > \n\nappear at the top of most pages, along with the lucene logo and link to their home page. ", "comments": ["OK, so we are using Forrest (a Cocoon system) for generating the website. It uses a skin to decide on the appearance of the site. Now we happen to be using a skin called Lucene - since we were part of Lucene.\n\n\nI am getting these silly little errors when building the site:\nX [0]                                     skin/images/current.gif\tBROKEN: /Users/alex/projects/forest/apache-forrest-0.8/main/webapp/. (No such file or directory)\nX [0]                                     images/instruction_arrow.png\tBROKEN: /Users/alex/projects/nutch/trunk/src/site/src/documentation/content/xdocs/images.instruction_arrow.png (No such file or directory)\nX [0]                                     skin/images/chapter.gif\tBROKEN: /Users/alex/projects/forest/apache-forrest-0.8/main/webapp/. (No such file or directory)\nX [0]                                     skin/images/page.gif\tBROKEN: /Users/alex/projects/forest/apache-forrest-0.8/main/webapp/. (No such file or directory)\n\n\n\n\nBefore we create our own new skin we need to fix the old skin\n\nIt looks like the images directory \n\nhttp://svn.apache.org/viewvc/lucene/site/src/documentation/skins/lucene/images\n\ndoes not appear to be in our area...We don't have an images directory in here\n\nhttp://svn.apache.org/viewvc/nutch/trunk/src/site/src/documentation/skins/lucene/\n\nPlease can a committer  svn copy one to the other. \n\n\nOnce that is done we should copy the whole skins/lucene directory as skins/nutch\n\n\n \n\n\n\n", "- fixed in r958174, r958175, r958176, r958177, and r958178. Thanks for the pointers on some of this stuff, Alex!", "Integrated in Nutch-trunk #1189 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1189/])\n    - changelog for NUTCH-833 Website is still Lucene branded\n- progress towards NUTCH-833 Website is still Lucene branded\n- progress towards NUTCH-833 Website is still Lucene branded\n- progress towards NUTCH-833 Website is still Lucene branded\n- progress towards NUTCH-833 Website is still Lucene branded\n- progress towards NUTCH-833 Website is still Lucene branded\nfix for NUTCH-833 Website is still Lucene branded.\nfix for NUTCH-833 Website is still Lucene branded.\n"], "tasks": {"summary": "Website is still Lucene branded", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Website is still Lucene branded"}, {"question": "What is the main context?", "answer": "The Nutch website still has a lot of Lucene branding and links which are confusing. eg the breadcrumbs\n\nApache > Lucene > Nutch  > \n\nappear at the top of most pages, along with the lucene logo and lin"}]}}
{"issue_id": "NUTCH-834", "project": "NUTCH", "title": "Separate the Nutch web site from trunk", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-06-24T13:58:19.524+0000", "updated": "2010-07-01T05:45:34.175+0000", "description": "As discussed on dev@, it would be useful to move the -PDFBox- Nutch web site sources from .../asf/nutch/trunk to .../asf/nutch/site and to use the svnpubsub mechanism for instant deployment of site changes.\n\nThe related issue for infra is https://issues.apache.org/jira/browse/INFRA-2822\nSee also https://issues.apache.org/jira/browse/PDFBOX-623", "comments": ["The mechanism has been enabled for the existing location of the site i.e.  nutch/trunk/site (see https://issues.apache.org/jira/browse/INFRA-2822). \nDo we want to move it to a separate branch e.g. nutch/site or leave it where it is?", "Hey Julien:\n\nMy recommendation would be to create a site directory at the top level of Nutch (e.g., http://svn.apache.org/repos/asf/nutch/site/) by moving http://svn.apache.org/repos/asf/nutch/trunk/site to there, and then maintain the latest version of the site in that directory. Then, we could ask Gavin to update svnpubsub to work from that new path. We would leave the source of the Nutch site inside of http://svn.apache.org/repos/asf/nutch/trunk/src/site then on each update of the website after site build via Forrest, we copy the latest website src to http://svn.apache.org/repos/asf/nutch/site/ and the website should automatically be up-to-date.\n\nSound good?\n\nCheers,\nChris\n", "I suppose you want to keep the source of the site in the trunk so that it is easier to include the javadoc as well? I haven't thought about that but it makes sense", "Hey Julien,\n\nYep that's what I was thinking. Also it makes it easier for people to contribute to the website static pages, by updating docs and attaching trunk patches, etc., I think...\n\nCheers,\nChris\n", "The javadoc would be updated only on a new release so we could simply copy it to asf/nutch/site when we need to. As for the non-xdoc generated pages they would be patchable in the same way if the sources of the site were also in asf/nutch/site. \nMy preference would be to not have anything related to the site in the trunk and keep the sources of the site in asf/nutch/site as well but I'd be happy to do as you suggested as well. The main point is to be able to use svnpubsub anyway ;-)", "Hey Julien,\n\nHmm, maybe you're right. So, what would you propose be the organization? Something like:\n\nhttp://svn.apache.org/repos/asf/nutch/site/forrest   (currently http://svn.apache.org/repos/asf/nutch/trunk/src/site/)\nhttp://svn.apache.org/repos/asf/nutch/site/deploy   (currently http://svn.apache.org/repos/asf/nutch/trunk/site/)\n\nIf so, +1 from me!\n\nCheers,\nChris\n", "{quote}\nSomething like:\n\nhttp://svn.apache.org/repos/asf/nutch/site/forrest (currently http://svn.apache.org/repos/asf/nutch/trunk/src/site/)\nhttp://svn.apache.org/repos/asf/nutch/site/deploy (currently http://svn.apache.org/repos/asf/nutch/trunk/site/)\n{quote}\n\nyep. If everybody's happy with it I will : \na) copy the stuff from trunk to site\nb) ask INFRA to configure svnpubsub with site instead of trunk\nc) remove trunk/site and trunk/src/site\nd) update the wiki page \n\nJ.\n ", "+1, makes sense to me!\n\nCheers,\nChris\n", "Is the javadoc copied manually to people.apache.org? Anyone knows how this fits with svnpubsub?", "Hey Julien,\n\nYep right now it's manual, but would be great to just check it in. Any objections to doing that? \n\nIf not, I think once you move the site over to its new home, just generate the javadoc (with ant javadoc) and then just SVN it. Sound good?\n\nCheers,\nChris\n", "Checking it in would be fine. We'd have it on something like \nhttp://svn.apache.org/repos/asf/nutch/site/publish/ (or deploy)\n\nApart from the javadoc for the latest release, is there anything else added manually to the site? ", "Ooh, I like \"/publish/\". That's snazzier than my original suggestion :) Let's do that.\n\nApart from the javadoc, no, there aren't anymore manual parts of the website deploy to my knowledge. I got rid of the links to the old javadoc on the Nutch site (pre 1.1), since I don't think it makes much sense to maintain links to APIs that are about to be 2 generations old. Those were the latest manual parts of the site...\n\nCheers,\nChris\n", "Committed revision 958996\n\nI have copied the stuff over to http://svn.apache.org/repos/asf/nutch/site and reopened INFRA-2822\nThe content of http://svn.apache.org/repos/asf/nutch/site/publish/ has been regenerated today with Forrest 0.9-dev. Once the changes in INFRA-2822 are tested I will remove the site related stuff from the trunk.", "The content of the site is now taken from http://svn.apache.org/repos/asf/nutch/site/publish/ (see INFRA-2822), I've tested with a small change and it works fine. \nI've updated http://wiki.apache.org/nutch/Website_Update_HOWTO accordingly and will now remove the src/site and site directories from the trunk", "Committed revision 959228.\n\nThanks Chris for your comments and help with this", "Integrated in Nutch-trunk #1194 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1194/])\n    (NUTCH-834) Separate the Nutch web site from trunk\n"], "tasks": {"summary": "Separate the Nutch web site from trunk", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Separate the Nutch web site from trunk"}, {"question": "What is the main context?", "answer": "As discussed on dev@, it would be useful to move the -PDFBox- Nutch web site sources from .../asf/nutch/trunk to .../asf/nutch/site and to use the svnpubsub mechanism for instant deployment of site ch"}]}}
{"issue_id": "NUTCH-835", "project": "NUTCH", "title": "document deduplication (exact duplicates) failed using MD5Signature", "status": "Closed", "priority": "Major", "reporter": "Sebastian Nagel", "assignee": "Andrzej Bialecki", "created": "2010-06-25T11:01:01.979+0000", "updated": "2013-05-22T03:53:34.113+0000", "description": "The MD5Signature class calculates different signatures for identical documents.\n\nThe reason is that\n  byte[] data = content.getContent();\n  ... StringBuilder().append(data) ...\nuses java.lang.Object.toString() to get a string representation of the (binary) content\nwhich results in unique hash codes (e.g., [B@30dc9065) even for two byte arrays\nwith identical content.\n\nA solution would be to take the MD5 sum of the binary content as first part of the\nfinal signature calculation (the parsed content is the second part):\n  ... .append(StringUtil.toHexString(MD5Hash.digest(data).getDigest())).append(parse.getText());\nOf course, there are many other solutions...", "comments": ["Yes, this is a bug. In fact the implementation makes things even worse by appending the parsed text, contrary to its specification that says it should use just the raw content... I'll fix this shortly.", "Fixed in rev. 959629. Thanks!", "Integrated in Nutch-trunk #1195 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1195/])\n    NUTCH-835 Document deduplication failed using MD5Signature (Sebastian Nagel via ab)\n", "This patch has been marked for 1.2 but has been committed to trunk only (2.0). \nShall we also apply it to /nutch/branches/branch-1.2 ?", "Sorry, I should've been more precise - I committed this to branch-1.2 as well (r95963)."], "tasks": {"summary": "document deduplication (exact duplicates) failed using MD5Signature", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "document deduplication (exact duplicates) failed using MD5Signature"}, {"question": "What is the main context?", "answer": "The MD5Signature class calculates different signatures for identical documents.\n\nThe reason is that\n  byte[] data = content.getContent();\n  ... StringBuilder().append(data) ...\nuses java.lang.Object.t"}]}}
{"issue_id": "NUTCH-836", "project": "NUTCH", "title": "Remove deprecated parse plugins", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-06-30T11:30:13.943+0000", "updated": "2010-07-04T04:23:16.848+0000", "description": "Some of the parser plugins in 1.1 are covered by the parse-tika plugin. These plugins have been kept in 1.1 but should be removed from 2.0 where we'll rely on parse-tika almost exclusively. Some existing plugins might be kept when there is no equivalent in Tika (to be discussed). The following plugins are removed : \n* parse-html\n* parse-msexcel\n* parse-mspowerpoint\n* parse-msword\n* parse-pdf\n* parse-oo\n* parse-text\n* lib-jakarta-poi\n* lib-parsems\n\nThe patch does not (yet) remove :\n* parse-ext\n* parse-js\n* parse-rss\n* parse-swf\n* parse-zip\n* feed\n\nPlease review the patch and vote for its inclusion in the trunk.\n\n\n", "comments": ["Actually creative-commons + languageidentifier currently have a dependency on parse-html and parse-zip has one on parse-text in their build script.\nThe tests for the Fetcher and ParserFactory also fail without parse-html and parse-text. \n\nI will modify the patch to prevent these issues", "New patch which fixes the issues mentioned earlier. \n\n*languageidentifier* and *parse-zip* : dependence was only in the plugin descriptor but the code works fine with Tika used as a default plugin\n*creative-commons* : had hard-coded dependence (fixed). Using Tika returns slightly different results - see adapted test code.\n \nAlso fixed the TestParserFactory.\n\nAll tests OK. ", "Some comments:\n\n* do we still need lib-nekohtml ?\n* the issue in creative-commons (Tika returns <a> instead of <rel>) should be reported to Tika - I'm not sure myself what's better, but at leas we should indicate a difference in treatment and ask for their reasons...\n* I think that our tests in parse-html are useful and we should keep them, just move them and relevant resources to parse-tika.\n\nOther than that this patch looks great.", "{quote}\n    *  do we still need lib-nekohtml ?\n{quote}\n\nprobably not. we can get rid of it later if there aren't any dependencies on it\n\n{quote}\n    * the issue in creative-commons (Tika returns <a> instead of <rel>) should be reported to Tika - I'm not sure myself what's better, but at leas we should indicate a difference in treatment and ask for their reasons...\n{quote}\n\nI bet it is related to the filtering/normalisation of tags in Tika. The next version of Tika should give us more control on how this is done. Will check tomorrow\n\n{quote}\n    * I think that our tests in parse-html are useful and we should keep them, just move them and relevant resources to parse-tika.\n{quote}\n\nGood idea. Let's do that in a separate issue\n\n\n", "+1, let's move on with this issue then - I'll create a separate JIRA for the parse-html tests, and we'll copy them from svn history.", "Committed revision 959948.\n\nThanks Andrzej for reviewing it", "Integrated in Nutch-trunk #1197 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1197/])\n    "], "tasks": {"summary": "Remove deprecated parse plugins", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove deprecated parse plugins"}, {"question": "What is the main context?", "answer": "Some of the parser plugins in 1.1 are covered by the parse-tika plugin. These plugins have been kept in 1.1 but should be removed from 2.0 where we'll rely on parse-tika almost exclusively. Some exist"}]}}
{"issue_id": "NUTCH-837", "project": "NUTCH", "title": "Remove search servers and Lucene dependencies ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Andrzej Bialecki", "created": "2010-06-30T15:42:02.504+0000", "updated": "2011-06-08T21:34:24.151+0000", "description": "One of the main aspects of 2.0 is the delegation of the indexing and search to external resources like SOLR. We can simplify the code a lot by getting rid of the : \n* search servers\n* indexing and analysis with Lucene\n* search side functionalities : ontologies / clustering etc...\nIn the short term only SOLR / SOLRCloud will be supported but the plan would be to add other systems as well. ", "comments": ["Warning - Nutch veterans may want to sit down before reading, because it looks like half of Nutch code is deleted in this patch... ;)\n\nThis patch implements the changes. All tests (that remain) pass, and a full crawl cycle plus  Solr indexing works as before. There is no single entry point in Nutch at this moment for searching - we may want to add a minimal test search setup based on Solr in another patch.", "hahah uh oh!\n\nI'll try and take a look before next Tuesday...", "I think we can also get rid of  :\n\n* docs/\n* WAR related tasks in ANT\n* src/web/\n* src/xmlcatalog/\n* src/engines/\n", "Hey Julien:\n\nHow are we going to replace the Nutch webapp? \n\nCheers,\nChris", "Hi Chris, \n\nMy position on this is that we simply wouldn't replace it. We delegate the search to SOLR and expect people to reuse existing front ends for SOLR or build custom ones (as I expect real world deployments of Nutch would do anyway). Maintaining the webapps takes some effort that I doubt we can afford given the limited number of active committers that we have. I'd rather we focused on crawl-related functionalities.  \n\nWDYT? \n\nJ.", "I'm not sure I agree :) \n\nThe Nutch webapp is just a set of web pages that let someone know that Search is working. They are decent web pages, have a great look and feel and are something I've seen nearly every newbie Nutch user I've been around leverage to tell whether or not Nutch installed correctly.\n\nI'm also a fan of the \"let's not loose functionality on a technology upgrade task\" mantra. That is, we are reorganizing the architecture of Nutch to improve it, not to take away functionality. We should at least support the baseline of functionality that was present in 1.x.\n\nThat said, I'm not sure the existing webapp should be maintained in its current form. Maybe we should take a pass at updating the webapp to work with the Nutch 2.0 architecture underneath. I'm happy to pick up a shovel and dig on that one.\n\nCheers,\nChris\n", "Thanks for your comments Chris\n\n{quote}\nThe Nutch webapp is just a set of web pages that let someone know that Search is working. They are decent web pages, have a great look and feel and are something I've seen nearly every newbie Nutch user I've been around leverage to tell whether or not Nutch installed correctly.\n{quote}\n\nwell the SOLR webapps would be just as good if not better for debugging. You get all sorts of stats + can debug your queries etc... The front end and its configuration is also a common source of trouble for beginners. \n\n{quote}\nI'm also a fan of the \"let's not loose functionality on a technology upgrade task\" mantra. That is, we are reorganizing the architecture of Nutch to improve it, not to take away functionality. We should at least support the baseline of functionality that was present in 1.x.\n{quote}\n\nI don't think it is completely lost we still do have the webapps from SOLR :-) \nRegardless of the debug aspect mentioned earlier I really think that any real application based on Nutch would customise the front end anyway. \n\n{quote}\nThat said, I'm not sure the existing webapp should be maintained in its current form. Maybe we should take a pass at updating the webapp to work with the Nutch 2.0 architecture underneath. I'm happy to pick up a shovel and dig on that one.\n{quote}\n\nThis would need doing indeed i.e. get the cached data or inlinks straight from the webtable via GORA. Speaking of which we should probably think in terms of \"what functionalities do we have in Nutch that are currently missing in SOLR\", one of them being to be able to get the cache from HDFS/GORA/etc... without having to store the content in the index.\n\n\n\n\n\n", "Hey Julien,\n\nYep that's the point. Solr != Nutch, so Solr's Webapp can't be expected to be = Nutch's webapp. The example you cited about cached data is a great one, because Solr's webapp doesn't really support that (nor should it IMHO).\n\nSo, I think we should still have a Nutch webapp and in my mind it's a must-have for a 2.0 release...not to worry though I'm volunteering to help do it! :)\n\nCheers,\nChris", "Updated patch against r959954 (after NUTCH-836).", "bq. So, I think we should still have a Nutch webapp and in my mind it's a must-have for a 2.0 release...\n\nI agree. But for the moment it's better to delete the old webapp stuff that we know for sure doesn't work with the current Nutch, and it will be completely reimplemented anyway. Refactoring it to work with the new Solr-based app is likely not worth it - we can achieve a similar effect to the current webapp by just tweaking the styling of the Solritas handler.", "Okey dok, I created NUTCH-841 to track it. Julien, Andrzej, you have my +1 to take your axe to the old one :)", ":-)", "Comments on the latest patch : \n* default.properties : some entries can be removed\n{code}\ndocs.dir = ./docs\ndocs.src = ${basedir}/src/web\nxmlcatalog.dir = ${basedir}/src/xmlcatalog\nbuild.webapps = ${build.dir}/webapps\nweb.src.dir = ./src/web\nsrc.webapps = ./src/webapps\n{code}\n* docs/ : still there\n* src/web/ : ditto\n\napart from that +1\n", "Committed in r960064. Thanks for review!", "Integrated in Nutch-trunk #1197 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1197/])\n    "], "tasks": {"summary": "Remove search servers and Lucene dependencies ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove search servers and Lucene dependencies "}, {"question": "What is the main context?", "answer": "One of the main aspects of 2.0 is the delegation of the indexing and search to external resources like SOLR. We can simplify the code a lot by getting rid of the : \n* search servers\n* indexing and ana"}]}}
{"issue_id": "NUTCH-838", "project": "NUTCH", "title": "Add timing information to all Tool classes", "status": "Closed", "priority": "Major", "reporter": "Jeroen van Vianen", "assignee": "Chris A. Mattmann", "created": "2010-06-30T21:30:24.246+0000", "updated": "2013-05-22T03:53:35.942+0000", "description": "Am happily trying to crawl a few hundred URLs incrementally. Performance is degrading suddenly after the index reaches approximately 25000 URLs.\n\nAt first each inject (generate, fetch, parse, updatedb) * 3, invertlinks, solrindex, solrdedup batch takes approximately half an hour with topN 500, but elapsed times now increase to 00h45m,  01h15m, 01h30m with every batch. As I'm uncertain which of the phases takes so much time I decided to add start and finish times to al classes that implement Tool so I at least have a feeling and can review them in a log file.\n\nAm using pretty old hardware, but I am planning to recrawl these URLs on a regular basis and if every iteration is going to take more and more time, index updates will be few and far between :-(\n\nI added timing information to *all* Tool classes for consistency whereas there are only 10 or so Tools that are really interesting.", "comments": ["Here's the patch to add timings to all Tool classes.\n\nAdditionally, it removes some @Override where they were used incorrectly and adds the ability to use '#' to mark a line as a comment while injecting new URLs", "I'll backport this to the 1.2 branch as well.", "- Patch applied to trunk in r960246 and backported to 1.2-branch in r960248. I had to make some minor CR-LF mods and avoid patching a few files that were removed in the latest trunk. Thanks, Jeroen!", "Integrated in Nutch-trunk #1197 (See [http://hudson.zones.apache.org/hudson/job/Nutch-trunk/1197/])\n    - fix for NUTCH-838 Add timing information to all Tool classes\n"], "tasks": {"summary": "Add timing information to all Tool classes", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add timing information to all Tool classes"}, {"question": "What is the main context?", "answer": "Am happily trying to crawl a few hundred URLs incrementally. Performance is degrading suddenly after the index reaches approximately 25000 URLs.\n\nAt first each inject (generate, fetch, parse, updatedb"}]}}
{"issue_id": "NUTCH-839", "project": "NUTCH", "title": "nutch doesnt run under 0.20.2+228-1~karmic-cdh3b1 version of hadoop", "status": "Closed", "priority": "Major", "reporter": "Robert Gonzalez", "assignee": null, "created": "2010-07-01T19:27:18.189+0000", "updated": "2011-11-28T13:13:34.837+0000", "description": "new versions of hadoop appear to put jars in a different format now, instead of file:/a/b/c/d/job.jar, its now jar:file:/a/b/c/d/job.jar!, which breaks nutch when its trying to load its plugins.  Specifically, the stack trace looks like:\n\nCaused by: java.lang.RuntimeException: x point org.apache.nutch.net.URLNormalizer not found.\n\tat org.apache.nutch.net.URLNormalizers.<init>(URLNormalizers.java:124)\n\tat org.apache.nutch.crawl.Injector$InjectMapper.configure(Injector.java:57)\n\nA simple test class was written the used the URLFilters class, and the following stack trace resulted:\n\n10/07/01 14:25:25 INFO mapred.JobClient: Task Id : attempt_201006171624_46525_m_000000_1, Status : FAILED\njava.lang.RuntimeException: org.apache.nutch.net.URLFilter not found.\n\tat org.apache.nutch.net.URLFilters.<init>(URLFilters.java:52)\n\tat com.maxpoint.crawl.BidSampler$BIdSMapper.setup(BidSampler.java:42)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\n\nRunning this on an older version of hadoop works.\n", "comments": ["It would appear that a very similar ticket exists as of NUTCH-937\n\nThe log output posted by Claudio on that ticket also refers to the RuntimeException - URLNormalizer not found, which is the same exception shown above.\n\nIs it fair to say that seeing as there is more correspondence on that particular ticket this issue has been superseded?", "https://issues.apache.org/jira/browse/NUTCH-937"], "tasks": {"summary": "nutch doesnt run under 0.20.2+228-1~karmic-cdh3b1 version of hadoop", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "nutch doesnt run under 0.20.2+228-1~karmic-cdh3b1 version of hadoop"}, {"question": "What is the main context?", "answer": "new versions of hadoop appear to put jars in a different format now, instead of file:/a/b/c/d/job.jar, its now jar:file:/a/b/c/d/job.jar!, which breaks nutch when its trying to load its plugins.  Spec"}]}}
{"issue_id": "NUTCH-84", "project": "NUTCH", "title": "Fetcher for constrained crawls", "status": "Closed", "priority": "Minor", "reporter": "Kelvin Tan", "assignee": null, "created": "2005-08-25T08:01:13.000+0000", "updated": "2011-04-13T23:09:34.314+0000", "description": "As posted http://marc.theaimsgroup.com/?l=nutch-developers&m=112476980602585&w=2", "comments": ["Javadocs included in the zip and also available online at http://www.supermind.org/code/oc/api/index.html.\n\nCode is released under APL, but I've also included the Spring jars you'll need to run it.", "Updated build.xml and build.properties so it works both with unpacked distributions or SVN copies.", "Corrected compilation and build snafus.", "This is a very old issue and documented externally. It handles issues that can easiliy be handled using some smart scripting and variable NUTCH_HOME values. The issue also covers too many smaller improvements. It's also a single patch file with a lot of data, it's highly unlikely to patch successfully on 1.x."], "tasks": {"summary": "Fetcher for constrained crawls", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fetcher for constrained crawls"}, {"question": "What is the main context?", "answer": "As posted http://marc.theaimsgroup.com/?l=nutch-developers&m=112476980602585&w=2"}]}}
{"issue_id": "NUTCH-840", "project": "NUTCH", "title": "Port tests from parse-html to parse-tika", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2010-07-02T10:27:06.570+0000", "updated": "2019-10-13T22:35:47.088+0000", "description": "We don't have test for HTML in parse-tika so I'll copy them from the old parse-html plugin", "comments": ["Patch which adds the HTML tests to the Tika Parser\n\nThe tests currently rely on some DOM related code from Neko-HTML which introduces a dependency to the plugin lib-nekohtml.\nApart from parse-tika lib-nekohtml is used only in clustering-carrot which will be removed shortly. Once this is done we can delete lib-nekohtml as well then either : \na) add the neko jar to the parse-tika lib via IVY\nb) replace it with another implementation already available from the tika dependencies or the main Nutch dependencies (e.g. dom4j)\n\n\n\n", "Hi Julien. I have absolutely no idea how or when I ended up working on this, but I think the attachment nearly addresses this issue. It is from a while back and to be honest I can't really remeber working on it...\n\nAnyway, I think the parse-tika tests fail as it is not quite working properly yet. The patch also changes the directory structure to o.a.n.p.tika rather than existing o.a.n.tika which is inconsistent with other parser plugin implementation we ship with Nutch.\n\nSorry for hijacking this one slightly.", "Set and Classify", "This is for trunk.\nThere is a problem here where the new tests (for parse-tika) also seem to be executed against (within?) other plugin testing scenarios... I am stuck atm as to why this is.\nOnce we fix we will port to 2.x", "Modified version of the patch to fix the tests post NUTCH-797", "The tests now run OK with the patch I just attached.\n\nbq. There is a problem here where the new tests (for parse-tika) also seem to be executed against (within?) other plugin testing scenarios\n\ncan you give more detail on this please Lewis?", "Hi Julien. I cleaned my local ivy repos (new years clean out), applied and tested the patch. It's A OK with me and as it is a test I'm +1 for committing this one for trunk even though the Jenkins builds are slightly dodgy due to the Hadoop upgrade and Jenbkins incompatibilities. If you keep it open for 2.x we can work on this in due course.", "Thanks Lewis. Will commit shortly unless someone has any objections", "+1", "Trunk => Committed revision 1435101.\n\nAnyone to port to 2x?", "Integrated in Nutch-trunk-Windows #6 (See [https://builds.apache.org/job/Nutch-trunk-Windows/6/])\n    NUTCH-840 Port tests from parse-html to parse-tika (Revision 1435101)\n\n     Result = FAILURE\njnioche : http://svn.apache.org/viewvc/nutch/trunk/?view=rev&rev=1435101\nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/src/plugin/parse-html/src/test/org/apache/nutch/parse/html/TestDOMContentUtils.java\n* /nutch/trunk/src/plugin/parse-tika/build.xml\n* /nutch/trunk/src/plugin/parse-tika/plugin.xml\n* /nutch/trunk/src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/DOMContentUtils.java\n* /nutch/trunk/src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/HTMLMetaProcessor.java\n* /nutch/trunk/src/plugin/parse-tika/src/test/org/apache/nutch/tika/TestDOMContentUtils.java\n* /nutch/trunk/src/plugin/parse-tika/src/test/org/apache/nutch/tika/TestRobotsMetaProcessor.java\n", "Integrated in Nutch-trunk #2091 (See [https://builds.apache.org/job/Nutch-trunk/2091/])\n    NUTCH-840 Port tests from parse-html to parse-tika (Revision 1435101)\n\n     Result = SUCCESS\njnioche : http://svn.apache.org/viewvc/nutch/trunk/?view=rev&rev=1435101\nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/src/plugin/parse-html/src/test/org/apache/nutch/parse/html/TestDOMContentUtils.java\n* /nutch/trunk/src/plugin/parse-tika/build.xml\n* /nutch/trunk/src/plugin/parse-tika/plugin.xml\n* /nutch/trunk/src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/DOMContentUtils.java\n* /nutch/trunk/src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/HTMLMetaProcessor.java\n* /nutch/trunk/src/plugin/parse-tika/src/test/org/apache/nutch/tika/TestDOMContentUtils.java\n* /nutch/trunk/src/plugin/parse-tika/src/test/org/apache/nutch/tika/TestRobotsMetaProcessor.java\n", "Patch for 2.X.\nThere currently appears to be a discrepancy in the detection of Outlunks. We are detecting more than the test expects\n\n{code}\n  1 Testsuite: org.apache.nutch.parse.tika.TestDOMContentUtils\n  2 Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.496 sec\n  3\n  4 Testcase: testGetTitle took 0.331 sec\n  5 Testcase: testGetText took 0.069 sec\n  6 Testcase: testGetOutlinks took 0.08 sec\n  7         FAILED\n  8 got wrong number of outlinks (expecting 3, got 5)\n  9 answer:\n 10 toUrl: http://www.nutch.org/ anchor: home\n 11 toUrl: http://www.nutch.org/docs/1 anchor: 1\n 12 toUrl: http://www.nutch.org/docs/2 anchor: 2\n 13\n 14 got:\n 15 toUrl: http://www.nutch.org/ anchor: home\n 16 toUrl: http://www.nutch.org/ anchor:\n 17 toUrl: http://www.nutch.org/docs/1 anchor: 1\n 18 toUrl: http://www.nutch.org/docs/1 anchor:\n 19 toUrl: http://www.nutch.org/docs/2 anchor: 2\n 20\n 21\n 22 junit.framework.AssertionFailedError: got wrong number of outlinks (expecting 3, got 5)\n 23 answer:\n 24 toUrl: http://www.nutch.org/ anchor: home\n 25 toUrl: http://www.nutch.org/docs/1 anchor: 1\n 26 toUrl: http://www.nutch.org/docs/2 anchor: 2\n 27\n 28 got:\n 29 toUrl: http://www.nutch.org/ anchor: home\n 30 toUrl: http://www.nutch.org/ anchor:\n 31 toUrl: http://www.nutch.org/docs/1 anchor: 1\n 32 toUrl: http://www.nutch.org/docs/1 anchor:\n 33 toUrl: http://www.nutch.org/docs/2 anchor: 2\n 34\n 35\n 36         at org.apache.nutch.parse.tika.TestDOMContentUtils.compareOutlinks(TestDOMContentUtils.ja    va:315)\n 37         at org.apache.nutch.parse.tika.TestDOMContentUtils.testGetOutlinks(TestDOMContentUtils.ja    va:296)\n{code}", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "Port tests from parse-html to parse-tika", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Port tests from parse-html to parse-tika"}, {"question": "What is the main context?", "answer": "We don't have test for HTML in parse-tika so I'll copy them from the old parse-html plugin"}]}}
{"issue_id": "NUTCH-841", "project": "NUTCH", "title": "Create a Wicket-based Web Application for Nutch", "status": "Closed", "priority": "Major", "reporter": "Chris A. Mattmann", "assignee": "Lewis John McGibbney", "created": "2010-07-02T15:56:37.702+0000", "updated": "2024-03-13T14:51:28.292+0000", "description": "In light of the conversation on NUTCH-837, we are removing the old Nutch webapp and will replace it with a 2.0 one that works with GORA + Solr. \n\nApache Nutch versions prior to 1.3 used to ship with a web application that allowed basic search, and browse of the information captured in the Nutch index. Since 1.3, we deprecated and removed the webapp mainly due to the fact that the segment API changed (we moved to Solr), and also due to the fact that we didn't want to maintain a webapp b/c those JSPs were a pain.\n\nI am going to propose having a Nutch web application using Apache Wicket http://wicket.apache.org/. This would be very cool and since I know Wicket, I'm willing to help maintain it. \n\nThe webapp should implement all of the old web pages and functionality, and also should support the basic views, and connection to Solr instead of to Lucene, and of should also consider both the trunk branch, and the 2.0 branch (Gora based).\n\nI'm putting this out there as a GSoC project for 2013.", "comments": ["Seems to be the best existing resource from our wiki.\n\nhttp://wiki.apache.org/nutch/NutchAdministrationUserInterface", "Yep, agreed. Thanks for the reminder Lewis!", "unfortunately the links provided (although they would have been terribly outdated anyway) at the bottom of the wiki entry either return 404's or else the tar files seem to be corrupted. Quite a shame, however I think although this is a blocker, it will be one of the latter tasks which needs to be addressed prior to a 2.0 release.", "Yep not a blocker!", "As usually Nutch and the web application which manages Nutch are not in same machine. So could we expose some Rest API to call Nutch to crawl webpages or other tasks: just like call nutch/crawl scripts remotely? Users are able to enable or disable these functions.\n\nThanks...", "Hi Chris,\nI am student from Estonia (Tartu University). I have experience in Java web application development.\nTools and frameworks: Wicket, Spring, JUnit, Mockito, RESTful services, git, mercurial, linux.\nI am looking forward to participate in this project during Google Summer of Code 2013.\nCould you give me some advice concerning next steps to continue proposal?\n\nThanks,\nIvan", "Thanks Ivan. Unfortunately the deadline to participate in GSoC 2013 is behind us.\n\nHowever if you are still interested in the project, you are welcome to work on it just not through GSoC.", "Yuan Yun : yes we should expose and leverage Nutch REST APIs, and extend them using JAX-RS.", "Hello all! \nMy name is Fedor Vershinin, and I study computer science in Tallinn University of Technology in Estonia.\nMy brother shared some info about Google Summer of Code and I see that ASF takes part in this project, so I’d decided to get in touch. I am quite interested in development using Apache Wicket framework, and even more interested in contribution to open-source projects.\nMy background: some Java and python development, can use git/mercurial, eclipse, maven, restful, html, SQL and so on. I dont have much experience, but I have desire to learn.\nAlso, brother said he can help me with initial setup, brief picture of whole project and some architectural decisions.\nSo, it would be nice if you give me some advice, where I can start to explore Nutch, and I wish to join your community in future.\nBest regards,\nFedor", "Hey [~FVershinin], this is excellent news. [~chrismattmann] and myself are interested in kicking this project off so we need to decide between ourselves the mentoring position. We will do this and get back to you :) \n\nbq. ...where I can start to explore Nutch\n\nPlease sign up to user@nutch and dev@nutch mailing lists. This will keep you up-to-date will everything that is going on.\nFinally, please see the official tutorial [0] for running Nutch and learning about its design and caveats.\n\n[0] https://wiki.apache.org/nutch/NutchTutorial", "Reassigning to myself [~chrismattmann], I've created a wiki page so you can track the progress during the summer.\nhttps://wiki.apache.org/nutch/GoogleSummerOfCode", "GSOC patch", "This patch includes a complete update of [~fjodor.vershinin] achivements over the summer. \nThe other patch on this issue does not compile against Nutch 2.X HEAD.\nI intend to deploy this to a webservice which will be public facing.\nThis patch passes all tests including the ones which have been added as part of the GSoC project.", "Committed @revision 1626786 in 2.x HEAD\nI waited ~72 hours for feedback on this one and got some from [~chrismattmann] [0] thanks for that.\nWe have an opportunity to fill out this patch if it is within 2.X HEAD as I am using it with a number of non-technical $ technical people on a current project. The user feedback will be really valuable for us to imrprove the patch further. \nGreat work [~fjodor.vershinin]. \n[0] http://www.mail-archive.com/dev%40nutch.apache.org/msg15187.html\n", "SUCCESS: Integrated in Nutch-nutchgora #1165 (See [https://builds.apache.org/job/Nutch-nutchgora/1165/])\nNUTCH-841 Create a Wicket-based Web Application for Nutch (lewismc: http://svn.apache.org/viewvc/nutch/branches/2.x/?view=rev&rev=1626786)\n* /nutch/branches/2.x/CHANGES.txt\n* /nutch/branches/2.x/build.xml\n* /nutch/branches/2.x/default.properties\n* /nutch/branches/2.x/ivy/ivy.xml\n* /nutch/branches/2.x/src/bin/nutch\n* /nutch/branches/2.x/src/java/org/apache/nutch/api/NutchServer.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/api/impl/NutchServerPoolExecutor.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/api/impl/RAMJobManager.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/api/model/request/SeedList.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/api/model/request/SeedUrl.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/api/resources/SeedResource.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/NutchUiApplication.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/NutchUiApplication.properties\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/NutchUiServer.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/NutchClient.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/NutchClientFactory.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/impl\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/impl/CrawlingCycle.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/impl/CrawlingCycleListener.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/impl/NutchClientImpl.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/impl/RemoteCommand.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/impl/RemoteCommandBuilder.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/impl/RemoteCommandExecutor.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/impl/RemoteCommandsBatchFactory.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/model\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/model/ConnectionStatus.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/model/Crawl.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/model/JobConfig.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/model/JobInfo.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/client/model/NutchStatus.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/config\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/config/CustomDaoFactory.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/config/CustomTableCreator.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/config/NutchGuiConfiguration.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/config/SpringConfiguration.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/model\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/model/NutchConfig.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/model/NutchInstance.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/model/SeedList.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/model/SeedUrl.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/AbstractBasePage.html\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/AbstractBasePage.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/DashboardPage.html\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/DashboardPage.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/LogOutPage.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/SchedulingPage.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/SearchPage.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/StatisticsPage.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/UrlsUploadPage.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/UserSettingsPage.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/assets\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/assets/NutchUiCssReference.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/assets/nutch-style.css\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/components\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/components/ColorEnumLabel.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/components/ColorEnumLabelBuilder.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/components/CpmIteratorAdapter.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/crawls\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/crawls/CrawlPanel.html\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/crawls/CrawlPanel.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/crawls/CrawlsPage.html\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/crawls/CrawlsPage.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/instances\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/instances/InstancePanel.html\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/instances/InstancePanel.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/instances/InstancesPage.html\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/instances/InstancesPage.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/menu\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/menu/VerticalMenu.html\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/menu/VerticalMenu.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/seed\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/seed/SeedListsPage.html\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/seed/SeedListsPage.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/seed/SeedPage.html\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/seed/SeedPage.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/settings\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/settings/SettingsPage.html\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/pages/settings/SettingsPage.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/service\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/service/CrawlService.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/service/NutchInstanceService.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/service/NutchService.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/service/SeedListService.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/service/impl\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/service/impl/CrawlServiceImpl.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/service/impl/NutchInstanceServiceImpl.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/service/impl/NutchServiceImpl.java\n* /nutch/branches/2.x/src/java/org/apache/nutch/webui/service/impl/SeedListServiceImpl.java\n* /nutch/branches/2.x/src/test/org/apache/nutch/webui\n* /nutch/branches/2.x/src/test/org/apache/nutch/webui/client\n* /nutch/branches/2.x/src/test/org/apache/nutch/webui/client/TestCrawlCycle.java\n* /nutch/branches/2.x/src/test/org/apache/nutch/webui/client/TestNutchClientFactory.java\n* /nutch/branches/2.x/src/test/org/apache/nutch/webui/client/TestRemoteCommandExecutor.java\n* /nutch/branches/2.x/src/test/org/apache/nutch/webui/client/TestRemoteCommandsBatchFactory.java\n* /nutch/branches/2.x/src/test/org/apache/nutch/webui/service\n* /nutch/branches/2.x/src/test/org/apache/nutch/webui/service/NutchServiceTest.java\n* /nutch/branches/2.x/src/test/org/apache/nutch/webui/view\n* /nutch/branches/2.x/src/test/org/apache/nutch/webui/view/AbstractWicketTest.java\n* /nutch/branches/2.x/src/test/org/apache/nutch/webui/view/SpringConfigForTests.java\n* /nutch/branches/2.x/src/test/org/apache/nutch/webui/view/TestColorEnumLabel.java\n", "Bulk progression of all “Resolved” issues to “Closed”. Performed my lewismc on 2024-03-13."], "tasks": {"summary": "Create a Wicket-based Web Application for Nutch", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Create a Wicket-based Web Application for Nutch"}, {"question": "What is the main context?", "answer": "In light of the conversation on NUTCH-837, we are removing the old Nutch webapp and will replace it with a 2.0 one that works with GORA + Solr. \n\nApache Nutch versions prior to 1.3 used to ship with a"}]}}
{"issue_id": "NUTCH-842", "project": "NUTCH", "title": "AutoGenerate WebPage code", "status": "Closed", "priority": "Major", "reporter": "Dogacan Guney", "assignee": "Dogacan Guney", "created": "2010-07-03T07:45:37.455+0000", "updated": "2013-05-22T03:53:18.570+0000", "description": "This issue will track the addition of an ant task that will automatically generate o.a.n.storage.WebPage (and ProtocolStatus and ParseStatus) from src/gora/webpage.avsc.", "comments": ["ANT task (generate-gora-src) for the generation of Java source code from the GORA schema. Shall we do it systematically as part of the build procedure? We should also add the generated classes to svn ignore ", "I think we can put it as depends in the compile target, but then we need to first check if there's any need to generate, i.e. check the timestamp of WebPage.avsc and WebPage.java and generate only when .avsc is more recent.\n\nPerhaps we should check in generated sources anyway, because if we don't do it then it will be difficult to compile in IDE-s without running Ant first... though to tell the truth it got difficult already when we switched to ivy. Perhaps we need a separate \"eclipse\" target to resolve dependencies, run gora compiler and generate Eclipse project files?", "{quote}\nI think we can put it as depends in the compile target, but then we need to first check if there's any need to generate, i.e. check the timestamp of WebPage.avsc and WebPage.java and generate only when .avsc is more recent.\n{quote}\n\ngood idea. Do you know how to do that?\n\n{quote}\nPerhaps we should check in generated sources anyway, because if we don't do it then it will be difficult to compile in IDE-s without running Ant first... though to tell the truth it got difficult already when we switched to ivy. \n{quote}\n\nyou are right, let's not make a special case for these sources. If the schemas were to be modified by a user then it would definitely be better to be warned by svn that the files have changed. I can picture accidental changes that would go unnoticed, errors that others would not be able to reproduce etc... \n\n{quote}\nPerhaps we need a separate \"eclipse\" target to resolve dependencies, run gora compiler and generate Eclipse project files?\n{quote}\nI found that IvyDE was pretty good at managing the dependencies. Let's just leave the generated *.java files in Subversion and keep things as simple as possible\n\n", "Hey Guys,\n\nI was able to get everything working in Eclipse with IvyDE and with the mods I made to ivy.xml and ivy-dependencies.xml for Gora. After that, and clicking resolve dependencies in Eclipse (after installing Gora to Ivy locally), all worked fine.\n\nAbout storing the generated java files in SVN -- I could go either way on this. If we can cook up an easy way to autogenerate them, then why not, and then add to svn:ignore?\n\nCheers,\nChris\n", "Good while ago that this issue was last in view. Does anyone have an opinion on where we are with this one. The patch doesn't incorporate the latter comments as above, is this something which would be required?", "Set and Classify", "I've been experimenting with this one today and I am nearly there. Its become a bit of a pain in the back side if I'm honest as the problems further down stream e.g. NUTCH-1477, then subsequently GORA-174 (the current patch for which I still don't think works properly). I'll post an updated patch once I've sorted out the Ant stuff... XML is really crap for scripting.", "Hi Lewis, Can you kindly upload the changes that you had done ? That way, your changes would be in the jira and people can look into it.. maybe even take it forward and address this issue.", "Here you go Tejas. We are working further downstream in Gora so that the compiler will accept multiple [] schemas. I will update the target when we commit the code in Gora.", "Committed @revision 1453593 in 2.x HEAD", "Integrated in Nutch-nutchgora #519 (See [https://builds.apache.org/job/Nutch-nutchgora/519/])\n    NUTCH-842 AutoGenerate WebPage code (Revision 1453593)\n\n     Result = SUCCESS\nlewismc : http://svn.apache.org/viewvc/nutch/branches/2.x/?view=rev&rev=1453593\nFiles : \n* /nutch/branches/2.x/CHANGES.txt\n* /nutch/branches/2.x/build.xml\n", "Integrated in Nutch-2.x-Windows #56 (See [https://builds.apache.org/job/Nutch-2.x-Windows/56/])\n    NUTCH-842 AutoGenerate WebPage code (Revision 1453593)\n\n     Result = FAILURE\nlewismc : http://svn.apache.org/viewvc/nutch/branches/2.x/?view=rev&rev=1453593\nFiles : \n* /nutch/branches/2.x/CHANGES.txt\n* /nutch/branches/2.x/build.xml\n"], "tasks": {"summary": "AutoGenerate WebPage code", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "AutoGenerate WebPage code"}, {"question": "What is the main context?", "answer": "This issue will track the addition of an ant task that will automatically generate o.a.n.storage.WebPage (and ProtocolStatus and ParseStatus) from src/gora/webpage.avsc."}]}}
{"issue_id": "NUTCH-843", "project": "NUTCH", "title": "Separate the build and runtime environments", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-07-07T15:38:45.046+0000", "updated": "2013-05-22T03:53:25.079+0000", "description": "Currently there is no clean separation of source, build and runtime artifacts. On one hand, it makes it easier to get started in local mode, but on the other hand it makes the distributed (or pseudo-distributed) setup much more challenging and tricky. Also, some resources (config files and classes) are included several times on the classpath, they are loaded under different classloaders, and in the end it's not obvious what copy and why takes precedence.\n\nHere's an example of a harmful unintended behavior caused by this mess: Hadoop daemons (jobtracker and tasktracker) will get conf/ and build/ on their classpath. This means that a task running on this cluster will have two copies of resources from these locations - one from the inherited classpath from tasktracker, and the other one from the just unpacked nutch.job file. If these two versions differ, only the first one will be loaded, which in this case is the one taken from the (unpacked) conf/ and build/ - the other one, from within the nutch.job file, will be ignored.\n\nIt's even worse when you add more nodes to the cluster - the nutch.job will be shipped to the new nodes as a part of each task setup, but now the remote tasktracker child processes will use resources from nutch.job - so some tasks will use different versions of resources than other tasks. This usually leads to a host of very difficult to debug issues.\n\nThis issue proposes then to separate these environments into the following areas:\n\n* source area - i.e. our current sources. Note that bin/ scripts will belong to this category too, so there will be no top-level bin/. nutch-default.xml belongs to this category too. Other customizable files can be moved to src/conf too, or they could stay in top-level conf/ as today, with a README that explains that changes made there take effect only after you rebuild the job jar.\n\n* build area - contains build artifacts, among them the nutch.job jar.\n\n* runtime (or deploy) area - this area contains all artifacts needed to run Nutch jobs. For a distributed setup that uses an existing Hadoop cluster (installed from plain vanilla Hadoop release) this will be a {{/deploy}} directory, where we put the following:\n{code}\nbin/nutch\nnutch.job\n{code}\nThat's it - nothing else should be needed, because all other resources are already included in the job jar. These resources can be copied directly to the master Hadoop node.\n\nFor a local setup (using LocalJobTracker) this will be a {{/runtime}} directory, where we put the following:\n{code}\nbin/nutch\nlib/hadoop-libs\nplugins/\nnutch.job\n{code}\nDue to limitations in the PluginClassLoader the local runtime requires that the plugins/ directory be unpacked from the job jar. And we need the hadoop libs to run in the local mode. We may later on refine this local setup to something like this:\n{code}\nbin/nutch\nconf/\nlib/hadoop-libs\nlib/nutch-libs\nplugins/\nnutch.jar\n{code}\nso that it's easier to modify the config without rebuilding the job jar (which actually would not be used in this case).", "comments": ["Super +1 \n\nI've wanted to do something like this for a looong time http://markmail.org/thread/osmfz6pknr4n4unf\n\n;)\n\nLet me think about the deployment structure a little bit and comment back on this issue...", "OK, so I read this more. I think it would be great if we didn't have to maintain 2 diff deployment structures based on local/remote. Some comments on your local proposal:\n\n{code}\nbin/nutch           - the main nutch script\nconf/                 - all relevant Nutch conf files\nlib/hadoop-libs  - static Hadoop lib files - are these jar files?\nlib/nutch-libs     - what are these? jar files?\nplugins/             - are these the plugin directories, or plugin jar files? \nnutch.jar           -  why wouldn't this go into the lib directory?\n{code}\n\nI could envision having one simple deployment structure that looked like this:\n\n{code}\n./bin/          - nutch script goes into here\n./etc/          - all Nutch configuration property files, like nutch-default.xml, nutch-site.xml\n./lib/           - all shared Nutch jar files (including the nutch.jar and hadoop.jar, as well as deps). Also it would be great to be able to generate a per-plugin jars that we could include in this lib directory as well. \n./logs/        - where all log files are written to\n./run/         - where PID files (if generated) are written to\n{code}\n\nThoughts?\n ", "This patch moves bin/nutch to src/bin/nutch, and creates /runtime/deploy and /runtime/local areas, populated with the right pieces. bin/nutch has been modified to work correctly in both cases.\n\n(Edit) Sorry, I just read your comment - I'm afraid of having a single area, because then again it's not clear what bits and pieces need to be deployed to Hadoop master.", "Hey Andrzej:\n\nWouldn't my proposed deployment structure in theory be equivalent to say creating a .job file as you proposed above? You can think of the proposed dir structure as an exploded version of the unpacked .job?\n\nCheers,\nChris\n", "We need to create the job file anyway. Actually, the patch I attached does something like this for the local setup (lib/ is flattened), but still I would argue for setting up two areas, /runtime/deploy and /runtime/local - it's painfully obvious then what parts you need to deploy to a Hadoop cluster.", "Updated patch that moves nutch.jar to lib/ for the local runtime.", "+1 I think this patch makes great progress! I think it would be good to tease out a single deployment structure in the future, but this works perfect for now...", " Committed in r961498. Thanks Chris for the review!", "I really like this. \n\nWhat shall we do with the hadoop scripts in /bin and the native libs in /lib? Should they go to runtime/local as well?\n", "runtime/local doesn't need Hadoop scripts, by definition it uses local FS and local job tracker, so Hadoop scripts are of no use. Native libs .. see NUTCH-845.", "OK - for some reason I thought we could use runtime/local in pseudo distributed mode as well. Probably need another coffee :-)", "Pseudo-distributed (i.e. a real JobTracker with a single TaskTracker) suffers from the same classpath issues that I described above, so even in such case it's best to run jobs in a separate environment, using /runtime/deploy artifacts.", "Hi,\n\nI found that after building runtime,\n\nIn nutch-2.0-dev.job and local\\lib directory contains different versions of the same library \n\nant-1.7.1.jar\nant-1.6.5.jar\n\nservlet-api-2.5-20081211.jar\nservlet-api-2.5-6.1.14.jar\n\nThanks,", " revision 963217 : removed task extract-hadoop from Ant build to avoid creation of hadoop scripts in bin dir\n\n@pham : your comment is not relevant to this issue. please create a separate issue, thanks ", "Thanks Julien, just call me Minh! \n\nrevision 963217 resolved my comment on issue NUTCH-846\n\nhttps://issues.apache.org/jira/browse/NUTCH-846\n\nThanks,\n\n"], "tasks": {"summary": "Separate the build and runtime environments", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Separate the build and runtime environments"}, {"question": "What is the main context?", "answer": "Currently there is no clean separation of source, build and runtime artifacts. On one hand, it makes it easier to get started in local mode, but on the other hand it makes the distributed (or pseudo-d"}]}}
{"issue_id": "NUTCH-844", "project": "NUTCH", "title": "Improve NutchConfiguration", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-07-08T12:59:28.992+0000", "updated": "2013-05-22T03:53:34.951+0000", "description": "This patch cleans up NutchConfiguration from servlet dependency, and modifies the API to allow bootstrapping via API from Properties. This is important for use cases where Nutch is embedded in a larger application.\n\nAlso, while I'm at it, remove the support for alternative \"crawl\" configuration when running Crawl tool, which has always been a source of confusion.", "comments": ["Updated patch. This also addresses an issue in PluginRepository that uses Configuration as a key in its internal cache - the problem though is that Configuration doesn't implement hashCode, so the cache would have been ineffective in situations like this:\n{code}\nConfiguration conf = NutchConfiguration.create();\nPluginRepository repo1 = PluginRepository.get(conf);\nJobConf job = new NutchJob(conf);\nPluginRepository repo2 = PluginRepository.get(job);\n// repo2 is a new instance, but should be the same instance!\n{code}\n\nThe new code sets a UUID property, so the cache knows it's still the same instance. There's a new unit test to ensure this works properly when using NutchConfiguration.create(), and illustrates that it fails without the uuid.", "The latest patch removes src/plugin/parse-tika/src/test/org/apache/nutch/tika/TestPdfParser.java which I expect is not on purpose\nApart from that +1 for the rest of it", "Indeed! Ok, I'll apply the patch sans the deletion. Thanks!", "Committed in r964063. Thanks for review!", "I believe this fix also addresses that old issue."], "tasks": {"summary": "Improve NutchConfiguration", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Improve NutchConfiguration"}, {"question": "What is the main context?", "answer": "This patch cleans up NutchConfiguration from servlet dependency, and modifies the API to allow bootstrapping via API from Properties. This is important for use cases where Nutch is embedded in a large"}]}}
{"issue_id": "NUTCH-845", "project": "NUTCH", "title": "Native hadoop libs not available through maven", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-07-08T13:34:31.610+0000", "updated": "2013-05-22T03:53:22.202+0000", "description": "There are no maven artifacts for the native libs (I verified this on Hadoop ML). I think it's better to delete the libs, after all we don't want to keep bits and pieces of dependencies in our svn, but let's leave a placeholder and a README that explains how to get them.", "comments": ["+1!!", "+1\n\nThere is always a possibility to use the job in a separate Hadoop setup and have the native libs there anyway \n\n", "Committed in rev. 961778. Thanks for review!"], "tasks": {"summary": "Native hadoop libs not available through maven", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Native hadoop libs not available through maven"}, {"question": "What is the main context?", "answer": "There are no maven artifacts for the native libs (I verified this on Hadoop ML). I think it's better to delete the libs, after all we don't want to keep bits and pieces of dependencies in our svn, but"}]}}
{"issue_id": "NUTCH-846", "project": "NUTCH", "title": "Remove Hadoop related scripts in /bin", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2010-07-09T08:06:03.851+0000", "updated": "2010-07-09T15:17:51.111+0000", "description": "The build and runtime environments have been separated in https://issues.apache.org/jira/browse/NUTCH-843 and we now have two separate folders for local and deployed configurations. The Hadoop scripts in /bin are of no use in local mode. As for the deployed configuration it is meant to be used in a separate Hadoop setup which will have these scripts already. I don't think we have any use for /bin, do we?", "comments": ["+1. We don't need Hadoop scripts in runtime/local/bin, correct. Of course we still need runtime/local/bin/nutch.", "Actually the /bin directory has already been removed as part of : http://svn.apache.org/viewvc?view=revision&revision=961498 \nFor some reason it was still in my local copy of the repository after the update. Problem sorted then", "Hi,\n\nI use Window XP SP3 with cygwin environment.\n\nAfter run ant clean command, it still remain some hadoop scripts in %NUTCH_HOME%\\bin directory\n\n\\nutch\\bin\n\\nutch\\bin\\stop-mapred.sh\n\\nutch\\bin\\stop-dfs.sh\n\\nutch\\bin\\stop-balancer.sh\n\\nutch\\bin\\stop-all.sh\n\\nutch\\bin\\start-mapred.sh\n\\nutch\\bin\\start-dfs.sh\n\\nutch\\bin\\start-balancer.sh\n\\nutch\\bin\\start-all.sh\n\\nutch\\bin\\slaves.sh\n\\nutch\\bin\\rcc\n\\nutch\\bin\\hadoop-daemons.sh\n\\nutch\\bin\\hadoop-daemon.sh\n\\nutch\\bin\\hadoop-config.sh\n\\nutch\\bin\\hadoop\n\nThanks,"], "tasks": {"summary": "Remove Hadoop related scripts in /bin", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove Hadoop related scripts in /bin"}, {"question": "What is the main context?", "answer": "The build and runtime environments have been separated in https://issues.apache.org/jira/browse/NUTCH-843 and we now have two separate folders for local and deployed configurations. The Hadoop scripts"}]}}
{"issue_id": "NUTCH-847", "project": "NUTCH", "title": "Wrong version of SOLR in Ivy.xml", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2010-07-09T12:04:53.438+0000", "updated": "2010-07-09T15:20:54.978+0000", "description": "We'd upgraded to SOLR 4.0 before releasing 1.1 but the Ivy config currently still mentions 1.3.0", "comments": ["Committed revision 962497.\n", "Hi Julien,\n\nHow about other libraries in ivy config? They should be latest stable version.\n\ncommons-httpclient: 3.0.1 => 3.1\nlucene-core: 2.4.0 => 3.0.2\njetty: 6.1.7 => 6.1.22 \njetty-util: 6.1.7 => 6.1.22 \n\nThanks,", "Thanks. I'll commit the change", "Also \nplugin\\lib-nekohtml\\ivy.xml\nnekohtml: 0.9.5 => 1.9.6.2\n\nplugin\\lib-xml\\ivy.xml\nxercesImpl: 2.6.2 => 2.9.1\n\nThanks", "Nutch 1.1 came with the version 0.9.4 for lib-nekohtml not 1.9.6.2\n\nThis issue is about having the same version of the dependencies as 1.1 not upgrading to the latest available. We'd need to check that the API have not changed and that it does not break anything in Nutch etc... These 2 plugins will certainly be removed in a not too distant future anyway\n\nJ.", "Thanks Julien!"], "tasks": {"summary": "Wrong version of SOLR in Ivy.xml", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Wrong version of SOLR in Ivy.xml"}, {"question": "What is the main context?", "answer": "We'd upgraded to SOLR 4.0 before releasing 1.1 but the Ivy config currently still mentions 1.3.0"}]}}
{"issue_id": "NUTCH-848", "project": "NUTCH", "title": "Error when calling 'nutch solrindex' in deployed configuration", "status": "Closed", "priority": "Blocker", "reporter": "Julien Nioche", "assignee": null, "created": "2010-07-09T15:37:29.242+0000", "updated": "2010-07-14T13:57:12.648+0000", "description": "See https://issues.apache.org/jira/browse/NUTCH-843\n\nIn a deployed environment with just bin/nutch and the job file on an independent Hadoop configuration, a call to 'nutch solrindex' yields : \n\n{code}\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/solr/client/solrj/SolrServer\nCaused by: java.lang.ClassNotFoundException: org.apache.solr.client.solrj.SolrServer\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:200)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:188)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:252)\n\tat java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)\nCould not find the main class: org.apache.nutch.indexer.solr.SolrIndexer.  Program will exit.\n{code}\n\nSurprisingly some tasks work fine, but not this one. \n", "comments": ["I use following to test solrindex command\n- Nutch trunk (run from runtime\\local directory)\n- Solr/lucene branch 3x \n- Window XP SP3, Cygwin\n\n(I follow the steps defined at http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/)\n\nResult: After run: bin/nutch solrindex http://127.0.0.1:8983/solr/ crawl/crawldb crawl/linkdb crawl/segments/*, \nIt return to console:\n\n-------------\nSolrIndexer: starting at 2010-07-13 16:57:52\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\njava.io.IOException: Job failed!\n-------------\n\nThanks,\n\n\n\n", "According to http://www.slf4j.org/codes.html#StaticLoggerBinder, I added required library when running command solrindex. Anyone help me to add this library using Ivy to build.xml?\n\n(Use one of slf4j-nop.jar, slf4j-simple.jar, slf4j-log4j12.jar, slf4j-jdk14.jar or logback-classic.jar)\n\nHowever, I still got another issue, it throws to console:\n---------------------\nSolrIndexer: starting at 2010-07-13 17:24:23\norg.apache.solr.common.SolrException: Not Found\n\nNot Found\n\nrequest: http://127.0.0.1:8983/solr/update\n-----------------------\nSurprisingly, org.apache.solr.common.SolrException is in solr-solrj-1.4.1.jar\n\nThanks,\n\n", "The issue I described is due to the fact that the classloader can't find the jars in the /lib directory of the job archive (if that makes any sense). This is not the case when the library is called within mapreduce as Hadoop unpacks everything it needs from the job and puts it in the classpath. In the case of the command 'nutch solrindex', the driver class has a dependency to the SOLR library and it cannot be found from the job.\n\nOne possible solution would be to have a classloader which would be able to deal with jars within jars but IMHO an even simpler one would be to modify the nutch script used in the deployed configuration so that it starts the job using \"hadoop jar NUTCH.job\". I will attach a patch shortly.\n\n@Minh : your comments are interesting but (again) not related to the issue. Your comment is about dependencies in local mode whereas the problem I described is about the deployed configuration. Thanks", "Hi Julien, all:\n\n{quote}\nOne possible solution would be to have a classloader which would be able to deal with jars within jars but IMHO an even simpler one would be to modify the nutch script used in the deployed configuration so that it starts the job using \"hadoop jar NUTCH.job\". I will attach a patch shortly.\n{quote}\n\nFWIW, Dennis Kubes mentioned it might be good to load plugin jars from within jars in NUTCH-609. We punted on it since it seemed to be something that DI frameworks handle nicely and have rich communities around already.\n\n\nCheers,\nChris\n", "Modified version of the nutch script to use in a deployed environment. Note that this will not work in local mode.\nWill modify the build script so that this is put in the right place at compilation time\n\n@Chris : thanks for the pointer. I think Andrzej has found an implementation of a ClassLoader which would allow to do just that. It will probably require some time to use and test it in Nutch so in the short term maybe we could go with the script attached. We can't really leave the trunk broken much longer", "Hey Julien: +1, the trunk can't be broken, so let's get a fix in...", "Patch which adds 2 variants of the nutch script to src/bin and uses them at compilation time.\nNote that the scripts are now called nutch-local and nutch-deploy but we could eventually rename name into nutch as it used to be before", "Hi Julien,\n\nIf you separate nutch-local and nutch-deploy, in nutch-local should not contain related things to nutch job,... etc\n\n\\# distributed runtime\nfor f in $NUTCH_HOME/nutch-*.job; do\n  CLASSPATH=${CLASSPATH}:$f;\ndone\n\nThanks,", "Minh - why don't you send a patch?", "This patch included Julien patch. Thanks!", "Thinking about it more, I don't like the current patch... it means maintaining two nearly identical scripts, which is clunky and prone to errors.\n\nI propose a different way to solve this: since we already know in the script whether it's run as a distributed or local, why not use the same uber-script in both cases, just use a different \"execute\" command:\n\n* in case of a local runtime use {{EXECUTE=\"$JAVA $CLASSPATH\"}}, then you call {{\"$EXECUTE <args>\"}}\n* in case of a distributed runtime use {{EXECUTE=\"$HADOOP_HOME/bin/hadoop $NUTCH_HOME/$NUTCH_JOB\"}}. BTW, we need to discover the right value of HADOOP_HOME - if it's not already set then use NUTCH_HOME, otherwise use that value (nutch could be run from a different path than the hadoop installation)", "Andrzej your suggestion makes sense. Here is a patch which does it, I've tested it and it seems to work fine in local and distributed mode\nSome of the tasks have been removed and I'll deal with them in a separate issue\nThanks ", "Committed revision 964050\n\nThanks for the comments and review"], "tasks": {"summary": "Error when calling 'nutch solrindex' in deployed configuration", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Error when calling 'nutch solrindex' in deployed configuration"}, {"question": "What is the main context?", "answer": "See https://issues.apache.org/jira/browse/NUTCH-843\n\nIn a deployed environment with just bin/nutch and the job file on an independent Hadoop configuration, a call to 'nutch solrindex' yields : \n\n{code"}]}}
{"issue_id": "NUTCH-849", "project": "NUTCH", "title": "different versions of the same library in nutch-2.0-dev.job and local\\lib directory ", "status": "Closed", "priority": "Minor", "reporter": "Pham Tuan Minh", "assignee": null, "created": "2010-07-12T10:01:35.907+0000", "updated": "2023-01-08T19:49:45.583+0000", "description": "Hi,\n\nI found that after building runtime, In nutch-2.0-dev.job and local\\lib directory contains different versions of the same library\n\nant-1.7.1.jar\nant-1.6.5.jar\n\nservlet-api-2.5-20081211.jar\nservlet-api-2.5-6.1.14.jar\n\nI predict these libraries come from different dependencies branch. Anyone help me to fix it?\n\nThanks,\n", "comments": ["you can type \"ant report\" to get more details about the dependencies managed by Ivy. This will create a file build/org.apache.nutch-Nutch-test.html with all the details\n\n ", "I checked out the latest trunk 2.0 from svn and after compiling checked all jar files in runtime/deploy/nutch-2.0-dev.job and /runtime/local/lib.\n\nAll jar files in both libraries are identical and versions are consistent therefore I propose we close this issue as fixed. Perhaps someone committed a change and didn't realise they had addressed this issue. ", "I see it in my 1.4-build too with several deps. ant report only throws alot of\n\n{code}\n[ivy:resolve]   unknown resolver maven2\n{code}\n\nmessages.", "Confirmed to affect 1.X also, my gut instinct is that what pham describes with dependencies getting dragged up. Therefore this sounds like more of an ivy review and configuration.\n\nSet and classify", "(closing this old dependency issue) - thanks anyway, [~phamtuanminh2004] !"], "tasks": {"summary": "different versions of the same library in nutch-2.0-dev.job and local\\lib directory ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "different versions of the same library in nutch-2.0-dev.job and local\\lib directory "}, {"question": "What is the main context?", "answer": "Hi,\n\nI found that after building runtime, In nutch-2.0-dev.job and local\\lib directory contains different versions of the same library\n\nant-1.7.1.jar\nant-1.6.5.jar\n\nservlet-api-2.5-20081211.jar\nservle"}]}}
{"issue_id": "NUTCH-85", "project": "NUTCH", "title": "pdf parser caused fetcher hangs.", "status": "Closed", "priority": "Major", "reporter": "Stefan Groschupf", "assignee": null, "created": "2005-08-25T18:12:18.000+0000", "updated": "2005-09-20T16:09:17.000+0000", "description": "We notice that fetcher hangs caused by pdfbox.\nA thread handles a pdf parsing and may hangs and is never again available. \nThis happens as many times as threads are active and than the complete fetch process hangs.\n \n\n\nFull thread dump Java HotSpot(TM) Client VM (1.4.2_08-b03 mixed mode):\n\n\"fetcher160\" prio=1 tid=0x083c9720 nid=0x16de runnable [b1669000..b166a238]\n\tat org.pdfbox.cmaptypes.CMap.addMapping(CMap.java:119)\n\tat org.pdfbox.cmapparser.CMapParser.parse(CMapParser.java:183)\n\tat org.pdfbox.pdmodel.font.PDFont.parseCmap(PDFont.java:532)\n\tat org.pdfbox.pdmodel.font.PDFont.encode(PDFont.java:358)\n\tat org.pdfbox.util.PDFStreamEngine.showString(PDFStreamEngine.java:261)\n\tat org.pdfbox.util.operator.ShowText.process(ShowText.java:63)\n\tat org.pdfbox.util.PDFStreamEngine.processOperator(PDFStreamEngine.java:405)\n\tat org.pdfbox.util.PDFStreamEngine.processOperator(PDFStreamEngine.java:385)\n\tat org.pdfbox.util.PDFStreamEngine.processStream(PDFStreamEngine.java:168)\n\tat org.pdfbox.util.PDFTextStripper.processPage(PDFTextStripper.java:232)\n\tat org.pdfbox.util.PDFTextStripper.processPages(PDFTextStripper.java:205)\n\tat org.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:180)\n\tat org.pdfbox.util.PDFTextStripper.getText(PDFTextStripper.java:108)\n\tat org.apache.nutch.parse.pdf.PdfParser.getParse(PdfParser.java:123)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.handleFetch(Fetcher.java:239)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:148)\n\n\n\n\"fetcher82\" prio=1 tid=0xb4637d78 nid=0x59aa runnable [b4379000..b437a238]\n\tat java.nio.charset.CoderResult$1.create(CoderResult.java:207)\n\tat java.nio.charset.CoderResult$Cache.get(CoderResult.java:196)\n\t- locked <0xb94fa908> (a java.nio.charset.CoderResult$1)\n\tat java.nio.charset.CoderResult$Cache.access$200(CoderResult.java:178)\n\tat java.nio.charset.CoderResult.malformedForLength(CoderResult.java:217)\n\tat sun.nio.cs.UnicodeDecoder.decodeLoop(UnicodeDecoder.java:71)\n\tat java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:538)\n\tat java.lang.StringCoding$CharsetSD.decode(StringCoding.java:192)\n\tat java.lang.StringCoding.decode(StringCoding.java:230)\n\tat java.lang.String.<init>(String.java:320)\n\tat java.lang.String.<init>(String.java:346)\n\tat org.pdfbox.cmapparser.CMapParser.createStringFromBytes(CMapParser.java:230)\n\tat org.pdfbox.cmapparser.CMapParser.parse(CMapParser.java:182)\n\tat org.pdfbox.pdmodel.font.PDFont.parseCmap(PDFont.java:532)\n\tat org.pdfbox.pdmodel.font.PDFont.encode(PDFont.java:358)\n\tat org.pdfbox.util.PDFStreamEngine.showString(PDFStreamEngine.java:261)\n\tat org.pdfbox.util.operator.ShowText.process(ShowText.java:63)\n\tat org.pdfbox.util.PDFStreamEngine.processOperator(PDFStreamEngine.java:405)\n\tat org.pdfbox.util.PDFStreamEngine.processOperator(PDFStreamEngine.java:385)\n\tat org.pdfbox.util.PDFStreamEngine.processStream(PDFStreamEngine.java:168)\n\tat org.pdfbox.util.PDFTextStripper.processPage(PDFTextStripper.java:232)\n\tat org.pdfbox.util.PDFTextStripper.processPages(PDFTextStripper.java:205)\n\tat org.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:180)\n\tat org.pdfbox.util.PDFTextStripper.getText(PDFTextStripper.java:108)\n\tat org.apache.nutch.parse.pdf.PdfParser.getParse(PdfParser.java:123)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.handleFetch(Fetcher.java:239)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:148)\n\n", "comments": ["This has been reported and fixed in the newer versions of PDFBox. These versions haven't been released yet as an official release, so I decided not to bring this into our repository, until it's released. In the meantime, you can avoid this issue by replacing the PDFBox library with the nightly build downloaded from http://www.pdfbox.org/dist/ .", "The parser has been updated to use PDFBox-0.7.2, which should solve this issue. Please re-open if that's not the case."], "tasks": {"summary": "pdf parser caused fetcher hangs.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "pdf parser caused fetcher hangs."}, {"question": "What is the main context?", "answer": "We notice that fetcher hangs caused by pdfbox.\nA thread handles a pdf parsing and may hangs and is never again available. \nThis happens as many times as threads are active and than the complete fetch "}]}}
{"issue_id": "NUTCH-850", "project": "NUTCH", "title": "SolrDeleteDuplicates needs to clone the SolrRecord objects ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-07-12T15:57:16.943+0000", "updated": "2010-07-12T16:11:14.203+0000", "description": "The reduce() method of SolrDeleteDuplicates deduplicates SOLRRecords given their signature. The first SOLRRecord is stored in a variable _recordToKeep_ and is compared to the following SOLRRecords found with the same signature. The only trouble being that the first instance is reused by Hadoop when calling values.next() and hence  _recordToKeep_ gets the same values as the latest call to values.next(). \n\nThe patch attached clones the SOLRRecord before assigning them to _recordToKeep_ in order to avoid the problem.", "comments": ["Committed revision 963328 (1.2)\nCommitted revision 963330 (trunk)\n"], "tasks": {"summary": "SolrDeleteDuplicates needs to clone the SolrRecord objects ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "SolrDeleteDuplicates needs to clone the SolrRecord objects "}, {"question": "What is the main context?", "answer": "The reduce() method of SolrDeleteDuplicates deduplicates SOLRRecords given their signature. The first SOLRRecord is stored in a variable _recordToKeep_ and is compared to the following SOLRRecords fou"}]}}
{"issue_id": "NUTCH-851", "project": "NUTCH", "title": "Port logging to slf4j", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-07-13T16:10:30.012+0000", "updated": "2010-08-10T08:46:59.151+0000", "description": "We are already inheriting a dependency on slf4j from Solr so we might as well use it :-)\nAny thoughts on this?", "comments": ["Definitely, +1.", "Boo! :)\n\n...fan of JDK logging...", "You are being facetious, aren't you. JDK logging is arguably the worst logging API (if you dismiss System.out.println), giving you the least control and flexibility. Slf4j is the opposite, and allows you even to use JDK logging, if that's what you like...\n\nAnd we have to include slf4j anyway.", "LOL yep I'm sort of a fan of JDK logging b/c it doesn't introduce extra dependencies. To be honest, most of the logging frameworks all look the same to me, but it's probably b/c I haven't compared them in detail. :)\n\nThe facetious part though is that I don't really feel strongly about it to do anything but boo to adding extra dependencies :)", "I'm a fan of Log4j, but I love something is simple, compact and no more dependencies like JDK logging.\n\nBy this, it allows end user to choose their desired logging framework at deployment time. It's very interesting feature!\n\n", "Used the migrator tool from slf4j http://www.slf4j.org/migrator.html + manually replaced fatal() by error() + various minor fixes\n\nI have commented out the dependency on commons-logging but left the log4j adapter (slf4j-log4j12) so that we are now using log4j through slf4j. The beauty of it is that users can swap to their favourite logging system by replacing the ivy dependencies\n\nThe patch compiles and goes through the tests OK\n\n\n\n", "Updated the patch to the 2.0 code. \n\nWill commit tomorrow if there aren't any objections", "Committed revision 983885"], "tasks": {"summary": "Port logging to slf4j", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Port logging to slf4j"}, {"question": "What is the main context?", "answer": "We are already inheriting a dependency on slf4j from Solr so we might as well use it :-)\nAny thoughts on this?"}]}}
{"issue_id": "NUTCH-852", "project": "NUTCH", "title": "parser not found for contentType=application/xhtml+xml", "status": "Closed", "priority": "Major", "reporter": "Pham Tuan Minh", "assignee": "Julien Nioche", "created": "2010-07-13T19:29:09.406+0000", "updated": "2011-04-13T23:48:07.550+0000", "description": "I config nutch trunk to crawl sample site (http://www.lucidimagination.com/), then it post to solr server for indexing, however, I got following error. It seems tika parser is not working properly or tika libraries is not recognized!\n----------------------\n$ bin/nutch-local crawl urls -solr http://127.0.0.1:8983/solr/ -dir crawl -depth 3 -topN 50\ncrawl started in: crawl\nrootUrlDir = urls\nthreads = 10\ndepth = 3\nsolrUrl=http://127.0.0.1:8983/solr/\ntopN = 50\nInjector: starting at 2010-07-14 02:08:20\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2010-07-14 02:08:31, elapsed: 00:00:11\nGenerator: starting at 2010-07-14 02:08:32\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 50\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: crawl/segments/20100714020838\nGenerator: finished at 2010-07-14 02:08:42, elapsed: 00:00:10\nFetcher: Your 'http.agent.name' value should be listed first in 'http.robots.age\nnts' property.\nFetcher: starting at 2010-07-14 02:08:42\nFetcher: segment: crawl/segments/20100714020838\nFetcher: threads: 10\nQueueFeeder finished: total 1 records + hit by time limit :0\nfetching http://www.lucidimagination.com/\n-finishing thread FetcherThread, activeThreads=4\n-finishing thread FetcherThread, activeThreads=1\n-finishing thread FetcherThread, activeThreads=2\n-finishing thread FetcherThread, activeThreads=3\n-finishing thread FetcherThread, activeThreads=5\n\n-finishing thread FetcherThread, activeThreads=7\n-finishing thread FetcherThread, activeThreads=8\n-finishing thread FetcherThread, activeThreads=9\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0\nError parsing: http://www.lucidimagination.com/: org.apache.nutch.parse.ParseException: parser not found for contentType=application/xhtml+xml url=http://www.lucidimagination.com/\n        at org.apache.nutch.parse.ParseUtil.parse(ParseUtil.java:74)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.output(Fetcher.java:879)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:647)\n\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: finished at 2010-07-14 02:08:54, elapsed: 00:00:12\nCrawlDb update: starting at 2010-07-14 02:08:54\nCrawlDb update: db: crawl/crawldb\nCrawlDb update: segments: [crawl/segments/20100714020838]\nCrawlDb update: additions allowed: true\n$\nCrawlDb update: URL filtering: true\nCrawlDb update: Merging segment data into db.\nCrawlDb update: finished at 2010-07-14 02:09:01, elapsed: 00:00:07\n$\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 50\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=1 - no more URLs to fetch.\nLinkDb: starting at 2010-07-14 02:09:06\nLinkDb: linkdb: crawl/linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: true\nLinkDb: adding segment: file:/D:/work/workspace/nutch/runtime/local/crawl/segments/20100714014136\nLinkDb: adding segment: file:/D:/work/workspace/nutch/runtime/local/crawl/segments/20100714015544\nLinkDb: adding segment: file:/D:/work/workspace/nutch/runtime/local/crawl/segments/20100714020206\nLinkDb: adding segment: file:/D:/work/workspace/nutch/runtime/local/crawl/segments/20100714020232\nLinkDb: adding segment: file:/D:/work/workspace/nutch/runtime/local/crawl/segments/20100714020838\nLinkDb: merging with existing linkdb: crawl/linkdb\nLinkDb: finished at 2010-07-14 02:09:19, elapsed: 00:00:12\nSolrIndexer: starting at 2010-07-14 02:09:19\nSolrIndexer: finished at 2010-07-14 02:09:36, elapsed: 00:00:17\nSolrDeleteDuplicates: starting at 2010-07-14 02:09:41\nSolrDeleteDuplicates: Solr url: http://127.0.0.1:8983/solr/\nSolrDeleteDuplicates: finished at 2010-07-14 02:09:45, elapsed: 00:00:04\ncrawl finished: crawl\n----------------------\n\nThanks", "comments": ["I ran the crawl command using the latest trunk and did not get the parse error. I also tried parsing directly with Tika and everything went fine. Could you check that your configuration does not differ from the trunk? Thanks ", "Hi Julien,\n\nThank for supports!\n\nI checked again, I missed some plug-in in plugin.includes attribute in nutch-site.xml. Currently, this file contains no attribute  and suggestion, so it is quite difficult for end user. I will add it in other issue for improvement.\n\nThanks,", "Closing all resolved issues with a non-fixed status."], "tasks": {"summary": "parser not found for contentType=application/xhtml+xml", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "parser not found for contentType=application/xhtml+xml"}, {"question": "What is the main context?", "answer": "I config nutch trunk to crawl sample site (http://www.lucidimagination.com/), then it post to solr server for indexing, however, I got following error. It seems tika parser is not working properly or "}]}}
{"issue_id": "NUTCH-853", "project": "NUTCH", "title": "Remove unused parameter files from conf/ ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-07-14T15:05:32.949+0000", "updated": "2010-07-14T16:19:54.479+0000", "description": "Since we separated the build and runtime environments (NUTCH-843) some of the resources in conf/ are now useless. This is the case for instance of slave,master, hadoop-policy.xml, hdfs-site.xml etc... which are generated automatically from the templates. In local mode the hadoop related resources are not used and in deployed mode we already have them in the hadoop setup proper. Others like context.xsl were relevant for the servlets which have now been removed", "comments": ["Committed revision 964084.\n", "Hrm: I'm not sold on context.xsl being removed, since Nutch 2.0 needs to have a webapp, as I've indicated before.\n\nCan you add it back?", "There are quite a few things that might be used by the webapp which have been already removed (jars, etc...). We might as well add this back at the same time as the rest of the webapp components. We might discover that it is not needed etc... \nAm happy to add it back if you feel strongly about it though", "Meh, potentially. I think having the XSL file was nice b/c it slurped out the relevant config params from the Nutch conf files into a webapp context file. We'll want to do pretty much the same thing in some form I'm guessing for the eventual 2.0 webapp. \n\nI'm happy to wait until then, but how about waiting longer than 5 minutes to commit and close the issue out next time? ;)"], "tasks": {"summary": "Remove unused parameter files from conf/ ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove unused parameter files from conf/ "}, {"question": "What is the main context?", "answer": "Since we separated the build and runtime environments (NUTCH-843) some of the resources in conf/ are now useless. This is the case for instance of slave,master, hadoop-policy.xml, hdfs-site.xml etc..."}]}}
{"issue_id": "NUTCH-854", "project": "NUTCH", "title": "Define standard attributes with values and explaination to configuration files in conf directory", "status": "Closed", "priority": "Major", "reporter": "Pham Tuan Minh", "assignee": null, "created": "2010-07-14T18:00:55.947+0000", "updated": "2011-04-01T15:07:20.683+0000", "description": "It would make nutch easier to use if all configuration file in conf directory is defined standard attributes with values and explanation. For example, currently nutch-site.xml.template contains no attributes and no explanation, we should define them.\n\n-------------\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<!-- site-specific property overrides in this file. -->\n\n<configuration>\n\n<!-- Agent name-->\n<property>\n<name>http.agent.name</name>\n<value>nutch-solr-integration</value>\n</property>\n\n<!---->\n<property>\n<name>generate.max.per.host</name>\n<value>100</value>\n</property>\n<property>\n\n<!-- plug-in using in this site -->\n<name>plugin.includes</name>\n<value>protocol-http|urlfilter-regex|parse-tika|scoring-opic|urlnormalizer-(pass|regex|basic)</value>\n</property>\n</configuration>\n-------------\n\nThanks,", "comments": ["My idea is we define standard attributes for nutch work. In case user want some customizations in crawling their web site (data source), they will define their attributes in nutch-site.xml to override.", "nutch-default.xml already does that : all the parameters are listed and commented along with their default values.", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Define standard attributes with values and explaination to configuration files in conf directory", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Define standard attributes with values and explaination to configuration files in conf directory"}, {"question": "What is the main context?", "answer": "It would make nutch easier to use if all configuration file in conf directory is defined standard attributes with values and explanation. For example, currently nutch-site.xml.template contains no att"}]}}
{"issue_id": "NUTCH-855", "project": "NUTCH", "title": "ScoringFilter and IndexingFilter: To allow for the propagation of URL Metatags and their subsequent indexing.", "status": "Closed", "priority": "Major", "reporter": "Scott Gonyea", "assignee": "Chris A. Mattmann", "created": "2010-07-15T01:48:07.151+0000", "updated": "2013-05-22T03:53:35.818+0000", "description": "This plugin is designed to enhance the NUTCH-655 patch, by doing two things:\n1. Meta Tags that are supplied with your Crawl URLs, during injection, will be propagated throughout the outlinks of those Crawl URLs.\n2. When you index your URLs, the meta tags that you specified with your URLs will be indexed alongside those URLs--and can be directly queried, assuming you have done everything else correctly.\n\nThe flat-file of URLs you are injecting should, per NUTCH-655, be tab-delimited in the form of:\nwww.url.com\\tkey1=value1\\tkey2=value2\\t...\\tkeyN=valueN\nor:\nhttp://slashdot.org/\tcorp_owner=Geeknet\twill_it_blend=indubitably\nhttp://engadget.com/\tcorp_owner=Weblogs\tgenre=geeksquad_thriller\n\nTo activate this plugin, you must modify two properties in your nutch-sites.xml:\n1. plugin.includes\n   add: urlmeta\n   to:   <value>...</value>\n   ie: <value>urlmeta|parse-tika|scoring-opic|...</value>\n2. urlmeta.tags\n   Insert a comma-delimited list of metatags. Using the above example:\n   <value>corp_owner, will_it_blend, genre</value>\n   Note that you do not need to include the tag with every URL. However, you must specify each tag if you want it to be propagated and later indexed.\n", "comments": ["This is my revised patch, with some small bug fixes.", "Updated comments, revised patch is now available. It's more robust to the nefarious \"null\" and his NullPointerException cabal.", "- Applied to 1.2-branch in r979079. Cleaned up comments, removed author tags (Nutch decided a long time ago that the project would move away from author tags), cleaned up formatting. Patch doesn't apply to trunk or Nutchbase branch because LuceneWriter doesn't exist anymore for Nutch 2.0. If someone wants to port this to Nutchbase-ville, by all means, but if so, please open a new issue for it. Thanks very much, Scott!", "FYI for anyone who might use this:\n\nThe \"urlmeta.tags\" must be comma-delimited, with no white-space to pad the boundaries.", "If it wasn't clear from my prior comment, the property for urlmeta in nutch-site should look like:\n<property>\n<name>urlmeta.tags</name>\n<value>tags,are,sooo,web2.0,man</value>\n</property>\n\nIt might be nice if someone updates the \"nutch-default.xml\" entry for \"urlmeta.tags\" to the following:\n\n<property>\n<name>urlmeta.tags</name>\n<value></value>\n<description>\nTo be used in conjunction with features introduced in NUTCH-655, which allows\nfor custom metatags to be injected alongside your crawl URLs. Specifying those\ncustom tags here will allow for their propagation into a pages outlinks, as\nwell as allow for them to be included as part of an index.\nValues should be comma-delimited. (\"tag1,tag2,tag3\") Do not pad the tags with\nwhite-space at their boundaries, if you are using Hadoop releases prior to 0.21.\n</description>\n</property>\n\nUnless, of course, Nutch-1.2 ships with Hadoop-0.21... Then it's a wash. I do think it's good to note that in there, as someone may stumble across that tidbit while troubleshooting some unrelated timewaster.  I'm looking out for you, long lost not-twin.\n", "My preference is that rather than reopen issues (which is a real pain for JIRA and CHANGES.txt where they have already been marked resolved) just open a new issue and link it to this.\n\nI see that you reopened it I'm guessing b/c you'd like the description updated in the nutch-default.xml. I'll do that now.", "updated the docs with your new comments Scott, in r983257. Thanks!"], "tasks": {"summary": "ScoringFilter and IndexingFilter: To allow for the propagation of URL Metatags and their subsequent indexing.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "ScoringFilter and IndexingFilter: To allow for the propagation of URL Metatags and their subsequent indexing."}, {"question": "What is the main context?", "answer": "This plugin is designed to enhance the NUTCH-655 patch, by doing two things:\n1. Meta Tags that are supplied with your Crawl URLs, during injection, will be propagated throughout the outlinks of those "}]}}
{"issue_id": "NUTCH-856", "project": "NUTCH", "title": "Use Tika for parsing feed", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-07-16T15:17:53.442+0000", "updated": "2013-05-22T03:53:27.184+0000", "description": "We currently have 2 plugins for dealing with feeds : \n* feeds\n* parse-rss\n\nI have proposed https://issues.apache.org/jira/browse/TIKA-466 which would at least cover the functionalities of parse-rss. If/when this is added to Tika then we should be able to remove parse-rss and rely on Tika instead", "comments": ["thanks Chris for reviewing and committing TIKA-466. I will mark the issue as closed as soon as Tika 0.8 is released and used in Nutch"], "tasks": {"summary": "Use Tika for parsing feed", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Use Tika for parsing feed"}, {"question": "What is the main context?", "answer": "We currently have 2 plugins for dealing with feeds : \n* feeds\n* parse-rss\n\nI have proposed https://issues.apache.org/jira/browse/TIKA-466 which would at least cover the functionalities of parse-rss. I"}]}}
{"issue_id": "NUTCH-857", "project": "NUTCH", "title": "DistributedBeans should not close their RPC counterparts", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2010-07-19T20:59:28.675+0000", "updated": "2010-07-26T11:57:59.845+0000", "description": "DistributedSearch and Segment Beans currently call close on their RPC counterparts from their own close methods.  This results in killing (closing) all distributed servers when the main bean (website, application, etc) is shutdown.  DistributedSearchServer (SegmentServer) are run independent from the main NutchBean or website calling those servers in shard type environments.  With the current code the distributed servers are closed and any further search requests throw IndexAlreadyClosed exceptions.  The distributed servers have to be restarted before searching can resume.  Obviously this doesn't work in a large distributed search where multiple beans could be called the distributed servers and where distributed servers could be coming up and down frequently.\n\nThe solution is simple though.  The Distributed beans shouldn't call close on their RPC counterparts.  Patch is attached.", "comments": ["Fixes distributed search bug calling close on RPC counterparts.", "Committed with revision 979252", "done"], "tasks": {"summary": "DistributedBeans should not close their RPC counterparts", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "DistributedBeans should not close their RPC counterparts"}, {"question": "What is the main context?", "answer": "DistributedSearch and Segment Beans currently call close on their RPC counterparts from their own close methods.  This results in killing (closing) all distributed servers when the main bean (website,"}]}}
{"issue_id": "NUTCH-858", "project": "NUTCH", "title": "No longer able to set per-field boosts on lucene documents", "status": "Closed", "priority": "Major", "reporter": "Edward Drapkin", "assignee": "Andrzej Bialecki", "created": "2010-07-21T19:41:17.091+0000", "updated": "2013-05-22T03:53:30.553+0000", "description": "I'm working on upgrading from Nutch 0.9 to Nutch 1.1 and I've noticed that it no longer seems possible to set boosts on specific fields in lucene documents.  This is, in my opinion, a major feature regression and removes a huge component to fine tuning search.  Can this be added?", "comments": ["This was fixed in trunk/ - there will be a 1.2 release, so we may prepare a patch to do bring this feature back into 1.x.", "Is there a patch against 1.1 that exists anywhere so I don't have to use a SVN checkout?\n\nThanks!", "Unfortunately no. The patch was included in a fix to NUTCH-837, which is relative to trunk, and it's not directly applicable to 1.x, needs to be ported.", "Ah, okay.  Is there an ETA on a 1.2 release or potential backport in a 1.1 point release?", "Hey Andrzej, do you know what rev this was fixed in trunk? I'd be happy to backport it...", "It was r960064, but I have to admit I sneaked in this improvement as a part of NUTCH-837, which contained a lot of other stuff...", "Ahh, kk, maybe you are the best person to sneak it in then on this. Let me know what you think and thanks!", "Here's the ported functionality. Please note that the standard plugins still don't use per-field boosts.\n\nIf no objections I'll commit this shortly.", "+1!", "Committed to branch-1.2, revision 982970."], "tasks": {"summary": "No longer able to set per-field boosts on lucene documents", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "No longer able to set per-field boosts on lucene documents"}, {"question": "What is the main context?", "answer": "I'm working on upgrading from Nutch 0.9 to Nutch 1.1 and I've noticed that it no longer seems possible to set boosts on specific fields in lucene documents.  This is, in my opinion, a major feature re"}]}}
{"issue_id": "NUTCH-859", "project": "NUTCH", "title": "Diff trunk and NutchBase ", "status": "Closed", "priority": "Blocker", "reporter": "Julien Nioche", "assignee": null, "created": "2010-07-23T13:17:58.147+0000", "updated": "2010-08-09T08:39:09.443+0000", "description": "Before we turn NutchBase into trunk we need to make sure that all (more or less) recent changes in the trunk have been ported to NutchBase. I have done that recently but given that there is a very large number of changes I might have missed a few things here and there.  ", "comments": ["Hi Julien,\n\nI've just looked at the new source code of Nutchbase:\n\n1. Build.xml file\norg/apache/nutch/scoring/webgraph/**/*.java,org/apache/nutch/tools/compat/**/*.java is no longer existed\n\n(target compile-core-test, compile-core)\n\n\n2. solrindex-mapping.xml is not added\nWe shoud convert SolrIndexer.java to run solrindex command in local mode\n\n3. /java/org/apache/nutch/crawl/FetchScheduleFactory.java\n\nNo need to use Class<?>, just Class is enough\n\n4. nutch-default.xml\n\nquery-(basic|site|url)|summary-basic is not used\n\n5. nutchbase use fetcher.threads.per.queue, why not use other feature like fetcher.threads.per.host, fetcher.threads.per.host.by.ip\n\n6.Some java source contain $Id, some java source not, it is not unique style\n\n// $Id: PrefixURLFilter.java 823614 2009-10-09 17:02:32Z ab $\n// $Id$\n\nWe should define a standard format rules for source code, then using eclipse to format the whole source code in the same style\n \nThanks,\n\n", "{quote}\n\n1. Build.xml file\norg/apache/nutch/scoring/webgraph/*/.java,org/apache/nutch/tools/compat/*/.java is no longer existed\n\n(target compile-core-test, compile-core)\n\n{quote}\n\nthe webgraph has not been ported to NutchBase yet, however since there is no code in these packages this line in build.xml has no effect and could be removed.\n\n{quote}\n2. solrindex-mapping.xml is not added\n{quote}\n\nindeed. well spotted\n\n{quote}\nWe shoud convert SolrIndexer.java to run solrindex command in local mode\n{quote}\n\nwhat do you mean by that? Embedding SOLR in the reducers in order to generate an index locally? You can put that as a new feature for 2.0, what we are trying to do here is to make sure we are not missing any of the improvements done in the last 12+ months on the trunk.\n\n{quote}\n3. /java/org/apache/nutch/crawl/FetchScheduleFactory.java\n\nNo need to use Class<?>, just Class is enough\n{quote}\n\nok - not really relevant here (see comment above)\n\n{quote}\n4. nutch-default.xml\n\nquery-(basic|site|url)|summary-basic is not used\n{quote}\n\ntrue. we'll also need to remove the params in nutch-default.xml which won't be used anymore. This can be done after we move to trunk though. \n\n{quote}\n5. nutchbase use fetcher.threads.per.queue, why not use other feature like fetcher.threads.per.host, fetcher.threads.per.host.by.ip\n{quote}\n\nthings are handled differently in the code but the feature is the same. look at the code for more details\n\n{quote}\n6.Some java source contain $Id, some java source not, it is not unique style\n\n// $Id: PrefixURLFilter.java 823614 2009-10-09 17:02:32Z ab $\n// $Id$\n{quote}\n\nok, can be part of the code cleanup after the move to trunk\n\n{quote}\nWe should define a standard format rules for source code, then using eclipse to format the whole source code in the same style\n{quote}\n\nagreed - can you create a separate JIRA for this please?\n\nThanks for your comments!\n\n", "revision 981959 : \n\n- *nutch-default.xml*\nquery-(basic|site|url)|summary-basic removed from plugin.includes\n\n- added *solrindex-mapping.xml*\n\n- *build.xml* removed exclusions in targets compile-core-test and compile-core\n\n", "- update to Nutchbase (which is the new 2.0)", "note since I'm about to create branch-1.3 from trunk, this issue really boils down to making sure that nutchbase (soon to be the Nutch 2.0 trunk) doesn't have any black magic compared to the branch-1.3 folder in SVN.", "NutchBase has become 2.0 and lives in the trunk. I had another look at its differences with 1.2 and could not find any improvement or recent change to the 1.x branch that was missing from NutchBase. However, since the move to the GORA API changed the code drastically it is possible that I missed something but hopefully this won't be the case."], "tasks": {"summary": "Diff trunk and NutchBase ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Diff trunk and NutchBase "}, {"question": "What is the main context?", "answer": "Before we turn NutchBase into trunk we need to make sure that all (more or less) recent changes in the trunk have been ported to NutchBase. I have done that recently but given that there is a very lar"}]}}
{"issue_id": "NUTCH-86", "project": "NUTCH", "title": "LanguageIdentifier API enhancements", "status": "Closed", "priority": "Minor", "reporter": "Jerome Charron", "assignee": "Chris A. Mattmann", "created": "2005-08-31T19:31:05.000+0000", "updated": "2011-04-01T15:07:20.466+0000", "description": "More informations can be found on the following thread on Nutch-Dev mailing list:\nhttp://www.mail-archive.com/nutch-dev%40lucene.apache.org/msg00569.html\n\nSummary:\n\n1. LanguageIdentifier API changes. The similarity methods should return an ordered array of language-code/score pairs instead of a simple String containing the language-code.\n\n2. Ensure consistency between LanguageIdentifier scoring and NGramProfile.getSimilarity().\n\n", "comments": ["removing from 1.0 queue since there has been no activity lately", "I've reported TIKA-465 to take care of this, if it's still relevant. I need to do some research more on what was being proposed, but either way, we can do Language analysis in Tika now since it has an explicit language identifier component and since that was the one of the library's original goals. In addition, there has been no activity on this issue in 5+ years...", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "LanguageIdentifier API enhancements", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "LanguageIdentifier API enhancements"}, {"question": "What is the main context?", "answer": "More informations can be found on the following thread on Nutch-Dev mailing list:\nhttp://www.mail-archive.com/nutch-dev%40lucene.apache.org/msg00569.html\n\nSummary:\n\n1. LanguageIdentifier API changes. "}]}}
{"issue_id": "NUTCH-860", "project": "NUTCH", "title": "package task fails", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2010-07-23T14:01:54.036+0000", "updated": "2011-04-01T15:07:20.994+0000", "description": "The ant 'package' tasks fails since we reorganised the code and removed the bin directory. The patch attached fixed the issue and adds the /runtime directory to the package.\nAn objections? ", "comments": ["Committed revision 979227 => Nutchbase\n\nCommitted revision 979228 => trunk", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "package task fails", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "package task fails"}, {"question": "What is the main context?", "answer": "The ant 'package' tasks fails since we reorganised the code and removed the bin directory. The patch attached fixed the issue and adds the /runtime directory to the package.\nAn objections? "}]}}
{"issue_id": "NUTCH-861", "project": "NUTCH", "title": "Rename HTMLParserFilter ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-07-23T15:43:09.982+0000", "updated": "2010-08-11T09:58:01.260+0000", "description": "The name 'HTMLParserFilter' is slightly confusing as it gives the impression that the implementations of this endpoint are getting only HTML documents. \nThe plugin parse-tika calls the HTMLParserFilters and passes them a DOM representation of the XHTML-like documents it got from the underlying Tika parsers. This means that we are getting a DOM representation for documents in any format recognised by Tika and not only HTML.\n\nWhat about renaming HTMLParserFilter into ParserFilter? Any other suggestions?", "comments": ["+1 this makes sense now.", "Patch which renames the HTMLParseFilter endpoint into ParseFilter", "will commit this tomorrow if no one has any objections", "Can I stress that if this is done then some documentation will probably need changing too?\n\n(But it sounds like a good change - badly named classes are confusing)\n\nThanks", "Which documentation are you thinking about specifically? We'll need to document the whole of 2.0 anyway and that would be part of it :-)", "Committed revision 984352.\n"], "tasks": {"summary": "Rename HTMLParserFilter ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Rename HTMLParserFilter "}, {"question": "What is the main context?", "answer": "The name 'HTMLParserFilter' is slightly confusing as it gives the impression that the implementations of this endpoint are getting only HTML documents. \nThe plugin parse-tika calls the HTMLParserFilte"}]}}
{"issue_id": "NUTCH-862", "project": "NUTCH", "title": "HttpClient null pointer exception", "status": "Closed", "priority": "Minor", "reporter": "Sebastian Nagel", "assignee": "Andrzej Bialecki", "created": "2010-07-27T11:54:47.344+0000", "updated": "2013-05-22T03:54:50.335+0000", "description": "When re-fetching a document (a continued crawl) HttpClient throws an null pointer exception causing the document to be emptied:\n\n2010-07-27 12:45:09,199 INFO  fetcher.Fetcher - fetching http://localhost/doc/selfhtml/html/index.htm\n2010-07-27 12:45:09,203 ERROR httpclient.Http - java.lang.NullPointerException\n2010-07-27 12:45:09,204 ERROR httpclient.Http - at org.apache.nutch.protocol.httpclient.HttpResponse.<init>(HttpResponse.java:138)\n2010-07-27 12:45:09,204 ERROR httpclient.Http - at org.apache.nutch.protocol.httpclient.Http.getResponse(Http.java:154)\n2010-07-27 12:45:09,204 ERROR httpclient.Http - at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:220)\n2010-07-27 12:45:09,204 ERROR httpclient.Http - at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:537)\n2010-07-27 12:45:09,204 INFO  fetcher.Fetcher - fetch of http://localhost/doc/selfhtml/html/index.htm failed with: java.lang.NullPointerException\n\nBecause the document is re-fetched the server answers \"304\" (not modified):\n\n127.0.0.1 - - [27/Jul/2010:12:45:09 +0200] \"GET /doc/selfhtml/html/index.htm HTTP/1.0\" 304 174 \"-\" \"Nutch-1.0\"\n\nNo content is sent in this case (empty http body).\n\nIndex: trunk/src/plugin/protocol-httpclient/src/java/org/apache/nutch/protocol/httpclient/HttpResponse.java\n===================================================================\n--- trunk/src/plugin/protocol-httpclient/src/java/org/apache/nutch/protocol/httpclient/HttpResponse.java        (revision 979647)\n+++ trunk/src/plugin/protocol-httpclient/src/java/org/apache/nutch/protocol/httpclient/HttpResponse.java        (working copy)\n@@ -134,7 +134,8 @@\n         if (code == 200) throw new IOException(e.toString());\n         // for codes other than 200 OK, we are fine with empty content\n       } finally {\n-        in.close();\n+        if (in != null)\n+          in.close();\n         get.abort();\n       }\n", "comments": ["patch", "Same happens in 1.1 and 1.2rc2.\nIt is a simple safe fix that shold make it to 1.2. Please?", "Fix applied to branch-1.2 (rev. 998156), branch-1.3 (rev. 998158) and trunk (998160). Thank you!"], "tasks": {"summary": "HttpClient null pointer exception", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "HttpClient null pointer exception"}, {"question": "What is the main context?", "answer": "When re-fetching a document (a continued crawl) HttpClient throws an null pointer exception causing the document to be emptied:\n\n2010-07-27 12:45:09,199 INFO  fetcher.Fetcher - fetching http://localho"}]}}
{"issue_id": "NUTCH-863", "project": "NUTCH", "title": "Benchmark and a testbed proxy server", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-07-29T13:07:19.028+0000", "updated": "2013-05-22T03:53:31.687+0000", "description": "This issue adds two components:\n\n* a testbed proxy server that can serve various content: pre-fetched Nutch segments, forward requests to original URLs, or create a lot of unique but predictable fake content (with outlinks) on the fly.\n* a simple Benchmark class to measure the time taken to complete several crawl cycles using fake content.\n* 'ant proxy' and 'ant benchmark' targets to execute a benchmark run.\n\nTogether these tools should provide a more or less objective method to measure the end-to-end crawl performance. This initial version can be further instrumented to collect statistics about various stages of data processing.", "comments": ["Patch with the tools. If there are no objections I'd like to commit this soon, and track improvements in other JIRA issues.", "Committed in rev. 980932.", "Hey Andrzej,\n\nOddly enough, this test fails for me on a fresh checkout of Nutch (r981962 as of today):\n\n{noformat}\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.063 sec\n    [junit] Test org.apache.nutch.tools.proxy.TestbedProxy FAILED\n{noformat}\n\n{noformat}\n[chipotle:~/src/nutch] mattmann% more build/test/TEST-org.apache.nutch.tools.proxy.TestbedProxy.txt \nTestsuite: org.apache.nutch.tools.proxy.TestbedProxy\nTests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.063 sec\n\nTestcase: initializationError took 0.006 sec\n        Caused an ERROR\nNo runnable methods\njava.lang.Exception: No runnable methods\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n\n[chipotle:~/src/nutch] mattmann% java -version\njava version \"1.6.0_20\"\nJava(TM) SE Runtime Environment (build 1.6.0_20-b02-279-9M3165)\nJava HotSpot(TM) 64-Bit Server VM (build 16.3-b01-279, mixed mode)\n[chipotle:~/src/nutch] mattmann% \n\n{noformat}\n\nAny clue as to why?\n\nCheers,\nChris", "Yes, the combined naivety of JUnit and mine contributed to this error ... The class is NOT a JUnit test, its name doesn't even start with a properly CamelCase-d \"Test\" keyword... but apparently JUnit only does a substring comparison. Pity, I will have to rename this class.", "haha not your fault! I blame Junit! :)", "okey dok, I cleaned this up in r982102, r982103, r982104, and r982107 by moving the Benchmark and its classes into src/java instead of src/test.", "Hey Guys:\n\nJust wanted to let you know that I kind of jumped the gun on committing my solution to this and in the future I'll wait a bit longer before trying to resolve it since the issue wasn't really assigned to me anyways! :) Great work on the tool, Andrzej, I really like it!\n\nCheers,\nChris"], "tasks": {"summary": "Benchmark and a testbed proxy server", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Benchmark and a testbed proxy server"}, {"question": "What is the main context?", "answer": "This issue adds two components:\n\n* a testbed proxy server that can serve various content: pre-fetched Nutch segments, forward requests to original URLs, or create a lot of unique but predictable fake "}]}}
{"issue_id": "NUTCH-864", "project": "NUTCH", "title": "Fetcher generates entries with status 0", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Dogacan Guney", "created": "2010-07-30T13:59:37.169+0000", "updated": "2019-10-13T22:36:24.738+0000", "description": "After a round of fetching which got the following protocol status :\n\n10/07/30 15:11:39 INFO mapred.JobClient:     ACCESS_DENIED=2\n10/07/30 15:11:39 INFO mapred.JobClient:     SUCCESS=1177\n10/07/30 15:11:39 INFO mapred.JobClient:     GONE=3\n10/07/30 15:11:39 INFO mapred.JobClient:     TEMP_MOVED=138\n10/07/30 15:11:39 INFO mapred.JobClient:     EXCEPTION=93\n10/07/30 15:11:39 INFO mapred.JobClient:     MOVED=521\n10/07/30 15:11:39 INFO mapred.JobClient:     NOTFOUND=62\n\nI ran : ./nutch org.apache.nutch.crawl.WebTableReader -stats\n\n10/07/30 15:12:37 INFO crawl.WebTableReader: Statistics for WebTable: \n10/07/30 15:12:37 INFO crawl.WebTableReader: TOTAL urls:\t2690\n10/07/30 15:12:37 INFO crawl.WebTableReader: retry 0:\t2690\n10/07/30 15:12:37 INFO crawl.WebTableReader: min score:\t0.0\n10/07/30 15:12:37 INFO crawl.WebTableReader: avg score:\t0.7587361\n10/07/30 15:12:37 INFO crawl.WebTableReader: max score:\t1.0\n10/07/30 15:12:37 INFO crawl.WebTableReader: status 0 (null):\t649\n10/07/30 15:12:37 INFO crawl.WebTableReader: status 2 (status_fetched):\t1177 (SUCCESS=1177)\n10/07/30 15:12:37 INFO crawl.WebTableReader: status 3 (status_gone):\t112 \n10/07/30 15:12:37 INFO crawl.WebTableReader: status 34 (status_retry):\t93 (EXCEPTION=93)\n10/07/30 15:12:37 INFO crawl.WebTableReader: status 4 (status_redir_temp):\t138  (TEMP_MOVED=138)\n10/07/30 15:12:37 INFO crawl.WebTableReader: status 5 (status_redir_perm):\t521 (MOVED=521)\n10/07/30 15:12:37 INFO crawl.WebTableReader: WebTable statistics: done\n\nThere should not be any entries with status 0 (null)\n\nI will investigate a bit more...\n\n\n", "comments": ["It seems that the entries with a status of 0 also have a metadatum indicating that they are the target of a redirection (metadata ___rdrdsc__ : \ty). I guess that the value of 0 is because the status is not explicitly set  and hence takes the default value of an int.\n\nThe metadatum ___rdrdsc__  is used during the update step and the URL is then marked as unfetched. I was wondering if there was a reason not to mark them as unfetched in the first place? \n\nDogacan - could you please explain the logic behind the use of this metadatum? \n\n", "- update to Nutchbase (which is the new 2.0)", "Hey everyone,\n\nRedirect metadatum is used to mark it as a redirect. The idea is you can have long redirect chains but you want ONE representative URL for all of them. For example, yahoo.com may redirect to www.yahoo.com which may redirect to www.yahoo.com/index.html. In this case (IIRC), we designate www.yahoo.com to represent all the redirections.\n\nThe reason we do not mark them as unfetched is they may already be fetched. Continuing from the above example, www.yahoo.com may already be FETCHED. During update step, these URLs should be recognized and then _injected_ if necessary. I can see how it may be a bit unintiutive that until updatedb they are essentially status-less. Julien, any suggestions?", "Thanks for the explanations.\n\n{quote}\nRedirect metadatum is used to mark it as a redirect. The idea is you can have long redirect chains but you want ONE representative URL for all of them. For example, yahoo.com may redirect to www.yahoo.com which may redirect to www.yahoo.com/index.html. In this case (IIRC), we designate www.yahoo.com to represent all the redirections.\n{quote}\n\nyep, that's what URLUtil.chooseRepr() is used for. As far as I can see the behaviour is mostly the same as in 1.x i.e we determine which one of the source or target looks nicer and store it in the target only. \n\n{quote}\nThe reason we do not mark them as unfetched is they may already be fetched. Continuing from the above example, www.yahoo.com may already be FETCHED. During update step, these URLs should be recognized and then injected  if necessary. I can see how it may be a bit unintiutive that until updatedb they are essentially status-less. Julien, any suggestions?\n{quote}\n\nWe could use an explicit status code instead of relying on the default. In theory there should be no 0 status left after an update so maybe it would be an overkill to create a status just for that. \n\n \n", "I want to close this issue if there are no objections. Julien, do you think an explicit status code is better? Or, just using 0 temporarily is OK?", "In theory we should not see any elements with a status of 0 after updating but having an explicit code would be much cleaner. This should not be the case in the code as it is now, but we could have webpages created with the default status somewhere else in the code and we would not be able to differentiate it from status 0 used by the redirections.", "I don't think that's possible to do without doing a DataStore#get first as we do not want to override current status on URL. I guess we could write redirect status as a temporary status somewhere, but it would be too complex IMHO.\n\nJulien, any ideas on how to set a redirect status without overwriting the current one?", "+1 to using a specific value != 0 as a redirect status. Value of 0 is helpful as a guard value, i.e. to detect things that were not properly initialized. I would even argue  that the default value of 0 should be explicitly named INVALID.", "OK, let's do it :)\n\nSo, should we do a DataStore#get  to read previous status? I am not sure how best to implement this....", "I think the difficulty comes from the simplification in 2.x as compared to 1.x, in that we keep a single status per page. In 1.x a side-effect of having two locations with two statuses (one \"db status\" in crawldb and one \"fetch status\" in segments) was that we had more information in updatedb to act upon.\n\nNow we should probably keep up to two statuses - one that reflects a temporary fetch status, as determined by fetcher, and a final (reconciled) status as determined by updatedb, based on the knoweldge of not only plain fetch status and old status but also possible redirects. If I'm not mistaken currently the status is immediately overwritten by fetcher, even before we come to updatedb, hence the problem..", "We can keep two statuses (stata?). But, in the case of a redirect to a newly discovered URL, the \"final\" status will still be unset and thus will be interpreted as 0 anyway. Is this OK? i.e, a URL with a temp fetch status but not a final one?", "Set and Classify", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "Fetcher generates entries with status 0", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fetcher generates entries with status 0"}, {"question": "What is the main context?", "answer": "After a round of fetching which got the following protocol status :\n\n10/07/30 15:11:39 INFO mapred.JobClient:     ACCESS_DENIED=2\n10/07/30 15:11:39 INFO mapred.JobClient:     SUCCESS=1177\n10/07/30 15:"}]}}
{"issue_id": "NUTCH-865", "project": "NUTCH", "title": "Format source code in unique style", "status": "Closed", "priority": "Major", "reporter": "Pham Tuan Minh", "assignee": "Lewis John McGibbney", "created": "2010-07-30T15:38:24.132+0000", "updated": "2024-03-13T14:50:53.143+0000", "description": "We should define a standard format rules for source code/comments, then using eclipse tool to format the whole source code in the same style. \n", "comments": ["The trunk (ex-Nutchbase) contains an eclipse format file eclipse-codeformat.xml which IIRC is simply the built-in \"Java Conventions\" formatter with the default \"Mixed\" tab policy with \"Spaces only\" and the tabulation + indentation size set to 2.\n\nIf that's OK with everybody we should apply it to the whole of the project + check that all the classes have the license header at the same time \n\n\n\n", "My feelings are that this could turn into a pretty huge task as I'm not exactly sure what this would this entail?\n\nAre we to implement this moving forward prior to release of trunk, branch-1.4, both?", "That's not very complex nor huge. All it takes is to associate the code format with the project in Eclipse and apply it. This would result in a lot of files changed for sure but these changes are only syntactic so the code will behave in exactly the same way. The only time it would be used after that is prior to committing a modification to the code.\n\n ", "I'm happy to have a crack at implementing in branch-1.4. Having had a look at eclipse-codeformat.xml in trunk you are right it doesn't appear to bad. If it is OK I will assign and start working on this? Thoughts...", "Maybe have a look at the diffs after applying it and see if all the changes are really that necessary? I think I saw cases where the original code looked nicer than the reformated one, in particular the sections in comments lost their \\n.", "How is this going to affect existing patches?\n\nI've a lot of them and am not sure a full diff will merge that well with subversion. When we added the ASL header it all went fine but that operated only in the head of the source files.", "This is a good point Markus. To be honest, I really don't know until the task is initiated, however I think to answer that question properly we are looking at a case-per-case basis.\n\nThe more I think about it the more similarity I think two JIRA issues bare e.g. this one and NUTCH-881 (if we are to seriously consider achieving this). It would be great to have these tasks done before the next release, however NUTCH-881 can be done in incremental stages, unfortunately it may not be the case with NUTCH-865 and a contingency period would need to be allocated to safely guarantee that no patches were effected in the process of resolving this ticket.", "It would be good to finish 1.4 with clean style indeed. Should we therefore not postpone this issue until we're ready to release? That would minimize impact on open issues and it wouldn't make this task any different.", "agreed :0)", "Marked 1.4 to keep it on the radar.", "Marked as blocker for 1.4. This issue should be the addressed as the final issue.", "OK, since the rather useless commit, I've compiled a patch using the code formatter application bundled with Eclipse. For some reason a rather nasty number of files have been skipped, these are as follows\n\n{code}\nlewis@lewis-desktop:~$ eclipse -application org.eclipse.jdt.core.JavaCodeFormatter -config ~/ASF/trunk/eclipse-codeformat.xml ~/ASF/trunk\nConfiguration Name: /home/lewis/ASF/trunk/eclipse-codeformat.xml\nStarting format job ...\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/test/org/apache/nutch/util/TestURLUtil.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/test/org/apache/nutch/crawl/TestInjector.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/test/org/apache/nutch/crawl/CrawlDBTestUtil.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/test/org/apache/nutch/crawl/TestGenerator.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/test/org/apache/nutch/fetcher/TestFetcher.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/plugin/scoring-link/src/java/org/apache/nutch/scoring/link/LinkAnalysisScoringFilter.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/plugin/urlmeta/src/java/org/apache/nutch/scoring/urlmeta/URLMetaScoringFilter.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/plugin/scoring-opic/src/java/org/apache/nutch/scoring/opic/OPICScoringFilter.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/plugin/index-static/src/java/org/apache/nutch/indexer/staticfield/StaticFieldIndexer.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/plugin/language-identifier/src/java/org/apache/nutch/analysis/lang/HTMLLanguageParser.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/plugin/tld/src/java/org/apache/nutch/scoring/tld/TLDScoringFilter.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/plugin/subcollection/src/java/org/apache/nutch/collection/CollectionManager.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/plugin/urlfilter-domain/src/java/org/apache/nutch/urlfilter/domain/DomainURLFilter.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/TikaConfig.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/DOMContentUtils.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/plugin/index-more/src/test/org/apache/nutch/indexer/more/TestMoreIndexingFilter.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/plugin/parse-html/src/java/org/apache/nutch/parse/html/DOMContentUtils.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/util/GenericWritableConfigurable.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/util/TrieStringMatcher.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/util/domain/DomainStatistics.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/util/domain/DomainSuffixes.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/util/domain/TopLevelDomain.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/util/domain/DomainSuffix.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/util/ObjectCache.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/util/EncodingDetector.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/util/NodeWalker.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/protocol/ProtocolException.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/protocol/ProtocolNotFound.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/protocol/ProtocolStatus.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/net/protocols/ProtocolException.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/net/URLFilterException.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/net/URLNormalizers.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/metadata/Metadata.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/metadata/SpellCheckedMetadata.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/scoring/ScoringFilter.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/scoring/webgraph/NodeDumper.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/scoring/webgraph/LinkRank.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/scoring/webgraph/WebGraph.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/scoring/webgraph/Loops.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/scoring/webgraph/LinkDumper.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/scoring/webgraph/ScoreUpdater.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/scoring/ScoringFilterException.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/scoring/ScoringFilters.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/CrawlDbReducer.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/CrawlDbFilter.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/Crawl.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/Injector.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/Generator.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/LinkDb.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/URLPartitioner.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/LinkDbMerger.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/LinkDbReader.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/CrawlDbReader.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/AdaptiveFetchSchedule.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/MapWritable.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/Inlinks.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/DefaultFetchSchedule.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/LinkDbFilter.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/CrawlDbMerger.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/TextProfileSignature.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/NutchWritable.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/crawl/CrawlDatum.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/IndexingFiltersChecker.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/solr/SolrIndexer.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/solr/SolrMappingReader.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/solr/SolrClean.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/solr/SolrDeleteDuplicates.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/solr/SolrWriter.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/NutchDocument.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/IndexerOutputFormat.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/NutchField.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/IndexingException.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/NutchIndexWriterFactory.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/IndexerMapReduce.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/plugin/ExtensionPoint.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/plugin/PluginDescriptor.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/plugin/PluginRepository.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/plugin/PluginManifestParser.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/plugin/Extension.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/segment/SegmentMerger.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/segment/SegmentMergeFilters.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/segment/ContentAsTextInputFormat.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/segment/SegmentMergeFilter.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/segment/SegmentReader.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/fetcher/OldFetcher.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/fetcher/FetcherOutputFormat.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/fetcher/Fetcher.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/parse/ParseCallable.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/parse/ParsePluginList.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/parse/ParseOutputFormat.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/parse/ParseResult.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/parse/ParserFactory.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/parse/ParsePluginsReader.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/parse/ParseException.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/parse/ParserChecker.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/parse/ParseSegment.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/tools/proxy/NotFoundHandler.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/tools/proxy/SegmentHandler.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/tools/proxy/LogDebugHandler.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/tools/proxy/DelayHandler.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/tools/proxy/FakeHandler.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/tools/proxy/AbstractTestbedHandler.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/tools/arc/ArcRecordReader.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/tools/arc/ArcInputFormat.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/tools/arc/ArcSegmentCreator.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/tools/FreeGenerator.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/tools/DmozParser.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/tools/Benchmark.java. Skip the file.\nThe Eclipse formatter failed to format /home/lewis/ASF/trunk/src/java/org/apache/nutch/tools/CrawlDBScanner.java. Skip the file.\nDone.\n{code}", "If I can get some time on this between today and tomorrow then I'll have my best look into sorting this out in due course.\n\nN.B. It is a rather trivial task formatting the code as the command line call can be seen in the snippet above.  ", "Integrated in Nutch-trunk-ant #57 (See [https://builds.apache.org/job/Nutch-trunk-ant/57/])\n    commit to address NUTCH-865, update to changes.txt and addition of eclipse-codeformat.xml\n\nlewismc : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1186941\nFiles : \n* /nutch/trunk/CHANGES.txt\n", "I'm not in a position to complete this task just now, I'm trying to work from windows and its driving me insane. The new Eclipse Indigo doesn't seem to have a code formatter and using the terminal after so long aaway from it is an unbeliveable pain. Is anyone else up for having a crack at this? I apologise for the late news, I thought I would be able to follow through, but I won't be back on my linux box until next Friday! Sorry guys.", "Formatting rules (eclipse-format.xml) are applied by Eclipse Indigo from GUI:\n 1. import formatting rules:\n   Properties > Java Code Style > Formatter\n   > Import > eclipse-format.xml\n 2. in project node context menu:\n   Source > Format\n   ", "This is excellent Sebastian. There is mention of rolling a RC possibly this week, so I'm very hopeful that this will make the cut. I hope you remembered to edit changes.txt! If not then I'm sure we will get this added at a later date.", "Hi Sebastian, did you check that this doesn't break the build? After checking out most recent trunk and nutchgora branch and applying patches, I'm getting test build failures! Can you (or anyone else) confirm whether this is working or not?\nIt was my fear that this would be the result.", "Apologies, I think that this is environment specific to me just now. My eclipse set-up is not behaving.", "As this is a pretty substantial aesthetic edit of the codebase, it would be good to have 1 or 2 guys give the go ahead prior to any commits. Thanks", "Does it change most files? There're a load of 1.5 patches waiting for inclusion so this may yet again cause a lot of issues. I haven't got Eclipse so i cannot test it if it works well with patches.\n\nCan you test some patches on a modified codebase? We can also create a temp branch with modified codebase so i can try out some intrusive patches.", "Yeah Markus, you're concerns are well heard. I am unable to appraise this until Friday, if we can't get any further comments for the time being I'm sure we can wait until then. In direct response to your question YES, it changes the overwhelming majority of files so the threat of something going wrong plus the legacy this leaves for existing patches is an area which needs some thought. As I said, if we can wait until Friday then I'll get right on it.  ", "Markus (or anyone else), the more I think about this, the more I have convinced myself that the patches will undoubtably have adverse affects on your (and everyone elses) existing patches. There are in some cases additional lines added and in other cases there are lines removed due to formatting. I would be extremely surpised if hunks were to find a place in every case. Although I cannot confirm this, I think it is safe to hope for the best but prepare for the worst. Where does this leave us with this issue???", "- push out to 1.5", "Integrated in Nutch-trunk #1646 (See [https://builds.apache.org/job/Nutch-trunk/1646/])\n    - prep changelog for RC1: 1.4 -- removed entry for NUTCH-865 since it's not done\n\nmattmann : http://svn.apache.org/viewvc/nutch/trunk/viewvc/?view=rev&root=&revision=1189790\nFiles : \n* /nutch/trunk/CHANGES.txt\n", "Integrated in Nutch-trunk-ant #63 (See [https://builds.apache.org/job/Nutch-trunk-ant/63/])\n    - prep changelog for RC1: 1.4 -- removed entry for NUTCH-865 since it's not done\n\nmattmann : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1189790\nFiles : \n* /nutch/trunk/CHANGES.txt\n", "Integrated in nutch-trunk-maven #3 (See [https://builds.apache.org/job/nutch-trunk-maven/3/])\n    - prep changelog for RC1: 1.4 -- removed entry for NUTCH-865 since it's not done\ncommit to address NUTCH-865, update to changes.txt and addition of eclipse-codeformat.xml\n\nmattmann : http://svn.apache.org/viewvc/nutch/trunk/viewvc/?view=rev&root=&revision=1189790\nFiles : \n* /nutch/trunk/CHANGES.txt\n\nlewismc : http://svn.apache.org/viewvc/nutch/trunk/viewvc/?view=rev&root=&revision=1186941\nFiles : \n* /nutch/trunk/CHANGES.txt\n", "20120304-push-1.6", "Anyone against me formatting the Nutch 1.10-SNAPSHOT codebase inline with the [eclipse-coreformat.xml|http://svn.apache.org/repos/asf/nutch/branches/2.x/eclipse-codeformat.xml]?", "+1\nI've checked the open issues with patch available: currently there are no hot issues with voluminous patches (excluding those which contain mostly new files).", "Committed @revision 1655526 in trunk", "Bulk progression of all “Resolved” issues to “Closed”. Performed my lewismc on 2024-03-13."], "tasks": {"summary": "Format source code in unique style", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Format source code in unique style"}, {"question": "What is the main context?", "answer": "We should define a standard format rules for source code/comments, then using eclipse tool to format the whole source code in the same style. \n"}]}}
{"issue_id": "NUTCH-866", "project": "NUTCH", "title": "STOP Nutch without breaking the crawled data", "status": "Closed", "priority": "Major", "reporter": "Pham Tuan Minh", "assignee": null, "created": "2010-07-30T16:44:23.338+0000", "updated": "2011-04-01T15:07:20.829+0000", "description": "How we can stop running nutch instance in local mode and in reducer mode without breaking the crawled data? \n\nFor example, you push a list of site that take a long time to complete crawl; then you want to stop nutch instance suddenly ...\n\n- For local mode, I suggest as below\n\nWe create a stop.txt file in specific directory, then for a piece of time, nutch instance will check whether this file existed or not; if existed, nutch instance will stop itself normally\n\n- For reducer mode, may we use zookeper to keep state of each instance?\n\nAny other suggestion?\n\nThanks,", "comments": ["I think this is no longer an issue in 2.0. You can just do CTRL+C or kill-job and nutch will resume from (nearly) the interrupted position now.", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "STOP Nutch without breaking the crawled data", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "STOP Nutch without breaking the crawled data"}, {"question": "What is the main context?", "answer": "How we can stop running nutch instance in local mode and in reducer mode without breaking the crawled data? \n\nFor example, you push a list of site that take a long time to complete crawl; then you wan"}]}}
{"issue_id": "NUTCH-867", "project": "NUTCH", "title": "Port Nutch benchmark to Nutchbase", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-07-31T18:06:02.014+0000", "updated": "2013-05-22T03:53:33.347+0000", "description": "Bring tools from NUTCH-863 to Nutchbase, and measure the performance of the Nutchbase branch vs. trunk.", "comments": ["Ported benchmark that uses HSQLDB as the store impl. If there are no objections I'll commit it shortly.", "Committed in rev. 982588."], "tasks": {"summary": "Port Nutch benchmark to Nutchbase", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Port Nutch benchmark to Nutchbase"}, {"question": "What is the main context?", "answer": "Bring tools from NUTCH-863 to Nutchbase, and measure the performance of the Nutchbase branch vs. trunk."}]}}
{"issue_id": "NUTCH-868", "project": "NUTCH", "title": "ParseSegment NullPointerException", "status": "Closed", "priority": "Major", "reporter": "Max Lynch", "assignee": "Julien Nioche", "created": "2010-08-02T02:51:46.219+0000", "updated": "2013-05-22T03:53:20.660+0000", "description": "The Nutch parser step will fail if ParseUtil.parse returns null.  Perhaps the issue is because I applied this patch: https://issues.apache.org/jira/browse/NUTCH-696 \n\nThe exception happens in ParseSegment.map at line 91.  The attached patch fixes the issue.", "comments": ["Fixes a NullPointerException if ParseUtil.parse returns null", "h3. I can confirm :\n\n{code}\njava.lang.NullPointerException at org.apache.nutch.parse.ParseSegment.map(ParseSegment.java:92)\nat org.apache.nutch.parse.ParseSegment.map(ParseSegment.java:42) \nat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50) \nat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358) \nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:307) \nat org.apache.hadoop.mapred.Child.main(Child.java:170) \n{code}\n\n*I'm using Nutch trunk with this patch*\n\n", "ParseUtil could return null before NUTCH-696, it's just that it probably did not happen very often. \nAccording to the javadoc :\n\n{quote}\n If the parse is unsuccessful, a message is logged to the <code>WARNING</code> level, and an empty parse is returned.\n{quote}\n\n I will fix ParseUtil so that it returns an empty parse. Since it is called in various places this will be cleaner than checking that the parse is not null\n\nThanks", "NutchBase : Committed revision 981438.\ntrunk (2.0) :  Committed revision 981439.\n1.2 : Committed revision 981440\n\nThanks for reporting the issue"], "tasks": {"summary": "ParseSegment NullPointerException", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "ParseSegment NullPointerException"}, {"question": "What is the main context?", "answer": "The Nutch parser step will fail if ParseUtil.parse returns null.  Perhaps the issue is because I applied this patch: https://issues.apache.org/jira/browse/NUTCH-696 \n\nThe exception happens in ParseSeg"}]}}
{"issue_id": "NUTCH-869", "project": "NUTCH", "title": "Add back parse-html", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Julien Nioche", "created": "2010-08-02T10:31:43.637+0000", "updated": "2010-08-04T10:29:41.559+0000", "description": "We need to add back parse-html. There are a few serious problems with HTML parsing in Tika 0.7, so it's not possible to do a quality crawl using parse-tika alone. The necessary improvements to Tika are on the way, so if a future version of Tika > 0.7 has a chance of passing our tests we can again remove this plugin and use parse-tika alone.", "comments": ["+1", "Nutchbase : Committed revision 982184\n1.2 : Committed revision 982185\ntrunk (2.0) : Committed revision 982197"], "tasks": {"summary": "Add back parse-html", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add back parse-html"}, {"question": "What is the main context?", "answer": "We need to add back parse-html. There are a few serious problems with HTML parsing in Tika 0.7, so it's not possible to do a quality crawl using parse-tika alone. The necessary improvements to Tika ar"}]}}
{"issue_id": "NUTCH-87", "project": "NUTCH", "title": "Efficient site-specific crawling for a large number of sites", "status": "Closed", "priority": "Major", "reporter": "AJ Chen", "assignee": null, "created": "2005-09-03T05:22:42.000+0000", "updated": "2011-04-01T15:03:18.857+0000", "description": "There is a gap between whole-web crawling and single (or handful) site crawling. Many applications actually fall in this gap, which usually require to crawl a large number of selected sites, say 100000 domains. Current CrawlTool is designed for a handful of sites. So, this request calls for a new feature or improvement on CrawTool so that \"nutch crawl\" command can efficiently deal with large number of sites. One requirement is to add or change smallest amount of code so that this feature can be implemented sooner rather than later. \n\nThere is a discussion about adding a URLFilter to implement this requested feature, see the following thread - \nhttp://www.mail-archive.com/nutch-dev%40lucene.apache.org/msg00726.html\nThe idea is to use a hashtable in URLFilter for looking up regex for any given domain. Hashtable will be much faster than list implementation currently used in RegexURLFilter.  Fortunately, Matt Kangas has implemented such idea before for his own application and is willing to make it available for adaptation to Nutch. I'll be happy to help him in this regard.  \n\nBut, before we do it, we would like to hear more discussions or comments about this approach or other approaches. Particularly, let us know what potential downside will be for hashtable lookup in a new URLFilter plugin.\n\nAJ Chen\n\n", "comments": ["The attached tarball contains sources to two Java classes:\n\nepile.crawl.plugin.whitelisturlfilter.WhitelistURLFilter.java\nepile.crawl.whitelist.WhitelistWriter.java\n\nLicense granted for inclusion in ASF works by Team Gigabyte, Inc.\n\nNote that there is one known logic bug in WhitelistWriter, which I'll be fixing in my own codebase shortly. Updates will be posted here. :^)", "Sample edits to nutch-site.xml for use with this plugin:\n\n\n<property>\n  <name>epile.crawl.whitelist.enableUndirectedCrawl</name>\n  <value>false</value>\n</property>\n\n<property>\n  <name>urlfilter.whitelist.file</name>\n  <value>/var/epile/crawl/whitelist_map</value>\n  <description>Name of file containing the location of the on-disk whitelist map directory.</description>\n</property>\n\n<property>\n  <name>plugin.includes</name>\n  <value>epile-whitelisturlfilter|urlfilter-(prefix|regex)|parse-(text|html)|index-basic|query-(basic|site|url)</value>\n</property>\n\n<property>\n  <name>urlfilter.order</name>\n  <value>org.apache.nutch.net.RegexURLFilter epile.crawl.plugin.WhitelistURLFilter</value> \n</property>\n", "Sample plugin.xml file for use with WhitelistURLFilter\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<plugin\n   id=\"epile-whitelisturlfilter\"\n   name=\"Epile whitelist URL filter\"\n   version=\"1.0.0\"\n   provider-name=\"teamgigabyte.com\">\n\n   <extension-point\n      id=\"org.apache.nutch.net.URLFilter\"\n      name=\"Nutch URL Filter\"/>\n\n   <runtime></runtime>\n\n   <extension id=\"org.apache.nutch.net.urlfiler\"\n      name=\"Epile Whitelist URL Filter\"\n      point=\"org.apache.nutch.net.URLFilter\">\n             \n      <implementation id=\"WhitelistURLFilter\"\n         class=\"epile.crawl.plugin.WhitelistURLFilter\"/>                    \n   </extension>\n</plugin>", "Matt,\n\nIs there a how-to or tutorial on how to get this plugin up and running?  I am running into problems (probably mine) on the integration of this.  Thanks!", "THIS REPLACES THE PREVIOUS TARBALL\nSEE THE INCLUDED README.txt FOR USAGE GUIDELINES\n\nPlace both of these files into ~nutch/src/plugin, then:\n- untar the tarball\n- apply the patch to ~nutch/src/plugin/build.xml to permit urifilter-whitelist to be built\n\nNext, cd ~nutch and build (\"ant\").\n\nA JUnit test is included. It will be run automatically by \"ant test-plugins\".\n\nThen follow the instructions in ~nutch/src/plugin/urlfilter-whitelist/README.txt", "JIRA-87-whitelistfilter.tar.gz is OBSOLETE. Use the newer tarball + patch file instead.", "The previous patch file is valid for 0.7. Here is one that works for 0.8-dev (trunk).\n\n(It's three separate one-line additions, to include the plugin in the \"deploy\", \"test\" , and \"clean\" targets.)", "Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Efficient site-specific crawling for a large number of sites", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Efficient site-specific crawling for a large number of sites"}, {"question": "What is the main context?", "answer": "There is a gap between whole-web crawling and single (or handful) site crawling. Many applications actually fall in this gap, which usually require to crawl a large number of selected sites, say 10000"}]}}
{"issue_id": "NUTCH-870", "project": "NUTCH", "title": "Injector should add the metadata before calling injectedScore", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Chris A. Mattmann", "created": "2010-08-02T12:26:33.086+0000", "updated": "2010-08-10T08:28:05.144+0000", "description": "The injector should add the metadata to the CrawlDatum *before* calling _scfilters.injectedScore(value, datum)_ as the metadata could be used by the ScoringFilters in this method.\n\nThe method _scfilters.injectedScore()_ should also be called regardless of whether the score is the one specified in the seed list or the one set by default but we should then modify the scoringOPICFilter so that it does not override it. \n\nAny thoughts? ", "comments": ["+1, this looks good to me. This and NUTCH-858 are the only remaining issues prior to a 1.2 point release, so my +1 to commit this back to 1.2", "- fixed in Nutch trunk (branch-1.3) r983278, Nutch 1.2 RC branch r983311 and in Nutchbase in r983315. Thanks for the patch, Julien. I didn't see any objections from anyone on this and it looked fine to me, so I applied it per my earlier email. This wraps up the dev for 1.2 I think so I will cut the RC later today.", "the method injectedScore in OPICScoringFilter (and also LinkAnalysisScoringFilter) should be modified in order not to override the score specified in the seed list metadata", "Opened an new issue re- scoring filters"], "tasks": {"summary": "Injector should add the metadata before calling injectedScore", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Injector should add the metadata before calling injectedScore"}, {"question": "What is the main context?", "answer": "The injector should add the metadata to the CrawlDatum *before* calling _scfilters.injectedScore(value, datum)_ as the metadata could be used by the ScoringFilters in this method.\n\nThe method _scfilte"}]}}
{"issue_id": "NUTCH-871", "project": "NUTCH", "title": "MoreIndexingFilter missing date format", "status": "Closed", "priority": "Minor", "reporter": "Max Lynch", "assignee": "Chris A. Mattmann", "created": "2010-08-02T16:42:37.669+0000", "updated": "2013-05-22T03:54:44.917+0000", "description": "Added another date format to MoreIndexingFilter.java which is part of the indexer-more plugin.  I had a date that wasn't being properly parsed so I fixed it.", "comments": ["The first patch had an extraneous change set that shouldn't have been included.  This is the correct patch.", "I'll port this to trunk and branch-1.2 as well.", "- fixed in trunk in r981971, in branch-1.2 in r981970 and in nutchbase in r981975. To apply the patch to the Nutchbase branch, I had to wire it up by hand since there appear to be some extraneous line reformattings present in MoreIndexingFilter.java in the Nutchbase branch. Thanks, Max!"], "tasks": {"summary": "MoreIndexingFilter missing date format", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "MoreIndexingFilter missing date format"}, {"question": "What is the main context?", "answer": "Added another date format to MoreIndexingFilter.java which is part of the indexer-more plugin.  I had a date that wasn't being properly parsed so I fixed it."}]}}
{"issue_id": "NUTCH-872", "project": "NUTCH", "title": "Change the default fetcher.parse to FALSE", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": null, "created": "2010-08-05T13:54:56.937+0000", "updated": "2012-09-10T11:52:13.844+0000", "description": "I propose to change this property to false. The reason is that it's a safer default - parsing issues don't lead to a loss of the downloaded content. For larger crawls this is the recommended way to run Fetcher. Users that run smaller crawls can still override it.", "comments": ["Good idea. We should probably remove the option '-noParsing' from the Fetcher and rely only on the value set in the parameter files or reverse its logic and call it '-parsing' in order to force the value to true", "+1 for calling it -parse or -parsing .", "I changed the name of the option to \"-parse\" to be consistent with the nutch-default.xml naming. I also updated the API to use this name, it's less confusing this way.\n\nCommitted in rev. 984401. Thanks for the feedback.", "To all: Andrzej has committed this to 1.3 as well in r1079746 at 2011-03-09.", "Something funky is still going on in 1.4-dev:\n\n* the -noParsing option is still output by fetcher when called without params;\n* the -parse option seems to be ignored;\n* the directive fetcher.parse seems to be ignored.\n\nIn other words, i cannot get the fetcher to parse in 1.4-dev. Debugging output clearly shows false for the setting coming from the config while it's actually set to true. ", "It seems something messed with my build and/or config. It works (again).", "I got a little follow-up question on this.\nIf i use fetcher.store.content / false couse i want to save space in my database i also need to set fetcher.parse to true since if it doesn't save the fetched content i can't parse it later, only during the fetch.\nIs this correct?\n", "Yes that is correct.", "I applied the patch and did a test run with nutch 2 against hbase but it still stores the\nf:cnt field with the entire source document. I have only done the fetch with the parse set to true and store to false.\n\nSnippet:\n------------\nf:bas                                          timestamp=1346459170179, value=http://www.wwwww.www/                                                                                 \n f:cnt                                          timestamp=1346459170179, value=<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\\x0A\"http://www.w3.org/TR/xhtml1/DTD/xhtml1\n                                                -transitional.dtd\">\\x0A<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\\x0A <head>\\x0A \n----------------\nThis shouldn't be there if i undertood it correct, right? :-)", "Christian, I ran a testcrawl with Nutch2.x branch and it does seem to work right:\n\nbin/nutch inject ~/urls/\nbin/nutch generate\nbin/nutch fetch -Dfetcher.parse=true -Dfetcher.store.content=false theBatchId\n\nNow I check my HBase and the content family is empty for the fetched/parsed urls. And they are parsed correctly.\n\nIf your problem persists, please try to explain in detail how you run the crawl. (Also it is better to put it onto mailing list next time.)", "Ok, sorry. Will use mailing list next time.\nIt works when i use this. \nbin/nutch fetch -Dfetcher.parse=true -Dfetcher.store.content=false $batchId\n\nbut with only 'bin/nutch fetch $batchid' and relying on the settings in the config it doesn't work.\n\nMy config is:\n<property>\n  <name>fetcher.parse</name>\n  <value>true</value>\n  <description>If true, fetcher will parse content. NOTE: previous releases would\n  default to true. Since 2.0 this is set to false as a safer default.</description>\n</property>\n\n<property>\n  <name>fetcher.store.content</name>\n  <value>false</value>\n  <description>If true, fetcher will store content.</description>\n</property>", "That IS really weird. Not sure why it doesn't work with the properties in config.."], "tasks": {"summary": "Change the default fetcher.parse to FALSE", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Change the default fetcher.parse to FALSE"}, {"question": "What is the main context?", "answer": "I propose to change this property to false. The reason is that it's a safer default - parsing issues don't lead to a loss of the downloaded content. For larger crawls this is the recommended way to ru"}]}}
{"issue_id": "NUTCH-873", "project": "NUTCH", "title": "Ivy configuration settings don't include Gora", "status": "Closed", "priority": "Blocker", "reporter": "Chris A. Mattmann", "assignee": "Chris A. Mattmann", "created": "2010-08-07T23:18:01.520+0000", "updated": "2013-05-22T03:53:21.178+0000", "description": "The Nutch 2.0 trunk now requires Gora, and even though it's not available in any repository, we should still configure Ivy to depend on it so that the build will work provided you follow the Gora instructions here:\n\nhttp://github.com/enis/gora\n\nI've fixed it locally and will commit an update shortly that takes care of it. In order to compile Nutch trunk now (before we get Gora into a repo), here are the steps (copied from http://github.com/enis/gora):\n\n{noformat}\n$ git clone git://github.com/enis/gora.git\n$ cd gora \n$ ant\n{noformat}\n\nThis will install Gora into your local Ivy repo. Then from there on out, just update your Ivy resolver (or alternatively just the Nutch build post this issue being resolved) and you're good.", "comments": ["- fixed in r983322.", "It did not work as seamless for me. The gora build created a ~/.ivy2/local/org.gora directory, not with org.apache.gora namespace.\n\nI guess you need to move away from Github and go to Apache:\n{noformat} \n$ svn co http://svn.apache.org/repos/asf/incubator/gora/trunk gora\n$ cd gora\n$ ant\n{noformat} "], "tasks": {"summary": "Ivy configuration settings don't include Gora", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Ivy configuration settings don't include Gora"}, {"question": "What is the main context?", "answer": "The Nutch 2.0 trunk now requires Gora, and even though it's not available in any repository, we should still configure Ivy to depend on it so that the build will work provided you follow the Gora inst"}]}}
{"issue_id": "NUTCH-874", "project": "NUTCH", "title": "Make sure all plugins in src/plugin are compatible with Nutch 2.0 and Gora", "status": "Closed", "priority": "Critical", "reporter": "Chris A. Mattmann", "assignee": "Chris A. Mattmann", "created": "2010-08-08T19:07:12.306+0000", "updated": "2019-10-13T22:35:28.596+0000", "description": "I just noticed while fixing NUTCH-564 that the ExtParser hasn't been brought up to date with Nutch 2.0 trunk. We should review the plugins in src/plugin to make sure they all work with Gora/Nutchbase now.", "comments": ["Some plugins have not been ported to the new API as it does not provide multi valued parse results. See See http://search.lucidimagination.com/search/document/844c48289f2d07db/nutchbase_multi_value_parseresult_missing#4ed6f352ebcce8ef\n\nThis is probably not the case for the ExtParser though. We could rely on Tika's mechanism for external parsing instead of maintaining ours. WDYT?", "Hey Julien,\n\nI think Jukka already worked on something really similar to the ExtParser in Tika. See: http://tika.apache.org/0.7/api/org/apache/tika/parser/ExternalParser.html\n\nIf we go that route here in Nutch, then I think we should add an encoding attribute similar to NUTCH-564 and flow it through in parse-tika then. If we can do that, I think we're good!\n\nCheers,\nChris\n", "{quote}\nI think Jukka already worked on something really similar to the ExtParser in Tika. See: http://tika.apache.org/0.7/api/org/apache/tika/parser/ExternalParser.html\n{quote}\nyes, that's the one I had in mind\n\nOne of the plugins which hasn't been ported yet is the feed parser. We could rely on the one we recently added to Tika, knowing that there is a substantial difference in the sense that the Tika feed parser generates a simple XHTML representation of the document where the feeds are simply represented as anchors whereas the Nutch version created new documents for each feed.\n\nThere is also the parse-rss plugin in Nutch which is quite similar - what's the difference with the feed one again? Since the Tika parser would handle all sorts of feed formats why not simply rely on it? ", "I know the heat has kind of shifted away from Nutchgora but it would be great to clarify what this issues actually encapsulates. Was/is it is the case that some plugins in Nutchgora are not actually working with the Nutchgora API? I kinda confused with this one! ", "Set and classify", "trivial patch to remove unused classes brought to our attention by Kiran Chitturi. Thanks for this Kiran, your contributions are greatly appreciated.", "part 1 e.g. removal of unused imports committed @revision 1396850 in 2.x head", "Integrated in Nutch-nutchgora #375 (See [https://builds.apache.org/job/Nutch-nutchgora/375/])\n    NUTCH-874 Make sure all plugins in src/plugin are compatible with Nutch 2.0 and Gora (part 1) (Revision 1396850)\n\n     Result = SUCCESS\nlewismc : \nFiles : \n* /nutch/branches/2.x/CHANGES.txt\n* /nutch/branches/2.x/src/plugin/feed/src/java/org/apache/nutch/indexer/feed/FeedIndexingFilter.java\n* /nutch/branches/2.x/src/plugin/feed/src/java/org/apache/nutch/parse/feed/FeedParser.java\n* /nutch/branches/2.x/src/plugin/feed/src/test/org/apache/nutch/parse/feed/TestFeedParser.java\n* /nutch/branches/2.x/src/plugin/parse-ext/src/java/org/apache/nutch/parse/ext/ExtParser.java\n* /nutch/branches/2.x/src/plugin/parse-ext/src/test/org/apache/nutch/parse/ext/TestExtParser.java\n* /nutch/branches/2.x/src/plugin/parse-swf/src/test/org/apache/nutch/parse/swf/TestSWFParser.java\n* /nutch/branches/2.x/src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/TikaParser.java\n* /nutch/branches/2.x/src/plugin/parse-zip/src/java/org/apache/nutch/parse/zip/ZipParser.java\n* /nutch/branches/2.x/src/plugin/parse-zip/src/java/org/apache/nutch/parse/zip/ZipTextExtractor.java\n* /nutch/branches/2.x/src/plugin/parse-zip/src/test/org/apache/nutch/parse/zip/TestZipParser.java\n", "The following plugins need to be ported for compatibility in 2.x \n\ni)   Feed\nii)  parse-swf\niii) parse-ext\niv) parse-zip\nv) parse-metatags ( I wrote patch for this earlier, NUTCH-1478)", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "Make sure all plugins in src/plugin are compatible with Nutch 2.0 and Gora", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Make sure all plugins in src/plugin are compatible with Nutch 2.0 and Gora"}, {"question": "What is the main context?", "answer": "I just noticed while fixing NUTCH-564 that the ExtParser hasn't been brought up to date with Nutch 2.0 trunk. We should review the plugins in src/plugin to make sure they all work with Gora/Nutchbase "}]}}
{"issue_id": "NUTCH-875", "project": "NUTCH", "title": "Port Webgraph to Nutch 2.0", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2010-08-09T08:42:24.491+0000", "updated": "2019-10-13T22:35:16.265+0000", "description": "The webgraph has not yet been ported to the GORA-based API.\n", "comments": ["Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "Port Webgraph to Nutch 2.0", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Port Webgraph to Nutch 2.0"}, {"question": "What is the main context?", "answer": "The webgraph has not yet been ported to the GORA-based API.\n"}]}}
{"issue_id": "NUTCH-876", "project": "NUTCH", "title": "Remove remaining robots/IP blocking code in lib-http", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-08-09T14:09:16.287+0000", "updated": "2013-05-22T03:53:32.250+0000", "description": "There are remains of the (very old) blocking code in lib-http/.../HttpBase.java. This code was used with the OldFetcher to manage politeness limits. New trunk doesn't have OldFetcher anymore, so this code is useless. Furthermore, there is an actual bug here - FetcherJob forgets to set Protocol.CHECK_BLOCKING and Protocol.CHECK_ROBOTS to false, and the defaults in lib-http are set to true.", "comments": ["Patch to fix the issue. If there are no objections I'll commit this shortly.", "Committed in rev. 984337."], "tasks": {"summary": "Remove remaining robots/IP blocking code in lib-http", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove remaining robots/IP blocking code in lib-http"}, {"question": "What is the main context?", "answer": "There are remains of the (very old) blocking code in lib-http/.../HttpBase.java. This code was used with the OldFetcher to manage politeness limits. New trunk doesn't have OldFetcher anymore, so this "}]}}
{"issue_id": "NUTCH-877", "project": "NUTCH", "title": "Allow setting of slop values for non-quote phrase queries on query-basic plugin", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Dennis Kubes", "created": "2010-08-09T18:47:46.532+0000", "updated": "2011-06-08T21:34:23.889+0000", "description": "Patch adds a configuration variable for setting slop values on phrase queries.  The default slop value, which currently can't be changed through configuration, is Integer.MAX_VALUE.  It produces something like this, which doesn't seem right to me.  If you are searching for a phrase you usually want it within a certain distance:\n\n2.9141337E-4 = weight(content:\"my phrase\"~2147483647 in 1029), product of:\n\n    * 0.07163286 = queryWeight(content:\"my phrase\"~2147483647), product of:\n          o 9.657982 = idf(content: my=13470 phrase=534)\n          o 0.0074169594 = queryNorm\n\nThis patch adds the query.phrase.slop configuration value to the nutch-default.xml file.  It has a default setting of 5.", "comments": ["Adds query.phrase.slop configuration variable to nutch-default and setting of variable in setConf method of query-basic plugin for phrase queries.", "+1 ", "+1 from me too on this Dennis. Commit away! Please add this to branches/branch-1.2. Once Julien applies NUTCH-878, I'll respin a 1.2 RC.\n\n", "Committed revision 989733.\n"], "tasks": {"summary": "Allow setting of slop values for non-quote phrase queries on query-basic plugin", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Allow setting of slop values for non-quote phrase queries on query-basic plugin"}, {"question": "What is the main context?", "answer": "Patch adds a configuration variable for setting slop values on phrase queries.  The default slop value, which currently can't be changed through configuration, is Integer.MAX_VALUE.  It produces somet"}]}}
{"issue_id": "NUTCH-878", "project": "NUTCH", "title": "ScoringFilters should not override the injected score ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-08-10T08:27:14.258+0000", "updated": "2013-05-22T03:53:26.655+0000", "description": "the method injectedScore in OPICScoringFilter (and also LinkAnalysisScoringFilter) should be modified in order not to override the score specified in the seed list metadata ", "comments": ["Patches for the branch 1.2 and trunk\n\nThe scoring filters do not override the score set by the injector as it might have been set per URL in the seed list. This has for result that the LinkAnalysisScoringFilter now implicitely relies on the parameter db.score.injected and not on link.analyze.injected.score. Both had the same default value anyway.\n\nThe patch for trunk also fixes the behaviour of the InjectorJob which now calls injectedscore() even if the URL has a custom score.\n\n  ", "+1 from me, Julien, thanks!", "1.2 : Committed revision 984075.\ntrunk : Committed revision 984082."], "tasks": {"summary": "ScoringFilters should not override the injected score ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "ScoringFilters should not override the injected score "}, {"question": "What is the main context?", "answer": "the method injectedScore in OPICScoringFilter (and also LinkAnalysisScoringFilter) should be modified in order not to override the score specified in the seed list metadata "}]}}
{"issue_id": "NUTCH-879", "project": "NUTCH", "title": "URL-s getting lost", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": null, "created": "2010-08-10T20:23:03.132+0000", "updated": "2019-10-13T22:35:53.658+0000", "description": "I ran the Benchmark using branch-1.3 and trunk (formerly nutchbase). With the same Benchmark parameters and the same plugins branch-1.3 collects ~1.5mln urls, while trunk collects ~20,000 urls. Clearly something is wrong.", "comments": ["DB stats and benchmark results.", "Did you have a chance to try with hbase backend? I wonder if it's a bug in Gora or nutchbase?", "I haven't tried hbase yet, but I'm going to - will update this issue soon.", "needs to be sorted in 2.0", "This looks heliishly serious and pretty worrying actually. Ferdy (or anyone else), can you please run this against one of your HBase instances, I will do the same with Cassandra and we can determine what is going on here. ", "This a pretty old issue. Nevertheless the bug might still be active. I'll look into it.", "bump to 2.1", "Agree to fix this issue later. Although I could not yet get to the bottom of this, I'm pretty sure the issue is not as severe as originally reported. (Based on current experencies with running Nutchgora in production).", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "URL-s getting lost", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "URL-s getting lost"}, {"question": "What is the main context?", "answer": "I ran the Benchmark using branch-1.3 and trunk (formerly nutchbase). With the same Benchmark parameters and the same plugins branch-1.3 collects ~1.5mln urls, while trunk collects ~20,000 urls. Clearl"}]}}
{"issue_id": "NUTCH-88", "project": "NUTCH", "title": "Enhance ParserFactory plugin selection policy", "status": "Closed", "priority": "Major", "reporter": "Jerome Charron", "assignee": "Jerome Charron", "created": "2005-09-08T18:06:28.000+0000", "updated": "2006-10-24T16:14:22.000+0000", "description": "The ParserFactory choose the Parser plugin to use based on the content-types and path-suffix defined in the parsers plugin.xml file.\nThe selection policy is as follow:\nContent type has priority: the first plugin found whose \"contentType\" attribute matches the beginning of the content's type is used. \nIf none match, then the first whose \"pathSuffix\" attribute matches the end of the url's path is used.\nIf neither of these match, then the first plugin whose \"pathSuffix\" is the empty string is used.\n\nThis policy has a lot of problems when no matching is found, because a random parser is used (and there is a lot of chance this parser can't handle the content).\nOn the other hand, the content-type associated to a parser plugin is specified in the plugin.xml of each plugin (this is the value used by the ParserFactory), AND the code of each parser checks itself in its code if the content-type is ok (it uses an hard-coded content-type value, and not uses the value specified in the plugin.xml => possibility of missmatches between content-type hard-coded and content-type delcared in plugin.xml).\n\nA complete list of problems and discussion aout this point is available in:\n  * http://www.mail-archive.com/nutch-user%40lucene.apache.org/msg00744.html\n  * http://www.mail-archive.com/nutch-dev%40lucene.apache.org/msg00789.html\n", "comments": ["Additional issue: the plugin descriptor currently allows to put just a single mime type. It is a realistic scenario that some plugins can handle multiple content types.\n\nMoreover, plugins could handle content types with varying degrees of \"faithfulness\" or precision - e.g. there could be three parsers for PDF, one that is able to do a simple text extraction (jpedal), and another that can also handle more complex PDF with metadata (pdfbox), and yet another that cannot handle metadata, but can preserve the layout (pdftohtml). Currently there is no way to express a preference of one plugin over another, if both support the same content type.", "I'm currently working on writing a proposal for addressing this issue. The proposal will include the following information:\n\n* summary of issue\n\n* suggested remedy\n\n* architectural impact\n\n* impact on current releases of Nutch\n - incompatabilities\n - any other issues\n\n* available resources\n\n* timeframe\n\nI hope to have it done by tomorrow afternoon, say 3pm Pacific Standard Time.\n\nThanks,\n  Chris", "Hi.\n\nI share your opinion -- this is an important issue. If I may add my few cents, the crawler should try to mimic a browser in handling mime types. This, of course, gets quite complex since Internet Explorer has a very confusing and unnecessarily complex mime type handling heuristic... which happens to change from version to version as well. Anyway, if you care to look, there are a few articles that explain the steps performed by IE to resolve a mime type of a Web page --\n\nhttp://msdn.microsoft.com/workshop/networking/moniker/overview/appendix_a.asp\nhttp://msdn.microsoft.com/workshop/networking/moniker/overview/mime_handling.asp\n\nD.", "Dawid,\nThanks for your pointers on IE MimeType resolution. We have in Nutch a MimeType resolver based on both file-ext and files \"magic\" sequences to find the content-type of a file. It is actually underused, and perhaps some enhancement must be added: such as the content-type mapping: allow to map a content-type to a normalized one (ie mapping for instance application/powerpoint to application/vnd.ms-powerpoint, so that only the normalized version must be registered in the plugin.xml file).\n\nChris,\nThanks in advance for your futur work. Could you please synchronize your efforts with Sébastien, since he seems very interested to contribute.\n\nAndrzej,\nThe way to express a preference of one plugin over another, if both support the same content type is to activate the plugin you want to handle a content-type and deactivate onther ones.\nNo?\n\nNote: Since the MimeResolver handles associations between file-extensions and content-types, the path-suffix in plugin.xml (and in ParserFactory policy for choosing a Parser) could certainly be removed in order to have only one central point for storing this knowledge.", "Yep, I know about byte-magic mime detector. I'm just pointing out Internet Explorer doesn't use it... or at least, it doesn't always use it the way you would expect it to. Whether Nutch should mimic IE in this behaviour is another question.", "A proposal is available on the Nutch Wiki:\nhttp://wiki.apache.org/nutch/ParserFactoryImprovementProposal", "A first step implementation of the proposal is committed: http://svn.apache.org/viewcvs.cgi?rev=292035&view=rev", "Second step implementation details: http://svn.apache.org/viewcvs.cgi?rev=292865&view=rev\nAnd final step implementation details: http://svn.apache.org/viewcvs.cgi?rev=321231&view=rev\n(some unit tests corrections: http://svn.apache.org/viewcvs.cgi?rev=321250&view=rev)\n\nBig thanks to Chris Mattmann and Sébastien Le Callonnec.\n ", "I am seeing some problems using this.\n\nFirst, the ParserFactory sometimes uses LOG.severe() which causes the Fetcher to exit.  Is there a reason this cannot be LOG.warning()?  LOG.severe() should only be used if you intend the application to exit.  This configuration problem does not seem to warrant that.  And I'm getting it with the default settings when an application/pdf is encountered.\n\nThe second problem I'm seeing is that most html pages are parsed by the ParseText parser.  I think this is because their HTTP content-type header is \"text/html; charset=ISO-8859-1\", which does not match \"text/html\".  Where should the content-type parameters be removed?\n", "1. It's true, LOG.severe() must be changed to LOG.warn()\n2. The MimeType already performs such process (removing the content-type parameters). I think that the content-type stored in the document meta-data must be unchanged (to avoid loosing some information that can be usefull for some other parts of code), so I propose to :\n  * Add a static method in MimeType that parse a content-type string and remove all it's parameters (keeping only primary type and sub type).\n  * The ParserFactory uses this content-type parsing method before trying to find the parser to apply.\n\nAre you ok with that?\n", "These both sound like good changes.  +1", " * Add a static method in MimeType that parse a content-type string and remove all it's parameters (keeping only primary type and sub type). \n\n>> I think that this sounds great, +1 \n\n * The ParserFactory uses this content-type parsing method before trying to find the parser to apply. \n\n>> I see one problem with having the ParserFactory call the MimeType method suggested in the first bullet above: in my mind that should be handled at the protocol level, no? The protocol handler should make sure that by the time it hands content off to a Parser that is generated by the ParserFactory, the contentType, or \"mimeType\" should have already been cleansed. What do you think about that?\n\nThanks,\n  Chris\n\n\n", "If it's to happen at parse time then it should happen in the Content constructor, so that it's only done in one place, and we don't rely on each protocol to normalize the mime type.\n\nThat said, I'm not sure that we should do it at parse time.  The \"charset=ISO-8859-1\" is a valid part of the mime type, so I'm not sure we should remove it.  But it is not required for parser selection.  So I think it makes sense for the parser selector to remove the charset specification.\n\nIn any case, we should patch these problems ASAP, as they make the trunk and mapred branches behave very poorly.", "Hey Doug,\n\n  Yeah I think you're right on this one. I'll work with Jerome (if he needs any help) to get this fixed ASAP.\n\nThanks,\n  Chris\n", "That's on the way. Corrections are coded.\nI'm just adding some unit tests and performing some minimal functional tests.\nSo it should be committed in the next hour.\n\nRegards", "Corrections are committed (http://svn.apache.org/viewcvs.cgi?rev=326889&view=rev).\nSorry for the delay, but I do my best...\n(thanks Chris for proposing your help)\n\nImplementation Note:\nIn this implementation, the MimeType.clean(String) method constructs a new MimeType object (the MimeType constructor clean the content-type) each type it is called.\nIt was the speedest way for solving this issue. But it is not optimal code, since it will better for performance (avoid instantiating very short time life objects) that:\n1. The clean method really contains the cleaning code.\n2. The MimeType constructors uses the clean method.\n\nRegards\n", "Jerome,  \n\nThis works well now.  I've merged your changes to the mapred branch.\n\nThanks!\n\nDoug\n", "closing issues for released versions"], "tasks": {"summary": "Enhance ParserFactory plugin selection policy", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Enhance ParserFactory plugin selection policy"}, {"question": "What is the main context?", "answer": "The ParserFactory choose the Parser plugin to use based on the content-types and path-suffix defined in the parsers plugin.xml file.\nThe selection policy is as follow:\nContent type has priority: the f"}]}}
{"issue_id": "NUTCH-880", "project": "NUTCH", "title": "REST API for Nutch", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-08-11T09:49:09.906+0000", "updated": "2013-05-22T03:53:18.071+0000", "description": "This issue is for discussing a REST-style API for accessing Nutch.\n\nHere's an initial idea:\n\n* I propose to use org.restlet for handling requests and returning JSON/XML/whatever responses.\n* hook up all regular tools so that they can be driven via this API. This would have to be an async API, since all Nutch operations take long time to execute. It follows then that we need to be able also to list running operations, retrieve their current status, and possibly abort/cancel/stop/suspend/resume/...? This also means that we would have to potentially create & manage many threads in a servlet - AFAIK this is frowned upon by J2EE purists...\n* package this in a webapp (that includes all deps, essentially nutch.job content), with the restlet servlet as an entry point.\n\nOpen issues:\n\n* how to implement the reading of crawl results via this API\n* should we manage only crawls that use a single configuration per webapp, or should we have a notion of crawl contexts (sets of crawl configs) with CRUD ops on them? this would be nice, because it would allow managing of several different crawls, with different configs, in a single webapp - but it complicates the implementation a lot.", "comments": ["Initial patch for discussion. This is a work in progress, so only some functionality is implemented, and even less than that is actually working ;)\n\nI would appreciate a review and comments.", "+1 from me. \n\nI think we can combine the approach you outlined in NUTCH-907 with this one. Instead of using confId-s to identify\ndifferent confs, we can use different crawl prefixes (or whatever we will call them) to identify different crawl sets (though\nwe still need a way to attach different conf-s to different crawl sets).\n\nI think API overall looks good. Maybe we can change all the Map<String, Object>s to be some classes though.\n\nA minor question:\n\nIn JobManager.java:\n\n+  public static enum JobType {INJECT, GENERATE, FETCH, PARSE, UPDATEDB, INDEX, CRAWL, CLASS};\n\nWhat is \"CLASS\"  ?\n\nBtw, Andrzej, I will be happy to help out with the implementation if you want.", "bq. I think we can combine the approach you outlined in NUTCH-907 with this one.\n\nI'm not sure... they are really not the same things - you can execute many crawls with different seed lists, but still using the same Configuration.\n\nbq. What is \"CLASS\" ?\n\nIt's the same as bin/nutch fully.qualified.class.name, only here I require that it implements NutchTool.\n\nbq. Btw, Andrzej, I will be happy to help out with the implementation if you want.\n\nBy all means - I didn't have time so far to progress beyond this patch...", "An improved version, which actually works :) The configuration and job management is implemented, there is also a unit test that exercises this API.\n\nIf there are no objections I'd like to commit this first version of the API, and continue improving it in other issues.", "The webapp part is tracked now in NUTCH-929.", "Committed in rev. 1028235. The webapp part of this issue is tracked now in NUTCH-929.", "This revision introduced a bug in the nutch inject command. It now throws a NullPointerException.\n\nPlease take a look at:\nhttp://svn.apache.org/viewvc/nutch/trunk/src/java/org/apache/nutch/crawl/InjectorJob.java?annotate=1028235&pathrev=1028235\n\nMake sure the first element in the array is not null:\n\n{noformat}\nIndex: src/java/org/apache/nutch/crawl/InjectorJob.java\n===================================================================\n--- src/java/org/apache/nutch/crawl/InjectorJob.java    (revision 1031881)\n+++ src/java/org/apache/nutch/crawl/InjectorJob.java    (working copy)\n@@ -242,6 +242,7 @@\n     job.setReducerClass(Reducer.class);\n     job.setNumReduceTasks(0);\n     job.waitForCompletion(true);\n+    jobs[0] = job;\n\n     job = new NutchJob(getConf(), \"inject-p2 \" + args[0]);\n     StorageUtils.initMapperJob(job, FIELDS, String.class,\n{noformat}\n", "Thanks - this issue is already fixed in NUTCH-932, to be committed soon."], "tasks": {"summary": "REST API for Nutch", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "REST API for Nutch"}, {"question": "What is the main context?", "answer": "This issue is for discussing a REST-style API for accessing Nutch.\n\nHere's an initial idea:\n\n* I propose to use org.restlet for handling requests and returning JSON/XML/whatever responses.\n* hook up a"}]}}
{"issue_id": "NUTCH-881", "project": "NUTCH", "title": "Good quality documentation for Nutch", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Lewis John McGibbney", "created": "2010-08-11T10:05:27.731+0000", "updated": "2015-01-09T06:07:38.945+0000", "description": "This is, and has been, a long standing request from Nutch users. This becomes an acute need as we redesign Nutch 2.0, because the collective knowledge and the Wiki will no longer be useful without massive amount of editing.\n\nIMHO the reference documentation should be in SVN, and not on the Wiki - the Wiki is good for casual information and recipes but I think it's too messy and not reliable enough as a reference.\n\nI propose to start with the following:\n\n 1. let's decide on the format of the docs. Each format has its own pros and cons:\n  * HTML: easy to work with, but formatting may be messy unless we edit it by hand, at which point it's no longer so easy... Good toolchains to convert to other formats, but limited expressiveness of larger structures (e.g. book, chapters, TOC, multi-column layouts, etc).\n  * Docbook: learning curve is higher, but not insurmountable... Naturally yields very good structure. Figures/diagrams may be problematic - different renderers (html, pdf) like to treat the scaling and placing somewhat differently.\n  * Wiki-style (Confluence or TWiki): easy to use, but limited control over larger structures. Maven Doxia can format cwiki, twiki, and a host of other formats to e.g. html and pdf.\n  * other?\n\n 2. start documenting the main tools and the main APIs (e.g. the plugins and all the extension points). We can of course reuse material from the Wiki and from various presentations (e.g. the ApacheCon slides).", "comments": ["+1 for storing the documentation in SVN\n\nAs for the format I don't really like the idea of writing in HTML and would rather use something that could generate different formats (html, pdf). I have no experience of docbook or noxia, but would favour anything that can be used easily with ANT so that we could generate the documentation as part of the build process.\n\n", "The \"other?\" would be DITA. This is in some ways \"DocBook Version 2\" in that it seems to have most of the good features of DocBook - but be better for large software projects rather than single documents. \n\nBasically the documentation is written and stored in XML (in svn/git/cvs whatever). XSLT / xsl:fo is used to generate html and pdf from that single source. \n\n\nThere is a precedent too. Apache Derby is using DITA for its documentation\n\nhttp://db.apache.org/derby/manuals/dita.html\n\nI don't have experience of this, but DITA was recommended to me by a friendly documentation professional. \n\n\nI'm happy to learn about this and try to set up a framework", "{quote}\nI'm happy to learn about this and try to set up a framework\n{quote}\n\nAlex, +1 from me, worth a try and looking forward to see what you come up with.\n\nCheers,\nChris\n", "So what is new in Nutch 2.0 which doesn't appear in Nutch 1.x ?\n\nGora is the main thing which comes to mind. \n\nHow do the config files differ?\nHow does Nutch's use of Hadoop differ? \nHow do the command lines differ? (Presumably you need different command lines to say *where* to store the crawldb, right?)\n\nanything else?", "bq. So what is new in Nutch 2.0 which doesn't appear in Nutch 1.x ? Gora is the main thing which comes to mind.\n\nYes. We also removed all search-related code from Nutch and rely exclusively on Solr to perform searching. This means that some APIs have been removed (e.g. query filters, text analysis, lucene indexing backend).\n\nbq. How do the config files differ?\n\nWe still use the same nutch-default/nutch-site.xml, plus per-plugin config files. Some properties have changes, e.g the ones to limit max. number of urls per host in generator. We added some Gora-related files, gora.properties and gora-*-mapping.xml, that define what driver to use and how to map webtable columns onto storage-specific columns/fields.\n\nbq. How does Nutch's use of Hadoop differ?\n\nAll jobs now use GoraInputFormat / GoraOutputFormat, which hides the details about the actual data storage backend.\n\nbq. How do the command lines differ? (Presumably you need different command lines to say where to store the crawldb, right?)\n\nYes. Actually, this could be a separate issue to be solved - currently we assume there is one Nutch webtable per storage backend, so we don't specify the \"db identifier\" anywhere... but this prevents us from defining multiple crawl configs that use the same backend, so it should be addressed.", "What is the current state of this issue and it's development?\n\nI can't help but think that this issue has been sidetracked but is a rather important prerequisite for a 2.0 release and for users and developers to understand the structure of Nutch >= 1.3\n\nAny thoughts of where I can pick this one up?", "In Nutch trunk we currently only have the wiki as a repository for any Nutch 2.0 information. Is this satisfactory?\n\nAs far as I can tell, the documentation for Gora_trunk is produced using Apache Forrest. I am reasonably familiar with using Forrest and it would be a great benefit, as well as lessening the burden upon mailing lists, if we could maintain a clean distribution of documentation bundled nicely into a /trunk/docs or/and branch-1.4/docs directory from now on and for all future official releases.\n\nI think the only addition to the documentation we require on the website is a formal tutorial (available as part of the Apache Nutch website), which we need to add to /site resources and which we could maintain and direct users to as a one stop resource for Nutch branch/tags, then similarly a separate resource for trunk.\n\nith specific reference to Nutch Trunk, in comparison on the Gora team they have provided a quick-start guide followed by a more in depth tutorial, which in our case we could apply to both branch-1.4 and 2.0 trunk. The quick-start guide would only show users how to get trunk up and running, then the formal tutorial would provide in-depth documentation on completing a crawl with either Nutch 1.4 or trunk 2.0. Does this sound reasonable?\n\nAndrzej provided some good comments in the correspondence on NUTCH-881 which should be addressed within any comprehensive documentation. I am very happy, and pretty keen to get this issue resolved but I think we need to agree on a specific tasks which need to be addressed, basically laying the path for everything this issue encompasses.", "For the time being, I think it only appropriate to shelve the 2.0 documentation.\n\nWith regards to 1.X documentation what are be agreeing on here? Taking in to consideration all of the above comments, I propose the following:\n\nAs the dev's agreed and moved documentation to SVN/site some time ago it provides the following resources.\n * About.html/pdf\n * bot.html/pdf\n * credits.html/pdf\n * FAQ.html/pdf - This now displays a direct link to the wiki. Which is easier for maintenance purposes\n * index.html/pdf\n * issue-tracking.html/pdf\n * link-map.html/pdf\n * mailing-lists.html/pdf\n * nightly.html/pdf\n * tutorial.html/pdf - this also contains some text and directs reads to the official Nutch tutorial\n * version-control.html/pdf\n * wiki.html/pdf - same as comments for tutorial.html/pdf\n\nI would consider the above the 'core' documentation for getting up and running with Nutch, however I'm not sure if we agree on whether the wiki is adequate enough to store the documentation, and whether it is tidy enough as well as being a rich enough resource for people to find and utilise the specific information they require. \n\nIn my opinion, the wiki has been working pretty well recently, there have been several 'on the fly' edits in recent weeks to accommodate a range of improvements suggested by the community. It has been a long process to get it looking and feeling like a better information resource for the community, however there is still lots to do. In addition, if we look at our sister projects e.g. Hadoop, Tika the community uses the wiki extensively.\n\nAny thoughts here about potential improvements or other Nutch specific documentation we need to add or need to update ASAP before we consider this issue resolved. I am working on an up-to-date Nutch & Hadoop tutorial and will hopefully have it completed for the end of the weekend.   ", "All child issues are addressed!!!\nAye right ;)\nLets close it off... we've done a good job over the years :)"], "tasks": {"summary": "Good quality documentation for Nutch", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Good quality documentation for Nutch"}, {"question": "What is the main context?", "answer": "This is, and has been, a long standing request from Nutch users. This becomes an acute need as we redesign Nutch 2.0, because the collective knowledge and the Wiki will no longer be useful without mas"}]}}
{"issue_id": "NUTCH-882", "project": "NUTCH", "title": "Design a Host table in GORA", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2010-08-11T11:26:54.997+0000", "updated": "2012-04-27T05:37:28.689+0000", "description": "Having a separate GORA table for storing information about hosts (and domains?) would be very useful for : \n* customising the behaviour of the fetching on a host basis e.g. number of threads, min time between threads etc...\n* storing stats\n* keeping metadata and possibly propagate them to the webpages \n* keeping a copy of the robots.txt and possibly use that later to filter the webtable\n* store sitemaps files and update the webtable accordingly\n\nI'll try to come up with a GORA schema for such a host table but any comments are of course already welcome ", "comments": ["Here is an initial version of the Host table. Very minimalistic for now but we'll make it evolve as we go. \n\n* src/gora/host.avsc : avro schema for the host table \n* modified gora mapping for sql backend\n* src/java/org/apache/nutch/host/HostDBReader.java : displays the info about an entry of the host table (or all of them)\n* src/java/org/apache/nutch/host/HostInjectorJob.java : mapreduce job which takes a seed host list and populates the host table with it\n* src/java/org/apache/nutch/host/HostMDApplierJob.java : mapreduce job which reads through the webtable and adds metadata taken from the host table\n\nThe key for the host table is reverted - just like the WebPages in the webtable. The hosts are represented with a URL in order to know about the protocol as well. The metadata are similar to the ones in the WebPages.\n\nObviously we'll need to add more things to it but this is already useful for projecting metadata by host onto the webtable. You can do that by : \n# Injecting the host metadata into the host table \n\n{code}\n./nutch org.apache.nutch.host.HostInjectorJob hostlist\n{code}\n\n# Applying the metadata to the WebTable\n\n{code}\n./nutch org.apache.nutch.host.HostMDApplierJob\n{code}\n\n# Check on the Webtable using \n\n{code}\n./nutch org.apache.nutch.crawl.WebTableReader\n{code}\n\nYou can of course do the same thing by putting the metadata in the URL seedlist then propagating them using  custom URL filters but this won't work for instance if a page is found from a page which belongs to a different host. The approach described here is a bit cleaner and can be used in a larger number of situations, e.g. when the metadata values are not known at the time of the URL seeding. We have developed a plugin for the detection of adult content which works quite well but found that the results were better after aggregating the stats at the host level, marking them as adult via a metadata, then project back onto the webtable and let a custom indexer use the value coming from the host to override the detection at the page level. Anyway, that was just an example :-)\n\nI am planning to add more to this code in the short term but would like to hear your comments on it. In particular I am planning to : \n* add a class which populate the host table given a webtable and (possibly) add statistics to it at the same time\n* maybe create a new plugin endpoint so that such statistics could be achieved using custom user functions\n* write an example of such an endpoint which would add stats per status\n* look into the management of robots.txt and sitemaps\n* see how we could leverage these host-related metadata to give specific instructions for the fetching (number of threads, time between calls) etc...\n\nJulien\n \n\n   \n", "This functionality is very useful for larger crawls. Some comments about the design:\n\n* the table can be populated by injection, as in the patch, or from webtable. Since keys are from different spaces (url-s vs. hosts) I think it would be very tricky to try to do this on the fly in one of the existing jobs... so this means an additional step in the workflow.\n\n* I'm worried about the scalability of the approach taken by HostMDApplierJob - per-host data will be multiplied by the number of urls from a host and put into webtable, which will in turn balloon the size of webtable...\n\nA little background: what we see here is a design issue typical for mapreduce, where you have to merge data keyed by keys from different spaces (with different granularity). Possible solutions involve:\n* first converting the data to a common key space and then submit both data as mapreduce inputs, or\n* submitting only the finer-grained input to mapreduce and dynamically converting the keys on the fly (and reading data directly from the coarser-grained source, accessing it randomly).\n\nA similar situation is described in HADOOP-3063 together with a solution, namely, to use random access and use Bloom filters to quickly discover missing keys.\n\nSo I propose that instead of statically merging the data (HostMDApplierJob) we could merge it dynamically on the fly, by implementing a high-performance reader of host table, and then use this reader directly in the context of map()/reduce() tasks as needed. This reader should use a Bloom filter to quickly determine nonexistent keys, and it may use a limited amount of in-memory cache for existing records. The bloom filter data should be re-computed on updates and stored/retrieved, to avoid lengthy initialization.\n\nThe cost of using this approach is IMHO much smaller than the cost of statically joining this data. The static join costs both space and time to execute an additional jon. Let's consider the dynamic join cost, e.g. in Fetcher - HostDBReader would be used only when initializing host queues, so the number of IO-s would be at most the number of unique hosts on the fetchlist (at most, because some of host data may be missing - here's Bloom filter to the rescue to quickly discover this without doing any IO). During updatedb we would likely want to access this data in DbUpdateReducer. Keys are URLs here, and they are ordered in ascending order - but they are in host-reversed format, which means that URLs from similar hosts and domains are close together. This is beneficial, because when we read data from HostDBReader we will read records that are close together, thus avoiding seeks. We can also cache the retrieved per-host data in DbUpdateReducer.", "Thanks for your comments Andrzej\n\n{quote}\nthe table can be populated by injection, as in the patch, or from webtable. Since keys are from different spaces (url-s vs. hosts) I think it would be very tricky to try to do this on the fly in one of \nthe existing jobs... so this means an additional step in the workflow.\n{quote}\n\nyes, that's what I meant by _add a class which populate the host table given a webtable and (possibly) add statistics to it at the same time_\n\n{quote}\n    *  I'm worried about the scalability of the approach taken by HostMDApplierJob - per-host data will be multiplied by the number of urls from a host and put into webtable, which will in turn balloon the size of webtable...\n{quote}\nthere would be a duplication of information indeed. In most cases that would be just a few bytes for a metadatum so no big deal compared to the overall size of a webpage object. \n\nRe-high performance reader, this is a nice idea and the example you gave for the Fetcher is very relevant. In terms of performance, it should not be that different from what I've implemented in the HostMDApplierJob, at least if most hosts are present in the host table. I suppose we could keep the HostMDApplier at least for the time being and open a separate JIRA for the BloomFiltered-Reader. Shall we put that in GORA?\n\nI'll start working on some code to populate/update the hostDB from an existing webtable\n\n\n\n\n", "I would like to start implementing the idea proposed by Andrzej. I have one question: I would like Host information to be accessible to plugins as well. Unfortunately, this will mean yet another API break for plugins. Should we do it like MR and introduce a Context object? So a typical plugin would look like this:\n\npublic void filter(String url, WebPage page, Context context); // context object will have a host in it for now. In the future, it may have other objects.\n\nWhat do you think?", "Hey Doğacan:\n\n+1 to introducing a NutchContext object. We're starting to get enough information and enough of a need to build out our own specific property set. \n\nCheers,\nChris", "+1 to NutchContext. See also NUTCH-907 because the changes required in Gora API will likely make this task easier (once implemented ;) ).", "I have  implemented a NutchContext object (which only has a Host in it for now). I also added a fast Host reader as Andrzej suggested by using bloom filters. For now, I also extended InjectorJob to have a NutchContext object and extended scoring filters to also accept NutchContext as an argument (only scoring filters for now, but I will extend this to all plugins). The fast host reader uses a new table (called metatable... yeah not very creative :), to read and write bloom filter data. The idea is, obviously, metatable stores information about other tables.\n\nUnfortunately, there is a huge problem that I need help with. I will try to explain it with an example. Let's say a ParserJob has 6 maps. We extended parse plugins so they also can use NutchContext objects. The problem is each map will update its OWN bloom filter and try to write its OWN bloom filter back to metatable. This, of course, breaks HostDb implementation as one map task overwrites bloom filter data. As a fix, I thought each task can write its own bloom filter to a temporary location using its task id. Once a job finishes, we can then read all tasks and write out a single bloom filter using data from all tasks. This is a very HACKISH solution though.\n\nWhat do you  guys think? Any better solutions?", "Here is an initial version. This doesn't have a bloom filter due to reasons I outlined in my previous comment. It adds a NutchContext object and adds NutchContext to a single method in ScoringFilters to demonstrate how it will look. I made a random change in OPICScoringFilter to demonstrate example usage.", "Doğacan, I missed your previous comment... the issue with partial bloom filters is usually solved that each task stores each own filter - this worked well for MapFile-s because they consisted of multiple parts, so then a Reader would open a part and a corresponding bloom filter.\n\nHere it's more complicated, I agree... though this reminds me of the situation that is handled by DynamicBloomFilter: it's basically a set of Bloom filters with a facade that hides this fact from the user. Here we could construct something similar, i.e. don't merge partial filters after closing the output, but instead when opening a Reader read all partial filters and pretend they are one.", "Thanks for the comments Andrzej. I think I can implement that solution as well, but first, I have a suggestion:\n\nI was thinking... Since we already store URLs in reverse-url form, they are ordered by host names. So instead of a bloom filter, we can write a hostdb that scans the host table for x rows starting from the requested host... This will probably make more sense with an example :)\n\nSo, let's say we have URLs from domains a.com, b.com, c.com, etc... So webtable keys will look like this:\n\ncom.a/....\ncom.a.www/....\ncom.a.www/....\ncom.b/...\ncom.b.pages/....\ncom.b.pages/...\n...\n....\n.....\netc...\n\nSo, if we ask for say, \"com.b\" from host table, then we can run a scan (say, for a hundred rows) starting from com.b then cache all results. During MapReduce, the next host we request will almost certainly be from com.b.* or com.{c,d,e} etc, and thus they will be cached. \n\nThe downside of this approach is quite obvious: Nutch will be reading a lot of hosts even if jobs do not need it (if you want to store host info for only 1 host per 100, this approach will read all hosts). Still, if we assume NutchContext  (and thus host) will exist for most URLs, this should not be a problem.\n\nWhat do you think?", "Last activity on this issue was more than a year ago. I'd like to get it rolling again.\nI suggest that I start with updating the patches to work with the latest nutchgora branch.  ", "Julien, did you make a start with \"I'll start working on some code to populate/update the hostDB from an existing webtable\"? If not, I'll start on that one too.", "nope, go ahead", "Status:\nI have updated the patches to match the current HEAD (nutchgora). Also added a HostDbUpdateJob which populates the host db from an existing web table (needed to fix an issue in GORA for this: https://issues.apache.org/jira/browse/GORA-105). \n\nI'm currently finishing some work on the NutchContext and will post the patch somewhere next week.\n\n", "Hi guys,\n\nI have second thoughts on implementing the NutchContext concept at this stage.\n\nAll Nutch processes are centered around the concept of a WebPage. And I agree, many of these processes and their plugins might benefit from additional input which is related to, but not directly part of a WebPage. Like host statistics, metadata or domain information.\n\nThe proposed NutchContext solution is elegant in the way that it makes this additional information available to plugins, in an extensible way. \nHowever, it indeed requires a big API break for plugins (since we don't use abstract base classes for all the plugins, we can't fix it there to keep them compatible).\n\nI'm afraid that a patch that tries to implement the Host table and the NutchContext at the same time, will have a hard time to make it to the repository ;)\n\nI propose to move the NutchContext approach to a new issue.\nPlugins and other components can still use Host information by using the HostDB class directly to perform efficient host lookups when needed. We can then decide later to make this part of the NutchContext.\n\nAgree?\n\n\n\n\n", "Mathijs, my opinion is that you have a clean sheet of paper to begin with certain aspects of this one (simply because you've stepped up to take it on). You obviously have you own idea about how you would like to see the new host table design and also have justification behind the eventual implementation (and API break/redesign) of NutchContext. I think it's wise to think sensibly about NOT breaking the plugin API at this stage and that an incremental approach to addressing this one is a suitable strategy. Feel free to open another issue for the NutchContext issue, as quite rightly this appears to have now morphed into it's own sub domain of the umbrella issue. ", "Hi, \n\nyou wrote, you have updated the patches for the Host table to pupulate it from the webpages.\n\nBut the files are from august and september. Where I can find your updated patch?", "Hey Patrick,\n\nWe are currently finishing the work for this issue. There is still one minor issue that is not fully working yet (namely host inlinks/outlinks are not populated), but we are still trying to make that work. If this is does not succeed in a few days, we will submit the patches anyhow.\n\nThanks for you interest.\n", "New version of patch. (On behalf of Mathijs I am finishing this issue. Nevertheless he has done much of the hard work!)\n\nBuilding hostdb links (inlinks and outlinks at the host level) works now too. Use:\norg.apache.nutch.host.HostDbUpdateJob -linkDb\n\nThis patch adds Host store definitions to the gora mapping for HBase only. (Other stores can be added easily later on). It needs GORA-105. So you can only use the added functionality when using a trunk version of Gora. Or wait until Nutchgora updates to Gora 0.2. (Should be soon).\n\nNo tests are included yet. For now this is okay, because by default this patch does not change existing functionality. (Also it's a bit of a pain to add tests because current tests depend on a valid SQLStore but updating Gora results in a dropped SQLStore so there an issue that needs to be solved first. In another issue that is).\n\nWill commit this in a few days.", "Committed. I realize that the current state is far from finished, however I figured it is enough to close this longstanding issue off. This makes room for people to easily play around with it and make improvements where necessary. (Adding definitions for other stores, new features such as storing stats etcetera.)\n\nI'll leave the final closing to Julien, since he is the original reporter.\n\nPlease let me know if any of you disagree.", "Ferdy I'll let you close it. I don't have time to give nutchgora a try and can't confirm that the patch does what it is supposed to. Thanks", "Ok.\n\nThanks to anyone who was involved.", "Integrated in Nutch-nutchgora #240 (See [https://builds.apache.org/job/Nutch-nutchgora/240/])\n    NUTCH-882 Design a Host table in GORA (Revision 1330728)\n\n     Result = SUCCESS\nferdy : \nFiles : \n* /nutch/branches/nutchgora/CHANGES.txt\n* /nutch/branches/nutchgora/build.xml\n* /nutch/branches/nutchgora/conf/gora-hbase-mapping.xml\n* /nutch/branches/nutchgora/default.properties\n* /nutch/branches/nutchgora/ivy/ivy.xml\n* /nutch/branches/nutchgora/src/gora/host.avsc\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/fetcher/FetcherReducer.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/host\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/host/HostDb.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/host/HostDbReader.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/host/HostDbUpdateJob.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/host/HostDbUpdateReducer.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/host/HostInjectorJob.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/indexer/IndexerReducer.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/storage/Host.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/storage/StorageUtils.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/storage/WebTableCreator.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/util/Histogram.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/util/TableUtil.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/util/domain/DomainStatistics.java\n"], "tasks": {"summary": "Design a Host table in GORA", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Design a Host table in GORA"}, {"question": "What is the main context?", "answer": "Having a separate GORA table for storing information about hosts (and domains?) would be very useful for : \n* customising the behaviour of the fetching on a host basis e.g. number of threads, min time"}]}}
{"issue_id": "NUTCH-883", "project": "NUTCH", "title": "Remove unused parameters from nutch-default.xml", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-08-11T12:22:12.067+0000", "updated": "2010-08-12T18:24:36.791+0000", "description": "The parameter file nutch-default.xml contains entries which are not used in 2.0 (e.g. configuration of the indexing with Lucene). I will submit a patch for this", "comments": ["Committed revision 984897.\n"], "tasks": {"summary": "Remove unused parameters from nutch-default.xml", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove unused parameters from nutch-default.xml"}, {"question": "What is the main context?", "answer": "The parameter file nutch-default.xml contains entries which are not used in 2.0 (e.g. configuration of the indexing with Lucene). I will submit a patch for this"}]}}
{"issue_id": "NUTCH-884", "project": "NUTCH", "title": "FetcherJob should run more reduce tasks than default", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-08-11T13:25:24.228+0000", "updated": "2013-05-22T03:53:22.301+0000", "description": "FetcherJob now performs fetching in the reduce phase. This means that in a typical Hadoop setup there will be many fewer reduce tasks than map tasks, and consequently the max. total throughput of Fetcher will be proportionally reduced. I propose that FetcherJob should set the number of reduce tasks to the number of map tasks. This way the fetching will be more granular.", "comments": ["Patch with the change. I also rearranged the arguments to FetcherJob.fetch(..) to make more sense (IMHO).", "+1. Maybe we can make the documentation on command line option a bit more verbose and say \"default number of map tasks\" instead of \"mapred.map.tasks\".\n\nIf I understood the code correctly, I think this part should be -all and not -threads:\n\n-    if (crawlId.equals(\"-threads\") || crawlId.equals(\"-resume\") || crawlId.equals(\"-parse\")) {\n+    if (!crawlId.equals(\"-threads\") || crawlId.startsWith(\"-\")) {\n\n\nFetcherJob.fetch makes much more sense with the rearranged arguments.", "Ok, I'll clarify the help message.\n\nbq. If I understood the code correctly, I think this part should be -all and not -threads:\n\nHeh, yes of course. Thanks!", "+1 good idea\n\n", "Corrected mistake in arg handling.", "Committed in rev. 986647."], "tasks": {"summary": "FetcherJob should run more reduce tasks than default", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "FetcherJob should run more reduce tasks than default"}, {"question": "What is the main context?", "answer": "FetcherJob now performs fetching in the reduce phase. This means that in a typical Hadoop setup there will be many fewer reduce tasks than map tasks, and consequently the max. total throughput of Fetc"}]}}
{"issue_id": "NUTCH-885", "project": "NUTCH", "title": "Error JAVA_HOME is Not Set", "status": "Closed", "priority": "Major", "reporter": "venkata hanuman choudari", "assignee": null, "created": "2010-08-12T06:32:04.632+0000", "updated": "2010-08-12T07:49:57.446+0000", "description": "After the Following the steps for Crawl the db in local system and run the ./start-all.sh i am getting following error\n./start-all.sh \nstarting namenode, logging to /var/www/nutch-0.9/bin/../logs/hadoop-chow-namenode-chow-desktop.out\nchow@localhost's password: \nlocalhost: starting datanode, logging to /var/www/nutch-0.9/bin/../logs/hadoop-chow-datanode-chow-desktop.out\nlocalhost: Error: JAVA_HOME is not set.\ncat: /var/www/nutch-0.9/bin/../conf/masters: No such file or directory\nstarting jobtracker, logging to /var/www/nutch-0.9/bin/../logs/hadoop-chow-jobtracker-chow-desktop.out\nchow@localhost's password: \nlocalhost: starting tasktracker, logging to /var/www/nutch-0.9/bin/../logs/hadoop-chow-tasktracker-chow-desktop.out\nlocalhost: Error: JAVA_HOME is not set.\n\n", "comments": ["Try setting JAVA_HOME in conf/hadoop-env.sh or in your environment."], "tasks": {"summary": "Error JAVA_HOME is Not Set", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Error JAVA_HOME is Not Set"}, {"question": "What is the main context?", "answer": "After the Following the steps for Crawl the db in local system and run the ./start-all.sh i am getting following error\n./start-all.sh \nstarting namenode, logging to /var/www/nutch-0.9/bin/../logs/hado"}]}}
{"issue_id": "NUTCH-886", "project": "NUTCH", "title": "A .gitignore file for Nutch", "status": "Closed", "priority": "Trivial", "reporter": "Dogacan Guney", "assignee": "Dogacan Guney", "created": "2010-08-12T07:52:10.740+0000", "updated": "2010-08-12T09:32:16.217+0000", "description": "We need a .gitignore file under nutch/ so git does not try to track many unnecessary files.", "comments": ["I think these should be enough:\n\nconf/*.txt\nconf/*.xml\nconf/hadoop-env.sh\nconf/slaves\nbuild/\nruntime/\nlogs/", "+1.", "Committed to trunk as of rev. 984703."], "tasks": {"summary": "A .gitignore file for Nutch", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "A .gitignore file for Nutch"}, {"question": "What is the main context?", "answer": "We need a .gitignore file under nutch/ so git does not try to track many unnecessary files."}]}}
{"issue_id": "NUTCH-887", "project": "NUTCH", "title": "Delegate parsing of feeds to Tika", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2010-08-14T08:30:59.689+0000", "updated": "2019-10-13T22:36:25.636+0000", "description": "[Starting a new thread from https://issues.apache.org/jira/browse/NUTCH-874]\n\nOne of the plugins which hasn't been ported yet is the feed parser. We could rely on the one we recently added to Tika, knowing that there is a substantial difference in the sense that the Tika feed parser generates a simple XHTML representation of the document where the feeds are simply represented as anchors whereas the Nutch version created new documents for each feed.\n\nThere is also the parse-rss plugin in Nutch which is quite similar - what's the difference with the feed one again? Since the Tika parser would handle all sorts of feed formats why not simply rely on it? \n\nAny thoughts on this?", "comments": ["Hey Julien:\n\n+1 to relying on Tika for RSS parsing. If there's something missing that Nutch needs, we'll add it to Tika and roll it into 0.8.\n\n{quote}\nThere is also the parse-rss plugin in Nutch which is quite similar - what's the difference with the feed one again? Since the Tika parser would handle all sorts of feed formats why not simply rely on it? \n{quote}\n\nI wrote parse-rss back in 2005, and used commons-feedparser from Kevin Burton and his crew. At the time it was well developed, and a little more flexible and easier for me to pick up than Rome. Since then however, its development has really become stagnant and it is no longer maintained.\n\nIn terms of real differences in terms of functionality, they are roughly equivalent so there isn't much difference. I would suggest we move forward with the feed plugin in Tika and roll it back in through Nutch.", "bq. If there's something missing that Nutch needs, we'll add it to Tika and roll it into 0.8.\n\nThere is something missing in Tika, and it's the support for compound documents, but it's not likely to be added in 0.8... not that we have such support in Nutch at the moment - it fell victim to the trunk/nutchbase switch, but it should be added back soon. I'd keep the \"feed\" plugin around for a while still, as an interim solution until Tika supports compound documents. +1 to getting rid of parse-rss.", "bq. There is something missing in Tika, and it's the support for compound documents, but it's not likely to be added in 0.8\n\nHuh, what do you mean? Nick just added a bunch of code to handle Compound document detection, and parsing, see TIKA-447 and the discussions on the wiki here: http://wiki.apache.org/tika/MetadataDiscussion. It may not be complete yet, but neither is 0.8. \n\nbq. I'd keep the \"feed\" plugin around for a while still, as an interim solution until Tika supports compound documents. +1 to getting rid of parse-rss.\n\n+1, I agree, but I still believe our goal should be to delegate this to Tika. I'm starting to feel the creep of parsing plugins make their way back into Nutch instead of just jumping over into Tika and working the process over there. In the end, if we start to add back all the parsing plugins, I'm not sure we've accomplished our goal...\n\n", "bq. Huh, what do you mean? Nick just added a bunch of code to handle Compound document detection, and parsing\n\nAh, good - I missed that, I need to take a closer look at this...\n\nbq. I'm starting to feel the creep of parsing plugins make their way back into Nutch instead of just jumping over into Tika\n\nThe \"creep\" so far is just parse-html, which we were forced to add back because Tika HTML parsing was totally inadequate to our needs. I know there have been some progress on this front, but I suspect it's still not sufficient. The ultimate goal is still to use Tika for all formats that it can handle, preferrably \"all formats\" without further qualifiers ;)", "bq. Ah, good - I missed that, I need to take a closer look at this...\n\nNp, let me know what you think. If it needs improvement, I'll be happy to pick up a shovel, and help out.\n\nbq. The \"creep\" so far is just parse-html, which we were forced to add back because Tika HTML parsing was totally inadequate to our needs. I know there have been some progress on this front, but I suspect it's still not sufficient. The ultimate goal is still to use Tika for all formats that it can handle, preferrably \"all formats\" without further qualifiers  ;)\n\nCoo coo, thanks Andrzej!\n\nCheers,\nChris\n", "Have created https://issues.apache.org/jira/browse/NUTCH-888 and will remove parse-rss tomorrow.\n\n", "Julien committed NUTCH-888 for 1.3 and trunk. I guess this issue can be closed?", "This issue is about parse-feeds and it requires some changes to the way Nutch 2.0 works (compound docs - see comments above). Let's leave it open for now", "Set and Classify", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "Delegate parsing of feeds to Tika", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Delegate parsing of feeds to Tika"}, {"question": "What is the main context?", "answer": "[Starting a new thread from https://issues.apache.org/jira/browse/NUTCH-874]\n\nOne of the plugins which hasn't been ported yet is the feed parser. We could rely on the one we recently added to Tika, kn"}]}}
{"issue_id": "NUTCH-888", "project": "NUTCH", "title": "Remove parse-rss", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-08-16T08:47:29.637+0000", "updated": "2011-06-29T04:01:01.864+0000", "description": "See https://issues.apache.org/jira/browse/NUTCH-887\n\n{quote}\nCM : I wrote parse-rss back in 2005, and used commons-feedparser from Kevin Burton and his crew. At the time it was well developed, and a little more flexible and easier for me to pick up than Rome. Since then however, its development has really become stagnant and it is no longer maintained.\n\nIn terms of real differences in terms of functionality, they are roughly equivalent so there isn't much difference.\n{quote}\n\nAlready +1 from Andrzej and Chris. Will remove it tomorrow if there aren't any objections in the meantime ", "comments": ["+1", "Let's wait a bit before we remove it. The feed parsing that I've recently added to Tika won't be available until Tika 0.8 is released and the feed parser has not been ported to the new API yet as it requires the support for compound documents in Nutch 2.0. We won't have anything to parse feeds if we remove parse-rss now.", "1.3 : Committed revision 1099483.\nStill need to do it for 2.0", "Trunk : committed revision 1099585", "Removed the parse-rss plugin and added test to parse-tika. All working fine on both 1.3 and 2.0", "Integrated in Nutch-trunk #1530 (See [https://builds.apache.org/job/Nutch-trunk/1530/])\n    "], "tasks": {"summary": "Remove parse-rss", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove parse-rss"}, {"question": "What is the main context?", "answer": "See https://issues.apache.org/jira/browse/NUTCH-887\n\n{quote}\nCM : I wrote parse-rss back in 2005, and used commons-feedparser from Kevin Burton and his crew. At the time it was well developed, and a l"}]}}
{"issue_id": "NUTCH-889", "project": "NUTCH", "title": "remove gora jars from lib dir", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-08-17T17:03:15.020+0000", "updated": "2010-08-18T09:05:07.225+0000", "description": "Gora does not yet have any published jars we could retrieve with Ivy, instead we currently need to install it and build it locally so that Nutch Ivy can resolve it.\nThe lib dir contains some gora jars which should be removed as they are not necessarily up to date.\n\nNote that depending on the Gora backend used for Nutch you might need to add some jars in the lib dir e.g. specific SQL drivers\n\nI will remove these jars to prevent any confusion\n\n", "comments": ["Committed revision 986601.\n"], "tasks": {"summary": "remove gora jars from lib dir", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "remove gora jars from lib dir"}, {"question": "What is the main context?", "answer": "Gora does not yet have any published jars we could retrieve with Ivy, instead we currently need to install it and build it locally so that Nutch Ivy can resolve it.\nThe lib dir contains some gora jars"}]}}
{"issue_id": "NUTCH-89", "project": "NUTCH", "title": "parse-rss null pointer exception", "status": "Closed", "priority": "Major", "reporter": "Michael Nebel", "assignee": null, "created": "2005-09-10T20:58:03.000+0000", "updated": "2005-09-24T01:16:09.000+0000", "description": "The rss-parser causes an exception. The reason is a syntax error in the page. Hitting this pages, the parser trys to add an outlink with \"null\" as anchor.  The anchor  of a outlink must no be null. \n\njava.lang.NullPointerException\n        at org.apache.nutch.io.UTF8.writeString(UTF8.java:236)\n        at org.apache.nutch.parse.Outlink.write(Outlink.java:51)\n        at org.apache.nutch.parse.ParseData.write(ParseData.java:111)\n        at org.apache.nutch.io.SequenceFile$Writer.append(SequenceFile.java:137)\n        at org.apache.nutch.io.MapFile$Writer.append(MapFile.java:127)\n        at org.apache.nutch.io.ArrayFile$Writer.append(ArrayFile.java:39)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.outputPage(Fetcher.java:281)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.handleFetch(Fetcher.java:261)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:148)\nException in thread \"main\" java.lang.RuntimeException: SEVERE error logged.  Exiting fetcher.\n        at org.apache.nutch.fetcher.Fetcher.run(Fetcher.java:354)\n        at org.apache.nutch.fetcher.Fetcher.main(Fetcher.java:488)\n        at org.apache.nutch.tools.CrawlTool.main(CrawlTool.java:140)\n\nI suggest the following patch:\n\nIndex: src/plugin/parse-rss/src/java/org/apache/nutch/parse/rss/RSSParser.java\n===================================================================\n--- src/plugin/parse-rss/src/java/org/apache/nutch/parse/rss/RSSParser.java     (revision 279397)\n+++ src/plugin/parse-rss/src/java/org/apache/nutch/parse/rss/RSSParser.java     (working copy)\n@@ -157,11 +157,13 @@\n                 if (r.getLink() != null) {\n                     try {\n                         // get the outlink\n-                        theOutlinks.add(new Outlink(r.getLink(), r\n-                                .getDescription()));\n+                       if (r.getDescription()!= null ) {\n+                           theOutlinks.add(new Outlink(r.getLink(), r.getDescription()));\n+                       } else {\n+                           theOutlinks.add(new Outlink(r.getLink(), \"\"));\n+                       }\n                     } catch (MalformedURLException e) {\n-                        LOG\n-                                .info(\"nutch:parse-rss:RSSParser Exception: MalformedURL: \"\n+                        LOG.info(\"nutch:parse-rss:RSSParser Exception: MalformedURL: \"\n                                         + r.getLink()\n                                         + \": Attempting to continue processing outlinks\");\n                         e.printStackTrace();\n@@ -185,12 +187,13 @@\n \n                     if (whichLink != null) {\n                         try {\n-                            theOutlinks.add(new Outlink(whichLink, theRSSItem\n-                                    .getDescription()));\n-\n+                           if (theRSSItem.getDescription()!=null) {\n+                               theOutlinks.add(new Outlink(whichLink, theRSSItem.getDescription()));\n+                           } else {\n+                               theOutlinks.add(new Outlink(whichLink, \"\"));\n+                           }\n                         } catch (MalformedURLException e) {\n-                            LOG\n-                                    .info(\"nutch:parse-rss:RSSParser Exception: MalformedURL: \"\n+                            LOG.info(\"nutch:parse-rss:RSSParser Exception: MalformedURL: \"\n                                             + whichLink\n                                             + \": Attempting to continue processing outlinks\");\n                             e.printStackTrace();\n", "comments": ["Hi Michael,\n\n  Thanks for creating an issue about this. I think that checking for null item and channel descriptions before indexing them is definitely a good thing :-) Could you create a patch file, attach it to this issue, and then request one of the commiters to commit the patch. You have my +1.\n\nCheers,\n  Chris\n", "Applied in trunk and 0.7 branch. Thanks."], "tasks": {"summary": "parse-rss null pointer exception", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "parse-rss null pointer exception"}, {"question": "What is the main context?", "answer": "The rss-parser causes an exception. The reason is a syntax error in the page. Hitting this pages, the parser trys to add an outlink with \"null\" as anchor.  The anchor  of a outlink must no be null. \n\n"}]}}
{"issue_id": "NUTCH-890", "project": "NUTCH", "title": "SqlStore doesn't work with nested types in Avro schema", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": null, "created": "2010-08-19T10:40:21.042+0000", "updated": "2013-05-22T03:53:26.563+0000", "description": "ParseStatus and ProtocolStatus are not properly serialized and stored when using SqlStore. This may indicate a broader issue in Gora with processing of nested types in Avro schemas.\n\nHBaseStore works properly, i.e. both types can be correctly stored and retrieved. SqlStore produces either NULL or '\\0\\0' value. This happens both when using HSQLDB and MySQL.", "comments": ["Fixed in Gora, see http://github.com/enis/gora/issues/issue/47"], "tasks": {"summary": "SqlStore doesn't work with nested types in Avro schema", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "SqlStore doesn't work with nested types in Avro schema"}, {"question": "What is the main context?", "answer": "ParseStatus and ProtocolStatus are not properly serialized and stored when using SqlStore. This may indicate a broader issue in Gora with processing of nested types in Avro schemas.\n\nHBaseStore works "}]}}
{"issue_id": "NUTCH-891", "project": "NUTCH", "title": "Nutch build should not depend on unversioned local deps", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": null, "created": "2010-08-19T10:54:29.805+0000", "updated": "2013-05-22T03:53:27.532+0000", "description": "The fix in NUTCH-873 introduces an unknown variable to the build process. Since local ivy artifacts are unversioned, different people that install Gora jars at different points in time will use the same artifact id but in fact the artifacts (jars) will differ because they will come from different revisions of Gora sources. Therefore Nutch builds based on the same svn rev. won't be repeatable across different environments.\n\nAs much as it pains the ivy purists ;) until Gora publishes versioned artifacts I'd like to revert the fix in NUTCH-873 and add again Gora jars built from a known external rev. We can add a README that contains commit id from Gora.", "comments": ["Can't we put Gora jars somewhere? I would like to put jars up somewhere, revision them with commit id (so they will look like gora-core-78ab312.jar), and make nutch depend on a gora version with a commit id... Would this be difficult to do with ivy?", "Hi Andrzej:\n\nCan I get some clarificatioin on this? First, local Ivy jars are versioned, by artifact id and by version #. So, we're talking about gora-0.1.jar here, right? So, your point is, if I'm off gitting and developing on Gora, at any point in time, I can run ant on gora and then it updates my local Ivy repo with a gora-0.1.jar file, right? And your point is, this file is different than the previous gora-0.1.jar file (-N minutes ago), and so thus, Nutch isn't really depending on a stable version, right?\n\nIf the above is true, what it suggests to me is that perhaps the process of installing Gora as a local Ivy dependency (independent of Nutch's deps) needs a bit more discipline. I'd say, why not make the Gora Ant build publish a gora-0.1-<some snapshot id aka SVN rev or UUID or whatever>.jar? In that fashion, you could develop on Gora, without fear of changing anything in the way that Nutch depends on it (because the 0.1 version that Nutch depends on could be frozen as is).\n\nI'd also be a fan if the above isn't true or doesn't make sense, of actually just uploading Gora to Maven central -- can we try that?\n\nCheers,\nChris\n\nP.S. I'm not trying to be difficult about NUTCH-873 b/c I was the one who did it. If in the end the consensus is to revert it, no egos here, go ahead. I'm just trying to figure out a solution to the problem that allows us to use Ivy as it should be and to not have to make exceptions. My other thought along these lines is that if we can't wrap our heads around Ivy, or getting to Maven Central in any short amount of time, then what about pulling Gora into Nutch SVN? It's ASLv2 licensed and there is nothing against doing this. From there, there would be a clean path to move to Incubation since the code would already be in Apache SVN anyways...", "bq. So, your point is [..]\n\nYes, that's exactly my point.\n\nbq. I'd say, why not make the Gora Ant build publish a gora-0.1-<some snapshot id aka SVN rev or UUID or whatever>.jar?\n\nSure, that would solve the problem for now - I'll bother the Gora devs, and you can create the patch, ok? :) Ultimately we should go with the other solution (publish to Maven), but it requires more involvement from Gora devs.\n\nbq. I'm not trying to be difficult about NUTCH-873 ...\n\nNeither am I, no egos here - I just find the current situation after the fix to be intractable, especially when doing bugfixing and testing - because even if APIs stay the same, hidden bugs may not be the same across revisions...", "bq. Sure, that would solve the problem for now - I'll bother the Gora devs, and you can create the patch, ok?  Ultimately we should go with the other solution (publish to Maven), but it requires more involvement from Gora devs.\n\nI like it! lol. Sure, I'll try and create a patch to make it do that. Installing Gora forced me to figure out how to use git the other day, so why not figure out how to patch Gora! ^_^\n\nbq. Neither am I, no egos here - I just find the current situation after the fix to be intractable, especially when doing bugfixing and testing - because even if APIs stay the same, hidden bugs may not be the same across revisions...\n\nI hear ya. OK let me think on this -- we definitely need a solution here. In the meanwhile I'll try and figure out how to patch Gora ant to make it version the jar on the Ivy install in a more meaningful way.", "Of course the best way that Nutch uses Gora is that Gora publishes it's artifacts to Maven and Nutch uses ivy to fetch the jars. But Gora is still in heavy development and we need some more time to make a first release. \n\nUntil then I think we can use the last commit sha1 in git for the revision number in git. We use this convention when uploading jars to guthub. Would that make sense? ", "Yes, this would help.", "Hey Guys, \n\nOK, i finally had time to do this. I went ahead and added a ${now} parameter to the gora jar file names. I could have done like a sha1 appended to the jar name, but kept running into a chicken and egg problem. The attached patch works great and just uses the same ${now} format used in the Ivy build-common.xml part. I don't know how to get this contributed to gora, so I'm attaching it here -- feel free to pull down into Gora.\n\nCheers,\nChris\n", "Nice patch, but I think changing the artifact name causes some problems in the other parts of the build (namely test and publish). I am attaching another patch for Gora which adds jar-snapshot and test-jar-snapshot targets to the top level build. \n\nNutch can use :\n $ ant test-jar-snapshot \n\nand copy the resulting jars at will. Is this acceptable ? \n\nBTW, gora uses github's issue tracker, you can also comment there. http://github.com/enis/gora/issues/issue/49", "+1. Great patch, Enis, I think we can use this. Are you going to apply it to Gora at Github?\n\nAlso thanks for the link to the issue tracker there!\n\nCheers,\nChris\n", "I have applied the patch to gora via http://github.com/enis/gora/issues/issue/49. \nAs part of this issue, I think we can replace unversioned gora jars with the snapshot versions. ", "Probably not an issue anymore. marking it as 2.x to triage unversioned issues, will check later", "Gora is now published to Maven Central and we have moved to Maven for builds over there as well."], "tasks": {"summary": "Nutch build should not depend on unversioned local deps", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Nutch build should not depend on unversioned local deps"}, {"question": "What is the main context?", "answer": "The fix in NUTCH-873 introduces an unknown variable to the build process. Since local ivy artifacts are unversioned, different people that install Gora jars at different points in time will use the sa"}]}}
{"issue_id": "NUTCH-892", "project": "NUTCH", "title": "nutch maven build support", "status": "Closed", "priority": "Minor", "reporter": "Marius Cristian Vulpe", "assignee": null, "created": "2010-08-19T17:16:30.340+0000", "updated": "2011-05-22T10:24:37.356+0000", "description": "I use nutch search mechanism form a standalone java application. I use maven to configure my dependencies and I have seen that nutch doesn't publish any artifacts to the public repositories.\nPlease let me know if somebody is working towards this direction.\nIf not, I think I can spent some time to \"mavenize\" the project and I can send you a version of that (I plan to do that for version 1.1).\n\nI would need feedback on this.", "comments": ["see https://issues.apache.org/jira/browse/NUTCH-825 for a discussion on publishing Nutch artifacts\nand https://issues.apache.org/jira/browse/NUTCH-821\n\nThere was some for of consensus that the publication would be done manually (we don't release very often). We have decided to use IVY starting with Nutch 2.0 so introducing Maven on top of it is definitely -1\n\n", "Julien, thanks for the quick response!\nI am looking forward for having the artifacts published to maven (Ivy is a good solution as well).\nI will close this one.", "I'm not sure how Ivy was better than Maven for nutch; Unlike Ivy, Maven is at least natively supported by most IDEs (NetBeans, IntelliJ,JDeveloper, m2eclipse). \nI at least recommend automating the 'mavenization' of Nutch, as is wonderfully done in Apache Solr (thanks Sarowe). In Solr dev-tools[1] there is a pom.xml template used by ant to mavenize the project. Mavenizing is done simply with (!):\n\nant get-maven-poms\nmvn -N -Pbootstrap install\n\nWith a fully-fledged NetBeans/IDE project one is able to learn alot about Solr/Lucene dynamics through the debugger (contribute patches and spam less the mlist) - it levels the playing field for newbies.\n\nI think Sarowe's dev-tools work could be ported to Nutch, but I'm not sure about Ivy in the equation; maybe makepom[2] is involved.\n\n\n\n[1] http://svn.apache.org/repos/asf/lucene/dev/branches/branch_3x/dev-tools/ ", "Cannot upload attachment to closed issue, here's my contribution to build.xml to at least allow installing nutch into the maven local repo (so that  nutch plugin maven developers can add nutch as dependency of their plugin):\n\n  <target name=\"publish-local-maven\" depends=\"release\">\n      <exec command=\"mvn install:install-file -DgroupId=${groupId} -DartifactId=${artifactId} -Dversion=1.0-SNAPSHOT -Dfile=${maven-jar} -Dpackaging=jar -DgeneratePom=true\"/>\n  </target>\n\nThis makes ant publish-local-maven execute after building the jar:\n$ mvn install:install-file -DgroupId=org.apache.nutch -DartifactId=nutch -Dversion=1.0-SNAPSHOT -Dfile=build/nutch-1.3.jar -Dpackaging=jar -DgeneratePom=true"], "tasks": {"summary": "nutch maven build support", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "nutch maven build support"}, {"question": "What is the main context?", "answer": "I use nutch search mechanism form a standalone java application. I use maven to configure my dependencies and I have seen that nutch doesn't publish any artifacts to the public repositories.\nPlease le"}]}}
{"issue_id": "NUTCH-893", "project": "NUTCH", "title": "DataStore.put() silently loses records when executed from multiple processes", "status": "Closed", "priority": "Blocker", "reporter": "Andrzej Bialecki", "assignee": null, "created": "2010-08-25T21:59:27.472+0000", "updated": "2010-09-13T14:18:50.596+0000", "description": "In order to debug the issue described in NUTCH-879 I created a test to simulate multiple clients appending to webtable (please see the patch), which is the situation that we have in distributed map-reduce jobs.\n\nThere are two tests there: one that uses multiple threads within the same JVM, and another that uses single thread in multiple JVMs. Each test first clears webtable (be careful!), and then puts a bunch of pages, and finally counts that all are present and their values correspond to keys. To make things more interesting each execution context (thread or process) closes and reopens its instance of DataStore a few times.\n\nThe multithreaded test passes just fine. However, the multi-process test fails with missing keys, as many as 30%.", "comments": ["Unit test to illustrate the issue.", "I'll go over this issue more carefully. But, in the meantime, did you try this test by adding DataStore#flush? Does it change anything?", "Marking as blocker and must be fixed for 2.0", "Dogacan, flush() doesn't help - there are still missing keys. What's interesting is that the missing keys form sequential ranges. Could this be perhaps an issue with connection management, or some synchronization issue?", "The code already calls close() so if flush() doesn't help, then yeah, this sounds like an issue with connection management or synchronization. I'll test what happens if we change SqlStore logic to not buffer statements at all, instead directly execute them.", "Dogacan and I spent a fair amount of time to figure out the problem with this test. We have checked and rechecked the code in gora-sql to make sure. However, the issue is that in TestGoraStorage#main(), setup() is called, which issues a deleteByQuery() to delete all the data in the store. When testMultiProcess() fires up lots of processes, some of the processes first start to write data (only some of them are committed), but the newly started ones just delete those newly written data. So this is a sync issue with the test itself. \n\nThe uploaded new patch passes the test. So I am afraid, we need to update the test to cover the issue in NUTCH-879. Andrzej, any suggestion for how to extend the test to reproduce NUTCH-879?\n\nIn the mean time, I will port this test to Gora as a part of http://github.com/enis/gora/issues#issue/50. Thanks for the excellent patch. \n  \n\n", "Very good catch - yes, the test now passes for me too. This is actually good news for Gora :) I'll continue digging regarding NUTCH-879 ... don't hesitate if you have any ideas how to solve that. I suspect we may be losing keys in Generator or Fetcher, due to partitioning collisions but this hypothesis needs to be tested.", "I want to close this one as INVALID and continue work on NUTCH-879 . Any objections?\n\n", "+1 and +1.", "Closing as INVALID. Work will continue on NUTCH-879 ."], "tasks": {"summary": "DataStore.put() silently loses records when executed from multiple processes", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "DataStore.put() silently loses records when executed from multiple processes"}, {"question": "What is the main context?", "answer": "In order to debug the issue described in NUTCH-879 I created a test to simulate multiple clients appending to webtable (please see the patch), which is the situation that we have in distributed map-re"}]}}
{"issue_id": "NUTCH-894", "project": "NUTCH", "title": "Move statistical language identification from indexing to parsing step", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Dogacan Guney", "created": "2010-08-27T08:46:39.852+0000", "updated": "2010-10-01T18:30:06.417+0000", "description": "The statistical identification of language is currently done part in the indexing step, whereas the detection based on HTTP header and HTML code is done during the parsing.\nWe could keep the same logic i.e. do the statistical detection only if nothing has been found with the previous methods but as part of the parsing. This would be useful for ParseFilters which need the language information or to use with ScoringFilters e.g. to focus the crawl on a set of languages.\n\nSince the statistical models have been ported to Tika we should probably rely on them instead of maintaining our own.\n\nAny thoughts on this?", "comments": ["I agree to merging language extraction into one plugin and delegating this work to tika where possible, I am putting together a patch to just do this. This is mainly a housekeeping patch where it merges the two models in the parsing step and modifies the unit tests. Since we now rely on tika for language identification, patch removes any identification code and its test cases along with the resources, so beware, that it looks like rather a big diff.\n\nPatch also introduces a new configuration option, lang.extraction.policy, to present users with an option to control the language extraction. So, the default action will stay the same, configured in the nutch-default.xml, the plugin will try to detect the language from headers and metadata, if this fails it will move on to use statistical identification. But, this way, users might be able to prefer one over another (only identification for instance).\n\nAny thoughts on the approach?", "+1 from me. \n\nIf there are no objections for the next couple days or so, I would like to commit this patch.", "+1, a nice clean up of our code base :)", "Nice one, that's exactly what I had in mind.\n+1 for commiting", "Committed as of rev. 1003608."], "tasks": {"summary": "Move statistical language identification from indexing to parsing step", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Move statistical language identification from indexing to parsing step"}, {"question": "What is the main context?", "answer": "The statistical identification of language is currently done part in the indexing step, whereas the detection based on HTTP header and HTML code is done during the parsing.\nWe could keep the same logi"}]}}
{"issue_id": "NUTCH-895", "project": "NUTCH", "title": "Urls with characters like [? = ] getting filtered out.", "status": "Closed", "priority": "Blocker", "reporter": "Jitendra", "assignee": null, "created": "2010-08-31T06:03:24.486+0000", "updated": "2010-09-01T08:09:19.415+0000", "description": "Hi,\n\nI am trying to write XpathBasedLinkExtractor which extracts links out of html page using xpaths.\nBut all the extracted links which contains characters like [? , = ] are being filtered out. I am not able to nail it down where it is happening. They are not going into segments.\nI have also commented out regular expression -[?*!@=] in regex-urlfilter.txt. Still It is showing same behaviour.\n\nCan any one give me idea about this. Where am I going wrong. I am stuck at this for last day.\n\nThanks\nJitendra", "comments": [], "tasks": {"summary": "Urls with characters like [? = ] getting filtered out.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Urls with characters like [? = ] getting filtered out."}, {"question": "What is the main context?", "answer": "Hi,\n\nI am trying to write XpathBasedLinkExtractor which extracts links out of html page using xpaths.\nBut all the extracted links which contains characters like [? , = ] are being filtered out. I am n"}]}}
{"issue_id": "NUTCH-896", "project": "NUTCH", "title": "Gora-based tests need to have their own config files ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-08-31T11:26:01.727+0000", "updated": "2012-05-03T15:30:45.367+0000", "description": "The tests extending AbstractNutchTest (Injector, Generator, Fetcher) have hard-coded properties for GORA. It would be better to be able to rely on a file gora.properties used only for the tests, just as we do with the nutch-*.xml config files (see CrawlTestUtil). This way we wouldn't use the configs set in the main /conf file as they could be specific to a given GORA backend e.g. Mysql vs hsqldb. This would also help running the tests with a non-default GORA backend. \n\nWe need to modify GORA and make the method DataStoreFactory.setProperties public. ", "comments": ["I am not quite sure why we choose to hide {{setProperties}} method in {{o.g.s.DataStoreFactory}} but instead of setting properties hard-coded in {{AbstractNutchTest}}, I guess we could one of the following; \n\n* We can place a different gora.properties file in src/test which includes these hard coded settings and let this one be used by test classes. This will require a slight change on GORA side as currently DataStoreFactory doesn't have a selection mechanism for the resource to read properties from (though, that will a minor change in GORA). The problem with this is that currently every subclass of {{AbstractNutchTest}} uses its own database by setting a different _jdbc.url_. Is there a specific reason why every subclass needs a different database? \n* We could create different properties file for each implementing test case and put these under src/test. This will again require the same change in GORA mentioned above. \n* OR, we can create a different configuration file containing these settings, and add this file to the {{Configuration}} object. At some point, we're again going to need to import these settings into DataStoreFactory possibly via changing the visibility of {{setProperties}} method. \n\nI am leaning towards the first but any comments on the track are welcome.", "This has taken some time to get round to but I am going to embark on the task of fixing o.a.n.storage.TestGoraStorage. We have some pretty nasty issues as described above, which need to be thought through before a fix is possible. I am also leaning towards the first option, having had a look at the tests after compiling it makes most sense atm. \nIs it possible for someone to explain why there are different jdbc.url's because I am getting the following\n{code}\nlewis@lewis-01:~/ASF/nutchgora/runtime/local$ bin/nutch junit org.apache.nutch.storage.TestGoraStorage\n.E.E\nTime: 0.931\nThere were 2 errors:\n1) testMultithread(org.apache.nutch.storage.TestGoraStorage)org.apache.gora.util.GoraException: java.io.IOException: java.sql.SQLTransientConnectionException: java.net.ConnectException: Connection refused\n\tat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:110)\n\tat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:118)\n\tat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:87)\n\tat org.apache.nutch.storage.StorageUtils.createDataStore(StorageUtils.java:43)\n\tat org.apache.nutch.storage.TestGoraStorage.setUp(TestGoraStorage.java:47)\nCaused by: java.io.IOException: java.sql.SQLTransientConnectionException: java.net.ConnectException: Connection refused\n\tat org.apache.gora.sql.store.SqlStore.getConnection(SqlStore.java:747)\n\tat org.apache.gora.sql.store.SqlStore.initialize(SqlStore.java:160)\n\tat org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:81)\n\tat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:104)\n\t... 14 more\n{code}\nwhich would suggest that there is a configuration error, that a service is not listening on a specific port? \nCan we simplify this any in an attempt to get this particular test working again.", "Set and classify", "Fixed with NUTCH-1205."], "tasks": {"summary": "Gora-based tests need to have their own config files ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Gora-based tests need to have their own config files "}, {"question": "What is the main context?", "answer": "The tests extending AbstractNutchTest (Injector, Generator, Fetcher) have hard-coded properties for GORA. It would be better to be able to rely on a file gora.properties used only for the tests, just "}]}}
{"issue_id": "NUTCH-897", "project": "NUTCH", "title": "Subcollection requires blacklist element", "status": "Closed", "priority": "Trivial", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2010-09-06T11:35:58.435+0000", "updated": "2011-05-08T22:34:48.890+0000", "description": "This is a very minor issue with in Subcollection.java. It throws an error if the (empty) blacklist element was omitted. I think it should either not silently fail in case of an omitted blacklist element or throw a decent error message that the blacklist element is required. The following exception gets thrown if the blacklist element is omitted in a subcollection block:\n\n2010-09-06 13:32:30,438 INFO  collection.CollectionManager - Instantiating CollectionManager                                                            \n2010-09-06 13:32:30,438 INFO  collection.CollectionManager - initializing CollectionManager                                                             \n2010-09-06 13:32:30,451 INFO  collection.CollectionManager - file has1 elements                                                                         \n2010-09-06 13:32:30,456 WARN  collection.CollectionManager - Error occured:java.lang.NullPointerException                                               \n2010-09-06 13:32:30,469 WARN  collection.CollectionManager - java.lang.NullPointerException                                                             \n2010-09-06 13:32:30,470 WARN  collection.CollectionManager - at org.apache.nutch.collection.Subcollection.initialize(Subcollection.java:173)            \n2010-09-06 13:32:30,470 WARN  collection.CollectionManager - at org.apache.nutch.collection.CollectionManager.parse(CollectionManager.java:98)          \n2010-09-06 13:32:30,470 WARN  collection.CollectionManager - at org.apache.nutch.collection.CollectionManager.init(CollectionManager.java:75)           \n2010-09-06 13:32:30,470 WARN  collection.CollectionManager - at org.apache.nutch.collection.CollectionManager.<init>(CollectionManager.java:56)         \n2010-09-06 13:32:30,471 WARN  collection.CollectionManager - at org.apache.nutch.collection.CollectionManager.getCollectionManager(CollectionManager.java:115)                                                                                                                                                  \n2010-09-06 13:32:30,471 WARN  collection.CollectionManager - at org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.addSubCollectionField(SubcollectionIndexingFilter.java:65)                                                                                                                   \n2010-09-06 13:32:30,471 WARN  collection.CollectionManager - at org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.filter(SubcollectionIndexingFilter.java:71)                                                                                                                                  \n2010-09-06 13:32:30,471 WARN  collection.CollectionManager - at org.apache.nutch.indexer.IndexingFilters.filter(IndexingFilters.java:109)               \n2010-09-06 13:32:30,471 WARN  collection.CollectionManager - at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:134)             \n2010-09-06 13:32:30,472 WARN  collection.CollectionManager - at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:50)              \n2010-09-06 13:32:30,472 WARN  collection.CollectionManager - at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:463)                  \n2010-09-06 13:32:30,472 WARN  collection.CollectionManager - at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)                            \n2010-09-06 13:32:30,472 WARN  collection.CollectionManager - at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216) ", "comments": ["Attached tested fix and if confirmed to work and not break existing configurations. Patch works for 1.3 and trunk.", "If there are no objections, i'll check this one in for 1.3 and trunk.", "Nitpick : What about calling *collection.getElementsByTagName(TAG_BLACKLIST)* only once?", "Yes, importing NodeList is less lazy. Updated in patch.", "Looks good to me", "Committed in trunk (rev. 1091389) and 1.3 (rev. 1091390).", "Committed in trunk (rev. 1091389) and 1.3 (rev. 1091390)."], "tasks": {"summary": "Subcollection requires blacklist element", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Subcollection requires blacklist element"}, {"question": "What is the main context?", "answer": "This is a very minor issue with in Subcollection.java. It throws an error if the (empty) blacklist element was omitted. I think it should either not silently fail in case of an omitted blacklist eleme"}]}}
{"issue_id": "NUTCH-898", "project": "NUTCH", "title": "Multi valued subcollection is not multi valued", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": null, "created": "2010-09-06T16:43:38.925+0000", "updated": "2010-09-07T11:15:37.513+0000", "description": "NUTCH-716 concatenates multiple values in a single string instead of adding single values to a multi valued field. For a test crawl i have defined the following two subcollection definitions:\n\n<subcollection>\n<name>asdf</name>\n<id>asdf-site</id>\n<whitelist>http://asdf/</whitelist>\n<blacklist/>\n</subcollection>\n\n<subcollection>\n<name>news</name>\n<id>asdf-news</id>\n<whitelist>http://asdf/news/</whitelist>\n<blacklist/>\n</subcollection>\n\nReindexing the segments by sending them to Solr will yield the following results for a news URL:\n\n<doc>\n<arr name=\"subcollection\">\n<str>asdf</str>\n</arr>\n<str name=\"url\">http://asdf/home/</str>\n</doc>\n<doc>\n<arr name=\"subcollection\">\n<str>asdf news</str>\n</arr>\n<str name=\"url\">http://asdf/news/</str>\n</doc>\n\nInstead, i expected the following result for the second document:\n\n<doc>\n<arr name=\"subcollection\">\n<str>asdf</str>\n<str>news</str>\n</arr>\n<str name=\"url\">http://asdf/news/</str>\n</doc>\n\nMy Solr schema.xml has the following declaration for the subcollection field:\n\n<field name=\"subcollection\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\" />\n", "comments": ["The old (only) nightly build i was using did allow multiple values but concatenated them. The current branch-1.2 already stored the values a multi valued field.\n\nIt was already fixed! "], "tasks": {"summary": "Multi valued subcollection is not multi valued", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Multi valued subcollection is not multi valued"}, {"question": "What is the main context?", "answer": "NUTCH-716 concatenates multiple values in a single string instead of adding single values to a multi valued field. For a test crawl i have defined the following two subcollection definitions:\n\n<subcol"}]}}
{"issue_id": "NUTCH-899", "project": "NUTCH", "title": "java.sql.BatchUpdateException: Data truncation: Data too long for column 'content' at row 1", "status": "Closed", "priority": "Minor", "reporter": "Faruk Berksöz", "assignee": null, "created": "2010-09-07T13:40:23.150+0000", "updated": "2010-12-18T19:30:40.855+0000", "description": "wenn i try to fetch a web page (e.g. http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html ) with mysql storage definition,\nI am seeing the following error in my hadoop logs. ,  (no error with hbase ) ;\n\njava.io.IOException: java.sql.BatchUpdateException: Data truncation: Data too long for column 'content' at row 1\n    at org.gora.sql.store.SqlStore.flush(SqlStore.java:316)\n    at org.gora.sql.store.SqlStore.close(SqlStore.java:163)\n    at org.gora.mapreduce.GoraOutputFormat$1.close(GoraOutputFormat.java:72)\n    at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:567)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\n\nThe type of the column 'content' is BLOB.\nIt may be important for the next developments of Gora.", "comments": ["You can either set a lower value for the parameter http.content.limit or modify the mapping and set\n\n<field name=\"content\" column=\"content\" jdbc-type=\"MEDIUMBLOB\"/>\n\nwhich should work for mysql.\n\nSee the discussion on http://github.com/enis/gora/issues/closed#issue/48", "Hi julien,\n\nboth suggestions that you wrote work fine.\n\nI close Issue.\n\nThanks", "I ran into the exact same issue, with MySQL. The blob column type can only store a string which length L is less than 2^16 = 65536 (not equal to) \nSee http://dev.mysql.com/doc/refman/5.0/en/storage-requirements.html\n\nI believe you just need to decrement http.content.limit from 65536 to 65535 in conf/nutch-default.xml...\n", "We stick with  the default gora schema for the MySQL backend, which says \"bytes\" in the Avro definition, that is translated into \"blob\" in MySQL. From src/gora/webpage.avsc;\n{\"name\": \"WebPage\",\n \"type\": \"record\",\n \"namespace\": \"org.apache.nutch.storage\",\n \"fields\": [\n        {\"name\": \"content\", \"type\": \"bytes\"},\n   ]\n}\n\n\nThere is potential bug in protocol-http. The http.content.limit value might be exceeded a little bit, hence the error saying that the value is too big for the MySQL blob column type, even tough we explicitly force http.content.limit to the 65535 max size.\n\nI tried to come up with a unit test for this, which is rather imperfect. Please see it in the attached patch. It changes http.content.limit from 65536 to 65535 when fetching a url which body content is big enough. The first test should see the error, the second should not.\n\nIdeally we want to generate the content with a local server for the unit test instead of using a random internet url. That remains to be implemented in the test."], "tasks": {"summary": "java.sql.BatchUpdateException: Data truncation: Data too long for column 'content' at row 1", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "java.sql.BatchUpdateException: Data truncation: Data too long for column 'content' at row 1"}, {"question": "What is the main context?", "answer": "wenn i try to fetch a web page (e.g. http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html ) with mysql storage definition,\nI am seeing the following error in my hadoop logs. ,  (no error with hbase "}]}}
{"issue_id": "NUTCH-9", "project": "NUTCH", "title": "JSP pages do not compile", "status": "Closed", "priority": "Major", "reporter": "Piotr Kosiorowski", "assignee": null, "created": "2005-03-13T01:19:39.000+0000", "updated": "2005-03-19T05:54:41.000+0000", "description": "In latest SVN version (but it was introduced in CVS I think) there is a simple error in two JSP pages.\nrefine-query-init.jsp :\nString urls = org.apache.nutch.util.NutchConf.get(\"extension.ontology.urls\");\nshould be:\nString urls = org.apache.nutch.util.NutchConf.get().get(\"extension.ontology.urls\");\n\nAnd in search.jsp:\n NutchConf.get().getInt(\"extension.clustering.hits-to-cluster\", 100);\nshould be:\n NutchConf.get().getInt(\"extension.clustering.hits-to-cluster\", 100); ", "comments": ["I believe this was fixed in r157456"], "tasks": {"summary": "JSP pages do not compile", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "JSP pages do not compile"}, {"question": "What is the main context?", "answer": "In latest SVN version (but it was introduced in CVS I think) there is a simple error in two JSP pages.\nrefine-query-init.jsp :\nString urls = org.apache.nutch.util.NutchConf.get(\"extension.ontology.url"}]}}
{"issue_id": "NUTCH-90", "project": "NUTCH", "title": "reduce logging output of IndexSegment", "status": "Closed", "priority": "Trivial", "reporter": "Michael Nebel", "assignee": null, "created": "2005-09-10T21:23:11.000+0000", "updated": "2007-04-18T15:46:16.372+0000", "description": "I think, LOG.fine would be enough :-) \n\nIndex: src/java/org/apache/nutch/indexer/IndexSegment.java\n===================================================================\n--- src/java/org/apache/nutch/indexer/IndexSegment.java (revision 279397)\n+++ src/java/org/apache/nutch/indexer/IndexSegment.java (working copy)\n@@ -142,10 +142,10 @@\n \n               // run filters to add more fields to the document\n               doc = IndexingFilters.filter(doc, parse, fetcherOutput);\n-    \n+     \n               // add the document to the index\n               NutchAnalyzer analyzer = AnalyzerFactory.get(doc.get(\"lang\"));\n-              LOG.info(\" Indexing [\" + doc.getField(\"url\").stringValue() + \"]\" +\n+              LOG.fine(\" Indexing [\" + doc.getField(\"url\").stringValue() + \"]\" + \n                        \" with analyzer \" + analyzer +\n                        \" (\" + doc.get(\"lang\") + \")\");\n               //LOG.info(\" Doc is \" + doc);\n", "comments": ["doesn't seem to apply anymore"], "tasks": {"summary": "reduce logging output of IndexSegment", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "reduce logging output of IndexSegment"}, {"question": "What is the main context?", "answer": "I think, LOG.fine would be enough :-) \n\nIndex: src/java/org/apache/nutch/indexer/IndexSegment.java\n===================================================================\n--- src/java/org/apache/nutch/ind"}]}}
{"issue_id": "NUTCH-900", "project": "NUTCH", "title": "Confusion in nutch-default between http.content.limit and file.content.limit", "status": "Closed", "priority": "Trivial", "reporter": "Markus Jelsma", "assignee": "Julien Nioche", "created": "2010-09-08T10:06:16.251+0000", "updated": "2010-10-27T10:39:25.050+0000", "description": "The http.content.limit and file.content.limit settings can be confusing and have fooled at least several users. The description element for these settings should be changed to reflect the difference between them so users won't be fooled that easy.\nSee also: http://lucene.472066.n3.nabble.com/ERROR-tika-TikaParser-org-apache-pdfbox-io-PushBackInputStream-td964353.html for a discussion.", "comments": ["To be fixed in the trunk as well", "Committed revision 994984 (trunk)\nCommitted revision 994985 (1.2)\n\nThanks!", "This patch is for branch-1.3 and fixes a typo in http.content.limit"], "tasks": {"summary": "Confusion in nutch-default between http.content.limit and file.content.limit", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Confusion in nutch-default between http.content.limit and file.content.limit"}, {"question": "What is the main context?", "answer": "The http.content.limit and file.content.limit settings can be confusing and have fooled at least several users. The description element for these settings should be changed to reflect the difference b"}]}}
{"issue_id": "NUTCH-901", "project": "NUTCH", "title": "Make index-more plug-in configurable", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": "Chris A. Mattmann", "created": "2010-09-08T10:42:20.940+0000", "updated": "2011-01-04T20:32:49.626+0000", "description": "In my case, i don't want the index-more plug-in to split content-types on slash. Tokenization is something a Solr instance should take care of. Instead of removing the code (which would break compatibility for users that rely on it), we need a way to configure the plug-in not to split the content-type.", "comments": ["Needs fixing in the trunk as well (v2.0)", "Hi Guys: I don't have time to put together a patch for this, and I haven't seen anything produced yet. Let's push this off to 2.0. If someone gets me a patch in the next day or so, I'll try and squeeze it in, but for now, I'm pushing to 2.0.", "Here's a patch for version 1.2 (that's the NUTCH-901-MarkusJelsma.998958.patch file). It includes a backward compatible setting in nutch-default.xml and handles the setting the the MoreIndexingFilter.java. It's tested and behaves as expected on my 1.2 up to date check out.", "Here's also a patch for 2.0 trunk. I could not test the code because i haven't managed to compile trunk as of yet.", "- fix for 1.2 as well (sigh, this means *another* RC). Oh well, for the greater good!", "- patch applied to trunk in r999181 and to branch-1.2 in r999200. Thanks so much Markus!\n\nOne nit: no unit tests. I've created one in the trunk (in r999203 and in r999204), and one in the branch-1.2 (in r999208).\n\nI won't be applying *any more* patches to the Nutch 1.2 RC. Let's get this thing VOTEd into release-dom with RC #4.", "Applied patch and added Mattmann's test to branch-1.3 ", "Markus - please remember to update CHANGES.txt when you commit a patch. Will add a comment about NUTCH-901 for branch-1.3 shortly.\nThanks!\n", "Thanks. Will remember next time.\n\n"], "tasks": {"summary": "Make index-more plug-in configurable", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Make index-more plug-in configurable"}, {"question": "What is the main context?", "answer": "In my case, i don't want the index-more plug-in to split content-types on slash. Tokenization is something a Solr instance should take care of. Instead of removing the code (which would break compatib"}]}}
{"issue_id": "NUTCH-902", "project": "NUTCH", "title": "Add all necessary files and configuration so that nutch can be used with different backends out-of-the-box", "status": "Closed", "priority": "Major", "reporter": "Enis Soztutar", "assignee": "Lewis John McGibbney", "created": "2010-09-08T13:38:18.118+0000", "updated": "2012-05-05T00:59:30.349+0000", "description": "As per the discussion in the mailing list and http://wiki.apache.org/nutch/GORA_HBase, it will be good to include all the necessary files and configuration. I propose that we maintain configuration for at least SQL, HBase and Cassandra. \n\nThe following changes are needed:\nconf/gora-sql-mapping.xml\nconf/gora-hbase-mapping.xml\nconf/gora-cassandra-mapping.xml\ncomments on nutch-default and ivy.xml \n\nShall we also include jars from gora-hbase, gora-cassandra and their dependencies ? ", "comments": ["Hi Enis, is it OK if I start work on this. Currently we support a gora-sql-mapping.xml and gora.properties file but I'm happy to begin working on your suggestions as above. Over time, I've already committed the comments and configuration properties to to nutch-default.xml therefore a patch would contain the remaining parts from above that you have highlighted. \n\nOne last thing, does the mapping document from the GORA_HBase tutorial suit as default options for gora-hbase-mapping?", "This is the beginning of a patch to address the ticket. It smartens up some files here and there, however as I've not been able to test recently on cassandra I don't know which additional dependencies are required to be added to ivy/ivy.xml (hector???).\n\nFinally, I've just used 'other' implementations from various resources for both cassandra and hbase xml mapping files. Obviously this is up for debate so please comment.", "By all means Lewis. \n\nPatch looks good, I have just two comments: \n - Why do we change the project name to be nutchgora in build.xml?\n - Can you add some comments at nutch-default.xml for property \"storage.data.store.class\". We already have values for HBase and Cassandra, but I think if we can add a brief comment there, this would be great. ", "Hi Enis,\n\n1) This was to disambiguate nutch build files within my Eclipse IDE. Both 1.4 trunk and Nutchgora branch are both called Nutch. This adds more overhead to the cleaning, testing, building etc from within the dev environment.\n2) Yes this is correct. I will substantiate the annotations, and will also determine whether or not we need some additional dependency targets when I fire this into a Cassandra instance. Thanks for commenting.", "Revised patch to incorporate additional comments.", "Committed @ revision 1195403 in Nutchgora branch.\n\nI would ask if Enis could now do a final check and now close. Thank you for the pointers.\n", "Patch looks good, but can you please test with cassandra. Theoretically, gora-cassandra should contain all dependencies itself, so I don't think we need to add other dependencies there. ", "Reopened as Cassandra configurations in ivy/ivy.xml are not complete.", "There are some slight problems here, I was getting a problem with unresolved dependencies when I was building Nutchgora with Cassandra as backend, so you need to add the following to ivy/ivy.xml\n{code}\n<!--\n    \tUncomment this to use Cassandra as Gora backend. \n-->\n\n\t\t<dependency org=\"org.apache.gora\" name=\"gora-cassandra\" rev=\"0.1.1-incubating\" conf=\"*->default\">\n\t\t\t<exclude org=\"org.apache.thrift\" />\n\t\t\t<exclude org=\"org.apache.cassandra\" />\n\t\t</dependency>\n{code}\n\nEnis, I completely agree with your comment, that every dependency should be managed from within Gora, however if the changes are not in the above gora dependency on maven repo then we cannot use them, therefore the exclusions need to be added in Nutchgora_home/ivy/ivy.xml prior to using the ant runtime target. I will attach a patch for the following in due course.", "patch to include previous config changes to NUTCHGORA/ivy/ivy.xml", "Integrated in Nutch-nutchgora #55 (See [https://builds.apache.org/job/Nutch-nutchgora/55/])\n    commit to address NUTCH-902 and update to changes.txt\n\nlewismc : http://svn.apache.org/viewvc/nutch/branches/nutchgora/viewvc/?view=rev&root=&revision=1195403\nFiles : \n* /nutch/branches/nutchgora/CHANGES.txt\n* /nutch/branches/nutchgora/build.xml\n* /nutch/branches/nutchgora/conf/gora-cassandra-mapping.xml\n* /nutch/branches/nutchgora/conf/gora-hbase-mapping.xml\n* /nutch/branches/nutchgora/conf/gora-sql-mapping.xml\n* /nutch/branches/nutchgora/conf/nutch-default.xml\n* /nutch/branches/nutchgora/ivy/ivy.xml\n", "Integrated in Nutch-nutchgora-ant #9 (See [https://builds.apache.org/job/Nutch-nutchgora-ant/9/])\n    commit to address NUTCH-902 and update to changes.txt\n\nlewismc : http://svn.apache.org/viewvc/nutch/branches/nutchgora/viewvc/?view=rev&root=&revision=1195403\nFiles : \n* /nutch/branches/nutchgora/CHANGES.txt\n* /nutch/branches/nutchgora/build.xml\n* /nutch/branches/nutchgora/conf/gora-cassandra-mapping.xml\n* /nutch/branches/nutchgora/conf/gora-hbase-mapping.xml\n* /nutch/branches/nutchgora/conf/gora-sql-mapping.xml\n* /nutch/branches/nutchgora/conf/nutch-default.xml\n* /nutch/branches/nutchgora/ivy/ivy.xml\n", "Note I changed gora-hbase-mapping.xml slightly: I added maxVersions=1 for each column family, since currently the HBaseStore for Gora is not prepared at all for multiple versions. It makes no sense to have it set to more than 1, for now.", "Integrated in Nutch-nutchgora #180 (See [https://builds.apache.org/job/Nutch-nutchgora/180/])\n    NUTCH-902 subcommit: change maxVersions to 1 of families in gora-hbase-mapping.xml (Revision 1295613)\n\n     Result = SUCCESS\nferdy : \nFiles : \n* /nutch/branches/nutchgora/conf/gora-hbase-mapping.xml\n", "I just committed a minor change to the sql mapping. (The content field should have the length that is the default max content in nutch-default.xml, namely 65536. Tested this and it works.", "Integrated in Nutch-nutchgora #185 (See [https://builds.apache.org/job/Nutch-nutchgora/185/])\n    NUTCH-902 Add all necessary files and configuration so that nutch can be used with different backends out-of-the-box (Revision 1297401)\n\n     Result = SUCCESS\nferdy : \nFiles : \n* /nutch/branches/nutchgora/conf/gora-sql-mapping.xml\n", "Committed change to the gora-hbase line in ivy: use 'default' dependancy instead of 'compile'. This way the gora-hbase module is actually placed in the lib target folder.", "Just made a second commit regarding gora-hbase: \n-Removed the comment that the hbase jar should be put in the lib folder manually as this is not needed anymore.\n-Removed the explicit zookeeper dependency, it is already included transitively: gora-hbase --> hbase --> zookeeper.\n-Exclude hsqldb because this is already explicitely included elsewhere in the ivy file.\n", "I made some commits on this to in include the memory store, AvroStore, DataFileAvroStore and Accumulo properties to nutch-site and some rough properties to gora.properties. I'm not clued up on the Accumulo mappings and we have no mappings for *AvroStore implementations therefore this one really should stay open. This being said I do however feel that what is currently committed in Nutchgora is enough for anyone to work with. wdygt?", "I think nutch-default.xml does not correctly use the description field of the \"storage.data.store.class\" property. The description should describe what the property is about, not what the value is about. So instead of the various entries:\n\n<property>\n  <name>storage.data.store.class</name>\n  <value>org.apache.gora.cassandra.store.CassandraStore</value>\n  <description>Gora class for storing data in Apache Cassandra</description>\n</property>\n-->\n\n<!--\n<property>\n  <name>storage.data.store.class</name>\n  <value>org.apache.gora.hbase.store.HBaseStore</value>\n  <description>Gora class for storing data in Apache HBase</description>\n</property>\n-->\n\nso on..\n\nI propose to add a single property entry with the following description like this:\n\n<property>\n  <name>storage.data.store.class</name>\n  <value>org.apache.gora.sql.store.SqlStore</value>\n  <description>The Gora DataStore class for storing/retrieving data.\n    Currently the following stores are available:\n\n    org.apache.gora.sql.store.SqlStore\n      A DataStore implementation for RDBMS with a SQL interface.\n      SqlStore uses JDBC drivers to communicate with the DB.\n\n    org.apache.gora.hbase.store.HBaseStore\n      DataStore implementation for Hadoop HBase.\n\n    etcetera\n\n  </description>\n</property>\n\nThis has the additional benefit to make the nutch-default.xml look cleaner, imho.", "Yeah +1.\nIs there anything else you find we require from Enis initial comments on this issue?", "Alright I'll change and commit the \"storage.data.store.class\" property description.\n\nAside from that I think we can close this issue. Effort can be put into NUTCH-1205 and after that actual testing of the stores to see if the current configuration is sufficient for out-of-the-box usage. If this is not the case for some stores, we can always create new issues for thosde. (To prevent too much clutter in this issue).", "Ok done. (Note that I did not actually check the stores, I simply merged the nutch-default.xml entries)", "Integrated in Nutch-nutchgora #240 (See [https://builds.apache.org/job/Nutch-nutchgora/240/])\n    NUTCH-902 (merge different \"storage.data.store.class\" entries into one) (Revision 1330807)\n\n     Result = SUCCESS\nferdy : \nFiles : \n* /nutch/branches/nutchgora/conf/nutch-default.xml\n", "Hey Enis. When you get a min can you please check and close. We are getting ever closer to a 2.0 RC here :0)\nThank you, great work Ferdy.", "Nice work guys. I'm closing the issue per discussion. It seems we have everything commented out and ready to be set free :) \nOne suggestion would be to add some documentation in the wiki or site showing how to use nutchgora with other stores, if we don't have it already. "], "tasks": {"summary": "Add all necessary files and configuration so that nutch can be used with different backends out-of-the-box", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add all necessary files and configuration so that nutch can be used with different backends out-of-the-box"}, {"question": "What is the main context?", "answer": "As per the discussion in the mailing list and http://wiki.apache.org/nutch/GORA_HBase, it will be good to include all the necessary files and configuration. I propose that we maintain configuration fo"}]}}
{"issue_id": "NUTCH-903", "project": "NUTCH", "title": "RESUME_KEY field in FetcherJob.Java has not been get correctly", "status": "Closed", "priority": "Minor", "reporter": "Faruk Berksöz", "assignee": null, "created": "2010-09-08T14:45:41.104+0000", "updated": "2010-09-08T15:01:51.724+0000", "description": "Source modification request for nutch 2.0 .\n\nFetcherJob.Java\n\t...\n\tFetcherMapper\n\t....\n\t\tprotected void setup(Context context) {\n\t\t\t  Configuration conf = context.getConfiguration();\n\t\t\t  shouldContinue = conf.getBoolean(\"job.continue\", false);\n\t\t>>>\t  \n\t\t>>>  job.continue has not beeen set anywhere\n\t\t>>> \"job.continue\" should be RESUME_KEY which is set before for this purpose\n\t\t>>>\t\t\t  \n\t\t\t  crawlId = new Utf8(conf.get(GeneratorJob.CRAWL_ID, Nutch.ALL_CRAWL_ID_STR));\n\t\t\t}\n\t...\n\n", "comments": ["I'm so sorry... \nDescription is not readable.Why i don't know.I close this one and open new.\n"], "tasks": {"summary": "RESUME_KEY field in FetcherJob.Java has not been get correctly", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "RESUME_KEY field in FetcherJob.Java has not been get correctly"}, {"question": "What is the main context?", "answer": "Source modification request for nutch 2.0 .\n\nFetcherJob.Java\n\t...\n\tFetcherMapper\n\t....\n\t\tprotected void setup(Context context) {\n\t\t\t  Configuration conf = context.getConfiguration();\n\t\t\t  shouldContin"}]}}
{"issue_id": "NUTCH-904", "project": "NUTCH", "title": "\"-resume\" option is always processed  as \"false\" in FetcherJob.", "status": "Closed", "priority": "Major", "reporter": "Faruk Berksöz", "assignee": null, "created": "2010-09-08T15:52:59.118+0000", "updated": "2010-09-14T13:45:10.101+0000", "description": "job.continue has not beeen set anywhere.\n\"job.continue\" should be RESUME_KEY which is set before for this purpose.\n\\\\\n\\\\\n{code:title=FetcherJob.java|borderStyle=solid}\n   ...\n   FetcherMapper\n   ....\n      protected void setup(Context context) {\n         Configuration conf = context.getConfiguration();\n         shouldContinue = conf.getBoolean(\"job.continue\", false);       \n         crawlId = new Utf8(conf.get(GeneratorJob.CRAWL_ID, Nutch.ALL_CRAWL_ID_STR));\n      }\n\t...\n {code} ", "comments": ["patch", "Fixed as of rev. 996898.\n\nThanks for the patch."], "tasks": {"summary": "\"-resume\" option is always processed  as \"false\" in FetcherJob.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "\"-resume\" option is always processed  as \"false\" in FetcherJob."}, {"question": "What is the main context?", "answer": "job.continue has not beeen set anywhere.\n\"job.continue\" should be RESUME_KEY which is set before for this purpose.\n\\\\\n\\\\\n{code:title=FetcherJob.java|borderStyle=solid}\n   ...\n   FetcherMapper\n   ....\n"}]}}
{"issue_id": "NUTCH-905", "project": "NUTCH", "title": "Configurable file protocol parent directory crawling", "status": "Closed", "priority": "Major", "reporter": "Chris A. Mattmann", "assignee": "Chris A. Mattmann", "created": "2010-09-11T01:57:06.247+0000", "updated": "2013-05-22T03:53:18.828+0000", "description": "See the discussion on NUTCH-407: apply the patch and backport to 1.2 and port to 2.0.", "comments": ["- patch for NUTCH-407 applied to 2.0 trunk in r996045, and in branch-1.2 in r996059. Thanks to Thorsten, and Andrzej and to the others who worked/commented on the patch!"], "tasks": {"summary": "Configurable file protocol parent directory crawling", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Configurable file protocol parent directory crawling"}, {"question": "What is the main context?", "answer": "See the discussion on NUTCH-407: apply the patch and backport to 1.2 and port to 2.0."}]}}
{"issue_id": "NUTCH-906", "project": "NUTCH", "title": "Nutch OpenSearch sometimes raises DOMExceptions due to Lucene column names not being valid XML tag names", "status": "Closed", "priority": "Major", "reporter": "Asheesh Laroia", "assignee": "Andrzej Bialecki", "created": "2010-09-13T19:13:47.227+0000", "updated": "2013-05-22T03:53:21.735+0000", "description": "The Nutch FAQ explains that OpenSearch includes \"all fields that are available at search result time.\" However, some Lucene column names can start with numbers. Valid XML tags cannot. If Nutch is generating OpenSearch results for a document with a Lucene document column whose name starts with numbers, the underlying Xerces library throws this exception: \n\norg.w3c.dom.DOMException: INVALID_CHARACTER_ERR: An invalid or illegal XML character is specified. \n\nSo I have written a patch that tests strings before they are used to generate tags within OpenSearch.\n\nI hope you merge this, or a better version of the patch!", "comments": ["Patch, including a test", "Fixed in rev. 998261. Thanks!"], "tasks": {"summary": "Nutch OpenSearch sometimes raises DOMExceptions due to Lucene column names not being valid XML tag names", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Nutch OpenSearch sometimes raises DOMExceptions due to Lucene column names not being valid XML tag names"}, {"question": "What is the main context?", "answer": "The Nutch FAQ explains that OpenSearch includes \"all fields that are available at search result time.\" However, some Lucene column names can start with numbers. Valid XML tags cannot. If Nutch is gene"}]}}
{"issue_id": "NUTCH-907", "project": "NUTCH", "title": "DataStore API doesn't support multiple storage areas for multiple disjoint crawls", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-09-15T15:00:24.195+0000", "updated": "2013-05-22T03:53:34.523+0000", "description": "In Nutch 1.x it was possible to easily select a set of crawl data (crawldb, page data, linkdb, etc) by specifying a path where the data was stored. This enabled users to run several disjoint crawls with different configs, but still using the same storage medium, just under different paths.\n\nThis is not possible now because there is a 1:1 mapping between a specific DataStore instance and a set of crawl data.\n\nIn order to support this functionality the Gora API should be extended so that it can create stores (and data tables in the underlying storage) that use arbitrary prefixes to identify the particular crawl dataset. Then the Nutch API should be extended to allow passing this \"crawlId\" value to select one of possibly many existing crawl datasets.", "comments": ["Gora already supports this somewhat. While creating a data store, you can optionally specify a table name:\n\n  public static <D extends DataStore<K,T>, K, T extends Persistent>\n  D createDataStore(Class<D> dataStoreClass\n      , Class<K> keyClass, Class<T> persistent, String schemaName)\n\nWe should be able to leverage that in Nutch to support different crawl datasets. If we extend Nutch's current API to allow names to be specified for crawls then Nutch can simply create tables prefixed with crawl names as Andrzej suggested. For example, a crawl dataset with name \"foo\" will have a table called \"foo_webtable\".\n\nWhat do you think Andrzej? I think Gora needs no extension here but if people think API is awkward we can change Gora too.", "That's very good news - in that case I'm fine with the Gora API as it is now, we should change Nutch to make use of this functionality.", "Here's a patch to allow Nutch to create different schemas to based on the same schema definition. Some points about the patch;\n\n* To be able to prefix a schema name with a value, Nutch needs to know the default schema name defined in the gora mapping file (e.g ...table=<name>...). Gora handles creation internally at the moment and it doesn't expose this name to outside. So, the patch introduces two new configuration options to pass the schema name to Nutch internals.\n** Nutch *ignores* the schema name setting in gora mapping file, instead, configuration option {{storage.schema}} will tell the Nutch which schema name it should use to access to data store. This value is defaulted to _webpage_.\n** {{storage.schema.id}} option defines the prefix to add to schema name in {{storage.schema}}, and by default this id is not provided, i.e. all jobs will run on _webpage_ store as before.\n* Apart from giving it as a configuration option, all jobs (injector, generator, fetcher, updatedb, indexer, benchmark and webtable reader) are modified to accept a schema id as an optional command line argument, {{-schemaId}}, which will override the configuration option ({{-schemaId}} may seem an odd name but I am not big on naming things).\n* Patch also modifies unit tests to use the same logic.\n\nAll unit tests pass without a problem and I have run a simple crawl with a)default configuration, b)by providing a schema id from configuration and c)giving the ids from command line and jobs seem to run well.", "Hi Sertan,\n\nThanks for the patch, this looks very good! A few  comments:\n\n* I'm not good at naming things either... schemaId is a little bit cryptic though. If we didn't already use crawlId I would vote for that (and then rename crawlId to batchId or fetchId), as it is now... I dont know, maybe datasetId ..\n\n* since we now create multiple datasets, we need somehow to manage them - i.e. list and delete at least (create is implicit). There is no such functionality in this patch, but this can be addressed also as a separate issue.\n\n* IndexerMapReduce.createIndexJob: I think it would be useful to pass the \"datasetId\" as a Job property - this way indexing filter plugins can use this property to populate NutchDocument fields if needed. FWIW, this may be a good idea to do in other jobs as well...", "Hi Andrzej,\n\nThanks for the review and the feedback.\n\n* Funny thing, I was actually going for {{datasetId}} for the name, but now that you mention, I prefer to use {{crawlId}} for this and rename the old {{crawlId}} to {{batchId}}. I am not entirely sure how much invasive that's going to be, but I don't think it will be much of a hassle to change both all at once.\n* I agree that arguments should override the configuration by actually setting it so that the setting could be accessible elsewhere. I'll modify the patch to work this way.\n* A utility to handle the datasets is a good idea, though, considering the current GORA architecture I think we may need to add a client interface there somewhere. I've opened up an [issue|http://github.com/enis/gora/issues/issue/56] for this, we can start thinking about the design there. We won't be able write a generic utility in Nutch, though, since this won't be available till we roll out a new version of Gora. I'll pitch in the utility once we have that but as that doesn't affect this issue directly, I'd rather go for a separate issue for that. And until that issue is solved, I think it would be safe to leave manipulation of stores (listing, removing, truncation.. etc) to user's responsibility.\n\nI'll modify the patch to reflect those two changes.", "Here's the modified version of the patch after Andrzej's review. The additional points to the original patch are as follows;\n\n* The old {{crawlId}} option is renamed to {{batchId}} for convenience.\n\n* All jobs now accept an optional argument, {{-crawlId <id>}}, to prefix the schema. Jobs now keep this property in the configuration allowing later use by, say, plugins.\n\nAll unit tests pass and again I have run a simple crawl w/o any problems. I have also tested the {{batchId}} option by generating two different sets of the injected urls and run a fetch-parse cycle on those sets. Jobs seem to recognize the correct {{batchId}} and select only the corresponding urls.\n\nLike I said before, I prefer to leave store manipulation utility out of this patch, and handle it in a separate issue once we have that functionality in Gora. What do you think?\n\n", "Committed in rev. 1025963. Thank you Sertan for a high-quality patch and unit tests!", "Thanks Andrzej, I've been waiting for this; I have a couple of use cases just for the functionality."], "tasks": {"summary": "DataStore API doesn't support multiple storage areas for multiple disjoint crawls", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "DataStore API doesn't support multiple storage areas for multiple disjoint crawls"}, {"question": "What is the main context?", "answer": "In Nutch 1.x it was possible to easily select a set of crawl data (crawldb, page data, linkdb, etc) by specifying a path where the data was stored. This enabled users to run several disjoint crawls wi"}]}}
{"issue_id": "NUTCH-908", "project": "NUTCH", "title": "Infinite Loop and Null Pointer Bugs in Searching", "status": "Closed", "priority": "Major", "reporter": "Dennis Kubes", "assignee": "Chris A. Mattmann", "created": "2010-09-16T20:10:18.183+0000", "updated": "2013-05-22T03:53:18.337+0000", "description": "It is possible for the NutchBean to drop into an infinite loop while trying to optimize a query to re-search for more results.  There are also two Null Pointer bugs in the search process.  One in NutchBean where there was an incorrect loop assignment and a second in DistributedSegementsBean when a segment is null (shouldn't happen but still should be handled.)  A patch is available for both.", "comments": ["Fixes infinite loop and null pointer bugs.", "Hey Dennis: OK, thanks. I'll backport this to 1.2 and roll another RC this weekend. If possible, if Nutch PMC'ers could check it out next week I'd love to get this release out the door and focus on 2.0! :)", "- applied to branch-1.2 in r998587. Thanks Dennis! I'll roll a new RC 1.2 tomorrow in time for VOTEing."], "tasks": {"summary": "Infinite Loop and Null Pointer Bugs in Searching", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Infinite Loop and Null Pointer Bugs in Searching"}, {"question": "What is the main context?", "answer": "It is possible for the NutchBean to drop into an infinite loop while trying to optimize a query to re-search for more results.  There are also two Null Pointer bugs in the search process.  One in Nutc"}]}}
{"issue_id": "NUTCH-909", "project": "NUTCH", "title": "Add alternative search-provider to Nutch site", "status": "Closed", "priority": "Minor", "reporter": "Alex Baranau", "assignee": "Chris A. Mattmann", "created": "2010-09-20T13:56:23.624+0000", "updated": "2013-05-22T03:54:49.682+0000", "description": "Add additional search provider (to existed Lucid Find) search-lucene.com. \n\nInitiated in discussion: http://search-lucene.com/m/2suCr1UnDfF1\n\nAccording to Andrzej's suggestion, \"when preparing the patch let's follow the same rationales as those in TIKA-488, since they are applicable here too\", so please refer to that issue for more insight on implementation details.", "comments": ["One thing I noted during working on the patch. The text in search box says \"Search the site with Solr\". While both providers offer search in more resources than just site. It might be better to see the message \"Search with Apache Solr\" (as on the TIKA's site).\n\nThoughts?", "bq. It might be better to see the message \"Search with Apache Solr\" (as on the TIKA's site).\n\nYes, let's make this uniform.", "- set fix version", "I did this on Tika's site, and would be happy to shepherd it over here. I'll review the patch and get back to you. Thanks Alex.", "Btw. I'll be presenting what's behind this service at the upcoming Lucene Revolution conference, so it would be great to see the new search up on nutch.apache.org.  Sorry to nag.", "- Patch applied in r1001571. Thanks Alex! Otis: enjoy your presentation."], "tasks": {"summary": "Add alternative search-provider to Nutch site", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add alternative search-provider to Nutch site"}, {"question": "What is the main context?", "answer": "Add additional search provider (to existed Lucid Find) search-lucene.com. \n\nInitiated in discussion: http://search-lucene.com/m/2suCr1UnDfF1\n\nAccording to Andrzej's suggestion, \"when preparing the pat"}]}}
{"issue_id": "NUTCH-91", "project": "NUTCH", "title": "empty encoding causes exception", "status": "Closed", "priority": "Major", "reporter": "Michael Nebel", "assignee": null, "created": "2005-09-10T21:35:48.000+0000", "updated": "2006-03-10T05:17:50.000+0000", "description": "I found some sites, where the header says:  \"Content-Type: text/html; charset=\". This causes an exception in the HtmlParser. My suggestion:\n\nIndex: src/plugin/parse-html/src/java/org/apache/nutch/parse/html/HtmlParser.java\n===================================================================\n--- src/plugin/parse-html/src/java/org/apache/nutch/parse/html/HtmlParser.java  (revision 279397)\n+++ src/plugin/parse-html/src/java/org/apache/nutch/parse/html/HtmlParser.java  (working copy)\n@@ -120,7 +120,7 @@\n       byte[] contentInOctets = content.getContent();\n       InputSource input = new InputSource(new ByteArrayInputStream(contentInOctets));\n       String encoding = StringUtil.parseCharacterEncoding(contentType);\n-      if (encoding!=null) {\n+      if (encoding!=null && !\"\".equals(encoding)) {\n         metadata.put(\"OriginalCharEncoding\", encoding);\n         if ((encoding = StringUtil.resolveEncodingAlias(encoding)) != null) {\n           metadata.put(\"CharEncodingForConversion\", encoding);\n", "comments": ["Commited with small extension. Thanks."], "tasks": {"summary": "empty encoding causes exception", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "empty encoding causes exception"}, {"question": "What is the main context?", "answer": "I found some sites, where the header says:  \"Content-Type: text/html; charset=\". This causes an exception in the HtmlParser. My suggestion:\n\nIndex: src/plugin/parse-html/src/java/org/apache/nutch/pars"}]}}
{"issue_id": "NUTCH-910", "project": "NUTCH", "title": "Cached.jsp has a bug with encoding", "status": "Closed", "priority": "Minor", "reporter": "Attila Pados", "assignee": null, "created": "2010-09-27T15:03:57.911+0000", "updated": "2013-05-22T03:54:52.874+0000", "description": "cached.jsp\n\nPages that has a non default encoding, or not utf-8 etc, the cached content is displayed screwed. This is quite annoying, but doesn't harm critically functionality.\n\nadd       :   Metadata parseData = bean.getParseData(details).getParseMeta();\noriginal :  Metadata metaData = bean.getParseData(details).getContentMeta();\n\nreplace: String encoding = (String) parseData.get(\"CharEncodingForConversion\");\n\nIn the cached jsp, the encoding variable is tried to retrieved from the wrong metadata source, contentMeta, which doesn't include this value.\nIt resides in the parseMetadata instead. \n\nFirst line is not a replacement above, it has to be added.  Original metadata is needed there for other things.\nThen below, the encoding value line has to be changed, that is a replacement.\n\nThis fix is for 1.0 nutch version, i didn't found an issue in the list that would cover this, just a mail found with google, on a mailing list that refered to it.", "comments": ["unset fix version -- 1.0.0 has already been released and therefore can't have any open issues. \n\nWe *may* potentially roll a Nutch 1.3 that this could fit into (honestly I would call it Nutch 1.2.1 as I don't think there's much 1.x dev going on, or folks willing to maintain it) but until that time I'm leaving it as unset.", "Mmmm... can we mark this as won't fix? Even since Chris' comments above things have progressed quire dramatically.", "this is a legacy issue so we won't be fixing it."], "tasks": {"summary": "Cached.jsp has a bug with encoding", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Cached.jsp has a bug with encoding"}, {"question": "What is the main context?", "answer": "cached.jsp\n\nPages that has a non default encoding, or not utf-8 etc, the cached content is displayed screwed. This is quite annoying, but doesn't harm critically functionality.\n\nadd       :   Metadata"}]}}
{"issue_id": "NUTCH-911", "project": "NUTCH", "title": "recrawls file protocol causes Errors/Exceptions when actually not modified or gone", "status": "Closed", "priority": "Minor", "reporter": "Peter Lundberg", "assignee": null, "created": "2010-10-07T20:39:40.917+0000", "updated": "2014-05-01T06:23:56.144+0000", "description": "When recrawling file systems file are marked as error and logging occurs such as:\n\njava.net.MalformedURLException\n\tat java.net.URL.<init>(URL.java:601)\n\tat java.net.URL.<init>(URL.java:464)\n\tat java.net.URL.<init>(URL.java:413)\n\tat org.apache.nutch.protocol.file.File.getProtocolOutput(File.java:85)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:627)\nfetch of file:/Users/peter.lundberg/Documents/valtech/scan-test/Peter Lundberg 20090929.pdf failed with: java.net.MalformedURLException\n\nThis is due to FileResponse and File not working well together. The same is true for files that after a while disappear from the file system being crawled (ie error instead of GONE). I am too new with nutch to know the design rational behind this or any sideaffect. Below is a patch that I have used that cleans up the segment data and removevs false errors in the log file.\n\n--- src/plugin/protocol-file/src/java/org/apache/nutch/protocol/file/File.java\t(revision 997976)\n+++ src/plugin/protocol-file/src/java/org/apache/nutch/protocol/file/File.java\t(working copy)\n@@ -79,6 +79,10 @@\n         if (code == 200) {                          // got a good response\n           return new ProtocolOutput(response.toContent());              // return it\n   \n+        } else if (code == 404) {                   // handle no such file\n+          return new ProtocolOutput(response.toContent(), ProtocolStatus.STATUS_GONE );  \n+        } else if (code == 304) {                   // handle not modified\n+          return new ProtocolOutput(response.toContent(), ProtocolStatus.STATUS_NOTMODIFIED );  \n         } else if (code >= 300 && code < 400) {     // handle redirect\n           if (redirects == MAX_REDIRECTS)\n             throw new FileException(\"Too many redirects: \" + url);\n", "comments": ["Good catch! Cf.[[1|http://mail-archives.apache.org/mod_mbox/nutch-dev/201211.mbox//%3CCAEncY+mi_KYKs5iSrs7g7TOr=sG9X-279614AY1T55w_-WERKQ@mail.gmail.com%3E]].\n\nRevised patch:\n* handle 401 (access denied)\n* file not found should be HTTP 404 (not found) instead of 410 (gone)", "Just to summarize: it's about translation of FileResponse.code to ProtocolStatus. Some translation pairs are missing, a protocol status exception is returned instead, e.g.:\n{code}\n% bin/nutch parsechecker file:///tmp/no_read_permission.html \nfetching: file:///tmp/no_read_permission.html\norg.apache.nutch.protocol.file.FileError: File Error: 401\n...\nFetch failed with protocol status: exception(16), lastModified=0: org.apache.nutch.protocol.file.FileError: File Error: 401\n{code}\nThe missing translation of NOTMODIFIED is fatal, indeed.", "Committed to trunk r1511479 and 2.x r1511496.", "SUCCESS: Integrated in Nutch-nutchgora #710 (See [https://builds.apache.org/job/Nutch-nutchgora/710/])\nNUTCH-911 protocol-file to return proper protocol status for notmodified, gone, access_denied (snagel: http://svn.apache.org/viewvc/nutch/branches/2.x/?view=rev&rev=1511496)\n* /nutch/branches/2.x/CHANGES.txt\n* /nutch/branches/2.x/src/plugin/protocol-file/src/java/org/apache/nutch/protocol/file/File.java\n", "SUCCESS: Integrated in Nutch-trunk #2311 (See [https://builds.apache.org/job/Nutch-trunk/2311/])\nNUTCH-911 protocol-file to return proper protocol status for notmodified, gone, access_denied (snagel: http://svn.apache.org/viewvc/nutch/trunk/?view=rev&rev=1511479)\n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/src/plugin/protocol-file/src/java/org/apache/nutch/protocol/file/File.java\n"], "tasks": {"summary": "recrawls file protocol causes Errors/Exceptions when actually not modified or gone", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "recrawls file protocol causes Errors/Exceptions when actually not modified or gone"}, {"question": "What is the main context?", "answer": "When recrawling file systems file are marked as error and logging occurs such as:\n\njava.net.MalformedURLException\n\tat java.net.URL.<init>(URL.java:601)\n\tat java.net.URL.<init>(URL.java:464)\n\tat java.n"}]}}
{"issue_id": "NUTCH-912", "project": "NUTCH", "title": "MoreIndexingFilter does not parse docx and xlsx date formats", "status": "Closed", "priority": "Major", "reporter": "Erlend Garåsen", "assignee": "Markus Jelsma", "created": "2010-10-12T10:56:28.053+0000", "updated": "2011-05-08T22:34:47.949+0000", "description": "The following error occurs in hadoop.log when MoreIndexingFilter tries to parse dates from MS Office formats:\n2010-10-08 13:56:32,555 WARN  more.MoreIndexingFilter - http://ridder.uio.no/test1.xlsx: can't parse erroneous date: 2010-10-08T13:55:54Z\n\nThis problem affects docx and xlsx formats, but probably the other XML-based MS Office formats as well.\n", "comments": ["I added the new date format according to http://download.oracle.com/javase/1.4.2/docs/api/java/text/SimpleDateFormat.html and escaped the T and Z literals => yyyy-MM-dd'T'HH:mm:ss'Z'\n\nHere are patches for the current 1.2 stable, branch 1.3 and trunk.", "Committed for 1.3 in 1037733\nCan't commit right now for trunk because i still cannot compile the check out. ", "branch 1.3 : NUTCH-912 added to CHANGES.txt in rev 105551\ntrunk : committed rev 1055518"], "tasks": {"summary": "MoreIndexingFilter does not parse docx and xlsx date formats", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "MoreIndexingFilter does not parse docx and xlsx date formats"}, {"question": "What is the main context?", "answer": "The following error occurs in hadoop.log when MoreIndexingFilter tries to parse dates from MS Office formats:\n2010-10-08 13:56:32,555 WARN  more.MoreIndexingFilter - http://ridder.uio.no/test1.xlsx: c"}]}}
{"issue_id": "NUTCH-913", "project": "NUTCH", "title": "Nutch should use new namespace for Gora", "status": "Closed", "priority": "Major", "reporter": "Dogacan Guney", "assignee": "Dogacan Guney", "created": "2010-10-13T14:13:49.291+0000", "updated": "2010-10-26T11:18:55.181+0000", "description": "Gora is in Apache Incubator now (Yey!). We recently changed Gora's namespace from org.gora to org.apache.gora. This means nutch should use the new namespace otherwise it won't compile with newer builds of Gora.", "comments": ["Patch for issue.", "Big +1 here, Doğacan.", "There are formatting issues in DomainStatistics.java - the file uses literal tabs, which we frown upon, but the patch introduces double-space indent in the changed lines. As ugly as it sounds I think this should be changed into tabs, and then reformatted in another commit.\n\nOther than that, +1, go for it.", "New patch that doesn't try to sneak in formatting changes :)\n\nBtw, I also changed default value for StorageUtils#getDataStoreClass from HBaseStore to SqlStore since that's the default value in nutch-default.xml.", "+1, let's commit it -  I want to start playing with GORA-9, and that patch is in the org.apache namespace...", "Fixed in rev. 1027471. Thanks for all the comments/reviews everyone."], "tasks": {"summary": "Nutch should use new namespace for Gora", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Nutch should use new namespace for Gora"}, {"question": "What is the main context?", "answer": "Gora is in Apache Incubator now (Yey!). We recently changed Gora's namespace from org.gora to org.apache.gora. This means nutch should use the new namespace otherwise it won't compile with newer build"}]}}
{"issue_id": "NUTCH-914", "project": "NUTCH", "title": "Implement Apache Project Branding Requirements", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Lewis John McGibbney", "created": "2010-10-19T10:49:09.790+0000", "updated": "2011-09-11T07:54:46.692+0000", "description": "We should implement the requirements from http://www.apache.org/foundation/marks/pmcs.html\n ", "comments": ["OK I have addressed points 2, 3 and 4 as above. I will hopefully committ tomorrow, then address 5, 6 in due course. In addition there are a number of additional points in a note which ships with the site source code. [1]\n\nIn general, what are our opinions regarding the general layout and functionality of this Apache Nutch site? \n\n[1] http://svn.apache.org/repos/asf/nutch/site/forrest/src/documentation/skins/nutch/note.txt", "How are we doing with this. As far as I am aware both subtasks have been addressed and although it would be nice to get a flashy site up and running incorporating newer suggestions I think it might be green light to think about resolving and closing these issues for the time being?", "issue to be closed as all subtasks have been addressed.\n\nThanks for the direction with this one.", "Thanks Lewis!", "Hi Julien can you please close this one off for us. Thank you very much.", "Thanks Lewis"], "tasks": {"summary": "Implement Apache Project Branding Requirements", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Implement Apache Project Branding Requirements"}, {"question": "What is the main context?", "answer": "We should implement the requirements from http://www.apache.org/foundation/marks/pmcs.html\n "}]}}
{"issue_id": "NUTCH-915", "project": "NUTCH", "title": "project website basics", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2010-10-19T10:52:04.758+0000", "updated": "2011-07-16T11:54:26.493+0000", "description": "http://www.apache.org/foundation/marks/pmcs#websites", "comments": ["The website already follows the requirements below\n\nbq.  Apache projects must host all official website content on an apache.org domain. This includes all content overseen by the project's PMC (including top level website, downloads, wikis, etc.), and ensures both that the ASF infrastructure team can maintain the services, as well informing any users that the content comes from the ASF or the project's PMC, and not a third party.\n\nThe homepage for any TLP must be served from http://project.apache.org, both to ensure consistent branding and to allow for automatically generated links (like the http://projects.apache.org site may do). All primary links to the project as a whole must point directly to the homepage, and not to alternate sites or domains.\n\nProjects are free to use any infrastructure supported technology for managing and deploying their websites, and are free to use any look and feel in their designs. In the future, we expect to ask projects to add a specific style or graphical element (from a choice of several variants) for their link back to www.apache.org that will help give users a better sense of the connection between all Apache projects.\n"], "tasks": {"summary": "project website basics", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "project website basics"}, {"question": "What is the main context?", "answer": "http://www.apache.org/foundation/marks/pmcs#websites"}]}}
{"issue_id": "NUTCH-916", "project": "NUTCH", "title": "Project Naming And Descriptions ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Lewis John McGibbney", "created": "2010-10-19T10:52:20.859+0000", "updated": "2011-07-16T11:53:18.419+0000", "description": null, "comments": ["See http://www.apache.org/foundation/marks/pmcs#naming", "Fixed as per ASF requirements"], "tasks": {"summary": "Project Naming And Descriptions ", "classification": "task", "qa_pairs": []}}
{"issue_id": "NUTCH-917", "project": "NUTCH", "title": "Website Navigation Links", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Lewis John McGibbney", "created": "2010-10-19T10:52:34.235+0000", "updated": "2011-08-10T12:08:39.866+0000", "description": null, "comments": ["http://www.apache.org/foundation/marks/pmcs#navigation", "Fixed as per ASF requirements", "Lewis, what is the commit number for this?\nCan't see the following links from our main page : \n\n\"Thanks\" should link to: http://www.apache.org/foundation/thanks.html\n\nor \n\n\"Security\" should link to either to a project-specific page detailing how users may securely report potential vulnerabilities, or to the main http://www.apache.org/security/ page\n\nDid you add them?\n", "Committed @ revision 1153108.\n\nI had linked the two elements from another section of the site, the commit above resolves this.\n\n", "That's great, thanks Lewis"], "tasks": {"summary": "Website Navigation Links", "classification": "task", "qa_pairs": []}}
{"issue_id": "NUTCH-918", "project": "NUTCH", "title": "Trademark Attributions", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Lewis John McGibbney", "created": "2010-10-19T10:52:43.221+0000", "updated": "2011-07-16T11:55:23.074+0000", "description": null, "comments": ["http://www.apache.org/foundation/marks/pmcs#attributions", "Fixed as per ASF requirements"], "tasks": {"summary": "Trademark Attributions", "classification": "task", "qa_pairs": []}}
{"issue_id": "NUTCH-919", "project": "NUTCH", "title": "Logos and Graphics", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Lewis John McGibbney", "created": "2010-10-19T10:52:51.074+0000", "updated": "2011-07-28T18:20:22.680+0000", "description": null, "comments": ["http://www.apache.org/foundation/marks/pmcs#graphics", "Having not been able to hack this myself I turned to the Apache Forrest user@ list and received the following information.\n\n----------------------------------------------------------------\nThe \"nutch-logo.gif\" needs to have a trademark symbol added\nto it and the graphic reproduced. That is not Forrest's job.\n\nWhen your graphics expert has the replacement logo,\nif it is still named \"nutch-logo.gif\" then all that\nis needed is to replace the image at\nnutch/site/forrest/src/documentation/resources/images/\nthen re-generate the site.\n\nIf it has a different filename, then declare the new\nfilename in nutch/site/forrest/src/documentation/skinconf.xml\nat the \"project-logo\" element.\n\nIf your team is not able to tweak the logo image\nto add the trademark symbol, i.e. you do not have\naccess to the sources and expertise that produced it,\nthen there is a Forrest plugin that might be able to\nassist.\n\nHowever it requires a PNG format logo. So need to\nuse a graphics tool, e.g. Gimp, to convert the GIF.\n\nThen Forrest can be instructed to do a once-off job\nto apply a \"trademark\" or \"registered trademark\"\nto the original PNG to produce a new PNG.\n-------------------------------------------------------\n\nDo we have the resources available to rework the Nutch logo to incorporate the 'TM' symbol?\n ", "So it looks like a new image incorporating the 'TM' logo is required. Where do we go about getting this from? This looks like the last issue required to close NUTCH-914 should the DOAP be OK.", "Ha, even i can use a color picker ;)", "Committed @ revision 1149263.\n", "slight problems with loading in new graphics. This has not been committed as I previously thought.\n\nI'll get this sorted out asap. Sorry about this.", "sorted and committed @ revision 1149508", "Thanks Lewis"], "tasks": {"summary": "Logos and Graphics", "classification": "task", "qa_pairs": []}}
{"issue_id": "NUTCH-92", "project": "NUTCH", "title": "DistributedSearch incorrectly scores results", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2005-09-16T04:27:49.000+0000", "updated": "2011-06-08T21:34:04.224+0000", "description": "When running search servers in a distributed setup, using DistributedSearch$Server and Client, total scores are incorrectly calculated. The symptoms are that scores differ depending on how segments are deployed to Servers, i.e. if there is uneven distribution of terms in segment indexes (due to segment size or content differences) then scores will differ depending on how many and which segments are deployed on a particular Server. This may lead to prioritizing of non-relevant results over more relevant ones.\n\nThe underlying reason for this is that each IndexSearcher (which uses local index on each Server) calculates scores based on the local IDFs of query terms, and not the global IDFs from all indexes together. This means that scores arriving from different Servers to the Client cannot be meaningfully compared, unless all indexes have similar distribution of Terms and similar numbers of documents in them. However, currently the Client mixes all scores together, sorts them by absolute values and picks top hits. These absolute values will change if segments are un-evenly deployed to Servers.\n\nCurrently the workaround is to deploy the same number of documents in segments per Server, and to ensure that segments contain well-randomized content so that term frequencies for common terms are very similar.\n\nThe solution proposed here (as a result of discussion between ab and cutting, patches are coming) is to calculate global IDFs prior to running the query, and pre-boost query Terms with these global IDFs. This will require one more RPC call per each query (this can be optimized later, e.g. through caching). Then the scores will become normalized according to the global IDFs, and Client will be able to meaningfully compare them. Scores will also become independent of the segment content or local number of documents per Server. This will involve at least the following changes:\n\n* change NutchSimilarity.idf(Term, Searcher) to always return 1.0f. This enables us to manipulate scores independently of local IDFs.\n\n* add a new method to Searcher interface, int[] getDocFreqs(Term[]), which will return document frequencies for query terms.\n\n* modify getSegmentNames() so that it returns also the total number of documents in each segment, or implement this as a separate method (this will be called once during segment init)\n\n* in DistributedSearch$Client.search() first make a call to servers to return local IDFs for the current query, and calculate global IDFs for each relevant Term in that query.\n\n* multiply the TermQuery boosts by idf(totalDocFreq, totalIndexedDocs), and PhraseQuery boosts by the sum of the idf(totalDocFreqs, totalIndexedDocs) for all of its terms\n\nThis solution should be applicable with only minor changes to all branches, but initially the patches will be relative to trunk/ .\n\nComments, suggestions and review are welcome!", "comments": ["I recall a discussion on lucene-dev list several (6+?) months back about this or very similar issue.  Lucene's MultiSearcher has the same problem.  Chuck lead the discussion and had some proposed solutions, if I recall correctly, but I don't think they ever made it into Lucene core.\n\nI'm saying this because maybe this can be fixed on a lower (Lucene) level and benefit both Lucene and Nutch.\n", "A minor detail:\n\nIn Searcher, instead of\n\n  int[] getDocFreqs(Term[]);\n\nThe new method will probably have to be something like\n\n  public int[] getDocFreqs(TermSet);\n\nAnd TermSet can implement Writable, as Nutch can't serialize Lucene Terms.\n\n", "Otis, I think this was actually comitted to Lucene, but the solution isn't quite appropriate for Nutch, which does not use Lucene's RMI-based RemoteSearchable, but instead has its own, leaner, RPC mechanism.", "Ah, you are right, I remember this getting in the core.  As a matter of fact, it might have been me who committed it in the end.  Re RMI vs. Nutch's RPC - right.", "Has there been any advancement on this front? ", "I ended up doing somethings very differently than ab initially suggested.\n\nHere is a breakdown:\n\n* change NutchSimilarity.idf(Term, Searcher) to always return 1.0f. \n\n* Add a new method getNumDocs to get total indexed docs per index server. (this will be called once during segment init) \n\n* add a new method to Searcher interface, MapWritable getDocFreqs(Query), which will return document frequencies for query terms. \n    - This is the \"very different\" part. A simple int[] getDocFreqs(Term[]) doesn't work here because DistributedSearch$Client doesn't know which terms will be generated from the query. And it is not enough to run  the query filters on client because each server may run different query filters too. So we send to query as it is, the servers each run their query filters on it, then return a map of form <Term, Frequency> (I ended up implementing WritableStringPair too, I didn't know how else to represent Terms).\n    - After each server returns their individual docFreqs, we combine them in the client.\n\n* in DistributedSearch$Client.search() first make a call to servers to return local IDFs for the current query, and calculate global IDFs for each relevant Term in that query. \n\n* multiply the TermQuery boosts by idf(totalDocFreq, totalIndexedDocs), and PhraseQuery boosts by the sum of the idf(totalDocFreqs, totalIndexedDocs) for all of its terms.\n\nThis patch ended up being rather larger than I initially concieved:D. Comments, suggestions, reviews, corrections are welcome.\n\n", "Here is my second attempt at this. Now DistributedSearch$Client keeps a mapping from addresses to numDocs, and in search(), computes total number of documents from live servers.", "Moving to 1.1 - needs further discussion, see also this thread: http://markmail.org/message/xyqdz3go6jwu4ozm", "Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "DistributedSearch incorrectly scores results", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "DistributedSearch incorrectly scores results"}, {"question": "What is the main context?", "answer": "When running search servers in a distributed setup, using DistributedSearch$Server and Client, total scores are incorrectly calculated. The symptoms are that scores differ depending on how segments ar"}]}}
{"issue_id": "NUTCH-920", "project": "NUTCH", "title": "Project Metadata", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Lewis John McGibbney", "created": "2010-10-19T10:53:00.806+0000", "updated": "2011-08-11T04:01:41.938+0000", "description": null, "comments": ["http://www.apache.org/foundation/marks/pmcs#metadata", "A new file should be created as per here as the older one is dated\n\nhttp://projects.apache.org/create.html\n\n", "DOAP attachment. It does not contain any of the older release metadata, however should this be required I could edit accordingly. This updates redundant data contained within the current DOAP file. ", "Committed @ revision 1149263.\n", "Guys, what about adding one in the trunk for 2.0 as well\nThanks\n\nJulien", "yes Julien I'll get it committed tonight", "DOAP file for Nutch 2.0 (trunk). Release date has been set as TBC to provide flexibility for developers", "If deemed suitable I could commit the trunk file to svn against /trunk directory. If we are to move towards moving documentation to svn as in NUTCH-881 then this would be a start. Comments? ", "Thanks", "Thanks indeed!", "doap.rdf for trunk committed @ revision 1156101.\n\nAfter I add the location of the trunk doap file to the ASF projects page this issue is well and truly closed.", "Integrated in Nutch-trunk #1573 (See [https://builds.apache.org/job/Nutch-trunk/1573/])\n    commit to address NUTCH-920 adding trunk 2.0 DOAP file to svn.\n\nlewismc : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156101\nFiles : \n* /nutch/trunk/doap.rdf\n"], "tasks": {"summary": "Project Metadata", "classification": "task", "qa_pairs": []}}
{"issue_id": "NUTCH-921", "project": "NUTCH", "title": "Reduce dependency of Nutch on config files", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-10-19T16:51:18.859+0000", "updated": "2013-05-22T03:53:36.065+0000", "description": "Currently many components in Nutch rely on reading their configuration from files. These files need to be on the classpath (or packed into a job jar). This is inconvenient if you want to manage configuration via API, e.g. when embedding Nutch, or running many jobs with slightly different configurations.\n\nThis issue tracks the improvement to make various components read their config directly from Configuration properties.", "comments": ["Patch that implements reading config parameters from Configuration, and falls back to config files if Configuration properties are unspecified.", "Hey Andrzej: see NUTCH-431: super +1 on this...", "Patch committed in rev. 1025960. Further improvements to be covered in other issues."], "tasks": {"summary": "Reduce dependency of Nutch on config files", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Reduce dependency of Nutch on config files"}, {"question": "What is the main context?", "answer": "Currently many components in Nutch rely on reading their configuration from files. These files need to be on the classpath (or packed into a job jar). This is inconvenient if you want to manage config"}]}}
{"issue_id": "NUTCH-922", "project": "NUTCH", "title": "SolrWriter should log source fields that are not mapped", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": null, "created": "2010-10-20T14:29:38.912+0000", "updated": "2011-04-14T10:43:10.224+0000", "description": "Currently the SolrWriter::write() method silently ignores source fields that have no mapping to a Solr field. Fields that are ignored should be logged. Any more thoughts on this one?", "comments": ["No problem, unmapped fields are written anyway."], "tasks": {"summary": "SolrWriter should log source fields that are not mapped", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "SolrWriter should log source fields that are not mapped"}, {"question": "What is the main context?", "answer": "Currently the SolrWriter::write() method silently ignores source fields that have no mapping to a Solr field. Fields that are ignored should be logged. Any more thoughts on this one?"}]}}
{"issue_id": "NUTCH-923", "project": "NUTCH", "title": "Multilingual support for Solr-index-mapping", "status": "Open", "priority": "Minor", "reporter": "Matthias Agethle", "assignee": "Markus Jelsma", "created": "2010-10-20T16:04:08.447+0000", "updated": "2025-07-09T20:25:52.230+0000", "description": "It would be useful to extend the mapping-possibilites when indexing to solr.\nOne useful feature would be to use the detected language of the html page (for example via the language-identifier plugin) and send the content to corresponding language-aware solr-fields.\n\nThe mapping file could be as follows:\n<field dest=\"lang\" source=\"lang\"/>\n<field dest=\"title_${lang}\" source=\"title\" />\nso that the title-field gets mapped to title_en for English-pages and tilte_fr for French pages.\n\nWhat do you think? Could this be useful also to others?\nOr are there already other solutions out there?", "comments": ["This is a very useful feature. +1", "This sounds useful, though the implementation needs to keep the following in mind:\n* you _assume_ that the lang field will have a nice predictable value, but unless you sanitize the values you can't assume anything... example: one page I saw had a language metadata set to a random string 8kB long with various control chars and '\\0'-s.\n\n* again, if you don't sanitize and control the total number of unique values in the source field, you could end up with a number of fields approaching infinity, and Solr would melt down...", "Andrzej is right. The LanguageIndexingFilter can return a value based on the value found in the HTTP header which can return garbage but shouldn't the filter itself make sure either `unknown` or a valid ISO-639-2 value is set?\n\nThis way client code can safely rely on the value of the lang field instead of sanitizing. What if more components come that do something with the lang field, must they also sanitize on their own?", "My point was simply that if you want to build your data schema dynamically, based on the actual input data, then you need to be aware that this process is inherently risky - now we could perhaps deal with \"lang\" and LanguageIdentifier, but tomorrow we may be dealing with dc.author or cc.license or something else, and then we will face the same issue, ie. a potentially unlimited number of fields created based on data.\n\nI don't have a good answer to this problem. On one hand this functionality is useful, on the other hand it's inherently risky in presence of less than ideal data, which is always a possibility... Perhaps introducing some sort of validation mechanism would make this safer to use.", "What about querying Solr for the configured fields (perhaps one can do this using LukeRequestHandler, I'm not sure)?\nWhen sending data to Solr one could check if they exist in the Solr schema; if not don't add this field and give a warning.\n\nThe other thing that comes to my mind is: what are valid field-names in solr? Obviously letters, numbers and so on, but is there a validation in Solr?\nOne could use this to check if a dynamically generated field name is compliant with solr (and in this way excluding control characters in field-names as Andrzej mentioned it).", "This doesn't solve the problem of potentially unbounded number of fields. Compliance is one thing, and you can clean up field names from invalid characters, but sanity is another thing - if you have {{title_*}} in your Solr schema then theoretically you are allowed to create unlimited number of fields with this prefix - Solr won't complain.", "Perhaps something like Solr DIH could be a solution. Adding scriptable transformers would allow to write custom logic and would be much more flexible. This way one could also add default field values if no value is provided etc. \nE.g.\n{code:xml}\n<script><![CDATA[\n                function addLanguage(row)        {\n                     //Implementation\n                }\n        ]]></script>\n<fields transformer=\"script:addLanguage\" >\n    <field dest=\"lang\" source=\"lang\"/>\n    <field dest=\"title\" source=\"title\"/>\n</fields>\n{code}\n\nIn the addLanguage script one could do all kind of validations to restrict explosion of field-names.", "This is really a useful feature and it matches to the drupal apache solr multilanguage modul. So how can I implement it? At the moment I have only a need for english and german,  so can we restrict it to this languages at first and if the language identifier is not clear just use a fallback language like english?\n\nat least subscribe +1", "Bronco, sorry for the late response.\nI attached my implementation, maybe it's useful to you.", "It's a while since this got updated. Are there any news here?\nI think this would be very useful. How would one do stemming, stopword removal etc without some extension like this?\nPerhaps we can add some validation to the solr indexer component in order to limit the detected languages to the supported ones?", "Solr now has a LangId request processor on board that can both detect languages and send values to the proper field. You can either do language identification in Nutch or delegate it to Solr.\n\nhttps://wiki.apache.org/solr/LanguageDetection", "Ah, very cool, thanks for the hint.\nSo the best solution would be to use Nutch's language-identifier plugin and use that language in Solr to remap the fields? I suppose this is better than plain Solr language identification as Nutch has access to http-headers and html-metadata to identify the language?", "Good to know about the language detection mechanism in Solr. But I think this nutch feature would be useful anyway. There are cases where you are crawling multilanguage pages and you know their language based on a url pattern, you don't even need to auto-detect it. You could then use the subcollection plugin to store the language in a field, but then you can't use stemming unless you enable the Solr language identification, which auto-detects the language that I already know in advance while crawling and indexes content in a different field based on the detected language. The solr mapping abilities should be made a little more flexible I guess.", "If you specify langid.overwrite=false and provide the language in the landid.langField the LangIdentifier skips detection and only maps the language fields. We use it that way with a custom language identifier so we can do proper analysis for many languages.", "That's brilliant. Thanks Markus for your insight."], "tasks": {"summary": "Multilingual support for Solr-index-mapping", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Multilingual support for Solr-index-mapping"}, {"question": "What is the main context?", "answer": "It would be useful to extend the mapping-possibilites when indexing to solr.\nOne useful feature would be to use the detected language of the html page (for example via the language-identifier plugin) "}]}}
{"issue_id": "NUTCH-924", "project": "NUTCH", "title": "Static field in solr mapping", "status": "Closed", "priority": "Major", "reporter": "David Stuart", "assignee": "Markus Jelsma", "created": "2010-10-22T10:26:25.226+0000", "updated": "2013-05-22T03:53:36.443+0000", "description": "Provide the facility to pass static data defined in solrindex-mapping.xml to solr during the mapping process.\n", "comments": ["Extends SolrWriter and SolrMappingReader to take static field data defined in solrindex-mapping.xml\n\ne.g.\n\n <staticField dest=\"entity\">node</staticField>", "The functionality is useful, +1. But the patch has formatting errors. Please fix them before committing.\n\nThe same functionality should be added to trunk, too.", "Yes, i'll look into it next week orso. The pro for this patch is that in some cases it can replace the subcollection plugin to set a static value.", "updated with formattng, think I need eclipse code styles", "Great! The patch almost works as i expected. It:\n1. adds a single static value to a specified field\n2. adds another value to a field already mapped by Nutch (requires multi valued field)\n3. adds the last defined staticField value (doesn't require a multi valued field).\n\nI think 1 and 2 are desired, this way we can append values to fields. But i think 3 isn't a desirable feature. If we can use this to append a value for a multi valued field, then shouldn't it also work if we define multiple staticField elements with the same dest attribute?", "I have reworked the patch slightly to accomplish the following\n\n1. adds a single static value to a specified field\n2. adds another value to a field already mapped by Nutch (requires multi valued field)\n3. The ability to define multiple staticField's with the same dest attribute will create multiple entires (requires multi valued field)", "Hey Markus,\n\nIs there anything else I need to do on this patch? Do you want me to roll a patch against trunk as per Andrzej?\n\nRegards,\n\nDave", "Yes, it needs to be added to trunk too. Please submit a patch if available.\n\nThanks", "Won't be fixed in 1.3. Marked as affecting 2.0 as well", "Hi Julien,\n\nThere is a patch outstanding on this for trunk but as far as I'm aware the 1.3 patch works as per Markus review. Was there anything outstanding as I think its good to go for 1.3??\n\n\nRegards,\n\nDave", "Hi David,\n\nI'd rather commit on both branches at the same time instead of having one functionality available in one version and not in the other. the plan is to release 1.3 quite soon hence my suggestion to commit your patch soon after that. \n\nDoes it make sense?\n\nJulien", "Yep totally.\n\nI start working on the 2.0 patch\n\nCheers", "Guys, \n\nWouldn't it be better to implement this functionality in a non-SOLR specific way as done in NUTCH-940?\nWe only support SOLR for now but could possibly have more indexing backends in the future, like for instance ElasticSearch.\n\nAny thoughts on this?", "Hi Julien,\n\nAgreed, would it also be better to provide the facility that the solr field mapping functionality provides to an indexer as well? I'll test out the 940 patch but would really like it to support\n\n1. adds a single static value to a specified field\n2. adds another value to a field already mapped by Nutch (requires multi valued field)\n3. The ability to define multiple staticField's with the same dest attribute will create multiple entires (requires multi valued field)", "I am also a proponent of NUTCH-940 for the reasons described by Julien. With that you can still add static values to already existing field to create a multi valued field. I don't know about case three David, you can simlpy copyField it around in Nutch mapping or Solr.", "Yep fine with me. Although it would be quite messy (but achievable), if you hav 10 static fields (categories say) you would have to give them individual names and copy them to a master in solr.", "NUTCH-940 has been committed for 1.4."], "tasks": {"summary": "Static field in solr mapping", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Static field in solr mapping"}, {"question": "What is the main context?", "answer": "Provide the facility to pass static data defined in solrindex-mapping.xml to solr during the mapping process.\n"}]}}
{"issue_id": "NUTCH-925", "project": "NUTCH", "title": "plugins stored in weakhashmap lead memory leak", "status": "Closed", "priority": "Major", "reporter": "congliu", "assignee": null, "created": "2010-10-23T09:35:33.290+0000", "updated": "2011-12-20T13:25:31.267+0000", "description": "I suffer serious memory leak using Nutch 1.2 though a very deep crawl. I get the error like this:\n\nException in thread \"Thread-113544\" java.lang.OutOfMemoryError: PermGen space\n\tat java.lang.Throwable.getStackTraceElement(Native Method)\n\tat java.lang.Throwable.getOurStackTrace(Throwable.java:591)\n\tat java.lang.Throwable.printStackTrace(Throwable.java:510)\n\tat org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:76)\n\tat org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:407)\n\tat org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:305)\n\tat org.apache.log4j.DailyRollingFileAppender.subAppend(DailyRollingFileAppender.java:359)\n\tat org.apache.log4j.WriterAppender.append(WriterAppender.java:160)\n\tat org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)\n\tat org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\n\tat org.apache.log4j.Category.callAppenders(Category.java:206)\n\tat org.apache.log4j.Category.forcedLog(Category.java:391)\n\tat org.apache.log4j.Category.log(Category.java:856)\n\tat org.slf4j.impl.Log4jLoggerAdapter.log(Log4jLoggerAdapter.java:509)\n\tat org.apache.commons.logging.impl.SLF4JLocationAwareLog.warn(SLF4JLocationAwareLog.java:173)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:256)\nException in thread \"main\" java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1252)\n\tat org.apache.nutch.fetcher.Fetcher.fetch(Fetcher.java:1107)\n\tat org.apache.nutch.crawl.Crawl.main(Crawl.java:133)\n\n\nI guess Plugin repository cache lead to memory leak.\n\nAs u know plugins is stored in weakhashmap <conf, plugins>, and new class classload\ncreate when u need plugins.\nUsually,WeakHashMap object can been gc, but class and classload is stored in Perm NOT stack and gc can't perform in Perm, SO (java.lang.OutOfMemoryError: PermGen space) occured..., is any nutch-issues have concerned this promble? or there is any solution? \n\nnutch-356 may help?", "comments": ["This has been fixed for 2.0 in NUTCH-844 but not in 1.x.", "This should have been part of the batch of issues which I backported from 2.0 to 1.x. Can't see it in the list of modifs in 1.3 though so it is possible that it slipped through the net.", "I've checked the PluginReposiry diff of NUTCH-844 and compared with 1.4. It's in!", "Bulk close of resolved issues of 1.4. bulkclose-1.4-20111220", "Your guess is right, there already are solution to this problem. \n\n\n\n\n\n"], "tasks": {"summary": "plugins stored in weakhashmap lead memory leak", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "plugins stored in weakhashmap lead memory leak"}, {"question": "What is the main context?", "answer": "I suffer serious memory leak using Nutch 1.2 though a very deep crawl. I get the error like this:\n\nException in thread \"Thread-113544\" java.lang.OutOfMemoryError: PermGen space\n\tat java.lang.Throwable"}]}}
{"issue_id": "NUTCH-926", "project": "NUTCH", "title": "Redirections from META tag don't get filtered", "status": "Closed", "priority": "Major", "reporter": "Marco Novo", "assignee": null, "created": "2010-10-27T14:45:00.428+0000", "updated": "2024-03-13T14:51:29.183+0000", "description": "We have nutch set to crawl a domain urllist and we want to fetch only passed domains (hosts) not subdomains.\nSo\n\nWWW.DOMAIN1.COM\n..\n..\n..\nWWW.RIGHTDOMAIN.COM\n..\n..\n..\n..\nWWW.DOMAIN.COM\n\nWe sets nutch to:\nNOT FOLLOW EXERNAL LINKS\n\n\nDuring crawling of WWW.RIGHTDOMAIN.COM\nif a page contains\n\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\n<html>\n<head>\n<title></title>\n    <META http-equiv=\"refresh\" content=\"0;\n    url=http://WRONG.RIGHTDOMAIN.COM\">\n</head>\n<body>\n</body>\n</html>\n\nNutch continues to crawl the WRONG subdomains! But it should not do this!!\n\nDuring crawling of WWW.RIGHTDOMAIN.COM\nif a page contains\n\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\n<html>\n<head>\n<title></title>\n    <META http-equiv=\"refresh\" content=\"0;\n    url=http://WWW.WRONGDOMAIN.COM\">\n</head>\n<body>\n</body>\n</html>\n\n\nNutch continues to crawl the WRONG domain! But it should not do this! If that we will spider all the web....\n\n\nWe think the problem is in org.apache.nutch.parse ParseOutputFormat. We have done a patch so we will attach it", "comments": ["Are you using the regex-urlfilter plugin? If so, please show us your regex-urlfilter.txt configuration file.", "Ok here is our File: /opt/nutch/conf/crawl-urlfilter.txt           \n\n\nbut we think it isn't here the problem....\n                                                                                           \n{noformat} \n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# The url filter file used by the crawl command.\n\n# Better for intranet crawling.\n# Be sure to change MY.DOMAIN.NAME to your domain name.\n\n# Each non-comment, non-blank line contains a regular expression\n# prefixed by '+' or '-'.  The first matching pattern in the file\n# determines whether a URL is included or ignored.  If no pattern\n# matches, the URL is ignored.\n\n# skip file:, ftp:, & mailto: urls\n-^(file|ftp|mailto):\n\n# skip image and other suffixes we can't yet parse\n# Original filter line (without images parsing) 2010-09-06\n# -\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe|jpeg|JPEG|bmp|BMP)$\n# Modified filter line to add images downloadin (and parsing?) 2010-09-06\n-\\.(css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe|js)$\n\n# skip URLs containing certain characters as probable queries, etc.\n#-[?*!@=]\n\n# skip URLs with slash-delimited segment that repeats 3+ times, to break loops\n-.*(/[^/]+)/[^/]+\\1/[^/]+\\1/\n\n# accept hosts in MY.DOMAIN.NAME\n# +^http://([a-z0-9]*\\.)*MY.DOMAIN.NAME/\n\n# skip everything else\n# -.\n\n# do everything\n+.\n{noformat} \n", "Sorry for the different indentation ;)", "Marco, the following parts of your config are interesting:\n\n{noformat}\n# accept hosts in MY.DOMAIN.NAME\n# +^http://([a-z0-9]*\\.)*MY.DOMAIN.NAME/\n{noformat}\n\nThis is commented out, if it wasn't, it would certainly allow all types of subdomain for MY.DOMAIN.NAME.\n\n{noformat}\n# skip everything else\n# -.\n{noformat}\n\nThis is also commented out, so nothing is skipped.\n\n{noformat}\n# do everything\n+.\n{noformat}\n\nHow about this? You allow everything.\n\nYou'll need to allow your injected URL's without also allowing subdomains so\n\n{noformat}\n+^http://www.MY.DOMAIN.NAME/ \n{noformat}\n\nand then disallow everything else is good enough..\n", "No we are using an external file with the urllist (to be injected). Really this list is a seeds list. And the list is dinamically generated (so it changes time to time). So we really want to do everything and skip nothing.\nWe attached a patch...have a look at it.\n\nThanks\n", "I've seen the patch but my Java is still too rusty and Nutch's lack of comments isn't helpful either. What does the patch exactly do? I still think this is to be solved by updating your regex filters as dynamically as you inject seeds. Perhaps someone else can shed some light on this one.\n\nThanks", "bq. Nutch continues to crawl the WRONG subdomains! But it should not do this!!\nNo need to shout, we hear you :)\n\nIndeed, Nutch behavior when following redirects doesn't play well with the rule of ignoring external outlinks. Strictly speaking, redirects are not outlinks, but the silent assumption behind ignoreExternalOutlinks is that we crawl content only from that hostname.\n\nAnd your patch would solve this particular issue. However, this is not as simple as it seems... My favorite example is www.ibm.com -> www8.ibm.com/index.html . If we apply your fix you won't be able to crawl www.ibm.com unless you inject all wwwNNN load-balanced hosts... so a simple equality of hostnames may not be sufficient. We have utilities to extract domain names, so we could compare domains but then we may mistreat money.cnn.com vs. weather.cnn.com ...", "I'm sorry I did not mean to shout, i know you are able to hear me, I was only despair, and capital letters were used to enhance the visibility of the problem. :)\n\nFrom what I understand, the problem was already known, we should add another property (and probably a plugin) to regulate the crawling of Web load balancer that have different hostname than the original but which contain relevant data.\nBut without our patch this time, with some unfortunate redirect outside of the domain (no load balancer), Nutch could end up downloading the entire web using high levels of depth ....", "Won't be addressed in 1.3, more discussion needed", "Hey guys, just looking at our critical issues and hadn't noticed this one previously, did anyone have a look at this issue and can we reproduce?", "Patch for current trunk:\n* meta refresh redirects are filtered by Fetcher regarding db.ignore.external.links, so they should also in ParseOutputFormat\n* all normalizing and filtering (also by external links) is now delegated to filterNormalize()", "Committed revision 1610659.\n\nThanks everyone!", "SUCCESS: Integrated in Nutch-trunk #2706 (See [https://builds.apache.org/job/Nutch-trunk/2706/])\nNUTCH-926 Redirections from META tag don't get filtered (jnioche: http://svn.apache.org/viewvc/nutch/trunk/?view=rev&rev=1610659)\n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/src/java/org/apache/nutch/parse/ParseOutputFormat.java\n", "Bulk progression of all “Resolved” issues to “Closed”. Performed my lewismc on 2024-03-13."], "tasks": {"summary": "Redirections from META tag don't get filtered", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Redirections from META tag don't get filtered"}, {"question": "What is the main context?", "answer": "We have nutch set to crawl a domain urllist and we want to fetch only passed domains (hosts) not subdomains.\nSo\n\nWWW.DOMAIN1.COM\n..\n..\n..\nWWW.RIGHTDOMAIN.COM\n..\n..\n..\n..\nWWW.DOMAIN.COM\n\nWe sets nutch "}]}}
{"issue_id": "NUTCH-927", "project": "NUTCH", "title": "Sub pages are not getting crawled", "status": "Closed", "priority": "Major", "reporter": "Rameez Raja", "assignee": null, "created": "2010-10-28T03:58:53.494+0000", "updated": "2010-10-28T08:10:15.813+0000", "description": "In my program the objective is to crawl all the pages and fetch the contents from it. The category wise fetching the information is done perfectly but the sub pages are not getting crawled. In the sense, the nextpages are in the form of links at the bottom of the webpage as shown below - \n\n<a href=\"http://reviews.logitech.com/7061/224/reviews.htm?page=2\" title=\"Next Page &gt;\" name=\"BV_TrackingTag_Review_Display_NextPage\">More Reviews for Z-5500 Digital 5.1 Speaker System</a>.\n\nI am using the below script to crawl the site.\n$NUTCH_HOME/search/scripts/crawl.sh testcrawlreviews 5 & > crawl.log\n\nwhere 5 is the depth\n\n\nShown below is the snapshot\n\ncd $NUTCH_HOME\nbin/nutch inject $BASEDIR/crawldb urls\nbin/nutch generate $BASEDIR/crawldb $BASEDIR/segments\nSEGMENT=`ls $BASEDIR/segments/ | tail -1`\necho processing segment $SEGMENT\nbin/nutch fetch $BASEDIR/segments/$SEGMENT -threads 10\nbin/nutch updatedb $BASEDIR/crawldb $BASEDIR/segments/$SEGMENT -filter\ndone\n", "comments": ["Not a bug. use the mailing lists to ask questions"], "tasks": {"summary": "Sub pages are not getting crawled", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Sub pages are not getting crawled"}, {"question": "What is the main context?", "answer": "In my program the objective is to crawl all the pages and fetch the contents from it. The category wise fetching the information is done perfectly but the sub pages are not getting crawled. In the sen"}]}}
{"issue_id": "NUTCH-928", "project": "NUTCH", "title": "Segmentation", "status": "Closed", "priority": "Major", "reporter": "Rameez Raja", "assignee": null, "created": "2010-10-28T05:06:05.672+0000", "updated": "2010-10-28T08:09:22.031+0000", "description": "Is there any configuration needed to create segments for each URL rather than for each depth?\n\nCan anyone suggest a way to do?", "comments": ["Please use the mailing lists to ask questions instead of reporting as bug"], "tasks": {"summary": "Segmentation", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Segmentation"}, {"question": "What is the main context?", "answer": "Is there any configuration needed to create segments for each URL rather than for each depth?\n\nCan anyone suggest a way to do?"}]}}
{"issue_id": "NUTCH-929", "project": "NUTCH", "title": "Create a REST-based admin UI for Nutch", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Fjodor Vershinin", "created": "2010-10-28T09:17:55.938+0000", "updated": "2024-03-13T14:51:50.575+0000", "description": "This is a follow up to NUTCH-880 - we need to expose the functionality of REST API in a user-friendly admin UI. Thanks to the nature of the API the UI can be implemented in any UI framework that speaks REST/JSON, so it could be a simple webapp (we already have jetty) or a Swing / Pivot / etc standalone application.", "comments": ["As we are using org.restlet as the underlying RESTlet framework, we will need to utilise the presentation technologies supported. e.g integration with three popular template technologies : XSLT, FreeMarker or Apache Velocity.\n\n[1] http://wiki.restlet.org/docs_2.0/13-restlet/21-restlet/378-restlet/116-restlet.html", "Bulk progression of all “Resolved” issues to “Closed”. Performed my lewismc on 2024-03-13."], "tasks": {"summary": "Create a REST-based admin UI for Nutch", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Create a REST-based admin UI for Nutch"}, {"question": "What is the main context?", "answer": "This is a follow up to NUTCH-880 - we need to expose the functionality of REST API in a user-friendly admin UI. Thanks to the nature of the API the UI can be implemented in any UI framework that speak"}]}}
{"issue_id": "NUTCH-93", "project": "NUTCH", "title": "DF error on long filesystem name", "status": "Closed", "priority": "Minor", "reporter": "Shuji Umino", "assignee": null, "created": "2005-09-17T03:21:08.000+0000", "updated": "2005-09-21T11:39:54.000+0000", "description": "java.util.NoSuchElementException happened on start datanode.\nmy system use LVM, default installed filesystem name is 'VolGroup00-LogVol00',\ndivided two lines on type 'df' command.\n\n---------------------------------------------------------------------------------------\n#df -k\n/dev/mapper/VolGroup00-LogVol00\n                     152559732   1279408 143530692   1% /                  <---  return next line\n/dev/sda2               101105      9098     86786  10% /boot\nnone                    257352         0    257352   0% /dev/shm\n---------------------------------------------------------------------------------------\n\n[org.apache.nutch.ndfs.DF] fixed source \n\n        StringTokenizer tokens =\n          new StringTokenizer(lines.readLine(), \" \\t\\n\\r\\f%\");\n        \n        this.filesystem = tokens.nextToken();       \n        if (!tokens.hasMoreTokens()) {\n        \t//for long filesystem name\n        \ttokens = new StringTokenizer(lines.readLine(), \" \\t\\n\\r\\f%\");\n        }", "comments": ["Fixed in the mapred branch."], "tasks": {"summary": "DF error on long filesystem name", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "DF error on long filesystem name"}, {"question": "What is the main context?", "answer": "java.util.NoSuchElementException happened on start datanode.\nmy system use LVM, default installed filesystem name is 'VolGroup00-LogVol00',\ndivided two lines on type 'df' command.\n\n-------------------"}]}}
{"issue_id": "NUTCH-930", "project": "NUTCH", "title": "Remove remaining dependencies on Lucene API", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-10-28T20:39:23.019+0000", "updated": "2013-05-22T03:53:28.071+0000", "description": "Nutch doesn't use Lucene API anymore, all indexing happens via Lucene-agnostic SolrJ API. The only place where we still use a minor part of Lucene is in index-basic, and that use (DateTools) can be easily replaced.", "comments": ["Patch to fix the issue. I'll commit this shortly.", "Committed in rev. 1028474."], "tasks": {"summary": "Remove remaining dependencies on Lucene API", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove remaining dependencies on Lucene API"}, {"question": "What is the main context?", "answer": "Nutch doesn't use Lucene API anymore, all indexing happens via Lucene-agnostic SolrJ API. The only place where we still use a minor part of Lucene is in index-basic, and that use (DateTools) can be ea"}]}}
{"issue_id": "NUTCH-931", "project": "NUTCH", "title": "Simple admin API to fetch status and stop the service", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-10-28T21:03:02.241+0000", "updated": "2013-05-22T03:53:28.617+0000", "description": "REST API needs a simple info / stats service and the ability to shutdown the server.", "comments": ["AdminResource, mostly skeleton for now that implements only the \"stop\" command.", "Committed in rev. 1028736 with some changes."], "tasks": {"summary": "Simple admin API to fetch status and stop the service", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Simple admin API to fetch status and stop the service"}, {"question": "What is the main context?", "answer": "REST API needs a simple info / stats service and the ability to shutdown the server."}]}}
{"issue_id": "NUTCH-932", "project": "NUTCH", "title": "Bulk REST API to retrieve crawl results as JSON", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-10-29T17:00:53.949+0000", "updated": "2013-05-22T03:53:20.212+0000", "description": "It would be useful to be able to retrieve results of a crawl as JSON. There are a few things that need to be discussed:\n\n* how to return bulk results using Restlet (WritableRepresentation subclass?)\n\n* what should be the format of results?\n\nI think it would make sense to provide a single record retrieval (by primary key), all records, and records within a range. This incidentally matches well the capabilities of the Gora Query class :)", "comments": ["This patch adds bulk retrieval of crawl results. This is still very rough, e.g. there's no way to select crawlId or limit the fields... but it returns proper JSON.\n\nThis patch also includes other enhancements and bugfixes - with this patch I was able to perform a complete crawl cycle via REST.", "Example DB content (this was passed through a JSON pretty-printer, otherwise it's just one giant line...).", "Updated patch - this recognizes now URL parameters such as fields, start/end keys, batch and crawl id.", "Examples (with the db equivalent to the one in db.formatted.gz):\n\n{code}\n$ curl -s 'http://localhost:8192/nutch/db?fields=url&end=http://www.freebsd.org/&start=http://www.egothor.org/'| ./json_pp\n[\n  {\n    \"url\": \"http://www.egothor.org/\"\n  }, \n  {\n    \"url\": \"http://www.freebsd.org/\"\n  }\n]\n{code}\n\n{code}\n$ curl -s 'http://localhost:8192/nutch/db?fields=url,outlinks,markers,protocolStatus,parseStatus,contentType&start=http://www.getopt.org/&end=http://www.getopt.org/'| ./json_pp\n[\n  {\n    \"contentType\": \"text/html\", \n    \"url\": \"http://www.getopt.org/\", \n    \"markers\": {\n      \"_updmrk_\": \"1288890451-1134865895\"\n    }, \n    \"parseStatus\": \"success/ok (1/0), args=[]\", \n    \"protocolStatus\": \"SUCCESS, args=[]\", \n    \"outlinks\": {\n      \"http://www.getopt.org/luke/\": \"Luke\", \n      \"http://www.getopt.org/ecimf/contrib/ONTO/REA\": \"REA Ontology page\", \n      \"http://www.getopt.org/CV.pdf\": \"CV here\", \n      \"http://www.getopt.org/utils/build/api\": \"API\", \n      \"http://svn.apache.org/viewvc/hadoop/hbase/trunk/src/java/org/apache/hadoop/hbase/util/JenkinsHash.java\": \"available here\", \n      \"http://www.getopt.org/murmur/MurmurHash.java\": \"MurmurHash.java\", \n      \"http://www.ebxml.org/\": \"ebXML / ebTWG\", \n      \"http://www.freebsd.org/\": \"FreeBSD\", \n      \"http://www.getopt.org/luke/webstart.html\": \"Launch with Java WebStart\", \n      \"http://www.freebsd.org/%7Epicobsd\": \"PicoBSD\", \n      \"http://home.comcast.net/~bretm/hash/6.html\": \"this discussion\", \n      \"http://protege.stanford.edu/\": \"Protege\", \n      \"http://jakarta.apache.org/lucene\": \"Lucene\", \n      \"http://www.getopt.org/ecimf/contrib/ONTO/ebxml\": \"ebXML Ontology\", \n      \"http://www.getopt.org/ecimf/\": \"here\", \n      \"http://www.isthe.com/chongo/tech/comp/fnv/\": \"his website\", \n      \"http://www.getopt.org/stempel/index.html\": \"Stempel\", \n      \"http://www.sigram.com/\": \"SIGRAM\", \n      \"http://www.egothor.org/\": \"Egothor\", \n      \"http://thinlet.sourceforge.net/\": \"Thinlet\", \n      \"http://www.getopt.org/utils/dist/utils-1.0.jar\": \"binary\", \n      \"http://www.ecimf.org/\": \"ECIMF\"\n    }\n  }\n]\n{code}\n", "Updated patch. This changes the NutchTool API to allow for execution steps that are not mapreduce jobs, and to pass arguments in arbitrary order, which was a side-effect of the Restlet API.\n\nAs a proof of concept I reimplemented the Crawler class (a one-shot crawler). If there are no objections I'll commit this shortly.", "This patch simplifies the NutchTool API and reduces changes to implementations of NutchTool. I'd like to commit this patch soon.", "NutchTool is an abstract class in this patch. This actually minimizes the amount of code throughout, though paradoxically the patch file is larger than before...", "Final version of the patch.", "Committed in rev. 1039014."], "tasks": {"summary": "Bulk REST API to retrieve crawl results as JSON", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Bulk REST API to retrieve crawl results as JSON"}, {"question": "What is the main context?", "answer": "It would be useful to be able to retrieve results of a crawl as JSON. There are a few things that need to be discussed:\n\n* how to return bulk results using Restlet (WritableRepresentation subclass?)\n\n"}]}}
{"issue_id": "NUTCH-933", "project": "NUTCH", "title": "Fetcher does not save a pages Last-Modified value in CrawlDatum", "status": "Closed", "priority": "Major", "reporter": "Joe Kemp", "assignee": null, "created": "2010-10-31T03:14:33.120+0000", "updated": "2021-01-28T14:03:34.825+0000", "description": "I added the following code in the output method just after the If (content !=null) statement.\n\n\n        String lastModified = metadata.get(\"Last-Modified\");\n        if (lastModified !=null && !lastModified.equals(\"\")) {\n\n        \ttry {\n\t\t\t\tDate lastModifiedDate = DateUtil.parseDate(lastModified);\n\t\t\t\tdatum.setModifiedTime(lastModifiedDate.getTime());\n\t\t\t} catch (DateParseException e) {\n\t\t\t\t\n\t\t\t}\n        }\n\n\nI now get 304 for pages that haven't changed when I recrawl.  Need to do further testing.  Might also need a configuration parameter to turn off this behavior, allowing pages to be forced to be refreshed.", "comments": ["The modifiedTime stored in a CrawlDatum record is not the \"Last-Modified\" time sent by the responding server (or the time stamp of a file, in case protocol-file is used) but the time a document was fetched.\n\nIs there any reason? \n\nDetermining the \"Last-Modified\" time is somewhat difficult since it may be specified in the HTTP header or in HTML as <META HTTP-EQUIV=\"Last-Modified\" ...>. But it would be a nice-to-have information. In addition, the index-more indexing filter which provides a field \"lastModified\" does the job not very well: it should take the value from content meta data (which seems to be mostly correct) and not from parse meta data.\n\nBeside: re-crawling with if-modified-since is not affected: there is no difference if the time of the last fetch is sent because only if the document has been modified since the last fetch it must be re-fetched.", "Marked as not a problem. PLease reopen if necessary"], "tasks": {"summary": "Fetcher does not save a pages Last-Modified value in CrawlDatum", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fetcher does not save a pages Last-Modified value in CrawlDatum"}, {"question": "What is the main context?", "answer": "I added the following code in the output method just after the If (content !=null) statement.\n\n\n        String lastModified = metadata.get(\"Last-Modified\");\n        if (lastModified !=null && !lastMod"}]}}
{"issue_id": "NUTCH-934", "project": "NUTCH", "title": "Upgrade to Tika 0.8", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-11-15T11:51:28.042+0000", "updated": "2011-04-13T23:48:08.037+0000", "description": null, "comments": ["This issue is superceded by NUTCH-967", "Closing all resolved issues with a non-fixed status."], "tasks": {"summary": "Upgrade to Tika 0.8", "classification": "task", "qa_pairs": []}}
{"issue_id": "NUTCH-935", "project": "NUTCH", "title": "remove unnecessary /./ in basic urlnormalizer", "status": "Closed", "priority": "Trivial", "reporter": "Stondet", "assignee": "Markus Jelsma", "created": "2010-11-17T10:00:52.617+0000", "updated": "2011-05-08T22:34:48.547+0000", "description": "remove unnecessary /./ in basic urlnormalizer, because this is a rather a sign of bad webserver configuration than of a wanted link.", "comments": ["remove current path signal", "This patch works as expected with 1.2 and 1.3. I can commit this one for 1.3 but not for trunk as i cannot compile it at the moment.\nThanks Stondubleyt!", "committed in trunk rev. 1055520\n1.3 : added to CHANGES file in rev 1055522\n\nThanks!"], "tasks": {"summary": "remove unnecessary /./ in basic urlnormalizer", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "remove unnecessary /./ in basic urlnormalizer"}, {"question": "What is the main context?", "answer": "remove unnecessary /./ in basic urlnormalizer, because this is a rather a sign of bad webserver configuration than of a wanted link."}]}}
{"issue_id": "NUTCH-936", "project": "NUTCH", "title": "LanguageIdentifier should not set empty lang field on NutchDocument", "status": "Closed", "priority": "Minor", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2010-11-19T17:55:17.169+0000", "updated": "2010-12-22T16:59:50.072+0000", "description": "For some reason the language identifier plugin sometimes sets an empty value for the lang field. It is confirmed to occur in 1.2 when parsing a scanned PDF file which cannot be OCR'd to proper text, resulting in an empty content field. Anyway, whether it's a problem with the parser or not, the plugin itself should not add an empty value because the content field can always be empty. The plugin already checks for a null value and then sets the lang field to `unknown`, which is fine. But when the lang string is empty, it should also be set to `unknown`.\n\nThis might break clients that have conditional logic on the empty value, but not on the `unknown` value because it may never have occurred in their set up and therefore they might not have added `unknown` to their logic. However, it might seem a little bit overkill to put this proposal behind a configuration option and let Nutch by default continue to behave as it currently does. Any thoughts on this one?\n\nHere's the troublesome URL : http://www.nrc.nl/redactie/binnenland/memo_buza_irak.pdf that returns an empty content field and an empty lang string in 1.2 and presumably in trunk and other versions as well.", "comments": ["Here are patches for the current 1.2 stable, branch 1.3 and trunk. It adds a lang.length() == 0 check to the already existing lang == null check without a configuration setting.", "Committed for 1.3 in 1037732\nCan't commit right now for trunk because i still cannot compile the check out.", "Committed in trunk under revision 1051985.\nThanks"], "tasks": {"summary": "LanguageIdentifier should not set empty lang field on NutchDocument", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "LanguageIdentifier should not set empty lang field on NutchDocument"}, {"question": "What is the main context?", "answer": "For some reason the language identifier plugin sometimes sets an empty value for the lang field. It is confirmed to occur in 1.2 when parsing a scanned PDF file which cannot be OCR'd to proper text, r"}]}}
{"issue_id": "NUTCH-937", "project": "NUTCH", "title": "When nutch is run on hadoop > 0.20.2 (or cdh) it will not find plugins because MapReduce will not unpack plugin/ directory from the job's pack (due to MAPREDUCE-967)", "status": "Closed", "priority": "Minor", "reporter": "Claudio Martella", "assignee": "Julien Nioche", "created": "2010-11-23T15:56:13.950+0000", "updated": "2011-12-20T11:30:22.648+0000", "description": "Jobs running in on hadoop 0.21 or cloudera cdh 0.20.2+737 will fail because of missing plugins (i.e.):\n\n10/10/28 12:22:21 WARN mapred.JobClient: Use GenericOptionsParser for\nparsing the arguments. Applications should implement Tool for the same.\n10/10/28 12:22:22 INFO mapred.FileInputFormat: Total input paths to\nprocess : 1\n10/10/28 12:22:23 INFO mapred.JobClient: Running job: job_201010271826_0002\n10/10/28 12:22:24 INFO mapred.JobClient:  map 0% reduce 0%\n10/10/28 12:22:39 INFO mapred.JobClient: Task Id :\nattempt_201010271826_0002_m_000000_0, Status : FAILED\njava.lang.RuntimeException: Error in configuring object\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)\n    at\norg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)\n    at\norg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:379)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:317)\n    at org.apache.hadoop.mapred.Child$4.run(Child.java:217)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at\norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063)\n    at org.apache.hadoop.mapred.Child.main(Child.java:211)\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)\n    ... 9 more\nCaused by: java.lang.RuntimeException: Error in configuring object\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)\n    at\norg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)\n    at\norg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n    at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34)\n    ... 14 more\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)\n    ... 17 more\nCaused by: java.lang.RuntimeException: x point\norg.apache.nutch.net.URLNormalizer not found.\n    at org.apache.nutch.net.URLNormalizers.<init>(URLNormalizers.java:122)\n    at\norg.apache.nutch.crawl.Injector$InjectMapper.configure(Injector.java:70)\n    ... 22 more\n10/10/28 12:22:40 INFO mapred.JobClient: Task Id :\nattempt_201010271826_0002_m_000001_0, Status : FAILED\njava.lang.RuntimeException: Error in configuring object\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)\n    at\norg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)\n    at\norg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:379)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:317)\n    at org.apache.hadoop.mapred.Child$4.run(Child.java:217)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at\norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063)\n    at org.apache.hadoop.mapred.Child.main(Child.java:211)\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)\n    ... 9 more\nCaused by: java.lang.RuntimeException: Error in configuring object\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)\n    at\norg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)\n    at\norg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n    at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34)\n    ... 14 more\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)\n    ... 17 more\nCaused by: java.lang.RuntimeException: x point\norg.apache.nutch.net.URLNormalizer not found.\n    at org.apache.nutch.net.URLNormalizers.<init>(URLNormalizers.java:122)\n    at\norg.apache.nutch.crawl.Injector$InjectMapper.configure(Injector.java:70)\n    ... 22 more\n\n\nThe bug is due to MAPREDUCE-967 (part of hadoop 0.21 and cdh 0.20.2+737) which modifies the way MapReduce unpacks the job's jar. The old way was to unpack the whole of it, now only classes/ and lib/ are unpacked. This way nutch is missing the plugins/ directory.\n\nA workaround is to force unpacking of the plugin/ directory by setting 'mapreduce.job.jar.unpack.pattern' configuration to \"(?:classes/|lib/|plugins/).*\"\n\n", "comments": ["Has this workaround been reviewed by anyone and validated? \n\nThis thread from the CDH google group and my own experience seem to suggest not : https://groups.google.com/a/cloudera.org/group/cdh-user/msg/79c0f772232b275f?", "that thread is still me and yes, it's not working. I stated it in the nutch mailinglist, probably forgot to put it here too.", "A workaround for this is:\n\n- Set the following in nutch-site.xml\n\n<property>\n<name>mapreduce.job.jar.unpack.pattern</name>\n<value>(?:classes/|lib/|plugins/).*</value>\n</property>\n\n<property>\n<name>plugin.folders</name>\n<value>${job.local.dir}/../jars/plugins</value>\n</property> \n\n- Recreate the nutch job file using ant.\n\nThis error will now vanish, but I'm guessing people will hit NUTCH-993 afterwards.", "thanks for the add.\n\nfortunately there's a patch available for 993.", "Marked for 1.4 and trunk. We should address this somehow as Hadoop 0.21 is on it's way.", "With these two properties added to the config everything works:\n* locally\n* on hadoop < 0.21\n* on hadoop > 0.20\n\nShould this be added to nutch-default?", "I would recommend adding to nutch-default..", "I agree, marking it as a fix for the Hadoop version. I'll commit in the next few days if there are no objections.", "-1 It seems something slipped through the testing: the plugin.folders property breaks a local running Nutch and Nutch on Hadoop 0.20.203.0. Need to take a look the solution again.", "I had the same problem in my build of nutch 2 and added the properties mentioned above to nutch-site.xml. I can run inject and generate but I get this error below when running a fetch. Do I need to do something the set the job.local.dir? I am using the nutch-2.0-dev.job jar with CDH3 on CentOs 5. \n\n\n11/08/02 11:01:45 WARN plugin.PluginRepository: Plugins: directory not found: ${job.local.dir}/../jars/plugins\n11/08/02 11:01:45 INFO plugin.PluginRepository: Plugin Auto-activation mode: [true]\n11/08/02 11:01:45 INFO plugin.PluginRepository: Registered Plugins:\n11/08/02 11:01:45 INFO plugin.PluginRepository:  NONE\n11/08/02 11:01:45 INFO plugin.PluginRepository: Registered Extension-Points:\n11/08/02 11:01:45 INFO plugin.PluginRepository:  NONE\nException in thread \"main\" java.lang.RuntimeException: x-point org.apache.nutch.protocol.Protocol not found.\n at org.apache.nutch.protocol.ProtocolFactory.<init>(ProtocolFactory.java:55)\n at org.apache.nutch.fetcher.FetcherJob.getFields(FetcherJob.java:144)\n at org.apache.nutch.fetcher.FetcherJob.run(FetcherJob.java:183)\n at org.apache.nutch.fetcher.FetcherJob.fetch(FetcherJob.java:224)\n at org.apache.nutch.fetcher.FetcherJob.run(FetcherJob.java:309)\n at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n at org.apache.nutch.fetcher.FetcherJob.main(FetcherJob.java:315)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n at java.lang.reflect.Method.invoke(Method.java:597)\n at org.apache.hadoop.util.RunJar.main(RunJar.java:186)\n", "Markus,\n\nThe param plugin.folder is multivalued so you can keep the existing value (plugins) as well as add the new one :\n\n<name>plugin.folders</name>\n<value>${job.local.dir}/../jars/plugins,plugins</value>\n\nI think it would be good to commit this (once tested of course) as it will make it easier for people to use with CDH and other distribs", "With regard to the property \"mapreduce.job.jar.unpack.pattern\", it should be used as an Hadoop deployment property, it cannot be used client side. (When submitting jobs using solely the job jar, embedding the property within nutch-site has no effect).", "One more thing, this is as of version CDH3u0. Perhaps newers versions of Hadoop allow client side configuration.", "we should stick with hadoop 0.20.203.0 not CDH and make nutch-1.4 to work with it.", "@Radim : Nutch is based on the Apache distribution of Hadoop and 1.4 already works with it. No one suggested that it should be based on something different. The point here is that if we can get it to work on other distributions by simply adding a default parameter then it is probably worth doing. \n\n@Ferdy : don't agree that embedding the property within nutch-site has no effect -> it does work. You probably have a different issue  ", "nutch-1.4 contains hadoop-core 0.20.2. If nutch 1.4 is compatible with higher version then included hadoop-core should be upgraded. i hope that it will fix my problems with running nutch locally.", "This in not strictly true, Nutch does not contain or include hadoop-core 0.20.2, instead it depends upon it as well as it depends on many other elements. The lengthy discussion above addresses the problem this issue was created to solve.\n\n@ Julien : I agree with this, it would be nice to offer this flexibility as, quite obviously there are a number of Hadoop distros we can (and people are) using Nutch on. ", "@Julien: I double checked and it seems you're right, \"mapreduce.job.jar.unpack.pattern\" does work as a client side property. (I cannot reproduce the problem).", "I finally found out what the problem is with the above suggestion. It was a terrible problem to debug because of the random elements involved.\n\nSetting the \"plugins.folders\" to \"${job.local.dir}/../jars/plugins\" works only in certain cases. If you have a single folder specified in \"mapred.local.dir\" there will be no trouble at all. However, when you have multiple folders specifed (which is a legit thing to do in Hadoop in order to spread tasks working folders over multiple disks), sometimes loading the plugins results in a NPE because the plugins folder does not exist.\n\nThis is caused by the fact that the jars directory (as unpacked by the TaskTracker) IS NOT ALWAYS ON THE SAME DISK AS THE WORKING FOLDER. This means for example if you have 2 folders in \"mapred.local.dir\" (let's say \"/mnt/disk1/mapred,/mnt/disk2/mapred\") the jars may be unpacked in \n\"/mnt/disk1/mapred/taskTracker/ferdy/jobcache/job_201108301201_0001/work/../jars/plugins\" but the working directory (wich the \"job.local.dir\" property is set to) could be\n\"/mnt/disk2/mapred/taskTracker/ferdy/jobcache/job_201108301201_0001/work/\".\n\nNow I'm not sure whether this is a good thing, perhaps it is because most of the time you will want to unpack a jar once for a job and still run task attempts on multiple disk for a TaskTracker. It is however very troublesome in cases such as this issue and therefore I strongly recommend against setting the \"plugins.folders\" to \"${job.local.dir}/../jars/plugins\", unless you only have one folder specified in \"mapred.local.dir\" of course.\n\nThe workaround I am currently using is to put the plugins folder not in the root of the jar, but in classes/plugins so that Hadoop unjars it and sets it on the classpath automatically. This way there is no need to change the \"mapreduce.job.jar.unpack.pattern\" property and \"plugins.folders\" can be left to it's default of \"plugins\". This suggestion requires a slight modification of Nutch's build.xml file.", "@Ferdy - good detective work! I like your suggestion. I won't affect runtime/local at all, haev you checked that it works with Hadoop 0.20 in distributed mode?", "I'm not entirely sure what you mean, but this is what I have tested: It works in distributed mode using CDH3u0. \n\nIt *should* also work with regular Hadoop versions, both 0.20 and 0.21. (Previously the whole jar was unpacked so that you could use any folder inside a jar, but nowadays only classes and lib are unpacked). I have not tested this but I will leave this up to any of the regular Hadoop users.", "Works fine on Hadoop-0.20.203.0 and CDH3. \n\nAnyone on 0.20? If so could you please give it a try? \n", "Yes Julien. Just getting back round to dealing with an issue I had so I will give it a spin once I get a chance.", "I've got a 0.20.203 cluster. Need that one confirmed?", "trunk : Committed revision 1176823\nnutchgora : Committed revision 1176824\n", "Integrated in Nutch-nutchgora #20 (See [https://builds.apache.org/job/Nutch-nutchgora/20/])\n    NUTCH-937 Put plugins in classes/plugins in job file (Claudio Martella, Ferdy Galema, jnioche)\n\njnioche : http://svn.apache.org/viewvc/nutch/branches/nutchgora/viewvc/?view=rev&root=&revision=1176824\nFiles : \n* /nutch/branches/nutchgora/CHANGES.txt\n* /nutch/branches/nutchgora/build.xml\n", "Bulk close of resolved issues of 1.4. bulkclose-1.4-20111220"], "tasks": {"summary": "When nutch is run on hadoop > 0.20.2 (or cdh) it will not find plugins because MapReduce will not unpack plugin/ directory from the job's pack (due to MAPREDUCE-967)", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "When nutch is run on hadoop > 0.20.2 (or cdh) it will not find plugins because MapReduce will not unpack plugin/ directory from the job's pack (due to MAPREDUCE-967)"}, {"question": "What is the main context?", "answer": "Jobs running in on hadoop 0.21 or cloudera cdh 0.20.2+737 will fail because of missing plugins (i.e.):\n\n10/10/28 12:22:21 WARN mapred.JobClient: Use GenericOptionsParser for\nparsing the arguments. App"}]}}
{"issue_id": "NUTCH-938", "project": "NUTCH", "title": "Imposible to fetch sites with robots.txt ", "status": "Closed", "priority": "Major", "reporter": "Enrique Berlanga", "assignee": null, "created": "2010-11-23T17:50:20.210+0000", "updated": "2010-11-30T09:34:37.609+0000", "description": "Crawling a site with a robots.txt file like this:  (e.g: http://www.melilla.es)\n-------------------\nUser-agent: *\nDisallow: /\n-------------------\nNo links are followed. \n\nIt doesn't matters the value set at \"protocol.plugin.check.blocking\" or \"protocol.plugin.check.robots\" properties, because they are overloaded in class org.apache.nutch.fetcher.Fetcher:\n\n// set non-blocking & no-robots mode for HTTP protocol plugins.\n    getConf().setBoolean(Protocol.CHECK_BLOCKING, false);\n    getConf().setBoolean(Protocol.CHECK_ROBOTS, false);\n\nFalse is the desired value, but in FetcherThread inner class, robot rules are checket ignoring the configuration:\n----------------\nRobotRules rules = protocol.getRobotRules(fit.url, fit.datum);\nif (!rules.isAllowed(fit.u)) {\n ...\nLOG.debug(\"Denied by robots.txt: \" + fit.url);\n...\ncontinue;\n}\n-----------------------\n\nI suposse there is no problem in disabling that part of the code directly for HTTP protocol. If so, I could submit a patch as soon as posible to get over this.\n\nThanks in advance", "comments": ["Patch solving NUTCH-938 issue with robots.txt file in some sites", "Nutch behavior in this case is correct. The goal of Nutch is to implement a well-behaved crawler that obeys robot rules and netiquette. Your patch simply disables these control mechanisms. If it works for you and you can risk the wrath of webmasters, that's fine, you are free to use this patch  - but Nutch as a project cannot encourage such practice.\n\nConsequently I'm going to mark this issue as Won't Fix.", "Thanks for your answer. I agree with you that Nutch as a project cannot encourage such practice, but maybe some code in Protocol or Fetcher class need to be removed from official source. If not, It's hard to understand why this lines appear in the main method of the class ...\n--------\n// set non-blocking & no-robots mode for HTTP protocol plugins.\ngetConf().setBoolean(Protocol.CHECK_BLOCKING, false);\ngetConf().setBoolean(Protocol.CHECK_ROBOTS, false);\n--------\n... and later in fetcher thread that values are ignored.\nMaybe some notes in crawl-urlfilter.txt showing these properties as deprecated would be great.\n\nMy question is: Is there any reason to force it to false? A well-behaved crawler that obeys robot rules and netiquette must force it to true, what makes me being a little confused about that part of the code. I would prefer to feel free to change the behaviour by changing \"protocol.plugin.check.robots\" value in crawl-urlfilter.txt file.\nThanks in advance", "These two properties are documented in nutch-default.xml, but they are mostly for internal use by Nutch. Other implementations of Fetcher (the OldFetcher) used to delegate the robot and politeness controls to protocol plugins. The current implementation of Fetcher performs these tasks itself, although in 1.2 protocol plugins still retain the code to implement these controls per protocol. In 1.3 (unreleased) and trunk this support has been removed from protocol plugins, so these lines will have no effect.", "Resolved as \"Won't Fix\" acording to Andrzej Bialecki."], "tasks": {"summary": "Imposible to fetch sites with robots.txt ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Imposible to fetch sites with robots.txt "}, {"question": "What is the main context?", "answer": "Crawling a site with a robots.txt file like this:  (e.g: http://www.melilla.es)\n-------------------\nUser-agent: *\nDisallow: /\n-------------------\nNo links are followed. \n\nIt doesn't matters the value "}]}}
{"issue_id": "NUTCH-939", "project": "NUTCH", "title": "Added -dir command line option to Indexer and SolrIndexer,  allowing to specify directory containing segments", "status": "Closed", "priority": "Minor", "reporter": "Claudio Martella", "assignee": "Andrzej Bialecki", "created": "2010-11-26T13:47:12.079+0000", "updated": "2011-06-25T12:53:50.347+0000", "description": "The patches add -dir option, so the user can specify the directory in which the segments are to be found. The actual mode is to specify the list of segments, which is not very easy with hdfs. Also, the -dir option is already implemented in LinkDB and SegmentMerger, for example.", "comments": ["This is a useful patch! Could you also submit a patch for trunk?", "Although I haven't checked these patches yet it would be good to have a consistent way of specifying segments - such as by using a -dir directory option. It would be good if the option behaved the same everywhere.", "to be honest i haven't had a big look at nutchbase. I just had a quick look right now and it looks like the mechanism is quite different. There are no such command lines requests for those jobs. Or am I missing something?", "Alex, that's exactly the idea behind this patch. Not all commands though need segments and it looks like that after this patch, they are all fixed. Did I miss some?", "Please note that trunk uses a very different method of working with segments (called batches there), and -dir is not applicable there.", "Yes, that's what I guessed looking at the code. I suppose this goes only for 1.x then.", "1.2 has been released. Marking as 1.3", "I modified the patch slightly to allow more flexibility (you can mix individual segment names and the -dir options) as well as allowing segments placed on different filesystems. Committed in rev. 1051505. Thank you!", "Great. what about the Indexer patch?", "1.2 release is out, and branch-1.2 is unlikely to result in a subsequent release - most users seem to be interested either in 1.3 or trunk.", "Bulk close of resolved issues for 1.3."], "tasks": {"summary": "Added -dir command line option to Indexer and SolrIndexer,  allowing to specify directory containing segments", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Added -dir command line option to Indexer and SolrIndexer,  allowing to specify directory containing segments"}, {"question": "What is the main context?", "answer": "The patches add -dir option, so the user can specify the directory in which the segments are to be found. The actual mode is to specify the list of segments, which is not very easy with hdfs. Also, th"}]}}
{"issue_id": "NUTCH-94", "project": "NUTCH", "title": "MapFile.Writer throwing 'File exists error'.", "status": "Closed", "priority": "Major", "reporter": "Michael Couck", "assignee": "Piotr Kosiorowski", "created": "2005-09-19T18:00:01.000+0000", "updated": "2006-03-25T19:48:49.000+0000", "description": "Running Nutch inside a server JVM or multiple times in the same JVM, MapFile.Writer doesn't get collected or closed by the WebDBWriter and the associated files and directories are not deleted, consequently throws a File exists error in the constructor of MapFile.Writer.\n\nSeems that this portion of code is very heavily integrated into Nutch and I am hesitant to look for a solution personally as a retrofit will be necessary with every release.\n\nHas anyone got any ideas, had the same issue, any solutions?\n\nRegards\n\nMichael", "comments": ["Ya Even I come across the same error \n\nThe problem seems to be in the MapFile Writer where it tries to create a newWeb Db and the the directory already exists\n\nI tried to remove the file but is being used by some other resource .So it is not gettign deleted\n\nMapFile.Writer newDb = (comparator == null) ? new MapFile.Writer(fs, newDbFile.getPath(), keyClass, valueClass) : new MapFile.Writer(fs, newDbFile.getPath(), comparator, valueClass);\n\nI tried to delete the newDbFile.getPath()\nFile dbfileexists=new File(newDbFile.getPath());\n                if (dbfileexists.exists())\n                {\n                System.out.println(\"already exists\");\n                if(FileUtil.fullyDelete(dbfileexists))\n                {\n                    System.out.println(\"deleted successfully\");\n                }\n                else\n                {\n                    System.out.println(\"not deleted\");\n                }\n                }\n\nand found that it already exists and is not getting deleted when you get the error", "Finally it seems that the solution would require too much re-writing and maintaince due to the integration, so we have implemented a spider.\n\nGood luck\n\nMichael", "Duplicate of   NUTCH-117."], "tasks": {"summary": "MapFile.Writer throwing 'File exists error'.", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "MapFile.Writer throwing 'File exists error'."}, {"question": "What is the main context?", "answer": "Running Nutch inside a server JVM or multiple times in the same JVM, MapFile.Writer doesn't get collected or closed by the WebDBWriter and the associated files and directories are not deleted, consequ"}]}}
{"issue_id": "NUTCH-940", "project": "NUTCH", "title": "static field plugin", "status": "Closed", "priority": "Minor", "reporter": "Claudio Martella", "assignee": "Lewis John McGibbney", "created": "2010-11-26T15:17:26.395+0000", "updated": "2015-06-30T17:29:33.854+0000", "description": "A simple plugin called at indexing that adds fields with static data. You can specify a list of <fieldname>:<fieldcontent> per nutch job.\nIt can be useful when collections can't be created by urlpatterns, like in subcollection, but on a job-basis.", "comments": ["Is it possible to add a static field per domain which contains the user id of cms like drupal or similar ? I am looking for a solution to group crawld domains on a user basis. For example i just want to search websites which are owned by user xyz and if yes how can I do it. \n\nThx in advance\n", "if you want to add fields based on urlpatterns i suggest you have a look at the subcollection plugin. that's exactly what it does.", "removed 1.2 flag as it has already been released.\nClaudio please attach a patch generated against the latest SVN code with 'svn diff', this will make it easier for others to review your contribution\nThanks", "you mean nutchbase or branch-1.3?", "nutchbase is not an active branch.  please diff against branch-1.3 and trunk thanks", "still needs a patch for trunk and 1.x branch\nwon't be part of the next 1.3 release", "sorry julien, i've been quite busy. I'll fix it next week. Still gotta understand how you worked out the delegation to solr to remove dependencies on lucene in the plugin.", "this one is done with svn diff and is synced with nutch 1.3", "Claudio, \n\nIt would be better to follow the implicit convention for naming the plugins and call it index-static for instance. This will be a better indication of what the plugin does.\n\nWould be better to be able to specify multiple values for a field as well i.e have a Map<String,String[]>\n\nJulien", "changed naming conventions from static-field to index-static", "About the multiple values, i split on commas and on colons, so values can already have multiple tokens with spaces. They will not be divided in the map, but does it make a difference at indexing time?\n\ni.e. this is reasonable:\n\nfield1:value1.1 value1.2 value1.3,field2:value2.1 value2.2 ...", "Map<String,String[]> => multiple values for the same key which is useful for mulitvalued fields in SOLR e.g. anchors.\n\nsee the functionalities of https://issues.apache.org/jira/browse/NUTCH-924 (and my comments there). Since your plugin is more generic I'd rather use it as soon as it provides at least the same functionalities .\n\nDon't forget to add to src/plugin/build.xml\n\n <ant dir=\"index-static\" target=\"clean\"/>\n\n\nThanks\n\nJulien", "Also please add a description of the parameter used by the plugin in nutch-default.xml", "- fixed src/plugin/build.xml\n- changed the map to HashMap<String,String[]>\n\nI hope this works this way. Looks like NutchField expects a Collection.\n\nIs it fine to do doc.add(String, String[]) for what you required?", "Just to understand, do you think anything more on this one?", "Hello Claudio,\nI see in the patch static-field.diff is not the last version of the java file.\nSo I think you should fix it.\n\nRegards,\nMarseld", "+1 for including this in 1.4. Objections?", "Still needs a description of the parameter used by the plugin in nutch-default.xml\nLooks OK, apart from that", "Any volunteers to assign this issue to? :P If not i'll pick it up for 1.4 in the next couple of weeks.", "From reading the reasonable amount of correspondence on this one I am happy to pick it up and get a (hopefully) final patch submitted next week if this is OK with everyone. Thanks", "cool, thanks! make sure to mention claudio in changes.txt ;)", "Final patch for review including the afore discussed property within nutch-site.xml. The only issue I have is the naming of the final directory as it currently exists as static field instead of index-static as per its parent folder. Also please review and comment re. nutch-site.xml description. ", "please ignore the patch and my comments as above. Too late I'm away to bed and have lost concentration. I will deal with this tomorrow.\n", "Looks fine. If the naming is inconsistent you can change everything to index-static including directory names.", "After a bit of confusion last night I've got this working and attach a new patch which passes all tests.\n\nMy concern was that the final directory was not consistently named and that this should change, but upon viewing the new patch it does not include all of the new files as per claudio's original.\n\nI'm creating the patch as per\n{code}\nsvn diff > patch-name.patch\n{code}\nbut it doesn't seem to want to include the new index-static plugin src and associated plugin .xml files... any ideas please. \n\nIn addition, is it required that we get some JUnit tests together to accompany this new plugin?", "Finally I've got this working and the attached patch passes tests and compiles without any errors. If I can get the thumbs up i'll commit.\nThanks ", "Thumbs up. Please commit! No need to wait on issues like this where there's been tons of discussion and time in between.", "Committed revision 1167651.\n\nI'll work on this work trunk and get a patch submitted shortly (hopefully tomorrow)", "A patch with some simple documentation to keep consistency across the plugins. This  completes the new index-static plugin for branch 1.4.", "Final patch for branch 1.4 committed @ revision 1169502.\nThis just adds some documentation to the class as well as adding a package.html description of the class.", "this patch is a work in progress. There are various issues as I am getting used to the changes in code base and classes etc. If anyone feels like picking this up and giving me some pointers then it would be appreciated. I will try and pick it back up shortly and complete.", "Hi Lewis,\n\nAll looks fine except the formatting is tabbed and not 2 spaces. You could also opt for a comma separated list of key/values. Hadoop Configuration offers a convenience method for that: http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/conf/Configuration.html#getStrings%28java.lang.String%29\n\nThen all you need is split by colon.\n\nCheers", "thanks Markus please give me time to get this sorted out. I appreciate your direction.", "My contribution for Nutch 2.0 is nearly there, so it should not be any bother should anyone wish to pick this up and complete it. The issue has been logged with NUTCH-1104 for clarity.", "Fixed as per the previous 1.4 commit. For 2.0 issues a 'patch' has been supplied as per previous comments.", "I took it to close this as Claudio is no longer around...\nHopefully this is OK.", "Yes, I'm around. Thanks for getting this to the end :)"], "tasks": {"summary": "static field plugin", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "static field plugin"}, {"question": "What is the main context?", "answer": "A simple plugin called at indexing that adds fields with static data. You can specify a list of <fieldname>:<fieldcontent> per nutch job.\nIt can be useful when collections can't be created by urlpatte"}]}}
{"issue_id": "NUTCH-941", "project": "NUTCH", "title": "Search returns blank page, when there is more than one SOLR server configured ", "status": "Closed", "priority": "Major", "reporter": "Charan Malemarpuram", "assignee": null, "created": "2010-12-02T19:54:21.258+0000", "updated": "2011-06-08T21:34:24.620+0000", "description": "Search returns a blank page throwing the following exception in the log file.\n\njava.lang.RuntimeException: Missing hit details! Found: 7, expecting: 8\n        at org.apache.nutch.searcher.SolrSearchBean.getDetails(SolrSearchBean.java:175)\n        at org.apache.nutch.searcher.DistributedSearchBean$DetailTask.call(DistributedSearchBean.java:92)\n\nThis happens, when there is more than one SOLR server configured for search.\n\nRoot cause of this issue is the \n\n NutchBean dedup logic does a \"contains\" check on a Map of Hit objects . \n Hit objects do not have hashcode and equals implemented. It is matching by reference, \n When NutchBean requests for more hits to process site based result grouping,  it gets a new object every time from SOLR result and the whole logic breaks.\n\n\n", "comments": ["Patch update", "Patch which includes the HashCode and Equals", "Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Search returns blank page, when there is more than one SOLR server configured ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Search returns blank page, when there is more than one SOLR server configured "}, {"question": "What is the main context?", "answer": "Search returns a blank page throwing the following exception in the log file.\n\njava.lang.RuntimeException: Missing hit details! Found: 7, expecting: 8\n        at org.apache.nutch.searcher.SolrSearchBe"}]}}
{"issue_id": "NUTCH-942", "project": "NUTCH", "title": "Add user uid from drupal or other cms to the author field of Nutch", "status": "Closed", "priority": "Minor", "reporter": "bronco", "assignee": null, "created": "2010-12-02T20:05:03.950+0000", "updated": "2011-04-13T23:24:37.929+0000", "description": "I wrote a modul which offers the user to add his domain to the solr search index of a website. The user website gets crawled with Nutch.\n\nTo search specially the sides of the user it is necessary to add the uid of drupal to the author field or to the uid field of solr. So the problem is how can I add this informations to nutch that he adds this extra info before he submits the crawler results for indexing to solr?\n\nMy problem is I am not a java programmer but willing to learn. So is someone here who could give me a hint  or some snippets or can describe what I have to do?  ", "comments": ["What's this? Inquiries must to the mailing list."], "tasks": {"summary": "Add user uid from drupal or other cms to the author field of Nutch", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add user uid from drupal or other cms to the author field of Nutch"}, {"question": "What is the main context?", "answer": "I wrote a modul which offers the user to add his domain to the solr search index of a website. The user website gets crawled with Nutch.\n\nTo search specially the sides of the user it is necessary to a"}]}}
{"issue_id": "NUTCH-943", "project": "NUTCH", "title": "Search Results default dedup field \"site\" should be stored in index.", "status": "Closed", "priority": "Major", "reporter": "Charan Malemarpuram", "assignee": null, "created": "2010-12-02T21:35:30.643+0000", "updated": "2011-06-08T21:34:24.867+0000", "description": "\"site\" is not configured as a stored field in SOLR schema.\n\nSearch returns only two results always and had \"See More Hits\" button, even if the results are from different sites.\n\n\"See More\"\n\nAttached patch changes the default schema.xml config to store site field.", "comments": ["Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Search Results default dedup field \"site\" should be stored in index.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Search Results default dedup field \"site\" should be stored in index."}, {"question": "What is the main context?", "answer": "\"site\" is not configured as a stored field in SOLR schema.\n\nSearch returns only two results always and had \"See More Hits\" button, even if the results are from different sites.\n\n\"See More\"\n\nAttached p"}]}}
{"issue_id": "NUTCH-944", "project": "NUTCH", "title": "Increase the number of elements to look for URLs and add the ability to specify multiple attributes by elements", "status": "Open", "priority": "Minor", "reporter": "Jean-Francois Gingras", "assignee": null, "created": "2010-12-03T17:24:39.781+0000", "updated": "2025-07-09T20:25:51.316+0000", "description": "Here a patch for DOMContentUtils.java that increase the number of elements to look for URLs. It also add the ability to specify multiple attributes by elements, for example:\n\nlinkParams.put(\"frame\", new LinkParams(\"frame\", \"longdesc,src\", 0));\nlinkParams.put(\"object\", new LinkParams(\"object\", \"classid,codebase,data,usemap\", 0));\nlinkParams.put(\"video\", new LinkParams(\"video\", \"poster,src\", 0)); // HTML 5\n\nI have a patch for release-1.0 and branch-1.3\n\nI would love to hear your comments about this.", "comments": ["I upload the patch for 1.0 because we currently use it.", "Fix a small, but kind of important, error when splitting attrName to create the HashSet", "Could you please send a patch for 2.0 (trunk) as well? Note that DOMCOntentUtils is used in parse-tika and would also need changing.\n\nCan't we simplify the parsing of the attrsName with string.split(',')? \n\nA JUnit test would be great (or a modification of the existing one if any)\n\nThanks fo ryour contribution\n\n", "We are currently moving to Nutch 1.2, I will provide a patch for it. I also change the code to use string.split('') as suggested.\n\nI will try to make time to provide a patch for 2.0, but I was not able to get 2.0 to compile yet.", "Moved out of 1.3. We need to review this patch thoroughly and check that it does not generate noisy URLs but this definitely looks like a good contribution", "I'm curious how this relates to [TIKA-463]. Is it that this code extracts the URLs from the attributes that (as of TIKA-463) should be getting returned by Tika's HtmlParser?\n\nAlso, TIKA-463 doesn't handle \"video\" - is that a legit XHTML 1.0 element?\n", "Set and Classify"], "tasks": {"summary": "Increase the number of elements to look for URLs and add the ability to specify multiple attributes by elements", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Increase the number of elements to look for URLs and add the ability to specify multiple attributes by elements"}, {"question": "What is the main context?", "answer": "Here a patch for DOMContentUtils.java that increase the number of elements to look for URLs. It also add the ability to specify multiple attributes by elements, for example:\n\nlinkParams.put(\"frame\", n"}]}}
{"issue_id": "NUTCH-945", "project": "NUTCH", "title": "Indexing to multiple SOLR Servers", "status": "Closed", "priority": "Major", "reporter": "Charan Malemarpuram", "assignee": null, "created": "2010-12-03T20:22:58.214+0000", "updated": "2013-08-07T07:34:44.741+0000", "description": "It would be nice to have a default Indexer in Nutch, which can submit docs to multiple SOLR Servers.\n\n> Partitioning is always the question, when writing to multiple SOLR Servers.\n> Default partitioning can be a simple hashcode based distribution with addition hooks to customization.\n\n ", "comments": ["Needed something along these lines, so did this and attaching the patch and new files for this.\n\nThe multiple SOLR servers are specified on the bin/nutch solrindex command line like so:\n\nbin/nutch solrindex http://localhost:8983/solr -all (for single SOLR server)\nbin/nutch solrindex http://localhost:8983/solr,http://localhost:8983/solr -all (for distributing across 2 SOLR servers).\n\nThe default partitioner used is Hadoop's o.a.h.mapreduce...HashPartitioner. For single server, the default partitioner used is a new NonPartitioningPartitioner class (which also extends o.a.h.mapreduce...Partitioner class. Custom partitioners can be specified in nutch-site.xml (or nutch-default.xml) by specifying full class name of custom partitioner in solr.partitioner.class, like so:\n\n{quote}\n<property>\n  <name>solr.partitioner.class</name>\n  <value>com.mycompany.nutch.indexer.solr.MurmurHashPartitioner</value>\n  <description/>\n</property>\n{quote}\n\nI am also attaching the custom partitioner referenced above (SOLR 4.0 will use this partitioning strategy per Markus).\n\nI have tested this locally (non-distributed environment) against a set of 6211 documents. Here are the results, fwiw:\n\nsingle server: 6211 records in 44.09s\n2 servers using (default) hashmod partitioning: 3113 and 3098 records in 44.42s\n2 servers using custom murmur partitioning: 3021 and 3190 records in 44.11s \n", "Patch file to make updates to SolrConstants (add new property), SolrIndexerJob (handle commit to multiple solr servers) and SolrWriter (instantiate and invoke partitioner).\n", "Partitioner that always returns 0 (for handling single SOLR server case).", "I built this as an example in order to verify that setting a custom partitioner works. Uses the o.a.h...MurmurHash as its base. Could probably be useful as an example of a custom Partitioner.", "Perhaps good to know for other readers, the patches submitted by Sujit are for the Nutch Gora branch.", "Thanks Markus, and sorry, I should have mentioned that in my comment.\n\nAlso, here is the link to the discussion on the mailing list.\nhttp://lucene.472066.n3.nabble.com/nutchgora-proposal-to-support-distributed-indexing-td3765521.html\n", "On user@ Julien passed some excellent comments on this one [0]. My opinion is that I would like to see these incorporated, admittedly I've not checked the patch out Sujit (so please excuse if these points are addressed). . My justification behind this is simply longevity. Markus stated \n\n{bq}\"If Solr 4.0 is released in the coming months (and that's what it looks like) i \nwould suggest to patch Nutch to allow for a list of Solr server URL's instead \nof doing partitioning on the client site.\"\n{bq}\n\nWhich I agree with, however until we witness a Solr 4.0 release (currently sitting @ 348 issues [2]) I don't see why this can't be integrated into Nutchgora.\n\n\n[0] http://www.mail-archive.com/user@nutch.apache.org/msg05664.html\n[1] http://www.mail-archive.com/user@nutch.apache.org/msg05674.html\n[2] https://issues.apache.org/jira/secure/IssueNavigator.jspa?reset=true&jqlQuery=project+%3D+SOLR+AND+resolution+%3D+Unresolved+AND+fixVersion+%3D+%224.0%22+ORDER+BY+priority+DESC&mode=hide", "Would be good to see both of these issues taking the same strand of through... if possible.", " I see that the issue is unresolved.Is this patch working?", "It should work. But where do you want to use it for? This is more for distributed indexing and not for sending the same documents to multiple servers. You can also check NUTCH-1377, it uses SolrCloud for indexing and supercedes this issue (unless you're stuck with Solr 3.x)", "Should this be closed as a dupe of NUTCH-1377 or NUTCH-1480?\n"], "tasks": {"summary": "Indexing to multiple SOLR Servers", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Indexing to multiple SOLR Servers"}, {"question": "What is the main context?", "answer": "It would be nice to have a default Indexer in Nutch, which can submit docs to multiple SOLR Servers.\n\n> Partitioning is always the question, when writing to multiple SOLR Servers.\n> Default partitioni"}]}}
{"issue_id": "NUTCH-946", "project": "NUTCH", "title": "cache.jsp does not recognize encoding conversion from content different to UTF-8", "status": "Closed", "priority": "Minor", "reporter": "Enrique Berlanga", "assignee": null, "created": "2010-12-21T09:34:35.091+0000", "updated": "2013-05-22T03:54:48.623+0000", "description": "Cache view does not recognize encoding conversion needed to show properly page content stored in a segment.\n\nThe problem is that it searchs \"CharEncodingForConversion\" meta in content metadata, but it's stored in parse metadata.\n\nHere is the patch I've generated for the fixed version:\n\n### Eclipse Workspace Patch 1.0\n#P branch-1.2\nIndex: src/web/jsp/cached.jsp\n===================================================================\n--- src/web/jsp/cached.jsp\t(revision 1027060)\n+++ src/web/jsp/cached.jsp\t(working copy)\n@@ -39,17 +39,18 @@\n     ResourceBundle.getBundle(\"org.nutch.jsp.cached\", request.getLocale())\n     .getLocale().getLanguage();\n \n-  Metadata metaData = bean.getParseData(details).getContentMeta();\n+  Metadata contentMetaData = bean.getParseData(details).getContentMeta();\n+  Metadata parseMetaData = bean.getParseData(details).getParseMeta();\n \n   String content = null;\n-  String contentType = (String) metaData.get(Metadata.CONTENT_TYPE);\n+  String contentType = (String) contentMetaData.get(Metadata.CONTENT_TYPE);\n   if (contentType.startsWith(\"text/html\")) {\n     // FIXME : it's better to emit the original 'byte' sequence \n     // with 'charset' set to the value of 'CharEncoding',\n     // but I don't know how to emit 'byte sequence' in JSP.\n     // out.getOutputStream().write(bean.getContent(details)) may work, \n     // but I'm not sure.\n-    String encoding = (String) metaData.get(\"CharEncodingForConversion\"); \n+    String encoding = (String) parseMetaData.get(\"CharEncodingForConversion\"); \n     if (encoding != null) {\n       try {\n         content = new String(bean.getContent(details), encoding);\n", "comments": ["The patch file", "Having tried this on some Greek websites with encoding Windows-1253, the correct meta name seems to be \"Content-Encoding\" instead of \"CharEncodingForConversion\". So, using the patch described above and adding a \n    if (encoding==null) encoding = (String) parseMetaData.get(\"Content-Encoding\");\nright after the CharEncodingForConversion search, seemed to do the trick for me.\n", "This issue is now deprecated and can't be fixed in current development."], "tasks": {"summary": "cache.jsp does not recognize encoding conversion from content different to UTF-8", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "cache.jsp does not recognize encoding conversion from content different to UTF-8"}, {"question": "What is the main context?", "answer": "Cache view does not recognize encoding conversion needed to show properly page content stored in a segment.\n\nThe problem is that it searchs \"CharEncodingForConversion\" meta in content metadata, but it"}]}}
{"issue_id": "NUTCH-947", "project": "NUTCH", "title": "text.jsp does not compile on Apache Tomcat, and charset is not specified", "status": "Closed", "priority": "Major", "reporter": "Enrique Berlanga", "assignee": null, "created": "2010-12-21T10:29:26.267+0000", "updated": "2011-04-13T23:36:58.910+0000", "description": "If you add index-more plugin, view as plain text option is shown in search result, but if you access the jsp a compilation error is shown:\norg.apache.jasper.JasperException: /text.jsp(60,29) Attribute value\ndetails.getValue(\"url\") is quoted with \" which must be escaped when used\nwithin the value\n\nTo solve this and the missing character encofing, i've generated the atached patch file.", "comments": [], "tasks": {"summary": "text.jsp does not compile on Apache Tomcat, and charset is not specified", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "text.jsp does not compile on Apache Tomcat, and charset is not specified"}, {"question": "What is the main context?", "answer": "If you add index-more plugin, view as plain text option is shown in search result, but if you access the jsp a compilation error is shown:\norg.apache.jasper.JasperException: /text.jsp(60,29) Attribute"}]}}
{"issue_id": "NUTCH-948", "project": "NUTCH", "title": "Remove Lucene dependencies", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2010-12-21T14:30:26.441+0000", "updated": "2011-06-25T12:53:51.868+0000", "description": "Branch-1.3 still has Lucene libs, but uses Lucene only in one place, namely it uses DateTools in index-basic. DateTools should be replaced with Solr's DateUtil, as we did in trunk, and then we can remove Lucene libs as a dependency.", "comments": ["Committed in rev. 1051509.", "Bulk close of resolved issues for 1.3."], "tasks": {"summary": "Remove Lucene dependencies", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove Lucene dependencies"}, {"question": "What is the main context?", "answer": "Branch-1.3 still has Lucene libs, but uses Lucene only in one place, namely it uses DateTools in index-basic. DateTools should be replaced with Solr's DateUtil, as we did in trunk, and then we can rem"}]}}
{"issue_id": "NUTCH-949", "project": "NUTCH", "title": "Conflicting ANT jars in classpath ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2010-12-22T14:50:49.168+0000", "updated": "2010-12-22T16:46:54.553+0000", "description": "When the locally installed version of ANT > 1.7.1 the test-plugins task crashes because of a conflict between the versions of  the ANT jars that can be found in the classpath. \nThis is due to Avro being referenced in the ivy.xml file despite the fact that it is not needed in 1.3\nWill commit the change shortly; will also check whether this is an issue for 2.0", "comments": ["branch 1.3 : Committed revision 1051932", "Found that 2.0 had exactly the same problem when calling test-plugins \nFixed in revision 1051977 by adding the following to ivy.xml\n{noformat}\n                <!--global exclusion-->\n             \t<exclude module=\"ant\" />\n{noformat}"], "tasks": {"summary": "Conflicting ANT jars in classpath ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Conflicting ANT jars in classpath "}, {"question": "What is the main context?", "answer": "When the locally installed version of ANT > 1.7.1 the test-plugins task crashes because of a conflict between the versions of  the ANT jars that can be found in the classpath. \nThis is due to Avro bei"}]}}
{"issue_id": "NUTCH-95", "project": "NUTCH", "title": "DeleteDuplicates depends on the order of input segments", "status": "Closed", "priority": "Major", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2005-09-21T05:58:56.000+0000", "updated": "2008-01-17T20:32:29.500+0000", "description": "DeleteDuplicates depends on what order the input segments are processed, which in turn depends on the order of segment dirs returned from NutchFileSystem.listFiles(File). In most cases this is undesired and may lead to deleting wrong records from indexes. The silent assumption that segments at the end of the listing are more recent is not always true.\n\nHere's the explanation:\n\n* Dedup first deletes the URL duplicates by computing MD5 hashes for each URL, and then sorting all records by (hash, segmentIdx, docIdx). SegmentIdx is just an int index to the array of open IndexReaders - and if segment dirs are moved/copied/renamed then entries in that array may change their  order. And then for all equal triples Dedup keeps just the first entry. Naturally, if segmentIdx is changed due to dir renaming, a different record will be kept and different ones will be deleted...\n\n* then Dedup deletes content duplicates, again by computing hashes for each content, and then sorting records by (hash, segmentIdx, docIdx). However, by now we already have a different set of undeleted docs depending on the order of input segments. On top of that, the same factor acts here, i.e. segmentIdx changes when you re-shuffle the input segment dirs - so again, when identical entries are compared the one with the lowest (segmentIdx, docIdx) is picked.\n\nSolution: use the fetched date from the first record in each segment to determine the order of segments. Alternatively, modify DeleteDuplicates to use the newer algorithm from SegmentMergeTool. This algorithm works by sorting records using tuples of (urlHash, contentHash, fetchDate, score, urlLength). Then:\n\n1. If urlHash is the same, keep the doc with the highest fetchDate  (the latest version, as recorded by Fetcher).\n2. If contentHash is the same, keep the doc with the highest score, and then if the scores are the same, keep the doc with the shortest url.\n\nInitial fix will be prepared for the trunk/ and then backported to the release branch.", "comments": ["A simpler fix would be to simply sort the list of segment names, since segment names are dates.  This is imperfect, when, e.g., one merge's segments, but it is very simple!", "Well, my point was that in some cases, when you move around indexes and possibly rename them, this no longer holds true, i.e. names may not correspond directly to dates. Reading the first FetcherOutput from a segment is not terribly inefficient, but it always gives the true date.", "I was renaming segments quite often so I would vote for reading the date from the segment instead of using dir name. ", "Number 2 sounds great, but wouldn't you always want the latest scoring document since that should reflect the latest updatedb and rank of the page even if it's lower or higher?", "Yes, you are right - we first need to order the segments by date, so that we always keep the latest revisions of a page (possibly with a lower score, if that's the case...).", "Should dedup automagically happen when you merge segments as well?", "Yes, it should. SegmentMergeTool should handle this correctly in 0.7. For 0.8 it is not (yet) supported...", "This issue is fixed in recent version of DeleteDuplicates - see NUTCH-371.", "See NUTCH-371."], "tasks": {"summary": "DeleteDuplicates depends on the order of input segments", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "DeleteDuplicates depends on the order of input segments"}, {"question": "What is the main context?", "answer": "DeleteDuplicates depends on what order the input segments are processed, which in turn depends on the order of segment dirs returned from NutchFileSystem.listFiles(File). In most cases this is undesir"}]}}
{"issue_id": "NUTCH-950", "project": "NUTCH", "title": "Content-Length limit, URL filter and few minor issues", "status": "Closed", "priority": "Major", "reporter": "Alexis", "assignee": null, "created": "2011-01-01T08:06:15.635+0000", "updated": "2013-05-22T03:53:21.520+0000", "description": "1. crawl command (nutch1.patch)\n\nThe class was renamed to Crawler but the references to it were not updated.\n\n\n2. URL filter (nutch2.patch)\n\nThis avoids a NPE on bogus urls which host do not have a suffix.\n\n\n3. Content-Length limit (nutch3.patch)\n\nThis is related to NUTCH-899.\nThe patch avoids the entire flush operation on the Gora datastore to crash because the MySQL blob limit was exceeded by a few bytes. Both protocol-http and protocol-httpclient plugins were problematic.\n\n\n4. Ivy configuration (nutch4.patch)\n- Change xercesImpl and restlet versions. These 2 version changes are required. The first one currently makes a JUnit test crash, the second one is missing in default Maven repository.\n\n- Add gora-hbase, zookeeper which is an HBase dependency. Add MySQL connector. These jars are necesary to run Gora with HBase or MySQL datastores. (more a suggestion that a requirement here)\n\n- Add com.jcraft/jsch, which is a protocol-sftp plugin dependency. \n", "comments": ["Will look into this next week, thanks for your contribution. In the future please open separate JIRA issues instead of putting everything into a single one", "Committed revision 1055604 in 1.3\nCommitted revision 1055608 for trunk\n\n{panel}\n NUTCH-950 DomainURLFilter throws NPE on bogus urls (Alexis Detreglode via jnioche)\n{panel}\n\nwill review the other submissions later\n\n", "Have committed the first 3 sub-issues.\n\nRegarding the last one, I haven't tested the first point (version changes) but here are a few comments about the other issues : \n* Hbase + MySQL : these backends should not be provided by default, same for the MySQL connector. One option would be to add them to the ivy file but comment them out and give a bit of an explanation e.g. \"uncomment this if you want to use xxx as a GORA backend\"\n* the dependency  com.jcraft/jsch should be placed in the ivy file of the corresponding plugin, not in the main one\n\nAlexis, could you please create a new issue for this then mark this issue as resolved? Having a single JIRA number for completely separated issues is a bad idea and does not help keeping things in sync with the svn commits.\n\nThanks a lot for your contributions\n\nJulien\n", "Sorry I missed the Ivy configuration file in the plugin directory.\n\nSee NUTCH-955 for the new Ivy issue."], "tasks": {"summary": "Content-Length limit, URL filter and few minor issues", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Content-Length limit, URL filter and few minor issues"}, {"question": "What is the main context?", "answer": "1. crawl command (nutch1.patch)\n\nThe class was renamed to Crawler but the references to it were not updated.\n\n\n2. URL filter (nutch2.patch)\n\nThis avoids a NPE on bogus urls which host do not have a su"}]}}
{"issue_id": "NUTCH-951", "project": "NUTCH", "title": "Backport changes from 2.0 into 1.3", "status": "Closed", "priority": "Blocker", "reporter": "Julien Nioche", "assignee": "Andrzej Bialecki", "created": "2011-01-05T10:24:32.817+0000", "updated": "2011-03-10T21:56:07.995+0000", "description": "I've compared the changes from 2.0 with 1.3 and found the following differences (excluding anything specific to 2.0/GORA)\n\n    *  NUTCH-564 External parser supports encoding attribute (Antony Bowesman, mattmann)\n    *  NUTCH-714 Need a SFTP and SCP Protocol Handler (Sanjoy Ghosh, mattmann)\n    *  NUTCH-825 Publish nutch artifacts to central maven repository (mattmann)\n    *  NUTCH-851 Port logging to slf4j (jnioche)\n    *  NUTCH-861 Renamed HTMLParseFilter into ParseFilter\n    *  NUTCH-872 Change the default fetcher.parse to FALSE (ab).\n    *  NUTCH-876 Remove remaining robots/IP blocking code in lib-http (ab)\n    *  NUTCH-880 REST API for Nutch (ab)\n    *  NUTCH-883 Remove unused parameters from nutch-default.xml (jnioche)\n    *  NUTCH-884 FetcherJob should run more reduce tasks than default (ab)\n    *  NUTCH-886 A .gitignore file for Nutch (dogacan)\n    *  NUTCH-894 Move statistical language identification from indexing to parsing step\n    *  NUTCH-921 Reduce dependency of Nutch on config files (ab)\n    *  NUTCH-930 Remove remaining dependencies on Lucene API (ab)\n    *  NUTCH-931 Simple admin API to fetch status and stop the service (ab)\n    *  NUTCH-932 Bulk REST API to retrieve crawl results as JSON (ab)\n\nLet's go through this and decide what to port to 1.3", "comments": ["ported NUTCH-883 to 1.3 in rev 1055503", "NUTCH-930 had already been done. added to CHANGES.txt in revision 1055509 ", "ported NUTCH-886 to 1.3 in rev 1055512", "NUTCH-894 : has been written for 2.0 and would need some effort to backport to 1.3 \nI suggest that we leave it there. \n\nThe list of things that IMHO are worth porting to 1.3 are now \n\n    * NUTCH-564 External parser supports encoding attribute (Antony Bowesman, mattmann)\n    * NUTCH-825 Publish nutch artifacts to central maven repository (mattmann)\n    * NUTCH-872 Change the default fetcher.parse to FALSE (ab).\n    * NUTCH-876 Remove remaining robots/IP blocking code in lib-http (ab)\n    * NUTCH-884 FetcherJob should run more reduce tasks than default (ab)\n    * NUTCH-921 Reduce dependency of Nutch on config files (ab)\n\nAny volunteers?\n", "Hey Julien,\n\nI'll take care of the ones with my name on them below (NUTCH-564 and NUTCH-825).\n\nCheers,\nChris\n\n\n\n\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nChris Mattmann, Ph.D.\nSenior Computer Scientist\nNASA Jet Propulsion Laboratory Pasadena, CA 91109 USA\nOffice: 171-266B, Mailstop: 171-246\nEmail: chris.a.mattmann@nasa.gov\nWWW:   http://sunset.usc.edu/~mattmann/\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nAdjunct Assistant Professor, Computer Science Department\nUniversity of Southern California, Los Angeles, CA 90089 USA\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n\n", "Cheekily assigned the task to AB as a friendly reminder :-) This is valid for Chris as well.\nGentlemen, any chance we could get that out of the way? If you are far too busy I could try and find the time to do it myself ", "NUTCH-564 backported in revision 1065552", "* Ported NUTCH-872 in rev. 1079746.\n* Ported NUTCH-876 in rev. 1079753.\n* Ported NUTCH-921 in rev. 1079760.\n* NUTCH-884 is not applicable to 1.3 because here fetching executes in map tasks, so there's a correct number of them already.", "All changes have been ported. Thanks everyone!", "OK, just NUTCH-825 to port and we're done", "NUTCH-825 committed in revision 1080368\nAll the known improvements from 2.0 have been backported into 1.3 now"], "tasks": {"summary": "Backport changes from 2.0 into 1.3", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Backport changes from 2.0 into 1.3"}, {"question": "What is the main context?", "answer": "I've compared the changes from 2.0 with 1.3 and found the following differences (excluding anything specific to 2.0/GORA)\n\n    *  NUTCH-564 External parser supports encoding attribute (Antony Bowesman"}]}}
{"issue_id": "NUTCH-952", "project": "NUTCH", "title": "fix outlink which started with '?' in html parser", "status": "Closed", "priority": "Major", "reporter": "Stondet", "assignee": null, "created": "2011-01-06T03:14:27.773+0000", "updated": "2021-01-28T14:03:50.415+0000", "description": "<a href=\"?w=ruby%20on%20rails&ty=c&sd=0\" >ruby on rails</a>(a snippet from http://bbs.soso.com/search?ty=c&sd=0&w=rails)\n\noutlink parsed from above link: http://bbs.soso.com/?w=ruby%20on%20rails&ty=c&sd=0\nbut expected is http://bbs.soso.com/search?w=ruby%20on%20rails&ty=c&sd=0\n\n\n\n", "comments": ["fix outlink which started with '?'", "the same reason", "remove query part of baseUrl in Content Object\nidea from NUTCH-797\n\n", "Was fixed by NUTCH-797 for v 1.4 (2.x will follow soon). Example link (attached) works now for 1.8 (both with parse-html and parse-tika):\n{code}\n% nutch parsechecker http://localhost/test_nutch_952.html\n...\nOutlinks: 1\n  outlink: toUrl: http://bbs.soso.com/search?w=ruby%20on%20rails&ty=c&sd=0\n{code}", "Linked to duplicate issues NUTCH-797 and NUTCH-566."], "tasks": {"summary": "fix outlink which started with '?' in html parser", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "fix outlink which started with '?' in html parser"}, {"question": "What is the main context?", "answer": "<a href=\"?w=ruby%20on%20rails&ty=c&sd=0\" >ruby on rails</a>(a snippet from http://bbs.soso.com/search?ty=c&sd=0&w=rails)\n\noutlink parsed from above link: http://bbs.soso.com/?w=ruby%20on%20rails&ty=c&"}]}}
{"issue_id": "NUTCH-953", "project": "NUTCH", "title": "Fix crawl command ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2011-01-06T13:15:12.332+0000", "updated": "2011-01-06T13:17:58.677+0000", "description": "1. crawl command (nutch1.patch)\n\nThe class was renamed to Crawler but the references to it were not updated.", "comments": ["Committed revision 1055862"], "tasks": {"summary": "Fix crawl command ", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fix crawl command "}, {"question": "What is the main context?", "answer": "1. crawl command (nutch1.patch)\n\nThe class was renamed to Crawler but the references to it were not updated."}]}}
{"issue_id": "NUTCH-954", "project": "NUTCH", "title": "Bugfix for Content-Length limit in http protocols", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2011-01-06T13:19:49.720+0000", "updated": "2011-06-25T12:53:51.407+0000", "description": "3. Content-Length limit (nutch3.patch)\n\nThis is related to NUTCH-899.\nThe patch avoids the entire flush operation on the Gora datastore to crash because the MySQL blob limit was exceeded by a few bytes. Both protocol-http and protocol-httpclient plugins were problematic.", "comments": ["1.3 : Committed revision 1056359\ntrunk : Committed revision 1056362\n\nThanks Alexis!", "Bulk close of resolved issues for 1.3."], "tasks": {"summary": "Bugfix for Content-Length limit in http protocols", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Bugfix for Content-Length limit in http protocols"}, {"question": "What is the main context?", "answer": "3. Content-Length limit (nutch3.patch)\n\nThis is related to NUTCH-899.\nThe patch avoids the entire flush operation on the Gora datastore to crash because the MySQL blob limit was exceeded by a few byte"}]}}
{"issue_id": "NUTCH-955", "project": "NUTCH", "title": "Ivy configuration", "status": "Closed", "priority": "Major", "reporter": "Alexis", "assignee": "Andrzej Bialecki", "created": "2011-01-10T10:18:13.635+0000", "updated": "2013-05-22T03:53:24.145+0000", "description": "As mentioned in NUTCH-950, we can slightly improve the Ivy configuration to help setup the Gora backend more easily.\nIf the user does not want to stick with default HSQL database, other alternatives exist, such as MySQL and HBase.\n\norg.restlet and xercesImpl versions should be changed as well.", "comments": ["In the patch, the required dependencies for MySQL and HBase are included in the Ivy config, but commented out as suggested in Julien's comment. It's up to the user to use his own backend to store the data.\n\nFollowing 3 points are minor issues but the fixes allow to play more nicely under Eclipse:\n\n- The call to \"nutch.root\" property set in build.xml for ant should be replaced in src/plugin/protocol-sftp/ivy.xml by the built-in \"basedir\" ivy property.\n- The 2.0.1 version of restlet dependency does not exist in the maven repository, so you want to manually change it to 2.0.0.\n- The xerces (XML parser) implementation needs to be upgraded from 2.6.2 to 2.9.1, otherwise you'll see exceptions while running a JUnit test.", "Sorry please disregard the nutch.root first bullet in the previous comment and in the patch. This would break the build: basedir variable holds the plugin's base directory (\"Nutch2.0/src/plugin/protocol-sftp\"). I get an error in the build saying ivy/ivy-configurations.xml is not found with this patch.\n\nI need to figure out how to load this nutch.root variable in the Ivy plugin in Eclipse.", "Committed with a tweak in rev. 1079770. Thanks!"], "tasks": {"summary": "Ivy configuration", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Ivy configuration"}, {"question": "What is the main context?", "answer": "As mentioned in NUTCH-950, we can slightly improve the Ivy configuration to help setup the Gora backend more easily.\nIf the user does not want to stick with default HSQL database, other alternatives e"}]}}
{"issue_id": "NUTCH-956", "project": "NUTCH", "title": "solrindex issues", "status": "Closed", "priority": "Major", "reporter": "Alexis", "assignee": "Sebastian Nagel", "created": "2011-01-13T21:31:39.826+0000", "updated": "2013-05-22T03:53:19.947+0000", "description": "I ran into a few caveats with solrindex command trying to index documents.\nPlease refer to http://techvineyard.blogspot.com/2010/12/build-nutch-20.html#solrindex that describes my tests.", "comments": ["Here are the changes:\n\n- Avoid multiple values for id field. (NUTCH-819)\n- Allow multiple values for tag field. Add tld (Top Level Domain) field.\n- Get the content-type from WebPage object's member. Otherwise, you will see NullPointerExceptions.\n- Compare strings with equalsTo. That's pretty random, but it avoids having some suprises.", "back on radar.", "Alexis, the first two issues are already in Nutch 1.3 and 2.0. Your content-type fix is for 2.0. What NPE's did you get? I haven't done extensive testing with 2.0 but don't remember seeing NPE. And what suprises do you avoid with the fourth issue?\n\n", "I do get the NPE when indexing this url\n\nhttp://www.truveo.com/ (Content-Type header is \"Content-Type: text/html; charset=utf-8\")\n\nwithout the src/plugin/index-more/src/java/org/apache/nutch/indexer/more/MoreIndexingFilter.java patch.\n\n{code}\njava.lang.NullPointerException\n\tat org.apache.nutch.indexer.more.MoreIndexingFilter.addType(MoreIndexingFilter.java:204)\n\tat org.apache.nutch.indexer.more.MoreIndexingFilter.filter(MoreIndexingFilter.java:78)\n\tat org.apache.nutch.indexer.IndexingFilters.filter(IndexingFilters.java:107)\n\tat org.apache.nutch.indexer.IndexerReducer.reduce(IndexerReducer.java:73)\n\tat org.apache.nutch.indexer.IndexerReducer.reduce(IndexerReducer.java:1)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)\n\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\n{code}\n\n\nSee attached patch \"solr.patch2\".\nIf you have time can you please go ahead an run the entire tests suite as well:\n\n1 InjectorJob\n2 GeneratorJob\n3 FetcherJob\n4 ParserJob\n5 DbUpdaterJob\n6 SolrIndexerJob\n(Finally chech the index with http://localhost:8983/solr/select/?q=video&indent=on in the browser)\n\nat least on this seed url:\n- http://www.truveo.com/\n\n\nRegarding the String comparison in Java, I believe people usually call String.equals instead of using the boolean comparator (==).", "- NPE related to content-type field\n- tld field in Solr schema\n- string comparison in Java", "Removed fix/version 1.4 which indexes the seed url fine. I'll try to fire up a 2.0 instance. Thanks for the pointer!", "Set and Classify \n\nmore work needs to be done here. Also unfortunately Alexis second patch is not ASF licensed!", "I could reproduce the NPE with nutch 2.1 (I at least believe it is the same):\n\n{noformat}\njava.lang.NullPointerException\n        at org.apache.nutch.indexer.more.MoreIndexingFilter.addType(MoreIndexingFilter.java:200)\n        at org.apache.nutch.indexer.more.MoreIndexingFilter.filter(MoreIndexingFilter.java:75)\n        at org.apache.nutch.indexer.IndexingFilters.filter(IndexingFilters.java:107)\n        at org.apache.nutch.indexer.IndexUtil.index(IndexUtil.java:64)\n        at org.apache.nutch.indexer.IndexerJob$IndexerMapper.map(IndexerJob.java:103)\n        at org.apache.nutch.indexer.IndexerJob$IndexerMapper.map(IndexerJob.java:61)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)\n{noformat}\n\nThis only happens when moreIndexingFilter.indexMimeTypeParts is set. Looking at the code, the following fix seems to be more consistent with the code around it, as it actually splits the *indexed* mimeType instead of a different, possibly null, string:\n\n{noformat}\nIndex: src/plugin/index-more/src/java/org/apache/nutch/indexer/more/MoreIndexingFilter.java\n===================================================================\n--- src/plugin/index-more/src/java/org/apache/nutch/indexer/more/MoreIndexingFilter.java\t(revision 1406665)\n+++ src/plugin/index-more/src/java/org/apache/nutch/indexer/more/MoreIndexingFilter.java\t(working copy)\n@@ -197,7 +197,7 @@\n \n     // Check if we need to split the content type in sub parts\n     if (conf.getBoolean(\"moreIndexingFilter.indexMimeTypeParts\", true)) {\n-      String[] parts = getParts(contentType.toString());\n+      String[] parts = getParts(mimeType);\n \n       for(String part: parts) {\n         doc.add(\"type\", part);\n{noformat}\n", "Updated patch for 2.2-SNAPSHOT. It would be great if this could be tested as it really does encapsulate quite a few issues which are quite appropriately summarized as solrindex issues. ", "new patch for this issue. It attempts to obtain the contentType by a number of means. Firstly from the HttpHeaders, then the page contentType. DEBUG logging accompanies these attempts. ", "Improved patch which uses fix from NUTCH-1552 for NPE in determination of content length.", "+1 push it Sebastian. Thanks for looking at this.", "final patches for 2.x and trunk:\n- fix NPE in determination of content-type for 2.x (also NUTCH-1552)\n- add field \"tld\"\n- improve javadocs\n\nRemoved \"string comparison\" change from patch because usage of {{!=}} is correct here (in SolrIndexWriter).\n\nWill commit tomorrow.", "Committed to trunk (r1480485) and 2.x (r1480484).", "Integrated in Nutch-trunk #2197 (See [https://builds.apache.org/job/Nutch-trunk/2197/])\n    NUTCH-956 solrindex issues (Revision 1480485)\n\n     Result = FAILURE\nsnagel : http://svn.apache.org/viewvc/nutch/trunk/?view=rev&rev=1480485\nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/conf/schema-solr4.xml\n* /nutch/trunk/conf/schema.xml\n* /nutch/trunk/src/plugin/index-more/src/java/org/apache/nutch/indexer/more/MoreIndexingFilter.java\n", "Integrated in Nutch-nutchgora #598 (See [https://builds.apache.org/job/Nutch-nutchgora/598/])\n    NUTCH-956 solrindex issues (Revision 1480484)\n\n     Result = SUCCESS\nsnagel : http://svn.apache.org/viewvc/nutch/branches/2.x/?view=rev&rev=1480484\nFiles : \n* /nutch/branches/2.x/CHANGES.txt\n* /nutch/branches/2.x/conf/schema-solr4.xml\n* /nutch/branches/2.x/conf/schema.xml\n* /nutch/branches/2.x/src/plugin/index-more/src/java/org/apache/nutch/indexer/more/MoreIndexingFilter.java\n"], "tasks": {"summary": "solrindex issues", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "solrindex issues"}, {"question": "What is the main context?", "answer": "I ran into a few caveats with solrindex command trying to index documents.\nPlease refer to http://techvineyard.blogspot.com/2010/12/build-nutch-20.html#solrindex that describes my tests."}]}}
{"issue_id": "NUTCH-957", "project": "NUTCH", "title": "fetcher.timelimit.mins is invalid when depth is greater than 1", "status": "Closed", "priority": "Major", "reporter": "Wade Lau", "assignee": null, "created": "2011-01-16T16:06:08.294+0000", "updated": "2011-06-25T12:53:51.039+0000", "description": "The setting value of  fetcher.timelimit.mins will be invalid when runing ./bin/nutch crawl with depth=n (n>1).\n\nThe reason is that the value of fetcher.timelimit.mins has been reset in the following paragraph ( org.apache.nutch.fetcher.Fetcher.java ), \n\nlong timelimit = getConf().getLong(\"fetcher.timelimit.mins\", -1);\nif (timelimit != -1) {\n  timelimit = System.currentTimeMillis() + (timelimit * 60 * 1000);\n  LOG.info(\"Fetcher Timelimit set for : \" + timelimit);\n  getConf().setLong(\"fetcher.timelimit.mins\", timelimit);\n}\n\nwhen the crawler goes down to next depth, the value will be the time value of last one which is timelimit.mins + currentTimeMillis.\n\nSome logs look like:\n\ndepth=1 \nFetcher: starting at 2011-01-16 20:58:53\nFetcher: segment: crawl/segments/20110116205851\nFetcher Timelimit set for : 1295182793540 now is:[1295182733540] timelimit:[1] new.sum:[1295182793540]\ndepth=2\nFetcher: starting at 2011-01-16 21:00:20\nFetcher: segment: crawl/segments/20110116210018\nFetcher Timelimit set for : 77712262795220167 now is:[1295182820167] timelimit:[1295182793540] new.sum:[77712262795220167]\n\nThe solution is easy to go as below:\n\nlong timelimit = getConf().getLong(\"fetcher.timelimit.mins.init\", -1);\nif( timelimit == -1)\n{\n    timelimit = getConf().getLong(\"fetcher.timelimit.mins\", -1);\n    getConf().setLong(\"fetcher.timelimit.mins.init\", timelimit);\n}\nif (timelimit != -1) {\n  timelimit = System.currentTimeMillis() + (timelimit * 60 * 1000);\n  LOG.info(\"Fetcher Timelimit set for : \" + timelimit);\n  getConf().setLong(\"fetcher.timelimit.mins\", timelimit);\n}\n\n\nHope  this will be helpful for the next release, and save time for others.\n\nrefer:\nhttp://ufqi.com/exp/x1183.html?title=apache.nutch.timelimit.bug\n\n\n\n", "comments": ["marked as to be fixed for 1.3 and 2.0", "Was an issue for 1.3 only. Fixed it by using the same approach as in 2.0 and having a separate parameter name. \n\nCommitted revision 1061815\n\nThanks for reporting the issue Wade", "Bulk close of resolved issues for 1.3."], "tasks": {"summary": "fetcher.timelimit.mins is invalid when depth is greater than 1", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "fetcher.timelimit.mins is invalid when depth is greater than 1"}, {"question": "What is the main context?", "answer": "The setting value of  fetcher.timelimit.mins will be invalid when runing ./bin/nutch crawl with depth=n (n>1).\n\nThe reason is that the value of fetcher.timelimit.mins has been reset in the following p"}]}}
{"issue_id": "NUTCH-958", "project": "NUTCH", "title": "Httpclient scheme priority order fix", "status": "Closed", "priority": "Major", "reporter": "Claudio Martella", "assignee": null, "created": "2011-01-17T10:56:09.080+0000", "updated": "2011-04-01T15:07:19.918+0000", "description": "Httpclient will try to authenticate in this order by default: ntlm, digest, basic.\n\nIf you set as default a scheme that comes in this list after a scheme that is negotiated by the server, and this authentication fails, the default scheme will not be tried.\n\nI.e. if you set digest as default scheme but the server negotiates ntlm, the client will still try ntlm and fail.\n\nThe fix sets the default scheme as the only possible scheme for authentication for the given realm by setting the authentication priorities of httpclient.", "comments": ["Hi Claudio. Is this desired behaviour? Shouldn't the default be used as fallback if the negotiated schema fails instead forcing default as only scheme?", "that is the problem. right now the system does not allow the default scheme to be used as a fallback, which is the reason i wrote this patch. that comes because of a bug in httpclient.\n\nSo, in order to have some control over the kind of authentication is used, which is the expected behavior you also describe, the only way is through this workaround.", "Claudio, i am not sure if this workaround should be committed at all. If the devs agree then it should:\n- be patched for 2.0 as well\n- add a configuration option to enable your workaround so to prevent breaking other user's HTTP authentication methods", "this workaround was necessary for my work and introduced an expected behavior. I understand it's not clean, but the actual behavior of nutch isn't correct either. Maybe it can be useful for somebody else and maybe it's enough to keep it here so people can find it and apply the patch if they like, so that it doesn't have to be commited.\n\nThe \"right\" way would probably just pass through moving to httpclient4.", "I had a look at upgrading to a more recent version of httpclient but it was a substantial job as most of the API had changed. We'll definitely do that for Nutch 2.0 at some point. \nWhat about marking this issue as won't fix and move it out of 1.3? As you said people will find your patch here if they have the same problem and can easily apply it. ", "yes, go on.", "See comments. This patch fixes a bug in the underlying httpclient library which will be upgraded later anyway", "Bulk close of resolved issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "Httpclient scheme priority order fix", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Httpclient scheme priority order fix"}, {"question": "What is the main context?", "answer": "Httpclient will try to authenticate in this order by default: ntlm, digest, basic.\n\nIf you set as default a scheme that comes in this list after a scheme that is negotiated by the server, and this aut"}]}}
{"issue_id": "NUTCH-959", "project": "NUTCH", "title": "use of \"ROWS\"  destroys result-lists: first hit appears also als last hit on each \"page\"  (search via search?query... -> xml ) ", "status": "Closed", "priority": "Major", "reporter": "Thomas ", "assignee": null, "created": "2011-01-19T10:27:57.781+0000", "updated": "2011-04-13T23:43:53.430+0000", "description": "http://localhost:8080/nutch-1.2/search?query=SEARCHTERM&start=0&dupes=10&rows=4\n\nin general I get the correct results, but as soon as i start using \"rows=\" the results become weird: \n\nthe first hit appears on the first \"page\" twice: as first hit, and as last entry of the page \n\n\nwhen I change the start-Value, the first hit remains as last entry of the page .... \n\n(sorry for my english, I hope the problem is understandable) \n", "comments": ["still the same problem as describe earlier ", "This smells like a question which should be addressed on the mailing list. Anyway, search has been delegated to Solr so it's regarded as legacy."], "tasks": {"summary": "use of \"ROWS\"  destroys result-lists: first hit appears also als last hit on each \"page\"  (search via search?query... -> xml ) ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "use of \"ROWS\"  destroys result-lists: first hit appears also als last hit on each \"page\"  (search via search?query... -> xml ) "}, {"question": "What is the main context?", "answer": "http://localhost:8080/nutch-1.2/search?query=SEARCHTERM&start=0&dupes=10&rows=4\n\nin general I get the correct results, but as soon as i start using \"rows=\" the results become weird: \n\nthe first hit ap"}]}}
{"issue_id": "NUTCH-96", "project": "NUTCH", "title": "MapFile.Writer throws directory exists exception if run multiple times in the same JVM or server JVM.", "status": "Closed", "priority": "Major", "reporter": "Michael Couck", "assignee": "Piotr Kosiorowski", "created": "2005-09-21T17:55:44.000+0000", "updated": "2006-03-25T19:49:32.000+0000", "description": "I added a bug to the 0.6 version, but I found the same behaviour in the 0.7 version. Specifically the MapFile.Writer doesn't get closed and deleted by WebDBWriter and throws an exception if the directory already exists. Still reluctant to solve this if the solution is not going to get integrated into the official code of Nutch as a retrofit will be necessary with every version released, however I will fix this and submit the patch for evaluation if this is not evaluated in the next couple of days by the Nutch team.\n\nRegards\n\nMichael", "comments": ["I have already commented on this bug and it is the same error  \n\nAnyway i am attaching the same one\n\nYa Even I come across the same error \n\nThe problem seems to be in the MapFile Writer where it tries to create a newWeb Db and the the directory already exists \n\nI tried to remove the file but is being used by some other resource .So it is not gettign deleted \n\nMapFile.Writer newDb = (comparator == null) ? new MapFile.Writer(fs, newDbFile.getPath(), keyClass, valueClass) : new MapFile.Writer(fs, newDbFile.getPath(), comparator, valueClass); \n\nI tried to delete the newDbFile.getPath() \nFile dbfileexists=new File(newDbFile.getPath()); \n                if (dbfileexists.exists()) \n                { \n                System.out.println(\"already exists\"); \n                if(FileUtil.fullyDelete(dbfileexists)) \n                { \n                    System.out.println(\"deleted successfully\"); \n                } \n                else \n                { \n                    System.out.println(\"not deleted\"); \n                } \n                } \n\nand found that it already exists and is not getting deleted when you get the error ", "Duplicate of  NUTCH-117."], "tasks": {"summary": "MapFile.Writer throws directory exists exception if run multiple times in the same JVM or server JVM.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "MapFile.Writer throws directory exists exception if run multiple times in the same JVM or server JVM."}, {"question": "What is the main context?", "answer": "I added a bug to the 0.6 version, but I found the same behaviour in the 0.7 version. Specifically the MapFile.Writer doesn't get closed and deleted by WebDBWriter and throws an exception if the direct"}]}}
{"issue_id": "NUTCH-960", "project": "NUTCH", "title": "Language ID - confidence factor", "status": "Closed", "priority": "Major", "reporter": "M Alexander", "assignee": null, "created": "2011-01-21T19:19:31.839+0000", "updated": "2013-05-22T03:53:28.960+0000", "description": "Hi\n\nIn JAVA implementation, what is the best way to calculate the confidence of the outcome of the language id for a given text?\n\nFor example:\nn-gram matching / total n-gram * 100.\n\nwhen a text is passed. The outcome would be \"en\" with 89% confidence. What is the best way to implement this to the existig nutch language id code?\n\nThanks\n\n", "comments": ["There are a number of Tika issues filed that relate to this. See TIKA-369, TIKA-496, TIKA-568.", "This is way too old and as Ken pointed out this should be dealt with upstream in Tika."], "tasks": {"summary": "Language ID - confidence factor", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Language ID - confidence factor"}, {"question": "What is the main context?", "answer": "Hi\n\nIn JAVA implementation, what is the best way to calculate the confidence of the outcome of the language id for a given text?\n\nFor example:\nn-gram matching / total n-gram * 100.\n\nwhen a text is pas"}]}}
{"issue_id": "NUTCH-961", "project": "NUTCH", "title": "Expose Tika's boilerpipe support", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-01-23T13:23:06.503+0000", "updated": "2024-03-13T14:51:17.740+0000", "description": "Tika 0.8 comes with the Boilerpipe content handler which can be used to extract boilerplate content from HTML pages. We should see how we can expose Boilerplate in the Nutch cofiguration.\n\nUse the following properties to enable and control Boilerpipe.\n\n{code}\n<property>\n  <name>tika.extractor</name>\n  <value>none</value>\n  <description>\n  Which text extraction algorithm to use. Valid values are: boilerpipe or none.\n  </description>\n</property>\n \n<property> \n  <name>tika.extractor.boilerpipe.algorithm</name>\n  <value>ArticleExtractor</value>\n  <description> \n  Which Boilerpipe algorithm to use. Valid values are: DefaultExtractor, ArticleExtractor\n  or CanolaExtractor.\n  </description>\n</property>\n{code}", "comments": ["Tika 0.8 has some issues with PDF parsing, it would be better to use the next release instead. This won't be done as part of the 1.3 release as this is a new functionality and not a bugfix ", "Boilerpipe comes with several algorithms for stripping away the boilerplate content. Although the ArticleExtractor is recommended, it certainly fails for many types of pages. Pages such as news overviews with blocks and lists are much better extracted with the CanolaExtractor instead. This poses a problem, we cannot have just one single configuration directive telling the parser which extractor to use for a whole crawl.\n\nSome thoughts on how to deal with it:\n- use Boilerpipe's estimator to automatically determine which extractor to use\n- have a facility to override false positives returned by the estimator and hardcode which extractor to use for URL groups (not unlike the subcollection plugin)\n", "Here's a WIP for 1.3 adding a repository (or factory) and patching pars-tika. Use the following settings to enable:\n\ntika.use_boilerpipe=true\ntika.boilerpipe.extractor=ArticleExtractor|CanolaExtractor\n\nTest with bin/nutch org.apache.nutch.parse.ParserChecker -dumpText <url>\n\nThere is an issue with extracting anchors of outlinks from the source text. There may also be issues with the repository of which im currently unaware of.", "@Markus - BoilerpipeExtractorRepository.java == NUTCH-961-1.3-tikaparser.patch, content-wise.", "Here's the correct file.", "@Markus - Thank you.\n\nWatch out for [1] in parse-plugins.xml. .html pages may indeed by xhtml. You can safely delete alla parse-html mimeType associations, as long as you have [2] (and you want to use parse-tika instead of parse-html ).\n\n[1]        \n<mimeType name=\"application/xhtml+xml\">\n\t\t<plugin id=\"parse-html\" />\n\t</mimeType>\n\n[2] \n<!--  by default if the mimeType is set to *, or \n        if it can't be determined, use parse-tika -->\n\t<mimeType name=\"*\">\n\t  <plugin id=\"parse-tika\" />\n\t</mimeType>\n ", "Not safely, there are still issues regarding HTML parsing with Tika, even without this nasty boilerpipe hack.", "yeah, I was looking for an issue i think was called to replace parse-html with parse-tika as the default but I found only NUTCH-869[1]. It have just been mentioned in the mailing list (by Julien) and I thought an issue was filed for it.\n\n[1] https://issues.apache.org/jira/browse/NUTCH-869", "Same as NUTCH-961-1.3-tikaparser.patch by Markus but adds necessary configuration to nutch-default.xml (!nutch-site.xml!) as discussed on the mailing list or privately time ago.", "Modified to include necessary changes to parse-plugins.xml also.", "Tested the patch against a checkout of 1.3 branch at revision 1101540, and made some trivial changes to TikaParser code.\nMore interestingly I've also removed the following from parse-plugins.xml:\n\n-        <mimeType name=\"application/xhtml+xml\">\n-\t\t<plugin id=\"parse-html\" />\n-\t</mimeType>\n-", "cleaned up patch. \nTo reproduce:\n{code}\nexport NUTCH_HOME=`pwd`\"/nutch\"; svn co -r 1101540 http://svn.apache.org/repos/asf/nutch/branches/branch-1.3 $NUTCH_HOME\ncp $MR_HOME/BoilerpipeExtractorRepository.java $NUTCH_HOME/src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/BoilerpipeExtractorRepository.java\ncd $NUTCH_HOME; patch -p0 -ui $MR_HOME/NUTCH-961v2.patch\nant\n{code}", "BTW, have you considered a more general patch to support (rather than expose) all of tika's options? I'm just thinking that perhaps no special Boilerpipe per-se support should (for the sake of code maintainability) be exposed at the Nutch level, but only an ability to pass parameters to tika. So at the nutch level one sets properties in nutch-site.xml (or even tika-site.xml) and those are forwarded to tika to the tika-delegating parser plugin.\nThere should therefore be no need for any Boilerpipe testing for example, but rather tika integration testing.\nI'm just thinking out loud (w/o any patch).", "This is not a general patch and won't be. It can, however, be a dependacy if for a broader Tika patch but i haven't seen other tickets as of yet.\n\nThis patch cannot work by just passing parameters to Tika as it needs to use a different ContentHandler in parse-tika itself.", "{quote}it needs to use a different ContentHandler in parse-tika itself.{quote}\n[Documentation opportunity] why?\n\nMy intuition is that the default sax ContentHandler returns the full page and then Tika handles it, this time with the boilerpipe option. ", "The way that Boilerpipe in Tika works is that it acts as a delegate, processing the SAX events generated by the default content handler that knows how to help clean up broken HTML.\n\nSo it's incremental processing (you don't need to get the full page first).\n\nSeparate note: Tika's Boilerpipe support now has an option to return HTML markup, so you could run it in this mode to get anchors/anchor text.\n", "Ah, that's great! Is this in 0.9 or trunk? We still bind with 0.9. This may be  useful because this patch doesn't add anchors to the detected outlinks. The last anchor(s) may contain the complete BP body! =D", "Patch to include mark up from Tika. Anchors are now detected but less outlinks are found! Anyone has a good suggestion on where to fetch our outlinks with the anchors from?", "With BP enabled you can get an java.util.EmptyStackException from DOMBuilder. This is fixed in this patch by adding another check around the peek 'n pop methods.\n\nhttp://mail-archives.apache.org/mod_mbox/nutch-user/201107.mbox/%3C201107151523.18511.markus.jelsma@openindex.io%3E\n\nThere is no answer yet to why this can occur yet i think checking before pop or peek is good anyway.", "It works in production but is still a big hack when dealing with outlinks. Mark as 1.5", "Here's a working patch we use in production. This includes a nasty work around in TikeParsers to collect all outlinks. Without it, only outlinks from the extracted text are collected.\n\nThis is a bit nasty and i'd appreciate if anyone with a bit more experience with Tika can shed some light on this.", "Fixed already. See NUTCH-1233 for a patch!", "20120304-push-1.6", "Markus, do you think this patch can also work for 2.x Series ? If not, is it easy to port to 2.x ? Please let me know your suggestions.", "Should work fine, parse plugins have not changed that much. Keep in mind that you may need bp1.2.0 and keep an eye on link extraction. See related issues.", "Kiran, did you already start porting it to 2.x?", "No Roland, not yet. I just switched to using 1.x series, but i will give a try at porting this to 2.x this week", "Status:\n- ported\n- compiles\n- yields same results as stock 2.1 if disabled (tika.use_boilerpipe=false)\n\nmore tests needed ;)", "- now with working config options\n- cleanup (removed unused useBoilerpipeEstimator)\n", "Roland, thanks for porting to 2.1. I'm having an issue where nutch is only successfully parsing the first fetched url, and all other urls fail to parse with a warning \"unable to successfully parse content [website] of type [x]\". If I run parseChecker on that url the parse runs successfully using tika/boilerplate, so it seems to be an issue that only occurs when trying to run the second parse or more in a batch job. \n\nI'm running Nutch 2.1 with MySQL. The problem occurs with both bp1.1.0 and 1.2.0. ", "Updated patch for trunk. Estimator code has been removed. Parser still relies on reparsing without BP for it to obtain all outlinks. See NUTCH-1233!", "I used patch NUTCH-961-2.1-v2.patch for nutch-2.2.1\ni found that the text parsed by nutch-tika (with boilerpipe support) is different from text parsed by demo site http://boilerpipe-web.appspot.com \nI did upgrade to boilerpipe 1.2.0 to be match with demo site.\n\nThe url i tested is http://www.medhelp.org/posts/Eye-Care/EYE/show/1199003\n\nThe text from nutch-tika (i use ArticleExtractor)\n\nEYE - Eye Care - MedHelp Experts My MedHelp Login or Signup Eye Care Community EYE Post a Question « Back to Community About This Community: This patient support community is for discussions relating to eye care, cataracts , glaucoma , retinal detachment , eye infections, misaligned eyes , intra-ocular implants, refractive surgery ( LASIK and CK), glasses, contact lenses, amblyopia , eye injuries, dry eyes , ocular allergy, eye pain and discomfort, pediatric eye disorders, eyelid and tearduct surgery, poor eyesight, and eye surgery. View community archives Font Size: A A ABackground: Search this Community: Go 3 Comments EYE My son is 4 and half years old and have + no .Our doctor told me six months ago that + no. decreases as time passed and he not to wear glasses after two -three years if he wears glasses regularly.But yesterday he told me that his + No. increases and he have to wear glasses always.If you wish u can go for laser surgery after 14 years i.e. when my son will have age of 17 years.please help me what to do ? Watch this discussion Tweet Related Discussions How to decide if glasses are needed for children? (8 replies):How can a Doctor tell if a child has amblyopia? Is t... [more] Astigmatism (1 replies):My 5 year old son has severe astigmatism. He wears glass... [more] Can someone help me in regards to my sons eyes? (6 replies):I had noticed my son had, had an eye issue when he was a... [more] Blurred vision with glasses (2 replies):Hi, I recently got new glasses and but the vision in my ... [more] Eyesight getting worse (2 replies):Hello! So here's the story. My eyesight had never been ... [more]\n\nAND from demo\n\n3 Comments\nEYE\nMy son is 4 and half years old and have + no .Our doctor told me six months ago that + no. decreases as time passed and he not to wear glasses after two -three years if he wears glasses regularly.But yesterday he told me that his + No. increases and he have to wear glasses always.If you wish u can go for laser surgery after 14 years i.e. when my son will have age of 17 years.please help me what to do ?\n\nthe result from demo is much better for this url.\nSo the parse-tike/boilerpipe not only extract main content from page but also include title and other node content.\nIs it expected?", "Looks like [~kkrugler] is offering to help with publishing Boilerpipe to a Sonatype Maven repo in TIKA-676 (this Nutch issue apparently depends on this Tika issue) - thanks Ken!\n\nBut note that simply moving Nutch to Boilerpipe 1.2.0 won't fix the issue [~tiennm] just reported.\n[~markus17], if [~tiennm] provides a patch that makes Nutch Boilerpipe output match that of the Boilerpipe demo, could you commit it to 2.x?", "Hi Otis - there are no significant improvements between the 1.1.0 and 1.2.0 of Boilerpipe, at least not when it comes to better extraction. I am very sure that when the demo was using 1.2.0, we got identical results with 1.2.0 as well, but still poor in cases not suitable such as overviews, blocks etc. I am also very sure that the current 1.2.0 is nowadays different than what the demo returns, it is not identical anymore, and improved quite a lot.\n\nWe don't use it BP anymore but i'm happy to commit whenever 1.2.0 is in maven or part of Tika if it gets donated to the ASF. We need to get NUTCH-1233 in as well then.", "bq. We don't use it BP anymore\n\nWhat do you mean by that?  I looked at parse-tika/plugins.xml earlier today and saw BP 1.1.0 there.  So I'm not sure what you mean...", "{quote}We don't use it BP anymore {quote}\n\nBP integration will be totally abandoned? Are there any plans to use other content extractor in favour of Boilerpipe?", "I am sorry, i did not mean to speak for the Nutch PMC at all; we not using BP means I am not using BP. As i said before, i am happy to commit this issue is the linked issues are resolved first.", "Hi Markus, is it possible to release a patch for nutch 1.9 ?\n", "Hello,\n\nSince I was not getting satisfactory results after upgrading to boilerpipe 1.2.0 with parse-tika (with boilerpipe support)  I have put some code to nutch-2.x parser to get the same results as the boilerpipe demo-website. Used some code from .v2.patch. \nAttaching the patch.\n\nThanks.\nAlex.", "Modified the NUTCH-961 patch for 1.11", "Any chance we could commit this, [~markus.jelsma@openindex.io]?", "Yes but it requires NUTCH-1233. ", "Update, i've updated NUTCH-1233 for current trunk as well as a fix for the outlink extraction in Tika via TIKA-1835.", "i'm using this patch NUTCH-961-1.11-1.patch, it works fine when run from eclipse & run in hadoop. It have problem when i run in local mode\nIt throws exception: \"Can't retrieve Tika parser for mime-type text/html\". It is not problem with parse-plugins.xml. It seem problem with TikaConfig constructor TikaConfig(ClassLoader loader), it failed to load some config via classLoader when run in local mode.", "Hello - that doesn't seem related to this issue as it doesn't interfere with how its loaded. Also, we cannot reproduce that locally nor in Hadoop mode. But there was some issue on the mailing list a couple of days ago that also mentioned an issue as you describe. ", "Some news, the upstream Tika issue has been committed and resolved and i have requested an earlier Tika RC at which Chris Mattmann responded positive. An early Tika 1.12 might come soon after which i can quickly resolve NUTCH-1233 and, of course, this issue.\n\nOne question to all of you and the PMC specifically, i would like to propose to enable Boilerpipe ArticleExtractor by default. I cannot think of any scenario at which a user would not want this. Please share your thoughts. :)", "One note with boilerpipe support, it is significant slower than parse-html. I tested to parse the same segment and here are results\nparse-html: 3hm, parse-tika with boilerpipe 5h10m and parse-tika without poilerpipe 4h.", "That is probably due to the patch parsing twice. Once with BP for text, and once without for link extraction. ", "AH yes, Could you explain why we need to parse it twice? with NUTCH-1233 we can use just 1 parse?", "With boilerpipe, you get only a very few outlinks, those found in the extracted text, and that is a problem :)", "Can NUTCH-1233: use tika to extract outlink solve that problem?", "Yes! :)", "Patch for trunk.", "Tests pass as expected and Boilerpipe as well. Will commit shortly.", "Updated patch. ExtractorRepository was missing.", "Committed to trunk in revision 1730694. Thanks everyone for contributions.", "SUCCESS: Integrated in Nutch-trunk #3347 (See [https://builds.apache.org/job/Nutch-trunk/3347/])\nNUTCH-961 Expose Tika's Boilerpipe support (markus: [http://svn.apache.org/viewvc/nutch/trunk/?view=rev&rev=1730694])\n* trunk/CHANGES.txt\n* trunk/conf/nutch-default.xml\n* trunk/src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/BoilerpipeExtractorRepository.java\n* trunk/src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/TikaParser.java\n", "GitHub user jeremie70 opened a pull request:\n\n    https://github.com/apache/nutch/pull/92\n\n    Add the boilerpipe parsing adapted from NUTCH-961\n\n    \n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/jeremie70/nutch my-branch\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/nutch/pull/92.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #92\n    \n----\ncommit f185bc4461c57a1a85578de0ecf0884c7026c3a6\nAuthor: Jérémie Bourseau <jeremie.bourseau@xilopix.com>\nDate:   2016-02-26T10:37:28Z\n\n    improve parser with boilerpipe\n\ncommit 93ea2e51f444447be41ec93b2c0b0b61c117eeb3\nAuthor: Jérémie Bourseau <jeremie.bourseau@xilopix.com>\nDate:   2016-02-26T10:37:28Z\n\n    NUTCH-961 improve parser with boilerpipe\n\ncommit be91764fdf59d4f6930fc3211a84a252e5452674\nAuthor: Jérémie Bourseau <jeremie.bourseau@xilopix.com>\nDate:   2016-02-26T11:00:36Z\n\n    Merge branch 'my-branch' of https://github.com/jeremie70/nutch into my-branch\n\n----\n", "Github user lewismc commented on a diff in the pull request:\n\n    https://github.com/apache/nutch/pull/92#discussion_r54332145\n  \n    --- Diff: conf/nutch-default.xml ---\n    @@ -876,6 +876,19 @@\n       </description>\n     </property>\n     \n    +<!-- tika properties -->\n    +\n    +<property>\n    +  <name>tika.boilerpipe</name>\n    +  <value>false</value>\n    --- End diff --\n    \n    Can you provide descriptions of these properties please?\n", "Github user lewismc commented on a diff in the pull request:\n\n    https://github.com/apache/nutch/pull/92#discussion_r54332155\n  \n    --- Diff: src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/BoilerpipeExtractorRepository.java ---\n    @@ -0,0 +1,62 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package org.apache.nutch.parse.tika;\n    +\n    +import java.lang.ClassLoader;\n    +import java.lang.InstantiationException;\n    +import java.util.WeakHashMap;\n    +import org.apache.commons.logging.Log;\n    --- End diff --\n    \n    Nutch currently uses Slf4j\n    \n    org.slf4j.Logger\n    org.slf4j.LoggerFactory\n    \n    I think!\n", "Github user lewismc commented on a diff in the pull request:\n\n    https://github.com/apache/nutch/pull/92#discussion_r54332193\n  \n    --- Diff: src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/BoilerpipeExtractorRepository.java ---\n    @@ -0,0 +1,62 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package org.apache.nutch.parse.tika;\n    +\n    +import java.lang.ClassLoader;\n    +import java.lang.InstantiationException;\n    +import java.util.WeakHashMap;\n    +import org.apache.commons.logging.Log;\n    +import org.apache.commons.logging.LogFactory;\n    +import org.apache.tika.parser.html.BoilerpipeContentHandler;\n    +import de.l3s.boilerpipe.BoilerpipeExtractor;\n    +import de.l3s.boilerpipe.extractors.*;\n    +\n    +class BoilerpipeExtractorRepository {\n    +\n    +    public static final Log LOG = LogFactory.getLog(BoilerpipeExtractorRepository.class);\n    +    public static final WeakHashMap<String, BoilerpipeExtractor> extractorRepository = new WeakHashMap<String, BoilerpipeExtractor>();\n    + \n    +    /**\n    +     * Returns an instance of the specified extractor\n    +     */\n    +    public static BoilerpipeExtractor getExtractor(String boilerpipeExtractorName) {\n    +      // Check if there's no instance of this extractor\n    +      if (!extractorRepository.containsKey(boilerpipeExtractorName)) {\n    +        // FQCN\n    +        boilerpipeExtractorName = \"de.l3s.boilerpipe.extractors.\" + boilerpipeExtractorName;\n    +\n    +        // Attempt to load the class\n    +        try {\n    +          ClassLoader loader = BoilerpipeExtractor.class.getClassLoader();\n    +          Class extractorClass = loader.loadClass(boilerpipeExtractorName);\n    +\n    +          // Add an instance to the repository\n    +          extractorRepository.put(boilerpipeExtractorName, (BoilerpipeExtractor)extractorClass.newInstance());\n    +\n    +        } catch (ClassNotFoundException e) {\n    +          LOG.error(\"BoilerpipeExtractor \" + boilerpipeExtractorName + \" not found!\");\n    --- End diff --\n    \n    In slf4j we can better structure the catch\n    http://www.slf4j.org/faq.html#logging_performance\n    e.g.\n    ```\n    logger.debug(\"The entry is {}.\", entry);\n    ```\n", "Github user lewismc commented on a diff in the pull request:\n\n    https://github.com/apache/nutch/pull/92#discussion_r54332201\n  \n    --- Diff: src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/TikaParser.java ---\n    @@ -109,7 +114,18 @@ public Parse getParse(String url, WebPage page) {\n         HTMLDocumentImpl doc = new HTMLDocumentImpl();\n         doc.setErrorChecking(false);\n         DocumentFragment root = doc.createDocumentFragment();\n    -    DOMBuilder domhandler = new DOMBuilder(doc, root);\n    +   // DOMBuilder domhandler = new DOMBuilder(doc, root);\n    +    ContentHandler domHandler;\n    +    // Check whether to use Tika's BoilerplateContentHandler\n    +    if (useBoilerpipe) {\n    +        LOG.debug(\"Using Tikas's Boilerpipe with Extractor: \" + boilerpipeExtractorName);\n    --- End diff --\n    \n    Can also use more efficient slf4j convention\n    logger.debug(\"The entry is {}.\", entry);\n", "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/nutch/pull/92\n", "Bulk progression of all “Resolved” issues to “Closed”. Performed my lewismc on 2024-03-13."], "tasks": {"summary": "Expose Tika's boilerpipe support", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Expose Tika's boilerpipe support"}, {"question": "What is the main context?", "answer": "Tika 0.8 comes with the Boilerpipe content handler which can be used to extract boilerplate content from HTML pages. We should see how we can expose Boilerplate in the Nutch cofiguration.\n\nUse the fol"}]}}
{"issue_id": "NUTCH-962", "project": "NUTCH", "title": "max. redirects not handled correctly: fetcher stops at max-1 redirects", "status": "Closed", "priority": "Major", "reporter": "Sebastian Nagel", "assignee": "Andrzej Bialecki", "created": "2011-01-26T15:20:04.286+0000", "updated": "2011-06-25T12:53:51.587+0000", "description": "The fetcher stops following redirects one redirect before the max. redirects is reached.\n\nThe description of http.redirect.max\n> The maximum number of redirects the fetcher will follow when\n> trying to fetch a page. If set to negative or 0, fetcher won't immediately\n> follow redirected URLs, instead it will record them for later fetching.\nsuggests that if set to 1 that one redirect will be followed.\n\nI tried to crawl two documents the first redirecting by\n <meta http-equiv=\"refresh\" content=\"0; URL=./to/meta_refresh_target.html\">\nto the second with http.redirect.max = 1\nThe second document is not fetched and the URL has state GONE in CrawlDb.\n\nfetching file:/test/redirects/meta_refresh.html\nredirectCount=0\n-finishing thread FetcherThread, activeThreads=1\n - content redirect to file:/test/redirects/to/meta_refresh_target.html (fetching now)\n - redirect count exceeded file:/test/redirects/to/meta_refresh_target.html\n\nThe attached patch would fix this: if http.redirect.max is 1 : one redirect is followed.\nOf course, this would mean there is no possibility to skip redirects at all since 0\n(as well as negative values) means \"treat redirects as ordinary links\".\n\n", "comments": ["patch for 1.3 to respect count of redirects literally:\n http.redirect.max = 0 (or negative) :: treat redirects as ordinary links\n http.redirect.max = 1 :: follow max. 1 redirect\n http.redirect.max = 2 :: follow max. 2 redirects, etc.", "Committed in 1079764 (trunk) and 1079765 (1.3). Thank you!", "Bulk close of resolved issues for 1.3."], "tasks": {"summary": "max. redirects not handled correctly: fetcher stops at max-1 redirects", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "max. redirects not handled correctly: fetcher stops at max-1 redirects"}, {"question": "What is the main context?", "answer": "The fetcher stops following redirects one redirect before the max. redirects is reached.\n\nThe description of http.redirect.max\n> The maximum number of redirects the fetcher will follow when\n> trying t"}]}}
{"issue_id": "NUTCH-963", "project": "NUTCH", "title": "Add support for deleting Solr documents with STATUS_DB_GONE in CrawlDB (404 urls)", "status": "Closed", "priority": "Minor", "reporter": "Claudio Martella", "assignee": "Markus Jelsma", "created": "2011-01-26T18:00:43.150+0000", "updated": "2011-05-08T22:34:48.094+0000", "description": "When issuing recrawls it can happen that certain urls have expired (i.e. URLs that don't exist anymore and return 404).\nThis patch creates a new command in the indexer that scans the crawldb looking for these urls and issues delete commands to SOLR.", "comments": ["Thanks Claudio. I'll fix the formatting and add a command to the bin/nutch script. If there are no objections, i'd like to have this one included in the 1.3 release. Despite the fact it's not a bugfix, it is a very useful feature. Without it, users must either patch it manually or build their own deleter.", "The class works fine although i did add a commit in the reducer's close method. I propose changing the name Solr404Deleter to something more generic, _solrclean_ or _solrpurge_ etc. This is because we might also use this feature to delete documents for pages that are moved but return a HTTP permanent redirect instead of a 404.", "It would be nice to couple that with the deduplication for SOLR as well. The current mechanism is not great : it pulls ALL the documents from SOLR, finds the duplicates then issues a delete command. It would be more efficient to find the duplicates with a mapreduce on the crawldb then send the deletions to SOLR. We could also delete the urls which are definitly gone at the same time", "@Markus: about the commit, i did also consider that, i just noticed that solrdeduplication doesn't have it and put the code on the same line. Sounds reasonable to me too. For the name, everything is fine for me.\nAlso the idea to delete redirects occurred to me, we could OR them in the Mapper.\n\n@Julien: you mean to use the signature on the CrawlDB?", "Here's a patch for the bin/nutch script adding the solrclean command. The patch also contains a rule for log4j to send some output of the solrclean command to stdout as well as the log file. I also attached the SolrClean class (was Solr404Deleter) and added a commit if > 0 deletes were found.\n\n@Claudio:\nabout the redirection. Nutch doesn't seem to set the values properly. Fetching a page (for the _first time_) returning HTTP 301 results in db_redir_temp for that CrawlDB entry instead. So something is wrong here that needs a fix first. If you fetch a page that was not-modified at first but later returns a HTTP 301 it holds the db_notmodified status.", "there's a little problem in where you put the commit. When you delete a number of documents that is multiple of NUM_MAX_DELETE_REQUEST, it will not commit.\n\nI'd put it in a: if(totalDeleted > 0) right after that block", "Of course! You reset numDeletes for each batch. Thanks!", "{quote}\n@Julien: you mean to use the signature on the CrawlDB?\n{quote}\n\nyes, along with the boost and timestamp as currently done in SolrDeleteDuplicates ", "Julien, shouldn't the deduplicate mechanism kept separate from purging 404's? I agree your proposal for finding dupes is better than the current but i believe it should be kept separate because:\n- people may use a Solr update request processor for finding and deleting dupes (it has several hashing algorithm incl. a fuzzy matching)\n- controlled environments where there are no dupes don't need a 404 purger that wastes cycles on finding dupes\n\nIf so, i believe this issue can be committed for 1.3 after further testing.", "Re-dedup on SOLR side : good point, although the SOLR dedup is based on signature only IIRC and does not take the score of a doc into account. \nThe dedup/404 remover would allow to do one or both of these operations so that people can deactivate what they don't need.\n\nWe're not likely to have the new deduplication any time soon anyway so am definitely OK for adding the 404 remover in 1.3, provided as you said that is has been tested and reviewed", "Solr deduplication makes its own (fuzzy) hashes on one or more fields. Separate algorithms on different fields can be combined. It does not take into account the score of a document if you mean the index-time boost on the document. But if there is a separate score (or boost) field then a combined signature on body, title and boost will work.\n\nAll aside, i agree we should go for a single Nutch command for cleaning an index, doing dedup and/or 404 cleaning in one swift go.\n\nI'll rereview this patch and do further testing and won't forget CHANGES.txt. After that i believe we can create a new related issue for the new deduplication.", "Committed for branch-1.3 in rev 1082944.\n- new command bin/nutch solrclean <crawldb> <solrurl>\n- added solrclean to log4j to allow output to stdout\n", "Shall we create a new issue to track the progress of solrclean on the trunk? I'd like to release 1.3 soon and this issue will look open until we do it on trunk, which might take some time", "Yes!"], "tasks": {"summary": "Add support for deleting Solr documents with STATUS_DB_GONE in CrawlDB (404 urls)", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add support for deleting Solr documents with STATUS_DB_GONE in CrawlDB (404 urls)"}, {"question": "What is the main context?", "answer": "When issuing recrawls it can happen that certain urls have expired (i.e. URLs that don't exist anymore and return 404).\nThis patch creates a new command in the indexer that scans the crawldb looking f"}]}}
{"issue_id": "NUTCH-964", "project": "NUTCH", "title": "ERROR conf.Configuration - Failed to set setXIncludeAware(true)", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-01-27T13:08:37.439+0000", "updated": "2011-05-08T22:34:48.724+0000", "description": "Each executed job results in a number of occurences of the exception below:\n\n2011-01-27 13:40:34,457 ERROR conf.Configuration - Failed to set setXIncludeAware(true) for parser org.apache.xerces.jaxp.DocumentBuilderFactoryImpl@3801318b:java.lang.UnsupportedOperationException: This parser does not support specification \"null\" version \"null\"\njava.lang.UnsupportedOperationException: This parser does not support specification \"null\" version \"null\"\n        at javax.xml.parsers.DocumentBuilderFactory.setXIncludeAware(DocumentBuilderFactory.java:590)\n        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1054)\n        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1040)\n        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:980)\n        at org.apache.hadoop.conf.Configuration.get(Configuration.java:436)\n        at org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:103)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:95)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:230)\n        at org.apache.nutch.crawl.Injector.run(Injector.java:248)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Injector.main(Injector.java:238)\n\nThis can be fixed by upgrading xercesImpl from 2.6.2 to 2.9.1. If modified ivy and lib-xml's ivy configuration and can commit it. The question is, is upgrading the correct method? I've tested Nutch with 2.9.1 and except the lack of the annoying exception everything works as expected.", "comments": ["Upgrades xercesImpl from 2.6.2 to 2.9.1", "This error has been bothering me for a while, too - it's great that an upgrade fixes it and doesn't break other stuff ;) One area that was sensitive to Xerces versions in the past was the Neko parser (in parse-html) but if its tests pass then +1 to commit the patch. We should upgrade trunk too.", "Patch for Nutch 2.0", "All tests pass. Committed for branch-1.3 in rev 1064121. Also attached patch for trunk but i haven't succesfully compiled trunk yet, can someone else confirm the tests pass and the exceptions are gone?", "don't forget to add an entry in the CHANGES.txt before you commit ;-)\n", "I remembered ;). I also updated the CHANGES and added the name of the person who provided a patch for the other issue.", "cool thanks : -)\nWhat problems are you having with 2.0? Did you checkout and compile gora first?\n", "I followed Chris' instruction in some issue on Gora for trunk. It failed and i haven't found the time to figure it out since. Perhaps i should so none of you need to bother with my half-closed tickets ;)", "Have a look at Alexis' tutorial for 2.0 : http://techvineyard.blogspot.com/2010/12/build-nutch-20.html\nSome of the issues that Alexis encountered have been fixed since. The more people can use the trunk, the merrier...", "Well, just building the most recent Gora did the trick this time, Nutch trunk compiles just right with the upgraded Xerces. I'll check the tests and commit if things are working as they should. Note, the exception itself does not seem to occur in trunk.", "Committed for trunk in rev 1064169."], "tasks": {"summary": "ERROR conf.Configuration - Failed to set setXIncludeAware(true)", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "ERROR conf.Configuration - Failed to set setXIncludeAware(true)"}, {"question": "What is the main context?", "answer": "Each executed job results in a number of occurences of the exception below:\n\n2011-01-27 13:40:34,457 ERROR conf.Configuration - Failed to set setXIncludeAware(true) for parser org.apache.xerces.jaxp.D"}]}}
{"issue_id": "NUTCH-965", "project": "NUTCH", "title": "Skip parsing for truncated documents", "status": "Closed", "priority": "Major", "reporter": "Alexis", "assignee": "Lewis John McGibbney", "created": "2011-02-08T17:37:06.209+0000", "updated": "2012-02-25T04:29:44.174+0000", "description": "The issue you're likely to run into when parsing truncated FLV files is described here:\nhttp://www.mail-archive.com/user@nutch.apache.org/msg01880.html\n\nThe parser library gets stuck in infinite loop as it encounters corrupted data due to for example truncating big binary files at fetch time.", "comments": ["In the parser mapper, compare Content-Length header to the size of the content buffer to see if they match.\n\nIf this HTTP header is available and in the case that the file was truncated, skip the parsing step to avoid that the parser gets stuck in infinite loop taking up all the CPU resources.\n\n\nBefore, in the logs, we would see:\n\n{noformat}2011-02-07 14:03:34,693 WARN  parse.ParseUtil - TIMEOUT parsing http://downtownjoes.com/botb1.flv with org.apache.nutch.parse.tika.TikaParser@8c0162\n2011-02-07 14:03:34,693 WARN  parse.ParseUtil - Unable to successfully parse content http://downtownjoes.com/botb1.flv of type video/x-flv\n2011-02-07 14:04:04,725 WARN  parse.ParseUtil - TIMEOUT parsing http://downtownjoes.com/dtj.flv with org.apache.nutch.parse.tika.TikaParser@8c0162\n2011-02-07 14:04:04,725 WARN  parse.ParseUtil - Unable to successfully parse content http://downtownjoes.com/dtj.flv of type video/x-flv\n2011-02-07 14:04:34,772 WARN  parse.ParseUtil - TIMEOUT parsing http://downtownjoes.com/botb2.flv with org.apache.nutch.parse.tika.TikaParser@8c0162\n2011-02-07 14:04:34,772 WARN  parse.ParseUtil - Unable to successfully parse content http://downtownjoes.com/botb2.flv of type video/x-flv\n{noformat} \n\nAfter:\n\n{noformat}2011-02-08 09:06:54,482 INFO  parse.ParserJob - http://downtownjoes.com/botb1.flv skipped. Content of size 4527822 was truncated to 63980\n2011-02-08 09:06:54,482 INFO  parse.ParserJob - http://downtownjoes.com/dtj.flv skipped. Content of size 2692082 was truncated to 63980\n2011-02-08 09:06:54,482 INFO  parse.ParserJob - http://downtownjoes.com/botb2.flv skipped. Content of size 35496213 was truncated to 61058\n{noformat} \n\n\n", "this should be optional but activated by default\nthe parsing is also done within the fetching so it would need modifying there as well\nwould be nice to have that in 1.3 \nnote : change the title to something like \"skip parsing for truncated documents\" would be more accurate description", "Can you provide a patch for 1.4 as well Alexis?", "This would be great to get into 1.4. Do you have time to get this in Alexis? If not, I am willing to try and get it working. ", "I'll address this for both versions after we release 1.4", "Hi Guys,\n\nI would ask you's to comment as this patch is not finished yet. Although I've made the functionality a boolean configurable, I've also intentionally neglected to address the second of your points Julien, regarding FetcherJob.java.\n\nI see that the boolean parsing value is set in this class [1], but would like you to confirm if the code I'm writing should live under the public Collection object on line 138.\n\nOnce this is addressed it would be great to get a patch for trunk.\n\nThanks for anyone that can comment on this. \n\n[1] http://svn.apache.org/viewvc/nutch/branches/nutchgora/src/java/org/apache/nutch/fetcher/FetcherJob.java?view=markup", "Hi can anyone advise if I should be looking @ ParseUtil class in trunk? I'm a bit confused and Eclipse doesn't seem to be helping out much.", "Hi Lewis,\n\nFYI: I'm currently looking into this one, for both nutchgora and trunk. Picked up your patch and changed a few things here and there to make work, including Julien's remarks. Side note: your patch was generated with a hardcoded project, namely on line 2 in the patch file \"#P nutchgora\". I couldn't apply it to a project that is named differently without removing this line. (Not too much of a worry, but just to let you know.. hadn't seen anything like that before).\n\nWill get back on this.", "Tested, verified and committed with both trunk and branch. (Both with parsing during fetch and separate parsing).", "Integrated in nutch-trunk-maven #162 (See [https://builds.apache.org/job/nutch-trunk-maven/162/])\n    NUTCH-965 Skip parsing for truncated documents (Revision 1292185)\n\n     Result = SUCCESS\nferdy : \nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/conf/nutch-default.xml\n* /nutch/trunk/src/java/org/apache/nutch/fetcher/Fetcher.java\n* /nutch/trunk/src/java/org/apache/nutch/parse/ParseSegment.java\n", "Patch for Nutchgora is much more comprehensive and a far cleaner implementation. In all honesty the patch for trunk shadowns this. Thanks Ferdy.", "Integrated in Nutch-nutchgora #170 (See [https://builds.apache.org/job/Nutch-nutchgora/170/])\n    NUTCH-965 Skip parsing for truncated documents (Revision 1292184)\n\n     Result = SUCCESS\nferdy : \nFiles : \n* /nutch/branches/nutchgora/CHANGES.txt\n* /nutch/branches/nutchgora/conf/nutch-default.xml\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/fetcher/FetcherReducer.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/parse/ParserJob.java\n", "Integrated in Nutch-trunk #1766 (See [https://builds.apache.org/job/Nutch-trunk/1766/])\n    NUTCH-965 Skip parsing for truncated documents (Revision 1292185)\n\n     Result = FAILURE\nferdy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1292185\nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/conf/nutch-default.xml\n* /nutch/trunk/src/java/org/apache/nutch/fetcher/Fetcher.java\n* /nutch/trunk/src/java/org/apache/nutch/parse/ParseSegment.java\n", "Will fix the test right away.", "The test works now. The pretty obvious fix was about the invertion of the \"isTruncated(content)\" check. (Not sure what went wrong yesterday as I stated that I had verified the changes; probably made a small modification afterwards with the assumption that it could not break the code...)", "Integrated in nutch-trunk-maven #164 (See [https://builds.apache.org/job/nutch-trunk-maven/164/])\n    integrate NUTCH-965 Skip parsing for truncated documents (commit 2) (Revision 1292667)\n\n     Result = SUCCESS\nferdy : \nFiles : \n* /nutch/trunk/src/java/org/apache/nutch/fetcher/Fetcher.java\n", "Doublechecked and it seems I made a few other bugs.\n\nNutchgora:Parsing will not be performed at all when 'checkTruncated' boolean is false.\nNutchtrunk:checkTruncated flag is ignored in ParseSegment.\n\nSorry about that. I ought to expand my test coverage. Fixing them now.", "Integrated in nutch-trunk-maven #165 (See [https://builds.apache.org/job/nutch-trunk-maven/165/])\n    integrate NUTCH-965 Skip parsing for truncated documents (commit 3) (Revision 1292686)\n\n     Result = SUCCESS\nferdy : \nFiles : \n* /nutch/trunk/src/java/org/apache/nutch/fetcher/Fetcher.java\n* /nutch/trunk/src/java/org/apache/nutch/parse/ParseSegment.java\n", "OK should be done now. I crosschecked both branches and tried to keep the implementations as similar as possible.\n\n(Please note that the v3 patches are now incorrect, obviously)", "Integrated in Nutch-nutchgora #171 (See [https://builds.apache.org/job/Nutch-nutchgora/171/])\n    integrate NUTCH-965 Skip parsing for truncated documents (commit 3) (Revision 1292684)\nintegrate NUTCH-965 Skip parsing for truncated documents (commit 2) (Revision 1292679)\n\n     Result = FAILURE\nferdy : \nFiles : \n* /nutch/branches/nutchgora/src/java/org/apache/nutch/parse/ParserJob.java\n\nferdy : \nFiles : \n* /nutch/branches/nutchgora/src/java/org/apache/nutch/fetcher/FetcherReducer.java\n", "Please note that the failure in Nutch-nutchgora #171 is unrelated. (It's the cursed TestAPI)", "Yeah this is confirmed Ferdy. I spun a build and your right. Another headache to deal with :) Relentless!", "Integrated in Nutch-trunk #1767 (See [https://builds.apache.org/job/Nutch-trunk/1767/])\n    integrate NUTCH-965 Skip parsing for truncated documents (commit 3) (Revision 1292686)\nintegrate NUTCH-965 Skip parsing for truncated documents (commit 2) (Revision 1292667)\n\n     Result = SUCCESS\nferdy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1292686\nFiles : \n* /nutch/trunk/src/java/org/apache/nutch/fetcher/Fetcher.java\n* /nutch/trunk/src/java/org/apache/nutch/parse/ParseSegment.java\n\nferdy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1292667\nFiles : \n* /nutch/trunk/src/java/org/apache/nutch/fetcher/Fetcher.java\n", "has this been fixed now?", "Hi Markus,\n\nFor nutchtrunk I performed the following testcrawls and it worked as expected (for urls that are NOT truncated)\n-fetching and separate parsing (parser.skip.truncated to true)\n-fetching with parsing (parser.skip.truncated to true)\n-fetching and separate parsing (parser.skip.truncated to false)\n-fetching with parsing (parser.skip.truncated to false)\n\nI did the same for nutchgora. So this is to verify that for nontruncated urls everything works as before.\n\nFor urls that _are_ truncated, I debugged a crawl and artifically changed the size to check that parsing is skipped. But only when the parser.skip.truncated is set to true. This works too.\n\nIn short, yes it has been fixed.\n", "Hi Ferdy,\n\nWith a parsing fetcher on trunk we see the ParseStatus.success counter rarely being incremented. A test crawl succesfully fetches 10.000 records but the success counter hangs around 15 records. Most, if not all, fetched pages are well below the truncating threshold.\n\nCheers", "Ok that's it, I have reverted the changes completely. I am not sure what your cause is exactly, but I give you benefit of the doubt. Trunk and nutchgora are back to their previous states. Sorry for the inconvenience.\n\nLewis, could you reopen this issue. Have my mind on some other matters now, but I will look later back at this one.", "Hmm, cleaning and rebuilding the job fixes that issue here. Please ignore :)", "I couldn't find an issue in your code either so i finally assumed it has to be my build messing things up. It works as expected now.\n", "Ok I will recommit it. Luckily I did notice a minor thing in nutchtrunk in Fetcher:\n\nWhen a fetch is skipped because of truncation, the following code was also skipped:\n\nif (parseResult == null) {\n              byte[] signature =\n                SignatureFactory.getSignature(getConf()).calculate(content,\n                    new ParseStatus().getEmptyParse(conf));\n              datum.setSignature(signature);\n            }\n\n", "Integrated in nutch-trunk-maven #169 (See [https://builds.apache.org/job/nutch-trunk-maven/169/])\n    REVERT NUTCH-965 Skip parsing for truncated document (Revision 1293225)\n\n     Result = SUCCESS\nferdy : \nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/conf/nutch-default.xml\n* /nutch/trunk/src/java/org/apache/nutch/fetcher/Fetcher.java\n* /nutch/trunk/src/java/org/apache/nutch/parse/ParseSegment.java\n", "Recommitted. Thanks all.", "Integrated in nutch-trunk-maven #170 (See [https://builds.apache.org/job/nutch-trunk-maven/170/])\n    RECOMMIT NUTCH-965 Skip parsing for truncated document (Revision 1293278)\n\n     Result = SUCCESS\nferdy : \nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/conf/nutch-default.xml\n* /nutch/trunk/src/java/org/apache/nutch/fetcher/Fetcher.java\n* /nutch/trunk/src/java/org/apache/nutch/parse/ParseSegment.java\n", "Integrated in Nutch-nutchgora #174 (See [https://builds.apache.org/job/Nutch-nutchgora/174/])\n    RECOMMIT NUTCH-965 Skip parsing for truncated document (Revision 1293277)\nREVERT NUTCH-965 Skip parsing for truncated document (Revision 1293228)\n\n     Result = SUCCESS\nferdy : \nFiles : \n* /nutch/branches/nutchgora/CHANGES.txt\n* /nutch/branches/nutchgora/conf/nutch-default.xml\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/fetcher/FetcherReducer.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/parse/ParserJob.java\n\nferdy : \nFiles : \n* /nutch/branches/nutchgora/CHANGES.txt\n* /nutch/branches/nutchgora/conf/nutch-default.xml\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/fetcher/FetcherReducer.java\n* /nutch/branches/nutchgora/src/java/org/apache/nutch/parse/ParserJob.java\n", "Integrated in Nutch-trunk #1768 (See [https://builds.apache.org/job/Nutch-trunk/1768/])\n    RECOMMIT NUTCH-965 Skip parsing for truncated document (Revision 1293278)\nREVERT NUTCH-965 Skip parsing for truncated document (Revision 1293225)\n\n     Result = SUCCESS\nferdy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1293278\nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/conf/nutch-default.xml\n* /nutch/trunk/src/java/org/apache/nutch/fetcher/Fetcher.java\n* /nutch/trunk/src/java/org/apache/nutch/parse/ParseSegment.java\n\nferdy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1293225\nFiles : \n* /nutch/trunk/CHANGES.txt\n* /nutch/trunk/conf/nutch-default.xml\n* /nutch/trunk/src/java/org/apache/nutch/fetcher/Fetcher.java\n* /nutch/trunk/src/java/org/apache/nutch/parse/ParseSegment.java\n"], "tasks": {"summary": "Skip parsing for truncated documents", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Skip parsing for truncated documents"}, {"question": "What is the main context?", "answer": "The issue you're likely to run into when parsing truncated FLV files is described here:\nhttp://www.mail-archive.com/user@nutch.apache.org/msg01880.html\n\nThe parser library gets stuck in infinite loop "}]}}
{"issue_id": "NUTCH-966", "project": "NUTCH", "title": "Behavior of NOINDEX,FOLLOW is not intuitive", "status": "Closed", "priority": "Minor", "reporter": "Josh Pavel", "assignee": null, "created": "2011-02-09T14:31:44.851+0000", "updated": "2019-10-13T22:35:38.323+0000", "description": "If a page has NOINDEX,FOLLOW for the ROBOTS metatag, Nutch will still create a document that can be found in the index via metatag or URL matching.  Instead, Nutch should rely on doc or parse metadata but nothing should be stored by the html parser. (thanks to Julien Nioche for helping me to understand the issue). ", "comments": ["A plugin that corrects the issue (again, thanks to Julien Nioche)\n\npublic class MetaNoIndexingFilter implements IndexingFilter {\n    public static final Log LOG =\nLogFactory.getLog(MetaNoIndexingFilter.class);\n\n    private Configuration conf;\n\n    public NutchDocument filter(NutchDocument doc, Parse parse, Text url,\n            CrawlDatum datum, Inlinks inlinks) throws IndexingException {\n        // should rely on doc or parse metadata but nothing stored\n        // by the html parser\n        String text = parse.getText();\n        String title = parse.getData().getTitle();\n        if ((text == null || text.equals(\"\"))\n                && (title == null || title.equals(\"\"))) {\n            // no text -> no indexing\n            return null;\n        }\n        return doc;\n    }\n\n    public void setConf(Configuration conf) {\n        this.conf = conf;\n    }\n\n    public Configuration getConf() {\n        return this.conf;\n    }\n\n}", "Not index a document just because it has no Text and Title? Maybe important content was written to other fields by parse filters.\n\nI think this solution is cleaner:\nhttps://github.com/saintybalboa/nutchmetarobots\n\nSame idea, but discards a document only when noindex directive is encountered.", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "Behavior of NOINDEX,FOLLOW is not intuitive", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Behavior of NOINDEX,FOLLOW is not intuitive"}, {"question": "What is the main context?", "answer": "If a page has NOINDEX,FOLLOW for the ROBOTS metatag, Nutch will still create a document that can be found in the index via metatag or URL matching.  Instead, Nutch should rely on doc or parse metadata"}]}}
{"issue_id": "NUTCH-967", "project": "NUTCH", "title": "Upgrade to Tika 0.9", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": "Julien Nioche", "created": "2011-02-17T16:02:25.546+0000", "updated": "2013-05-02T02:29:38.216+0000", "description": null, "comments": ["Note : Tika 0.9 cause the parse-zip plugin to crash in 1.3 (it hasn't been ported to 2.0 yet)", "That didn't show up in test nor in a crawl, but i'm not using parse-zip anyway. How to procede with a fix?", "patch for Tika 0.9 on Nutch 1.3\n\ncurrently fails when running the tests e.g. ant -f src/plugin/parse-zip/build.xml test", "Surprisingly I managed to run the tests on a different machine. The versions of java and ant on the laptop where parse-zip test fails are : \n\njava version \"1.6.0_24\"\nJava(TM) SE Runtime Environment (build 1.6.0_24-b07)\nJava HotSpot(TM) Server VM (build 19.1-b02, mixed mode)\n\nApache Ant version 1.7.1 compiled on July 2 2010\n", "I applied your patch (seems i didn't properly reconfigure plugin's own ivy.xml). Doing the usual ant test does pass all tests. Doing test manually on parse-zip's build.xml fails lik this:\n\nmarkus@midas:~/projects/apache/nutch/branches/branch-1.3$ ant -f src/plugin/parse-zip/build.xml test\nBuildfile: src/plugin/parse-zip/build.xml\n\nBUILD FAILED\n/home/markus/projects/apache/nutch/branches/branch-1.3/src/plugin/parse-zip/build.xml:20: The following error occurred while executing this line:\n/home/markus/projects/apache/nutch/branches/branch-1.3/src/plugin/build-plugin.xml:46: Problem: failed to create task or type antlib:org.apache.ivy.ant:settings\nCause: The name is undefined.\nAction: Check the spelling.\nAction: Check that any custom tasks/types have been declared.\nAction: Check that any <presetdef>/<macrodef> declarations have taken place.\nNo types or tasks have been defined in this namespace yet\n\nThis appears to be an antlib declaration. \nAction: Check that the implementing library exists in one of:\n        -/usr/share/ant/lib\n        -/home/markus/.ant/lib\n        -a directory added on the command line with the -lib argument\n\n\nTotal time: 0 seconds\n\nIs this what you're getting as well? I have the same Java version, my ant is a bit more recent though:\nApache Ant version 1.7.1 compiled on September 8 2010\n\n", "Strange. The call to ant test should have failed. Could you try calling ant test-plugins?\nAm not getting at the same error as you : the test for parse-zip runs but fails. in your case there seems to be some configuration problem as it does not start at all\n ", "ant test-plugins\n\nBUILD SUCCESSFUL\nTotal time: 20 seconds\n", "Julien, why doesn't your patch modify tika-parse plugin.xml to use tika-parsers-0.9 instead of tika-parsers-0.7?\nTrying to do so I get exception (for both html and pdfs): \n\nException in thread \"main\" java.io.IOException: Job failed!\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1252)\n    at org.apache.nutch.parse.ParseSegment.parse(ParseSegment.java:156)\n    at org.apache.nutch.parse.ParseSegment.run(ParseSegment.java:177)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.nutch.parse.ParseSegment.main(ParseSegment.java:163)\n\nIt's enough to set it back to 0.7 to have it work. This is not an issue with html only but also pdfs.", "Julien, your patch didn't include the Apache James Mime4j jar. It seems Tika 0.9 (or earlier) has this dependancy. I've modified parse-tika's plugin.xml and ivy.xml to use 0.9 parsers and load and copy the Mime4j jar. This patch is built on top of your patch! I can now parse without having parse-html enabled and everything seems fine. \n\nDoing an ant test still looks fine but our issue with parse-zip and your failing tests remain. \n\nEDIT: for some reason Apache Mime4j 0.6 and tike-parsers 0.7 are copied over as well and there is a tika-core-0.7 going too!? Double checked for occurences but can't find any. Bahh", "bq. Julien, why doesn't your patch modify tika-parse plugin.xml to use tika-parsers-0.9 instead of tika-parsers-0.7?\n\nWhy? Because I forgot to modify it, that's why. Note that the dependencies inherited from Tika should be update in plugin.xml, not just the main Tika one.\n\nbq. Julien, your patch didn't include the Apache James Mime4j jar. It seems Tika 0.9 (or earlier) has this dependency. \n\nIt seems to be pulled along with the other dependencies (try 'ant -f src/plugin/parse-tika/build-ivy.xml' and look at the content of the lib dir in parse-tika) so I don't think we need to do anything special about it - apart from adding it to plugin.xml like everything else.\n\nWill look at the tika-parser.0.7 issue\n\n\n\n\n", "Updated patch which changes plugin.xml so that it reflects the dependencies used by Tika 0.9", "OK guys, I've attached a new version of the patch which updates the content of plugin.xml (thanks Gabriele for reminding me). The tests now run succesfully.\n\nThe reason why I got it to work on my other machine was probably that I had done it properly then (grin).\n\nThere might be a few things we could do on the tika-parser like getting rid of the duplicate tika config object but we'll track that in a separate JIRA if necessary.\n\nLeaving the issue open as we still need to update the trunk. In the meantime I'll commit this patch on 1.3 tomorrow unless of course someone finds a good reason not to.\n\nThanks!\n\nJulien \n\n\n ", "Dependencies are in place now and all is well in 1.3 and various Boilerplate extractors work as well. If you commit, perhaps it's wise to add parse-tika for HTML mime's to parse-plugins (now there's only parse-html). Then we only have to set parse.includes in our configuration to switch between parse-tika and parse-html (if there is any need to).", "bq. perhaps it's wise to add parse-tika for HTML mime's to parse-plugins (now there's only parse-htm. Then we only have to set parse.includes in our configuration to switch between parse-tika and parse-html (if there is any need to).\n\nIf you look at parse-tika's plugin.xml you'll see that it is associated with any mimetype (value=*) so it does not need to be explicitely linked to the html mimetype in parse-plugins. I don't remember the exact mechanism, I think it tries to use any parser set explicitly for a given mime-type and if there aren't any or if they fail then it tries the default parsers.\n\nWe need to test parse-tika on html to make sure that it behaves exactly like the legacy html parser, which wasn't the case with 0.7; in the meantime it is advised to use parse-html and so we'll leave parse-html in parse-plugins.xml\n\nHope it makes sense\n\n\n", "trunk : Committed revision 1090181\n1.3 : Committed revision 1090182\n\n", "Integrated in Nutch-trunk #1530 (See [https://builds.apache.org/job/Nutch-trunk/1530/])\n    "], "tasks": {"summary": "Upgrade to Tika 0.9", "classification": "task", "qa_pairs": []}}
{"issue_id": "NUTCH-968", "project": "NUTCH", "title": "Crawling - File Error 404 when fetching file with an chinese word in the file name ", "status": "Open", "priority": "Major", "reporter": "Dominic Xu", "assignee": null, "created": "2011-03-04T01:19:29.441+0000", "updated": "2025-07-09T20:25:55.100+0000", "description": "I am performing a local file system crawling.\nMy problem is the following: all files that contain some chinese words in the file name do not get crawled.\nexample:\nfetching  /mnt/中文.txt\n\nI will get the error :org.apache.nutch.protocol.file.FileError: File Error: 404.\n\nand I read ISSUE NUTCH-824 https://issues.apache.org/jira/browse/NUTCH-824\nand I patch with trunk : Committed revision 1056394.\n\nbut the bug no fix.\n\nI fix the problem by modifying  the file : src/plugin/protocol-file/src/java/org/apache/nutch/protocol/file/FileResponse.java \n\n262    for (int i=0; i<list.length; i++) {\n263      f = list[i];\n264      String name = f.getName();\n265 +try {\n266 +      // specify the encoding via the config later?\n267 +      name = java.net.URLEncoder.encode(name, \"UTF-8\");\n268 +    } catch (UnsupportedEncodingException ex) {\n269 +    }\n270 +\n271 String time = HttpDateFormat.toString(f.lastModified());\n\nThere is must encode by utf8.\n\nand I modify the content with meta tag.\n251- StringBuffer x = new StringBuffer(\"<html><head>\");\n251+ StringBuffer x = new StringBuffer(\"<html><head><meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=utf-8\\\" />\");\n\n\n\n ", "comments": ["Hi, can you submit the modification as a patch?"], "tasks": {"summary": "Crawling - File Error 404 when fetching file with an chinese word in the file name ", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Crawling - File Error 404 when fetching file with an chinese word in the file name "}, {"question": "What is the main context?", "answer": "I am performing a local file system crawling.\nMy problem is the following: all files that contain some chinese words in the file name do not get crawled.\nexample:\nfetching  /mnt/中文.txt\n\nI will get the"}]}}
{"issue_id": "NUTCH-969", "project": "NUTCH", "title": "protocol-ftp with configurable encoding", "status": "Closed", "priority": "Major", "reporter": "Te mule", "assignee": null, "created": "2011-03-19T13:36:41.688+0000", "updated": "2019-10-13T22:36:32.935+0000", "description": "I use Nutch fetching a chinease ftp sites.But fetch ftp url's encoding is wrong.FTP used gb2312 encoding.but nutch used UTF-8 encoding.I want to change encoding to gb2312.How should I do?I hope nutch next version can change FTP encoding.Thank you.", "comments": ["The problem is that the URL encoded path (percent-encoded UTF-8 as per [[RFC3986|http://tools.ietf.org/html/rfc3986#section-2.5]] has to be \"translated\" into Ftp commands in the encoding used by the ftp server (or its file system).\nWe could make the encoding configurable as property \"ftp.encoding\" and call [[FTP.setControlEncoding|http://commons.apache.org/proper/commons-net/apidocs/org/apache/commons/net/ftp/FTP.html#setControlEncoding%28java.lang.String%29]].", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "protocol-ftp with configurable encoding", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "protocol-ftp with configurable encoding"}, {"question": "What is the main context?", "answer": "I use Nutch fetching a chinease ftp sites.But fetch ftp url's encoding is wrong.FTP used gb2312 encoding.but nutch used UTF-8 encoding.I want to change encoding to gb2312.How should I do?I hope nutch "}]}}
{"issue_id": "NUTCH-97", "project": "NUTCH", "title": "make datanode starting port configurable", "status": "Closed", "priority": "Minor", "reporter": "Stefan Groschupf", "assignee": null, "created": "2005-09-26T18:17:40.000+0000", "updated": "2005-09-29T22:28:33.000+0000", "description": "Actually the ndfs datanode port is hardcoded to 7000. In case 7000 is blocked the datanode will iterate up until it find a free port.\nThis behavior is difficult to manage in case a fire wall is installed.\nIt would be good in case it is possible to be able configure the port, as it is possible with the namenode port as well.", "comments": ["Is duplicate and fixed with:\nhttp://issues.apache.org/jira/browse/NUTCH-99"], "tasks": {"summary": "make datanode starting port configurable", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "make datanode starting port configurable"}, {"question": "What is the main context?", "answer": "Actually the ndfs datanode port is hardcoded to 7000. In case 7000 is blocked the datanode will iterate up until it find a free port.\nThis behavior is difficult to manage in case a fire wall is instal"}]}}
{"issue_id": "NUTCH-970", "project": "NUTCH", "title": "Injector job crashes with MySQL with table collation set to utf8_general_ci", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": null, "created": "2011-03-22T09:51:32.852+0000", "updated": "2014-04-16T22:28:00.304+0000", "description": "Running the injector of trunk with an already existing database where the default collation is utf8_* or ucs2_* the following GoraException is thrown:\n\nInjectorJob: starting\nInjectorJob: urlDir: urls\nInjectorJob: org.apache.gora.util.GoraException: java.io.IOException: com.mysql.jdbc.exceptions.MySQLSyntaxErrorException: Column length too big for column 'text' (max = 21845); use BLOB or TEXT instead\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:110)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:93)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:43)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:227)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:266)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:276)\nCaused by: java.io.IOException: com.mysql.jdbc.exceptions.MySQLSyntaxErrorException: Column length too big for column 'text' (max = 21845); use BLOB or TEXT instead\n        at org.apache.gora.sql.store.SqlStore.createSchema(SqlStore.java:226)\n        at org.apache.gora.sql.store.SqlStore.initialize(SqlStore.java:172)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:81)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:104)\n        ... 7 more\nCaused by: com.mysql.jdbc.exceptions.MySQLSyntaxErrorException: Column length too big for column 'text' (max = 21845); use BLOB or TEXT instead\n        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:936)\n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:2985)\n        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1631)\n        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:1723)\n        at com.mysql.jdbc.Connection.execSQL(Connection.java:3283)\n        at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1332)\n        at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:1604)\n        at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:1519)\n        at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:1504)\n        at org.apache.gora.sql.store.SqlStore.createSchema(SqlStore.java:224)\n        ... 10 more", "comments": ["Markus can you reproduce this? As I mentioned a while ago on the lists, I have been getting errors with a similar aesthetic look but what I fear are completely disjoint characteristics. They all appear @ injecting so they definitely share some commonality. ", "I haven't got a working trunk with backend anymore. At the time using Gora 0.1-incubating it happened consistently. Little doubt it's still an issue.", "I'm the same Markus. I have been working towards debugging and fixing the trunk test failures before I progess attempting to sort or pass my opinion on any trunk patches/issues.\n\nThis is quite concerning as I now belive that Trunk is completekly broken. I am getting no further than 'similar' exceptions/errors/failures when I attempt to use Nutch trunk for injecting URLs... this subsequently means that I cannot use trunk at all...", "Set and Classify", "We won't fix this."], "tasks": {"summary": "Injector job crashes with MySQL with table collation set to utf8_general_ci", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Injector job crashes with MySQL with table collation set to utf8_general_ci"}, {"question": "What is the main context?", "answer": "Running the injector of trunk with an already existing database where the default collation is utf8_* or ucs2_* the following GoraException is thrown:\n\nInjectorJob: starting\nInjectorJob: urlDir: urls\n"}]}}
{"issue_id": "NUTCH-971", "project": "NUTCH", "title": "IndexMerger produces indexes itself cannot merge anymore", "status": "Closed", "priority": "Minor", "reporter": "Gabriele Kahlout", "assignee": null, "created": "2011-03-26T10:34:00.421+0000", "updated": "2011-03-27T09:16:26.697+0000", "description": "Here's what I do:\n\n1. index the fetched segs\n$ rm -r $new_indexes $temp_indexes\n$ bin/nutch index $new_indexes $it_crawldb crawl/linkdb crawl/segments/*\n \nI examine the index with luke and it's as expected.\n\n2. merge the new index with the previous\n$ bin/nutch merge $temp_indexes $new_indexes $indexes\nIndexMerger: starting at 2011-03-26 10:24:58\nIndexMerger: merging indexes to: crawl/temp_indexes\nAdding file:/Users/simpatico/nutch-1.2/crawl/new_indexes/part-00000\nIndexMerger: finished at 2011-03-26 10:24:59, elapsed: 00:00:01\n\nOn the first iteration, when $indexes is empty it works fine by essentially duplicating  $new_indexes into $temp_indexes.\nBut on the 2nd iteration, after I mv $temp_indexes $indexes[1] the merged index $temp_indexes contains only #new_indexes and nothing from $indexes, which indeed still contains the data from the previous iteration. That is, it doesn't merge.\nThis unexpected merge behavior is NOT symmetric, i.e.\n\n$ bin/nutch merge $temp_indexes $indexes $new_indexes\nIndexMerger: starting at 2011-03-26 10:32:15\nIndexMerger: merging indexes to: crawl/temp_indexes\nAdding file:/Users/simpatico/nutch-1.2/crawl/new_indexes/part-00000\nIndexMerger: finished at 2011-03-26 10:32:16, elapsed: 00:00:01\n\nThe morale of the story is that a merged index cannot be merged with another, i.e. bin/nutch merge is meant to  merge only 2 indeces generated with bin/nutch index (or solrindex, perhaps).\nThe difference between the 2 indeces I can tell is that the merged index doesn't contain file index_done (and a hidden companion), but adding those to the merged index before merging it again doesn't solve either.\n\nThe way/workaround to make the merged index equivalent to the bin/nutch index generated index seems to be putting it in a \"part\" subdirectory:\n\nbin/nutch merge crawl/temp_indexes/part-1 crawl/indexes crawl/new_indexes\nIndexMerger: starting at 2011-03-26 11:18:10\nIndexMerger: merging indexes to: crawl/temp_indexes/part-1\nAdding file:/Users/simpatico/nutch-1.2/crawl/indexes/part-1\nAdding file:/Users/simpatico/nutch-1.2/crawl/new_indexes/part-00000\nIndexMerger: finished at 2011-03-26 11:18:12, elapsed: 00:00:01\n\nWhere was this documented? I'd recommend rather not documenting but have IndexMerger handle it as in the attached patch.", "comments": ["Checks if output path ends in a part dir and if not adds it.", "1.3 and 2.0 rely on SOLR for the indexing and search. This patch deals with the legacy Lucene-bsaed indexing and won't be applied to the code.\nNutch-users are encouraged to migrate to SOLR for indexing as this will be maintained in future versions of Nutch.\nYour patch should be useful for users who have to use 1.2 or older versions though, thanks for sharing it.", "I expect that installing solr and then replacing index with solrindex in 1. the merge should work. Am I correct?", "You will need to reindex your docs using the solrindex command. There is no need for merging the indices as SOLR will do that by updating its existing index. "], "tasks": {"summary": "IndexMerger produces indexes itself cannot merge anymore", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "IndexMerger produces indexes itself cannot merge anymore"}, {"question": "What is the main context?", "answer": "Here's what I do:\n\n1. index the fetched segs\n$ rm -r $new_indexes $temp_indexes\n$ bin/nutch index $new_indexes $it_crawldb crawl/linkdb crawl/segments/*\n \nI examine the index with luke and it's as exp"}]}}
{"issue_id": "NUTCH-972", "project": "NUTCH", "title": "Mergedb doesn't merge with empty directory, as is the case with merge (for indexes)", "status": "Closed", "priority": "Minor", "reporter": "Gabriele Kahlout", "assignee": null, "created": "2011-03-27T09:10:19.068+0000", "updated": "2011-06-25T12:53:50.854+0000", "description": "Just an issue of unexpected behavior. This series of commands works with bin/nutch merge to merge indexes but not with crawldb.\n\nallcrawldb=\"crawl/allcrawldb\"\ntemp_crawldb=\"crawl/temp_crawldb\"\nmerge_dbs=\"$it_crawldb $allcrawldb\"\n\t\n#\tif [[ ! -d $allcrawldb ]]\n#\tthen\n#\t\tmerge_dbs=\"$it_crawldb\"\n#\tfi\n# uncomment the above and mergedb will work fine.\t\nbin/nutch mergedb $temp_crawldb $merge_dbs\t\nrm -r $it_crawldb $allcrawldb crawl/segments crawl/linkdb\nmv $temp_crawldb $allcrawldb\n\nThis is the exception that occurs:\n\nbin/nutch mergedb crawl/temp_crawldb crawl/crawldb crawl/allcrawldb\nCrawlDb merge: starting at 2011-03-27 10:13:06\nAdding crawl/crawldb\nAdding crawl/allcrawldb\nCrawlDb merge: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/simpatico/nutch-1.2/crawl/allcrawldb/current\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:190)\n\tat org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:44)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:201)\n\tat org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:810)\n\tat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:781)\n\tat org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1249)\n\tat org.apache.nutch.crawl.CrawlDbMerger.merge(CrawlDbMerger.java:126)\n\tat org.apache.nutch.crawl.CrawlDbMerger.run(CrawlDbMerger.java:187)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.crawl.CrawlDbMerger.main(CrawlDbMerger.java:159)\n\nBeside the scripting workaround I've attached a patch which skips adding the empty folder to the collection of dbs to merge. I've also added it a log of which dbs actually get added, consistent with merge interface.\n", "comments": ["Committed revision 1090199.\n\nThanks Gabriele. In the future could you use 'svn diff' to generate patches? See [http://wiki.apache.org/nutch/Becoming_A_Nutch_Developer] for best practices", "Bulk close of resolved issues for 1.3."], "tasks": {"summary": "Mergedb doesn't merge with empty directory, as is the case with merge (for indexes)", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Mergedb doesn't merge with empty directory, as is the case with merge (for indexes)"}, {"question": "What is the main context?", "answer": "Just an issue of unexpected behavior. This series of commands works with bin/nutch merge to merge indexes but not with crawldb.\n\nallcrawldb=\"crawl/allcrawldb\"\ntemp_crawldb=\"crawl/temp_crawldb\"\nmerge_d"}]}}
{"issue_id": "NUTCH-973", "project": "NUTCH", "title": "Remove Segment Merger in 1.3", "status": "Closed", "priority": "Minor", "reporter": "Julien Nioche", "assignee": null, "created": "2011-03-27T17:07:33.578+0000", "updated": "2011-04-01T14:20:53.004+0000", "description": "The code for the segment merging is still in 1.3, as far as I understand its original function it was mostly useful for having a single data structure where the search app could get the cached data from. Now that we've delegated the indexing and search to SOLR we don't really need to worry about the cache anymore. Would it make sense to purge it or do you guys think it would still be useful? ", "comments": ["I'm not sure we should. In 1.x fetches still generate loose segments which i just might want to merge for simplicity. For example, i have a test setup where multiple segments are generated each day, then they get merged so i have a single segment per day. It's a bit easier to maintain and reindex fewer segments. I do delete segments older than the fetch interval but in my case that's just deleting one segment a day. This is in a local environment.", "You are right, let's leave it for now. It won't be a problem once we're on 2.0 anyway"], "tasks": {"summary": "Remove Segment Merger in 1.3", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove Segment Merger in 1.3"}, {"question": "What is the main context?", "answer": "The code for the segment merging is still in 1.3, as far as I understand its original function it was mostly useful for having a single data structure where the search app could get the cached data fr"}]}}
{"issue_id": "NUTCH-974", "project": "NUTCH", "title": "Parsing Error in Nutch 1.2 on Windows7", "status": "Closed", "priority": "Major", "reporter": "Niksa Jakovljevic", "assignee": "Markus Jelsma", "created": "2011-04-05T08:20:19.629+0000", "updated": "2013-05-22T03:53:21.864+0000", "description": "Hello World example of crawling does not work with Nutch 1.2 libs, but works fine with Nutch 1.1 libs. Note that same configuration is used in both Nutch 1.2 and Nutch 1.1.\n\nNutch 1.2 always throws following exception:\n\n2011-04-01 16:33:45,177 WARN  parse.ParseUtil - Unable to successfully parse content http://www.test.com/ of type text/html\n2011-04-01 16:33:45,177 WARN  fetcher.Fetcher - Error parsing: http://www.test.com/: failed(2,200): org.apache.nutch.parse.ParseException: Unable to successfully parse content\n\n\nThanks,\n\nNiksa Jakovljevic", "comments": ["Niksa, i tested a fetch and parse cycle of that URL with both Nutch 1.1 and Nutch 1.2 without any problems. You have something misconfigured, probably in somewhere in parse-plugins or something. Next time, please open a thread first on the Nutch user mailings list before opening an issue in Jira.\n\nThanks.", "Hi Markus,\n\nas I said I was using the same conf folder in both cases (nutch 1.2 and 1.1) so I guess configuration is not the issue. \nI didn't said that I started crawling using API, not script. I'll paste my code so you can check again.\n\nThanks!\n\npublic void crawl() throws IOException {\n\t\tConfiguration conf = NutchConfiguration.createCrawlConfiguration();\n\t\t\n\t\tconf.set(\"http.agent.name\", \"Test\");\n\t\tconf.set(\"http.agent.description\", \"Test Desc\");\n\t\tconf.set(\"http.agent.url\", \"testAgent\");\n\t\tconf.set(\"http.agent.email\", \"test@test.com\");\n\t\t\n\t\tNutchJob job = new NutchJob(conf);\n\t\tPath rootUrlDir = new Path(\"D:/Development/crawler/url\");\n\t\tPath dir = new Path(\"D:/tmp/crawl-\" + getDate());\n\t\tint threads = job.getInt(\"fetcher.threads.fetch\", 2);\n\t\tint depth = 2;\n\t\tlong topN = Long.MAX_VALUE;\n\t\tString indexerName = \"solr\";\n\t\tString solrUrl = \"http://localhost:8081/solr/\";\n\n\t\tboolean isSolrIndex = StringUtils.equalsIgnoreCase(indexerName, \"solr\");\n\t\tFileSystem fs = FileSystem.get(job);\n\n\t\tPath crawlDb = new Path(dir + \"/crawldb\");\n\t\tPath linkDb = new Path(dir + \"/linkdb\");\n\t\tPath segments = new Path(dir + \"/segments\");\n\t\tPath indexes = new Path(dir + \"/indexes\");\n\t\tPath index = new Path(dir + \"/index\");\n\n\t\tPath tmpDir = job.getLocalPath(\"crawl\" + Path.SEPARATOR + getDate());\n\t\tInjector injector = new Injector(conf);\n\t\tGenerator generator = new Generator(conf);\n\t\tFetcher fetcher = new Fetcher(conf);\n\t\tParseSegment parseSegment = new ParseSegment(conf);\n\t\tCrawlDb crawlDbTool = new CrawlDb(conf);\n\t\tLinkDb linkDbTool = new LinkDb(conf);\n\t\t\n\t\t // initialize crawlDb\n\t    injector.inject(crawlDb, rootUrlDir);\n\t    int i;\n\t    for (i = 0; i < depth; i++) {             // generate new segment\n\t      Path[] segs = generator.generate(crawlDb, segments, -1, topN, System\n\t          .currentTimeMillis());\n\t      if (segs == null) {\n\t        LOG.info(\"Stopping at depth=\" + i + \" - no more URLs to fetch.\");\n\t        break;\n\t      }\n\t      fetcher.fetch(segs[0], threads, org.apache.nutch.fetcher.Fetcher.isParsing(conf));  // fetch it\n\t      if (!Fetcher.isParsing(job)) {\n\t        parseSegment.parse(segs[0]);    // parse it, if needed\n\t      }\n\t      crawlDbTool.update(crawlDb, segs, true, true); // update crawldb\n\t    }\n\t    if (i > 0) {\n\t      linkDbTool.invert(linkDb, segments, true, true, false); // invert links\n\n\t      // index, dedup & merge\n\t      FileStatus[] fstats = fs.listStatus(segments, HadoopFSUtil.getPassDirectoriesFilter(fs));\n\t      if (isSolrIndex) {\n\t        SolrIndexer indexer = new SolrIndexer(conf);\n\t        indexer.indexSolr(solrUrl, crawlDb, linkDb, \n\t            Arrays.asList(HadoopFSUtil.getPaths(fstats)));\n\t      }\n\t      else {\n\t        \n\t        DeleteDuplicates dedup = new DeleteDuplicates(conf);        \n\t        if(indexes != null) {\n\t          // Delete old indexes\n\t          if (fs.exists(indexes)) {\n\t            LOG.info(\"Deleting old indexes: \" + indexes);\n\t            fs.delete(indexes, true);\n\t          }\n\n\t          // Delete old index\n\t          if (fs.exists(index)) {\n\t            LOG.info(\"Deleting old merged index: \" + index);\n\t            fs.delete(index, true);\n\t          }\n\t        }\n\t        \n\t        Indexer indexer = new Indexer(conf);\n\t        indexer.index(indexes, crawlDb, linkDb, \n\t            Arrays.asList(HadoopFSUtil.getPaths(fstats)));\n\t        \n\t        IndexMerger merger = new IndexMerger(conf);\n\t        if(indexes != null) {\n\t          dedup.dedup(new Path[] { indexes });\n\t          fstats = fs.listStatus(indexes, HadoopFSUtil.getPassDirectoriesFilter(fs));\n\t          merger.merge(HadoopFSUtil.getPaths(fstats), index, tmpDir);\n\t        }\n\t      }\n\t    }\n\t    LOG.info(\"crawl finished: \" + dir); \n\t}\n\n", "Hi,\n\nNutch 1.1 and 1.2 don't ship with the same configuration. The crawl and parse works as expected using the configuration shipped with the releases and using the bin/nutch shell commands."], "tasks": {"summary": "Parsing Error in Nutch 1.2 on Windows7", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Parsing Error in Nutch 1.2 on Windows7"}, {"question": "What is the main context?", "answer": "Hello World example of crawling does not work with Nutch 1.2 libs, but works fine with Nutch 1.1 libs. Note that same configuration is used in both Nutch 1.2 and Nutch 1.1.\n\nNutch 1.2 always throws fo"}]}}
{"issue_id": "NUTCH-975", "project": "NUTCH", "title": "Fix missing/wrong headers in source files", "status": "Closed", "priority": "Blocker", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-04-05T11:31:25.363+0000", "updated": "2011-06-25T12:53:50.679+0000", "description": "It seems several source files still do not contain the proper ASL headers. This includes older core in 1.3 (indexer.NutchField etc) and recent code in 2.0 (API for instance). This should be fixed (yet again). So if you spot one ;)", "comments": ["Here's a patch for 1.3 for files:\nsrc/java/org/apache/nutch/tools/Benchmark.java\nsrc/java/org/apache/nutch/parse/ParseCallable.java\nsrc/java/org/apache/nutch/indexer/NutchField.java\nrc/plugin/urlnormalizer-regex/src/test/org/apache/nutch/net/urlnormalizer/regex/TestRegexURLNormalizer.java", "Same patch including src/bin/nutch", "Committed the above files for 1.3 in revision 1089077. I've checked all .java source files and most build files in 1.3 so this should be fixed now for 1.3.", "Thanks Markus. Isn't there a tool that we could use to automatically check the headers? I think I saw something similar being used with other projects. Would save the hassle of doing it manually for the trunk", "There are several tools or small bash scripts that can help. Some will work, some won't. I think i'll rather take 15 minutes off and do it by hand =)", "Added missing headers for trunk using http://www.wdev91.com/ plugin for Eclipse", "Great stuff Julien! I'll also add the header for bin/nutch which is also missing and get ready to commit.", "Here's the patch for bin/nutch in trunk.", "Everything builds and runs fine with these patches. I've committed this for trunk in rev. 1092082. Thanks Julien for checking trunk!", "Bulk close of resolved issues for 1.3."], "tasks": {"summary": "Fix missing/wrong headers in source files", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fix missing/wrong headers in source files"}, {"question": "What is the main context?", "answer": "It seems several source files still do not contain the proper ASL headers. This includes older core in 1.3 (indexer.NutchField etc) and recent code in 2.0 (API for instance). This should be fixed (yet"}]}}
{"issue_id": "NUTCH-976", "project": "NUTCH", "title": "Rename properties solrindex.* to solr.* ", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-04-05T14:54:57.648+0000", "updated": "2011-05-08T22:34:48.410+0000", "description": "All Solr properties are now consistently using solr.* instead of solrindex.*. This has been changed for solrindex.mapping.file which was not configurable at all.\n\nWas: The shipped nutch-default.xml configuration file uses solrindex. as namespace for configuration parameters but the namespace (or prefix) in SolrConstants is solr instead. It should be solrindex.", "comments": ["Fix for 1.3 and trunk", "Any objections devs? Everythings is working fine with these patches.", "What about changing the name of the param in the default config instead? I suppose it has been named like this to reflect the name of the mapping file (solrindex-mapping.xml). SOLR is not used for anything else but indexing so using 'solrindex.' is a bit redundant. Not that it really matters mind you...\n\n", "Correct patch", "Yes, i thought about that too but changing the namespace to solr would break existing configurations that rely on solrindex.* params. Usually one would set commit.size to prevent OOMerrors in Nutch.", "Apart from 'solrindex.mapping.file' all the other params (including commit.size) rely on the existing 'solr.' prefix; changing the namespace *will* break them for sure.\n\nBetter to rename 'solrindex.mapping.file' so that it uses the same prefix as the existing params", "Patches for 1.3 and trunk. solr.commit.size has been added to nutch-default and solrindex.mapping is now solr.mapping. Mapping is now also added to SolrConstants.", "All seems to be alright now for trunk and 1.3, any further objections for this issue and NUTCH-977?", "Looks good to me, thanks Markus", "Fixed in 1.3 in rev 1092084 and for trunk in rev 1092085. Thanks Julien for commenting."], "tasks": {"summary": "Rename properties solrindex.* to solr.* ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Rename properties solrindex.* to solr.* "}, {"question": "What is the main context?", "answer": "All Solr properties are now consistently using solr.* instead of solrindex.*. This has been changed for solrindex.mapping.file which was not configurable at all.\n\nWas: The shipped nutch-default.xml co"}]}}
{"issue_id": "NUTCH-977", "project": "NUTCH", "title": "SolrMappingReader uses hardcoded configuration parameter name for mapping file", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-04-05T14:56:52.510+0000", "updated": "2011-05-08T22:34:47.833+0000", "description": "Because the SolrMappingReader uses a hard coded value for the name of the mapping file configuration parameter it actually works. It should rely on SolrConstants instead of using a hard coded value.", "comments": ["Mapping reader should use SolrConstants but that namespace value should be corrected first to prevent stuff from breaking.", "Fix for 1.3", "Fix for trunk", "Any objections devs? Everythings is working fine with these patches.", "Shouldn't MAPPING_FILE be added to SOLRContants as well? ", "It was added but https://issues.apache.org/jira/browse/NUTCH-976 seems to contain an old patch, i'll update the patch.\n", "Committed for trunk in rev. 1092090 for 1.3 in rev. 1092091."], "tasks": {"summary": "SolrMappingReader uses hardcoded configuration parameter name for mapping file", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "SolrMappingReader uses hardcoded configuration parameter name for mapping file"}, {"question": "What is the main context?", "answer": "Because the SolrMappingReader uses a hard coded value for the name of the mapping file configuration parameter it actually works. It should rely on SolrConstants instead of using a hard coded value."}]}}
{"issue_id": "NUTCH-978", "project": "NUTCH", "title": "A Plugin for extracting certain element of a web page on html page parsing.", "status": "Open", "priority": "Minor", "reporter": "Ammar Shadiq", "assignee": "Chris A. Mattmann", "created": "2011-04-06T15:09:10.987+0000", "updated": "2025-07-09T20:26:00.883+0000", "description": "Nutch use parse-html plugin to parse web pages, it process the contents of the web page by removing html tags and component like javascript and css and leaving the extracted text to be stored on the index. Nutch by default doesn't have the capability to select certain atomic element on an html page, like certain tags, certain content, some part of the page, etc.\n\nA html page have a tree-like xml pattern with html tag as its branch and text as its node. This branch and node could be extracted using XPath. XPath allowing us to select a certain branch or node of an XML and therefore could be used to extract certain information and treat it differently based on its content and the user requirements. Furthermore a web domain like news website usually have a same html code structure for storing the information on its web pages. This same html code structure could be parsed using the same XPath query and retrieve the same content information element. All of the XPath query for selecting various content could be stored on a XPath Configuration File.\n\nThe purpose of nutch are for various web source, not all of the web page retrieved from those various source have the same html code structure, thus have to be threated differently using the correct XPath Configuration. The selection of the correct XPath configuration could be done automatically using regex by matching the url of the web page with valid url pattern for that xpath configuration.\n\nThis automatic mechanism allow the user of nutch to process various web page and get only certain information that user wants therefore making the index more accurate and its content more flexible.\n\nThe component for this idea have been tested on nutch 1.2 for selecting certain elements on various news website for the purpose of document clustering. This includes a Configuration Editor Application build using NetBeans 6.9 Application Framework. though its need a few debugging.\n\nhttp://dl.dropbox.com/u/2642087/For_GSoC/for_GSoc.zip\n\n", "comments": ["Proposal for Google Summer of Code 2011\nhttp://www.google-melange.com/gsoc/homepage/google/gsoc2011", "If a mentor has been identified then please assign the issue to that mentor.\n\nhttp://community.apache.org/guide-to-being-a-mentor.html", "Wow, thank you very much Mr. Jelsma:-)", "Proposal Updated", "If it is about main text extraction then there's a plugin in Tika for this (boilerpipe) and there's the readability bookmarklet that has an alternative algorithm to determine the main text.", "Hi Thomas, thank you for the question and the information for boilerplate and readability bookmarklet. \n\nI think it's different.\n\nIt's not just about main text extraction, but also specific information like the value of some <meta> tags, picture illustration link of a news page article (a news page article usually have only one picture), or certain links (in form of anchors) for the next crawling iteration that you want to get and process. I think this type of specific configuration would be useful.\n\nIt's not using any training, from what i read on the boilerplate paper, boilerplate use training data for the algorithm and focusing mainly on the number of words to determine the main text. I myself wonder what if the input page are japanese or chinese. I think they developed a custom tokenizer for that, I haven't exploring the component more thoroughly, so I'm not sure. I myself use this component I'm working on to parse pages in Bahasa Indonesia.", "Can you please explain how your proposal differs from the HTMLParseFilter mechanism that Nutch already has?", "Please correct me if I'm wrong.\nIn my limited understanding, Nutch using plugin system, one of those are for parsing html pages (HTMLParseFilter class) whose  later selected appropriate plugin based on the configuration and runs it. \n\nInside parse-html the main thing it's extract are : Content, Title, and Outlinks.\n\nThe problem that I'm trying to solve are, for adding custom field like on : http://sujitpal.blogspot.com/2009/07/nutch-custom-plugin-to-parse-and-add.html\nfor various type of content and for various sites and add it to the index fields. Instead of creating a new plugin for each site, nutch user could simply create the xpath configuration file, put it on the configuration folder and the parsing of custom fields could be done automatically without writing/compiling any code.\n\nIn addition, user could also bypass Content, Title and Outlinks with a different result, for example, \nSet the title of page's from a news site (example: http://www.guardian.co.uk/world/2011/apr/08/ivory-coast-horror-recounted), \n\ninstead the value of <head><title> :\n=Ivory Coast horror recounted by victims and perpetrators | World news | The Guardian\n\nget the title only, by using xpath of : /html/body/div[@id='wrapper']/div[@id='box']/div[@id='article-header']/div[@id='main-article-info']/h1/text(), and get:\n=Ivory Coast horror recounted by victims and perpetrators\n\nor only follow outlinks of related news, ignore the rest:\n\n-Ouattara calls for Ivory Coast sanctions to be lifted \n-Ivory Coast crisis: Q&A \n-After Gbagbo, what next for Ivory Coast? \n-Ivory Coast: The final battle \n\nlike the screenshoot here : https://issues.apache.org/jira/secure/attachment/12475860/app_guardian_ivory_coast_news_exmpl.png\n\nSince the default parser are parse-html. I add the handler there, some kind of if-else bypass, if the parsed page have URL that match one of those Configuration, it's parsed by it, if there's no configuration matched with the URL, it's uses the default parser mechanism. \n\nI'm sorry for my English and if I'm not presenting my idea well enough.", "If there has been a plugin written for this, would it be possible to get the code added to the wiki? As we have both parse-tika and parse-html for text and outlink extraction for html format, I don't think that this plugin serves much purpose for the average user of Nutch. It really only adds value for users looking for a solution to the specific problem addressed... this is rare.\n\nIt would be disappointing if we were not able to harness and share the code from this small project with other members of the Nutch community via the wiki.", "Hi Chris did you mentor this project through GSoC? I've downloaded the .zip available in the description (which I've also attached in case the link goes AWOL) and I'm going to play about with it. I'll attach it as a patch if I get anywhere.", "In it's present form this is quite literally all over the place and is merely for safe keeping.", "Hey Lewis,\n\nI didn't end up mentoring this project b/c the proposal came too late and the GSoC Apache folks mentioned the program was already over by that time.\n\n+1 to continuing work on it though!\n\nCheers,\nChris\n", "Hi Lewis,\n\nSince the proposal is not accepted, I'm using my summer time to work on my undergrad thesis. I'm graduated from collage recently, and the time has freed up, so I'd love to help, and it's awesome if we could collaborate.\n\nthanks,\nAmmar", "I think it's best if we talk off list for the time being, please get in touch with me lewismc@apache.org and we can take this forward. GSoC expressions of interest need to be made by the end of the month and this would be great as a project for Nutch.", "I'll send you an email.", "Guys, I think it's fine to keep the conversation on list, in fact, I'd favor it unless there is a specific reason to take it there?", "No bother Chris. So far questions that have been asked\n1. provide a quick run down on the issue, summarizing all of the above\n2. what were the motivations, purpose and technical challenges encountered whilst working on it?\n3. Why did the issue drop away and what do you think is required to get it back on track and possibly in the codebase?", "Replies:\n\n1 & 2. The main motivation of this issue is for processing news document\nrequired for my undergrad thesis of Bahasa Indonesia news text\nclustering, it's needed a prepossessing to extract the title, news\ncontent, date, related news link separately.\n\n2. The most biggest technical challenge for me is processing the web page\nso it could be parsered as an XML document   and could be queried by\nXPath.\n\n3. The issue is drop away, because with a small tweak a could get it\nworking for \"only\" my thesis requirements, i haven't tested it with\nweb page other than the web pages i used for my thesis so i think it's\nnot anyway nearly finished yet. And since the proposal is not accepted\nas a GSOC project, i lost motivation to continue to work on this issue\nand decided to work on my thesis instead.\n\nrelated issue : https://issues.apache.org/jira/browse/NUTCH-185", "Generally speaking the plugin sounds sounds really useful, the only problem I see is that it is very specific and for it to be integrated into the code base usually we need to make it specific enough to address some given task fully and in a well defined and well justified manner, but we also need to make it general enough to be used in many different contexts. This increases usability and user feedback as well engagement.\n\n4. With regards to the biggest technical challenge being the processing of web page's, how far did you get with this? We're you able to process it with enough precision to satisfy your requirements?\n\n5. How were you querying it with XPath? You cannot query with XPath, but instead with XQuery. Do you maybe mean that this enabled you to navigate the document and address various parts of it is XPath?\n\n6. Ok I understand why it has crumbled slightly, but I think if the code is there is would be a huge waster if we didn't try to revive it, possibly getting it integrated into the code base, and maybe getting it added as a contrib component but not shipping it within the core codebase if the former was not a viable option.\n\nI've had a look at NUTCH-185, but I think we can discard this as it was addressed a very long time ago, it's also already integrated into the codebase. I was referring more to Jira issues which were currently open, which we could maybe merge or combine to give this a more general and possibly more justified arguement for inclusion in the codebase... what do you think? Does NUTCH-585 fit this?", ">>4.With regards to the biggest technical challenge being the processing of web page's, how far did you get with this? We're you able to process it with enough precision to satisfy your requirements?\n\nI get it to work for my text clustering algorithm, the application screenshoot provided here: http://www.facebook.com/media/set/?set=a.2075564646205.124550.1157621543&type=3&l=7313965254\\. Yes, it's quite satisfactory.\n\n>> 5. How were you querying it with XPath? You cannot query with XPath, but instead with XQuery. Do you maybe mean that this enabled you to navigate the document and address various parts of it is XPath?\n\nIn my understanding there are 3 ways to query an XML document, that is using XPath, XQuery and XLST, I'm sorry if i get it wrong. For navigating various parts of the page i uses java HTML parse lister extending  HTMLEditorKit.ParserCallback and then displaying it on the editor application (some kind of chromium Inspect element), this makes the web page structure visible and thus making the XPath expression easier to make.\n\n>> 6. Ok I understand why it has crumbled slightly, but I think if the code is there is would be a huge waster if we didn't try to revive it, possibly getting it integrated into the code base, and maybe getting it added as a contrib component but not shipping it within the core codebase if the former was not a viable option.\n\nI totally agree\n\nAs for Nutch 585, i think it's different in the idea that is Nutch 585 trying to block certain parts. This idea instead, only retrieve certain parts and in addition store it in certain lucene field (i havent looked into the Solr implementation yet) thus automatically discarding the rest.", "upload latest version, worked on 1.2", "Great Ammar. Are you wanting to add this as a GSoC2012 project? I am already mentoring one project, and time/work restrictions mean that I can't step up to take on another mentoring role. If you don't wish to make this a project this year, at least the code is on here for guys to pick it up in the future. ", "I don't think i could participate in this year GSoC Lewis, my status is not a student anymore. I put it here so it could be freely used/developed further by anyone.\n\ncheers\nAmmar", "This is as I thought. Look I've marked it for this years GSoC, students can apply up until April 6th iirc so if there is any interest then we can progress with it. Thanks Ammar", "Sweet, thanks Lewis.", "Set and Classify", "Hi everyone,\n\nI was interested in such a plugin and found a working implementation, described here: http://www.atlantbh.com/precise-data-extraction-with-apache-nutch/\n\nMaybe it would be a good candidate for integration to the main distribution? (I think adressing this problematic would be pretty useful: as soon as one needs to crawl a specific subset - say, an intranet - one wants to index specific information for better search)", "Hi Emmanuel, do you wish to address this issue. From memory, the dilemma we are having with this issue is to make it configurable enough for general use. Do you have some suggestion(s)? ", "Hi Lewis, thanks for your reply. \n\nIn fact I was suggesting that - in my opinion - the issue has been (very well) addressed by the filter-xpath plugin described in the blog entry I pointed to. Its approach for configuration leveraging the power of XPath queries is pretty flexible (and well described by comments in the config file). I also tested the plugin and it works fine. \n\nWhat I would add for some more flexibility on the 'field' configuration markup is: a 'locale' attribute to enable parsing of non English date formats; a 'fixedValue' attribute enabling to index fixed values into Solr for better categorization when indexing specific datasets; a 'regexPattern' and a 'regexReplace' attributes to enable some regex based postprocessing of the data extracted (for example to remove some unwanted prefix before the interesting data).\n\nWith this, the plugin would be pretty powerful and useful to anyone wanting to extract information from semistructured websites without writing their own plugin.", "It sounds like this is nearly read for a review. I would suggest you to please incorporate your suggestions into a fresh patch against the head version you are working on. Is it the 2.x branch or 1.x trunk?\nAs you said, it always seems that a plugin of this nature is sought after, therefore a reasonably documented and easily configurable implementation would be very welcome.  ", "It looks like there is a small misunderstanding, I must not have expressed myself very clearly: I am not the author of this plugin.\nThe author of the code is the writer of the blog post I pointed to, so I would not presume to submit his code ;)\nWhat I can do is leave a comment on his blog suggesting him to submit his code, since from our exchange it looks like such a submission would be welcome. (EDIT : done)", "+1 Emmanuel", "here is a solution that i am currently experimenting with:\nhttp://www.longconnections.com/blog/2015/6/3/using-apache-nutchsolr-to-build-a-search-engine-with-auto-complete-feature"], "tasks": {"summary": "A Plugin for extracting certain element of a web page on html page parsing.", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "A Plugin for extracting certain element of a web page on html page parsing."}, {"question": "What is the main context?", "answer": "Nutch use parse-html plugin to parse web pages, it process the contents of the web page by removing html tags and component like javascript and css and leaving the extracted text to be stored on the i"}]}}
{"issue_id": "NUTCH-979", "project": "NUTCH", "title": "Add support for deleting Solr documents with ProtocolStatusCodes.NOTFOUND", "status": "Closed", "priority": "Minor", "reporter": "Markus Jelsma", "assignee": null, "created": "2011-04-08T11:39:59.038+0000", "updated": "2019-10-13T22:35:49.257+0000", "description": "When issuing recrawls it can happen that certain urls have expired (i.e. URLs that don't exist anymore and return 404).\nThis issue creates a new command in the indexer that scans for WebPages with ProtocolStatusCodes.NOTFOUND and issues delete commands to Solr.", "comments": ["Here's a WIP in case i'll accidentally send it all to the litter bin. This doesn't include a patch for log4j nor the nutch script.", "Some work to be done\n\nSet and Classify", "Bulk closing all 2.5 issues as branch is no longer is maintenance mode. Closed by [~lewismc] on 2019-10-13. "], "tasks": {"summary": "Add support for deleting Solr documents with ProtocolStatusCodes.NOTFOUND", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add support for deleting Solr documents with ProtocolStatusCodes.NOTFOUND"}, {"question": "What is the main context?", "answer": "When issuing recrawls it can happen that certain urls have expired (i.e. URLs that don't exist anymore and return 404).\nThis issue creates a new command in the indexer that scans for WebPages with Pro"}]}}
{"issue_id": "NUTCH-98", "project": "NUTCH", "title": "RobotRulesParser interprets robots.txt incorrectly", "status": "Closed", "priority": "Minor", "reporter": "Jeff Bowden", "assignee": null, "created": "2005-09-29T15:27:34.000+0000", "updated": "2011-04-01T14:56:13.874+0000", "description": "Here's a simple example that the current RobotRulesParser gets wrong:\n\nUser-agent: *\nDisallow: /\nAllow: /rss\n\n\nThe problem is that the isAllowed function takes the first rule that matches and incorrectly decides that URLs starting with \"/rss\" are Disallowed.  The correct algorithm is to take the *longest* rule that matches.  I will attach a patch that fixes this.", "comments": ["Patch to fix interpretation of robots.txt", "Where is there a specification of robots.txt that defines how 'allow' and 'disallow' lines interact?  I can't even find anything that specifies the semantics of 'allow' lines at all!", "OK, so actually I'm wrong on two counts.  \n\n1.  The current accepted standard does not have Allow lines\n\n2. The draft standard does (http://www.robotstxt.org/wc/norobots-rfc.html), but it specifies that the robot should take the first match found (Nutch's current implementation)\n\nAny rule that is a prefix matched by an earlier rule is rendered completely non-effective according to the standard.  My patch was motivated by what I thought was the obvious interpretation given examples I've seen in the field.  The initial example I gave is from http://del.icio.us/robots.txt\n\n\n\n", "According to the Googlebot faq their implementation takes the longest matching URL as the one they obey.\n\nSee point 7 of http://www.google.com/webmasters/bot.html.\n\nAlso, there's a small difference between the way Googlebot handles the robots.txt file and the way the robots.txt standard says we should (keeping in mind the distinction between \"should\" and \"must\"). The standard says we should obey the first applicable rule, whereas Googlebot obeys the longest (that is, the most specific) applicable rule. This more intuitive practice matches what people actually do, and what they expect us to do. For example, consider the following robots.txt file:\n\nUser-Agent: *\nAllow: /\nDisallow: /cgi-bin ", "Bulk close of legacy issues:\nhttp://www.lucidimagination.com/search/document/2738eeb014805854/clean_up_open_legacy_issues_in_jira"], "tasks": {"summary": "RobotRulesParser interprets robots.txt incorrectly", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "RobotRulesParser interprets robots.txt incorrectly"}, {"question": "What is the main context?", "answer": "Here's a simple example that the current RobotRulesParser gets wrong:\n\nUser-agent: *\nDisallow: /\nAllow: /rss\n\n\nThe problem is that the isAllowed function takes the first rule that matches and incorrec"}]}}
{"issue_id": "NUTCH-980", "project": "NUTCH", "title": "Fix IllegalAccessError with slf4j used in Solrj.", "status": "Closed", "priority": "Blocker", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-04-12T12:36:08.537+0000", "updated": "2011-05-08T22:34:47.669+0000", "description": "Currently Solr commands fail because of:\n\n Exception in thread \"main\" java.lang.IllegalAccessError: tried to \n access field org.slf4j.impl.StaticLoggerBinder.SINGLETON from class \n org.slf4j.LoggerFactory\n         at \n org.slf4j.LoggerFactory.staticInitialize(LoggerFactory.java:83)\n         at org.slf4j.LoggerFactory.<clinit>(LoggerFactory.java:73)\n         at \n org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.<clinit>(CommonsHttpSolrServer.java:78)\n\nJulien looked it up http://www.slf4j.org/faq.html#IllegalAccessError , we need to change the versions in Ivy. I haven't yet come around to test it with trunk so we need to look for it there as well.", "comments": ["Patch for 1.3, upgrade of slf4j-log4j12 from 1.5.11 to 1.5.5 works as fine. Trunk has not yet been tested.", "Commited for 1.3 in rev 1091407.", "This issue illustrates the fact that we have no tests for the indexing with SOLR. We should add that in trunk at some point ", "Both 1.5.11 and 1.5.5 work fine on trunk. If there are no objections i'll upgrade trunk to use 1.5.5 as well.", "Committed for trunk in rev. 1092062."], "tasks": {"summary": "Fix IllegalAccessError with slf4j used in Solrj.", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fix IllegalAccessError with slf4j used in Solrj."}, {"question": "What is the main context?", "answer": "Currently Solr commands fail because of:\n\n Exception in thread \"main\" java.lang.IllegalAccessError: tried to \n access field org.slf4j.impl.StaticLoggerBinder.SINGLETON from class \n org.slf4j.LoggerFac"}]}}
{"issue_id": "NUTCH-981", "project": "NUTCH", "title": "Add tests for solr* tasks", "status": "Closed", "priority": "Minor", "reporter": "Markus Jelsma", "assignee": null, "created": "2011-04-12T14:36:40.676+0000", "updated": "2011-08-30T11:16:25.070+0000", "description": "As demonstrated in NUTCH-980, we come up with some tests the Solr indexer.", "comments": ["Hi Markus, as you mention in NUTCH-1046 this issue is completely duplicated, Ken has suggested some resources for tests which we could possibly port to Nutch. Is this deemed acceptable or do you have other suggestions? Would this issue be easier to track if we closed one issue?", "We can use either issue. Will close this one."], "tasks": {"summary": "Add tests for solr* tasks", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Add tests for solr* tasks"}, {"question": "What is the main context?", "answer": "As demonstrated in NUTCH-980, we come up with some tests the Solr indexer."}]}}
{"issue_id": "NUTCH-982", "project": "NUTCH", "title": "Remove copying of ID and URL field in solrmapping", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-04-12T15:21:09.416+0000", "updated": "2013-05-22T03:53:25.538+0000", "description": "Guys, the Solrindexer seems to be broken in trunk. With current solrmapping and code you'll get an exception complaining about multiple values in a non-multivalued field; the ID field which must of course be single valued. This happens because of the current mapping code and mapping config copy the url and id fields. The old 1.3 NutchDocument does not contain an ID field but in trunk it does.\n\nI propose to change the current solrmapping configuration by simply removing:\n                <field dest=\"id\" source=\"url\"/>\n                <copyField source=\"url\" dest=\"url\"/>\n\nIf not, we need to do something about the solrmapping code.\n", "comments": ["Removes copying of ID and URL fields from solrmapping config.", "Uploaded config instead of patch file.", "If there are no objection i'll commit this today. ", "Committed for trunk in rev 1091895."], "tasks": {"summary": "Remove copying of ID and URL field in solrmapping", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Remove copying of ID and URL field in solrmapping"}, {"question": "What is the main context?", "answer": "Guys, the Solrindexer seems to be broken in trunk. With current solrmapping and code you'll get an exception complaining about multiple values in a non-multivalued field; the ID field which must of co"}]}}
{"issue_id": "NUTCH-983", "project": "NUTCH", "title": "Upgrade SolrJ", "status": "Closed", "priority": "Minor", "reporter": "Markus Jelsma", "assignee": null, "created": "2011-04-13T13:44:44.142+0000", "updated": "2011-06-29T04:01:01.266+0000", "description": "Solr 3.1 has been released a while ago. The Javabin format between 1.4.1 and 3.1 has been changed so our SolrJ 1.4.1 cannot send documents to 3.1. Since Nutch 2.0 won't be released within a short period i believe it would be a good idea to upgrade our SolrJ to 3.1. New Solr users are encouraged to use Solr 3.1 or upgrade so i expect more users wanting to use 3.1 as well. Any thoughts?", "comments": ["Can someone take a look at this? Updating ivy from 1.4.1 to 3.1.0 results in an unmet-dependency-hell.", "I haven't been able to fetch 3.1 using Ivy. I suppose the artefacts have not been published yet.", "SolrJ itself comes in nicely but it seems it comes with its own dependencies, which cannot be found on maven.org:\n[ivy:resolve]           :: javax.jms#jms;1.1!jms.jar\n[ivy:resolve]           :: com.sun.jdmk#jmxtools;1.2.1!jmxtools.jar\n[ivy:resolve]           :: com.sun.jmx#jmxri;1.2.1!jmxri.jar\n\nThe pom's etc are there but the jar's are not. Is there an alternate method of getting those jars?\n\n", "Excellent, seems like things have changed since I tried. \n\nsee http://svn.apache.org/viewvc/nutch/trunk/ivy/ivy.xml?view=markup line 59\n\nwe could try excluding them in the same way\n\n<exclude org=\"com.sun.jdmk\"/>\n<exclude org=\"com.sun.jmx\"/>\n<exclude org=\"javax.jms\"/> ", "I can give it a try. What does the exclude exactly do besides not downloading the dep? If it's a dep and we don't download it, where must it then come from?\n\nedit: it doesn't seem to work, same errors are reported when adding the exclusions.", "It works as expected in trunk but i can't seem to find the issue with 1.3. Tried different configurations and stuff from trunk, same errors. I can commit the change for trunk but not 1.3.\n\nEdit: it gets even better. Trunk can build even without the exclusions listed in the dependency for SolrJ 3.1.0. ", "The only explanation I can think of is that we are getting these from some second hand dependencies which for some reason must be excluded in the trunk.\n\nAdding them as global exclusion seems to do the trick : \n\n{code:xml}\n\n            <!--global exclusion-->\n             \t<exclude module=\"jmxtools\" />\n             \t<exclude module=\"jms\" />\n             \t<exclude module=\"jmxri\" />\n\t</dependencies>\n{code} \n\nSince all we need is the SOLRJ library, these jars should not be necessary at all. \n\n\n", "That works indeed (seems the exclusions must not come before a dependency element). I'm not sure this is the right way to do this but we could commit this for 1.3 and trunk. I think we should, just in case, then also add the exlusions to trunk (although it works without). What do you think?", "It would help to have that in trunk as well. As you can see in http://svn.apache.org/viewvc/nutch/trunk/ivy/ivy.xml?view=markup the exclusions are used in various places (gora, log4j, ...) - having a single exclusion at the end would simplify things and be cleaner", "I'll take a look at it for trunk, hopefully tomorrow.\nCheers!\n\n\n-- \nMarkus Jelsma - CTO - Openindex\nhttp://www.linkedin.com/in/markus17\n050-8536620 / 06-50258350\n", "1.3 : committed revision 1099895\ntrunk : Committed revision 1099896", "Great stuff! I just got my first svn conflict in Nutch but it was you who already committed the issue. Thanks!", "Integrated in Nutch-trunk #1530 (See [https://builds.apache.org/job/Nutch-trunk/1530/])\n    "], "tasks": {"summary": "Upgrade SolrJ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Upgrade SolrJ"}, {"question": "What is the main context?", "answer": "Solr 3.1 has been released a while ago. The Javabin format between 1.4.1 and 3.1 has been changed so our SolrJ 1.4.1 cannot send documents to 3.1. Since Nutch 2.0 won't be released within a short peri"}]}}
{"issue_id": "NUTCH-984", "project": "NUTCH", "title": "Parse-tika throws some URL's away", "status": "Closed", "priority": "Critical", "reporter": "Markus Jelsma", "assignee": null, "created": "2011-04-18T16:35:25.724+0000", "updated": "2013-05-02T02:29:38.717+0000", "description": "For some reason using parse-tika a crawl just wouldn't dive into some website news archive. The paging through the news archive is done with simple anchors:\n\n<div class=\"page active\">1</div> <a href=\"/nieuws/overzicht/1/\"><div class=\"page\">2</div> </a> <a href=\"/nieuws/overzicht/2/\"><div class=\"page\">3</div> </a>\n\nI added some logging to DOMContentUtils:\n2011-04-18 18:26:09,788 INFO  tika.DOMContentUtils - Throw away link:  http://www.site.nl/nieuws/overzicht/1/\n2011-04-18 18:26:09,788 INFO  tika.DOMContentUtils - Throw away link:  http://www.site.nl/nieuws/overzicht/2/\n2011-04-18 18:26:09,788 INFO  tika.DOMContentUtils - Throw away link:  http://www.site.nl/nieuws/overzicht/3/\n...\n\nNow, this is rather funky. The code for private boolean shouldThrowAwayLink(Node node, NodeList children, int childLen, LinkParams params) is the same for parse-html and parse-tika. I also tested the two parsers between versions 1.2 and 1.3 for the following URL.\n\nhttp://news.bbc.co.uk/2/hi/europe/country_profiles/1154019.stm\n\n 1.2 - parse-tika: 196\n 1.2 - parse-html: 296\n 1.3 - parse-tika: 279\n 1.3 - parse-html: 296\n\nSomething clearly improved in 1.3 but not generating the remaining URL's are a blocker for parse-tika in my case. Relevant configurations are the same parser.html.outlinks.ignore_tags is not being used. Testing has been done with ParserChecker only.", "comments": ["Could you test the URLs above directly with Tika 0.9? I suppose this has to do with the default mappers used by Tika which we can override from Nutch.\n\nBTW this illustrates why parse-html is still the default option for html and parse-tika is used for the other mime-types. I'd suggest that we mark this as fixed in 2.0 as 1.3 is about to be RCed. More generally the tests that are used for checking the html parsing need to be ported to parse-tika as well\n\n\n", "Yes i can test these URL's with tika-parsers 0.9 but what do you want to see? They seem to be parsed correctly when using the -t option but not when using -h or -x. The anchors become\n\n<a shape=\"rect\" href=\"http://www.site.nl/nieuws/het-laatste-nieuws/overzicht/2/\"/>3\n\nSo in this case the anchor indeed doesn't contain data and is thus thrown away. Might be a Tika issue instead!", "Looks like this is a Tika issue. If not, please let someone know or file a new issue.\n\nThanks!", "Created issue for Tika.", "Bulk close of resolved issues for 1.3."], "tasks": {"summary": "Parse-tika throws some URL's away", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Parse-tika throws some URL's away"}, {"question": "What is the main context?", "answer": "For some reason using parse-tika a crawl just wouldn't dive into some website news archive. The paging through the news archive is done with simple anchors:\n\n<div class=\"page active\">1</div> <a href=\""}]}}
{"issue_id": "NUTCH-985", "project": "NUTCH", "title": "MoreIndexingFilter doesn't use properly formatted date fields for Solr", "status": "Closed", "priority": "Major", "reporter": "Dietrich Schmidt", "assignee": "Markus Jelsma", "created": "2011-04-19T18:27:13.827+0000", "updated": "2011-05-23T16:49:26.531+0000", "description": "I am using the index-more plugin to parse the lastModified data in web\npages in order to store it in a Solr data field.\n\nIn solrindex-mapping.xml I am mapping lastModified to a field \"changed\" in Solr:\n                <field dest=\"changed\" source=\"lastModified\"/>\n\nHowever, when posting data to Solr the SolrIndexer posts it as a long,\nnot as a date:\n<add><doc boost=\"1.0\"><field\nname=\"changed\">1079326800000</field><field\nname=\"tstamp\">20110414144140188</field><field\nname=\"date\">20040315</field>\n\nSolr rejects the data because of the improper data type.\n", "comments": ["this custom plugin will add a field to the Nutch document. The field is called \"lastModifiedDateFormat\" and is in the format expected by Solr.\nPlease make sure to add the plugin to your configuration file.", "This is similar to another issue described today about the failing dedup. Although i believe it would be a good idea to port longs to properly formatted dates for 1.3 i do think it'll be quite a task since it's not only reformatting before sending it over. Dedup for example relies on dates as long stored in Solr for it to work. I'm also unsure whether a simple reformat in the Solr indexer is a better idea than changing it in the plugins themselves.\n\nThoughts?", "Ideally org.apache.nutch.indexer.more.MoreIndexingFilter should store the lastModifiedDate in date format. Having limited knowledge about the Nutch source, I am not sure whether  dependencies exist that would break things by doing that, but at this point I can't see what that would be.  ", "Yes, something has to be done. What did you attach anyway, is that a recompiled plugin with your modification? If so, please include sources. Jar's are not really useful here ;)\n\nAnyway, thanks for pointing to this issue Dietrich.", "Markus,\nthe source code is the JAR. It's a custom plugin (not a hacked MoreIndexingFilter) that I use as a workaround. The code also might be useful to demonstrate how to properly format the date for Solr. It has been tested with hundreds of thousands of web pages. \n ", "Here's a working patch. It adds a date fieldtype to the schema and changes the index-more fields date and lastModified to use the new date fieldType. It also patches the plugin itself to use the proper date format for those fields. Works flawlessly.\n\nPlease check so i can commit.", "Changed title to reflect the actual issue with the plugin.", "Patch for trunk!", "SOLR is currently our current indexer but this might not be the case forever and we could have other backends like ElasticSearch which would expect a different format. For this reason I'd rather we stored Date objects in the various IndexingFilters implementations and do the SOLR-specific formatting in the SOLR indexer.\n\n", "You are right. But various index-* plugins write dates differently. index-basic already uses Solr formatted date with millis but index-feed uses longs. I'd like to see all writers use the same so conversions when indexing to another system will be more straightforward.\n\nIf devs agree i'll continue and fix index-feed as well. If most disagree i'll dump this patch and go with a quick conversion fix in solrwriter.\n\n", "From dev@nutch\n> For now a quick fix for the moreindexingfilter would be OK, but we can\n> maybe create a new issue for 1.4 and rely on Date objects everywhere then\n> format it properly in the SOLRWriter. We could of course to the latter now,\n> but since I have no time to do it in the short time and don't want to twist\n> your arm I'll let you decide\n\nIn that case i'll go with this quicker fix. Test and commit it and i'll open new issue for trunk and 1.4 once that version is added.", "Looks fine. Any reason why we build the date in a different way in BasicIndexingFilter i.e. using org.apache.solr.common.util.DateUtil? Why not relying on the same mechanism in all the plugins? ", "We should use the Solr's DateUtil in all such places, to avoid code duplication and confusion should the date format ever change... The patch does essentially the same what DateUtil does, only the DateUtil reuses SimpleDateFormat instances in a thread-safe way, so it's more efficient.", "Thanks for the explanation Andrzej. Am definitely not keen on having dependencies on SOLR in pretty much every indexing filter. I'll create a new issue for 1.3 where the Date objects are stored as is in the NutchDocument and converted into Strings in the SOLR Writer. It won't take too much effort to do it and it will be much cleaner", "This issue has been superceded by NUTCH-997, which has been committed to 1.3."], "tasks": {"summary": "MoreIndexingFilter doesn't use properly formatted date fields for Solr", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "MoreIndexingFilter doesn't use properly formatted date fields for Solr"}, {"question": "What is the main context?", "answer": "I am using the index-more plugin to parse the lastModified data in web\npages in order to store it in a Solr data field.\n\nIn solrindex-mapping.xml I am mapping lastModified to a field \"changed\" in Solr"}]}}
{"issue_id": "NUTCH-986", "project": "NUTCH", "title": "Dedup fails due to date format (long)", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-04-26T09:51:54.955+0000", "updated": "2011-06-29T04:01:00.457+0000", "description": "As already mentioned on the list, dedup also failes because of invalid date formats.\n\nApr 19, 2011 10:34:50 AM org.apache.solr.request.BinaryResponseWriter$Resolver \ngetDoc\nWARNING: Error reading a field from document : \nSolrDocument[{digest=7ff92a31c58e43a34fd45bc6d87cda03}]\njava.lang.NumberFormatException: For input string: \"2011-04-19T08:16:31.675Z\"\n        at \njava.lang.NumberFormatException.forInputString(NumberFormatException.java:48)\n        at java.lang.Long.parseLong(Long.java:419)\n        at java.lang.Long.valueOf(Long.java:525)\n        at org.apache.solr.schema.LongField.toObject(LongField.java:82)\n....\n\nStrange enough, Solr seems to allow updates of long fields with a formatted \ndate. In Nutch 1.2 the tstamp field is actually a long but in 1.3 the field is \na valid Solr date format. This exception is only triggered using the javabin \nresponse writer so there's something weird in Solr too.\n\nWe need to either change the tstamp field back to a long or update the Solr \nexample schema and fix SolrDeleteDuplicates to use the formatted date instead \nof the long.", "comments": ["Here's a patch. It leaves all code intact but only converts the incoming formatted date to the internally used long. Tested and confirmed to work as expected.", "Patch for trunk!", "If there are no objections i'll commit this one tomorrow and commit NUTCH-991 (dedup must commit) as well.", "Committed 1.3 in rev. 1097390 and for trunk in rev. 1097391.", "Previous patch was incorrect but committed. It did actually deduplicate but threw exceptions. These patches fix it all.", "Recommitted 1.3 in rev 1097410 and for trunk in rev. 1097411. Apologies!", "Integrated in Nutch-trunk #1530 (See [https://builds.apache.org/job/Nutch-trunk/1530/])\n    "], "tasks": {"summary": "Dedup fails due to date format (long)", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "Dedup fails due to date format (long)"}, {"question": "What is the main context?", "answer": "As already mentioned on the list, dedup also failes because of invalid date formats.\n\nApr 19, 2011 10:34:50 AM org.apache.solr.request.BinaryResponseWriter$Resolver \ngetDoc\nWARNING: Error reading a fi"}]}}
{"issue_id": "NUTCH-987", "project": "NUTCH", "title": "Support HTTP auth for Solr communication", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-04-26T15:50:09.722+0000", "updated": "2011-09-13T21:26:59.501+0000", "description": "At the moment we cannot send data directly to a public HTTP auth protected Solr instance. I've a WIP that passes a configured HTTPClient object to CommonsHttpSolrServer, it works. This issue should add this ability to indexing, dedup and clean and be configured from some configuration file.\n\nEnable Solr HTTP auth communication by setting the following parameters in your nutch-site config:\n* solr.auth=true\n* solr.auth.username=USERNAME\n* solr.auth.password=PASSWORD", "comments": ["Attached nasty hack for the sake of not losing it.", "Patch for 1.4. Also moved UTF-8 strip method to Solr utils. It's implemented using simple job properties, no fancy AuthScope stuff. Please comment.", "Some instances in SolrDedup were missing. Also cleaned up some mess.", "Are there objections? Pointers? Comments?", "Based upon the current patch you provided, I think this is a good suggestion for inclusion. I am not currently using an auth protected Solr core in production, but will get authentication set up in development and get this tested Markus. It would make sense for inclusion just now as it will inevitably become a requested feature in the future.\n\nFurther to this, to address you initial question, I agree with the comments regarding the location to configure the auth credentials for Solr communication as quite simply I cannot think of any other solution which would do anything other than clutter.", "If no objections i'll send this one in together with NUTCH-1036. This patch includes the changes made for NUTCH-1036, adding reporter increments here and there.", "don't forget to add the parameters you introduced to nutch-default.xml (with authentication off by default)\n+1 otherwise\n\nThanks!", "The previous patch has the config change for nutch-default, i missed it in the last patch.  Thanks Lewis and Julien!", "Committed for 1.4 in rev. 1146035.", "Hi Markus, will this be committed to trunk as well?", "Yes, at least partially. Solrclean isn't finished yet and dedup is broken. ", "Partial patch for 2.0 includes support for HTTP auth for solrindex and solrdedup and includes NUTCH-1026 and NUTCH-1036.\n\nAnyone who can test this would make me happy.", "Hi Markus, the patch for 2.0 does not apply cleanly for me I get the following output\n{code}\nlewis@lewis-01:~/ASF/trunk$ patch -p0 -i NUTCH-987-2.0-1.patch\npatching file src/java/org/apache/nutch/indexer/solr/SolrUtils.java\npatching file src/java/org/apache/nutch/indexer/solr/SolrDeleteDuplicates.java\npatching file src/java/org/apache/nutch/indexer/solr/SolrIndexerJob.java\npatching file src/java/org/apache/nutch/indexer/solr/SolrConstants.java\npatching file src/java/org/apache/nutch/indexer/solr/SolrWriter.java\npatching file src/java/org/apache/nutch/indexer/IndexerReducer.java\npatching file conf/nutch-default.xml\nHunk #1 FAILED at 728.\nHunk #2 succeeded at 1060 (offset 13 lines).\n1 out of 2 hunks FAILED -- saving rejects to file conf/nutch-default.xml.rej\n{code}\n\nI therefore I attach an updated patch which applies cleanly, however it breaks runtime builds and (already broken) tests with the following output\n{code}\n[javac] Compiling 5 source files to /home/lewis/ASF/trunk/build/classes\n    [javac] /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/solr/SolrDeleteDuplicates.java:231: cannot find symbol\n    [javac] symbol  : variable SolrUtils\n    [javac] location: class org.apache.nutch.indexer.solr.SolrDeleteDuplicates.SolrInputFormat\n    [javac]       SolrServer solr = SolrUtils.getCommonsHttpSolrServer(conf);\n    [javac]                         ^\n    [javac] /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/solr/SolrDeleteDuplicates.java:261: cannot find symbol\n    [javac] symbol  : variable SolrUtils\n    [javac] location: class org.apache.nutch.indexer.solr.SolrDeleteDuplicates.SolrInputFormat\n    [javac]       SolrServer solr = SolrUtils.getCommonsHttpSolrServer(conf);\n    [javac]                         ^\n    [javac] /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/solr/SolrDeleteDuplicates.java:306: cannot find symbol\n    [javac] symbol  : variable SolrUtils\n    [javac] location: class org.apache.nutch.indexer.solr.SolrDeleteDuplicates\n    [javac]        solr = SolrUtils.getCommonsHttpSolrServer(conf);\n    [javac]               ^\n    [javac] /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/solr/SolrIndexerJob.java:70: cannot find symbol\n    [javac] symbol  : variable SolrUtils\n    [javac] location: class org.apache.nutch.indexer.solr.SolrIndexerJob\n    [javac]       SolrServer solr = SolrUtils.getCommonsHttpSolrServer(getConf());\n    [javac]                         ^\n    [javac] /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/solr/SolrWriter.java:51: cannot find symbol\n    [javac] symbol  : variable SolrUtils\n    [javac] location: class org.apache.nutch.indexer.solr.SolrWriter\n    [javac]     solr = SolrUtils.getCommonsHttpSolrServer(conf);\n    [javac]            ^\n    [javac] /home/lewis/ASF/trunk/src/java/org/apache/nutch/indexer/solr/SolrWriter.java:64: cannot find symbol\n    [javac] symbol  : variable SolrUtils\n    [javac] location: class org.apache.nutch.indexer.solr.SolrWriter\n    [javac]           val2 = SolrUtils.stripNonCharCodepoints((String)val);\n    [javac]                  ^\n    [javac] 6 errors\n\nBUILD FAILED\n/home/lewis/ASF/trunk/build.xml:96: Compile failed; see the compiler error output for details.\n{code}", "Ah the config. You can easily add the config params yourself but they're not strictly required as the code already uses the same defaults.\n\nI'm off!! Cheers!", "Resolved for 1.4, see NUTCH-1104 for 2.0"], "tasks": {"summary": "Support HTTP auth for Solr communication", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Support HTTP auth for Solr communication"}, {"question": "What is the main context?", "answer": "At the moment we cannot send data directly to a public HTTP auth protected Solr instance. I've a WIP that passes a configured HTTPClient object to CommonsHttpSolrServer, it works. This issue should ad"}]}}
{"issue_id": "NUTCH-988", "project": "NUTCH", "title": "index-feed plugin also doesn't use proper date fields", "status": "Closed", "priority": "Minor", "reporter": "Markus Jelsma", "assignee": null, "created": "2011-04-27T10:59:18.540+0000", "updated": "2011-05-23T11:22:09.828+0000", "description": "Like some other fields, the date fields generated by the feed-plugin are not using the proper date format for Solr.", "comments": ["Fixed as part of https://issues.apache.org/jira/browse/NUTCH-999"], "tasks": {"summary": "index-feed plugin also doesn't use proper date fields", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "index-feed plugin also doesn't use proper date fields"}, {"question": "What is the main context?", "answer": "Like some other fields, the date fields generated by the feed-plugin are not using the proper date format for Solr."}]}}
{"issue_id": "NUTCH-989", "project": "NUTCH", "title": "index-basic plugin doesn't use Solr date fieldType", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-04-27T11:12:35.002+0000", "updated": "2011-06-29T04:01:00.117+0000", "description": "The index-basic plugin actually sends over a properly formatted date with millis but the schema isn't configured to use the dateField.", "comments": ["The supplied Solr schema must use a date fieldType instead of long. If not, deduplication won't work because Solr mysteriously allows formatted dates in a long field, but i cannot write them back in the javabin format.\n\nThis change will break an existing 1.2 schema. But since we want to upgrade to 3.1 anyway, users must reindex the data.", "Date fieldType added and updated tstamp field to use the new fieldType. Committed for 1.3 in rev. 1099800 and for trunk in rev. 1099802.\n\nNote: it is not a TrieDate fieldType. We probably need a separate issue to fine tune the schema for Solr 3.1.", "Integrated in Nutch-trunk #1530 (See [https://builds.apache.org/job/Nutch-trunk/1530/])\n    "], "tasks": {"summary": "index-basic plugin doesn't use Solr date fieldType", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "index-basic plugin doesn't use Solr date fieldType"}, {"question": "What is the main context?", "answer": "The index-basic plugin actually sends over a properly formatted date with millis but the schema isn't configured to use the dateField."}]}}
{"issue_id": "NUTCH-99", "project": "NUTCH", "title": "ports are hardcoded or random", "status": "Closed", "priority": "Critical", "reporter": "Stefan Groschupf", "assignee": null, "created": "2005-09-29T22:21:48.000+0000", "updated": "2005-11-15T07:40:34.000+0000", "description": "Ports of tasktracker are random and the port of the datanode is hardcoded to 7000 as strting port.", "comments": ["This patch make the port of datanode and tasktracker configurable in the nutch-default.xml.\nI changed as less as possible code,  to be sure this patch can be assigned as fast as possible to the sources, since for me this is a critical issue.\n\n", "I notice there are no tests for  ndfs and mapreduce trackers, so the test suite was running after patching the sources. But my manually tests today fails.\nSo the last patch is just for the trash. Sorry!\nFind attached a patch that at least pass my manually tests.\nNow the port for jetty is configurable as well, the code already had trying to load a value from the configuration file, but the node in nutch-default.xml was missing.\nBeside that I organize some how the ports that are default used a bit.\nI suggested following ports:\nndfs namenode \t7000 \t\nndfs data node \t7010 \t\nmr job tracker \t7020 \t\nmr job tracker info port \t7025 \t\nmr task tracker output \t7030 \t\nmr task tracker report \t7040 \t\n\nWell, now all of this ports should be configurable individually.\n\n\n", "I like  the cleanup of the port numbers.  And removing the use of random port numbers may make some network administrators happy.  But switching from random to fixed ports in the TaskTracker means that only a single task tracker can be run at a time.  Currently I frequently find it useful to debug things by running multiple task trackers on a single box.\n\nSo we need to either loop, trying a range of port numbers, or switch back to random allocation, or both (since random allocations may collide).\n\nAccording to the IANA, we should be able to randomly allocate stuff in 49152-65535.  But that still could make folks upset who wish to set up restrictive firewalls.\n\n\n", "OK, make sense. \nDo you prefer command line args for the ports for this 'lets search for a port' code?\nI personal would prefer command line args.\n", "What command line would you add this to?  I think this should simply start at the default port (e.g., 7030) and loop trying port+1 until BindException is not thrown.  A message should be logged for each failure.", "As discussed,  tasktracker iterates now until it is finding a free port, starting  with a configurable port from nutch-default.xml. Fails will be logged.\nOnly ports higher than  49152 are used.\nHope this patch match the requirements to be submitted.", "Is there anything I can improve so one of the developers commit this patch into the svn?\nThanks in case one of the people with svn write access can commit this. \n\n\n", "I cannot get patch on linux to accept this. The absolute DOS paths seem to cause problems.  Can you please regenerate this with relative paths?  Generating it on linux would also be preferable, as patch also has problems with EOL differences.\n\nAlso, ndfs.datanode.port would be a better name for that property.\n\nAnd catching Exception is overkill.  This should be java.net.BindException, no?\n", "I'm not sure what you are meaning with catching Exception is overkill. \nIn case the try to open a server on this port fails a axception is thrown and I have to catch this since I will iterate to a higher port number and try it again instead of exit the methdo my since a BindException is thrown.\n\n ", "+ rename the property as requested\n+ regenerate the patch with relative paths on a unix based system :-)\n+ as commented the exception catching is necessary and is also done in other exsting code that iterates to find a free port.\n\nTHANKS DOUG FOR TAKING THE TIME to get this into the sources! ", "I think Doug meant that we should have:\n} catch (BindException e) {\ninstead of  generic:\n} catch (Exception e) {\n\nAnd I agree with such suggestiom.\n\nIf such change is ok with you - Stefan - I can change this one small thing myself and commit it  - as I am starting to use  mapreduce branch.\n\n", "SURE! That is absolutly ok for me!\nThanks a lot Piotr!!!!\n\n", "Sounds good.  We should also probably note in the config property descriptions that these port numbers are the first in a range that will be tried.\n", "Patch committed. Thanks Stefan.\n"], "tasks": {"summary": "ports are hardcoded or random", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "ports are hardcoded or random"}, {"question": "What is the main context?", "answer": "Ports of tasktracker are random and the port of the datanode is hardcoded to 7000 as strting port."}]}}
{"issue_id": "NUTCH-990", "project": "NUTCH", "title": "protocol-httpclient fails with short pages", "status": "Closed", "priority": "Minor", "reporter": "Gabriele Kahlout", "assignee": null, "created": "2011-04-27T14:34:04.772+0000", "updated": "2011-11-10T09:26:03.127+0000", "description": "Using protocol-http with a few words html pages works fine. But with protocol-httpclient the same pages disappear from the index, although they are still fetched.\n\n\nThose small files are useful for quick testing. \n\nSteps to reproduce:\n$ svn co http://svn.apache.org/repos/asf/nutch/branches/branch-1.3 nutch-1.3\nChecked out revision 1097214.\n\n$ cd nutch-1.3\n$ xmlstarlet edit -L -u \"/configuration/property[name='http.agent.name']\"/value -v 'test' conf/nutch-default.xml\n$ ant\n\nDownload to runtime/local the following script and seeds list file. They assume a $HADOOP_HOME environment variable. It's a 1.3 adaptation of [1].\nhttp://dp4j.sf.net/debug/whole-web-crawling-incremental\nhttp://dp4j.sf.net/debug/urls\n\n$ cd runtime/local\n\nThis will empty your Solr index (-f) and crawl:\n$ ./whole-web-crawling-incremental -f .\n\nNow Check Solr index searching for artificial and you will find the page pointed to in urls.\nNow change plugin-includes in conf/nutch-default to use protocol-httpclient instead of protocol-http and re-run the script. No more results in solr. Try again with http and the results return.\n\n\n[1] http://wiki.apache.org/nutch/Whole-Web%20Crawling%20incremental%20script\n", "comments": ["What does the log say? I tried my home_dir with httpclient but it immediately failed to fetch my Apache generated directory index (which is a HTML file):\n\nfetch of http://localhost/~markus/ failed with: java.io.IOException: unzipBestEffort returned null\n\nDid you try httpclient with a clean crawl? Testing the behavior if different plugins is usually easier with clean crawldb's.\n\n", "I ran it on a fresh crawl (and index). I've tried it with a 'good' html page and 'bad' one and the bad doesn't get into the index while the good does.", "A test page:\n<!DOCTYPE html>\n<html>\n  <head>\n    <title></title>\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n  </head>\n  <body>\n      artificial rucksack\n  </body>\n</html>\n\nEven with a title it doesn't work.", "Could you post only the relevant parts of the log without the plugin info? This gives too much noise. One log for the same file with protocol-http and one for the same file with httpclient.\n\n", "the logs look full of INFO noise indeed. I don't see any error in there. What would be the relevant parts?\nI've posted detailed steps to re-produce.", "protocol-httpclient has a number of issues and should not be used. The underlying library has undergone massive changes in the meantime and porting protocol-httpclient would require a lot of effort. It would probably be simpler to rewrite it altogether. There is a JIRA about this already I think.\nFor these reasons I don;t think there is much point trying to patch the existing version.", "guess we can mark it as won't fix then and close the ticket.", "@Julien - can we mark this related to the other JIRA you are talking about so that I follow it up?\n\nI recall from the mailing list that only httpclient handled https and maybe even pdfs (?).", "@Markus : yes - won't fix \n\nIdeally we should come implement that as part of crawler-commons and use it as a dependency in Nutch. Crawler-commons has not received much interest lately but I am sure that Ken would be interested. \n\n@gabriele : can't find the issue. was probably within a different one\nhttpclient is indeed the only way to currently handle https, however it has absolutely nothing to do with pdf as it is about protocols, not content. Did you mean something else?\n\n\n", "@Julien - my bad with the pdfs.", "I have this problem too protocol-httpclient fails with short files (for example robots.txt) with unzipBestEffort returned null", "Same here - been trying to fetch https pages through protocol-httpclient but am getting the same error. If that library has a lot of underlying issues, which alternative library should we use? Any recommendation appreciated :)", "A patch has been committed recently that fixes the issues with compressed short pages - checkout the code from SVN\nSee https://issues.apache.org/jira/browse/NUTCH-1089\n\nNote that protocol-httpclient still needs replacing and is considered broken. See https://issues.apache.org/jira/browse/NUTCH-1086", "Hi this issue has been marked as fixed but not closed off. Should it be marked as won't fix as its a duplication then closed off as per Juliens comments above? I think we need to remove/resolve some of the issues muddying this umbrella topic of problems with protocol-httpclient"], "tasks": {"summary": "protocol-httpclient fails with short pages", "classification": "bug", "qa_pairs": [{"question": "What is the issue about?", "answer": "protocol-httpclient fails with short pages"}, {"question": "What is the main context?", "answer": "Using protocol-http with a few words html pages works fine. But with protocol-httpclient the same pages disappear from the index, although they are still fetched.\n\n\nThose small files are useful for qu"}]}}
{"issue_id": "NUTCH-991", "project": "NUTCH", "title": "SolrDedup must issue a commit", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-04-27T15:19:09.750+0000", "updated": "2011-06-29T04:01:01.569+0000", "description": "Title says it all. SolrDedup job doesn't commit but it should.", "comments": ["Added the commit operation to the close and cleanup methods for resp. 1.3 and trunk.", "Committed in 1.3 rev 1097415 and trunk 1097416.", "Integrated in Nutch-trunk #1530 (See [https://builds.apache.org/job/Nutch-trunk/1530/])\n    "], "tasks": {"summary": "SolrDedup must issue a commit", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "SolrDedup must issue a commit"}, {"question": "What is the main context?", "answer": "Title says it all. SolrDedup job doesn't commit but it should."}]}}
{"issue_id": "NUTCH-992", "project": "NUTCH", "title": "SolrDedup is broken in 2.x", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": null, "created": "2011-04-28T11:23:07.492+0000", "updated": "2014-05-01T06:23:56.295+0000", "description": "SolrDedup seems to have been broken for at least a few months, perhaps more. It does fetch the documents from Solr but when processing the rows we get the following exception:\n\nException in thread \"main\" java.lang.NullPointerException\n        at org.apache.hadoop.io.serializer.SerializationFactory.getSerializer(SerializationFactory.java:73)\n        at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:899)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:779)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:432)\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:447)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates.dedup(SolrDeleteDuplicates.java:350)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates.run(SolrDeleteDuplicates.java:360)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates.main(SolrDeleteDuplicates.java:370)", "comments": ["Set and Classify", "Fixed and committed @revision 1496628 in 2.x HEAD"], "tasks": {"summary": "SolrDedup is broken in 2.x", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "SolrDedup is broken in 2.x"}, {"question": "What is the main context?", "answer": "SolrDedup seems to have been broken for at least a few months, perhaps more. It does fetch the documents from Solr but when processing the rows we get the following exception:\n\nException in thread \"ma"}]}}
{"issue_id": "NUTCH-993", "project": "NUTCH", "title": "NullPointerException at FetcherOutputFormat.checkOutputSpecs", "status": "Closed", "priority": "Minor", "reporter": "Christian Guegi", "assignee": "Julien Nioche", "created": "2011-05-03T10:10:50.652+0000", "updated": "2011-12-20T11:30:19.675+0000", "description": "When running Nutch as a mapreduce job on an existing cluster I get an NullPointerException at org.apache.nutch.fetcher.FetcherOutputFormat.checkOutputSpecs.\n\nThe reason is that the passed in reference to the file system is null.\nThe attached patch ignores the parameter 'fs' and creates a new reference to the file system.", "comments": ["This issue also affects the ParseOutputFormat.java code. I've attached a patch here.\n", "This patch for ParseOutputFormat fixes the same issue (NPE) in this file.", "Shouldn't the ParseOutputFormat patch check for PARSE_DIR instead of FETCH_DIR?", "@Paper Cruncher - Yes indeed. I think I uploaded the wrong file in there. I'll update it shortly. Thanks for the catch.", "Viksit, can you provide an updated patch?", "-Viksit- Christian, can you mark you patch to be included in ASF works? We cannot include your fetcher patch in the dist.", "Markus - definitely. I'll have an updated version up in a day.", "Patch fixes both Fetcher and Parser + gets a new fs only if it is null", "correct version of the patch", "The latest patch runs smoothly. +1", "Thanks to the reporter and contributors\n\nCommitted revision 1143411.\n\n", "Bulk close of resolved issues of 1.4. bulkclose-1.4-20111220"], "tasks": {"summary": "NullPointerException at FetcherOutputFormat.checkOutputSpecs", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "NullPointerException at FetcherOutputFormat.checkOutputSpecs"}, {"question": "What is the main context?", "answer": "When running Nutch as a mapreduce job on an existing cluster I get an NullPointerException at org.apache.nutch.fetcher.FetcherOutputFormat.checkOutputSpecs.\n\nThe reason is that the passed in reference"}]}}
{"issue_id": "NUTCH-994", "project": "NUTCH", "title": "Fine tune Solr schema", "status": "Closed", "priority": "Major", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-05-05T13:54:55.386+0000", "updated": "2011-06-29T04:01:00.307+0000", "description": "The supplied schema is old and doesn't use more advanced fieldTypes such as Trie based (since Solr 1.4) and perhaps other improvements. We need to fine tune the schema.", "comments": ["This patch changes:\n* upgraded schema version from 1.1 to 1.3 to allow users to enable new features.\n* non-analyzed field types to their Trie-based equivalent. No high precisions used because little or no range queries are expected from data generated by Nutch.\n* removed RemoveDuplicatesTokenFilterFactory from URL field type. There is no stemmer involved that can blow up TF/IDF in conjunction with a WordDelimiterFilter.\n* adds cc field for creativecommons plugin. Not sure whether is should be tokenized to allow for more flexible search.\n\nFor clarity i have added fields created by plugin that come with the release. I haven't found any in parse-swf. I also didn't add fields from the urlmeta plugin since it is unclear which field names are found.\n\nI also didn't add the tag field for microformats-reltag plugin, it collides with the same field name for the feed plugin. Any thoughs on this? Change what?\n\nI'd still like to change date fields that do not use the date field type to use a proper date field type. This depends on NUTCH-985, the same goes for the feed plugin, if we still want to ship it in the release (julian?).\n\nI kept the 80-column `wordwrap` although it only fills up less than halve my screens ;)", "Markus - what about modifying the default schema.xml provided with 3.1 and add our fields definitions instead of trying to upgrade the old schema?\nThis way it would be easier to compare with the default + we'd have all the field types up to date", "I thought about that but Solr's example schema contains a lot of generic stuff and analyzed fieldTypes not suitable for a Nutch example (IMO). I did upgrade all fields that Nutch actually uses. I think i prefer a schema targetted for Nutch and fieldTypes required by new plugins should be added accordingly. The patch also contains a link to the real Solr example schema for people interested. ", "+1 Looks good to me\n\nbq. I also didn't add the tag field for microformats-reltag plugin, it collides with the same field name for the feed plugin. Any thoughs on this? Change what?\n\nAssuming they both use the same field type, why not let these 2 plugins share the same field?  \n\nbq. I'd still like to change date fields that do not use the date field type to use a proper date field type. This depends on NUTCH-985, the same goes for the feed plugin, if we still want to ship it in the release (julian?).\n\nDone\n", "I made the tag field multi valued (as it should, judging from code from both plugins) and added a comment that tag is also used by microformats-reltag plugin.\n\nCommitted for 1.3 in rev. 1126417.\n\nThere's a little depedency with 2.0 NUTCH-999 here because of the date fields. Perhaps we could avoid the hassle and commit this schema change for 2.0 when 999 is being committed.", "Markus, \n\nI've committed NUTCH-999. Could you apply the changes to the schema to the trunk as well?\n\nThanks\n\nJulien ", "Committed for 2.0 in rev. 1126425. The only difference is that i added a link to NUTCH-999 instead of 994, which was for 1.3", "Bulk close of resolved issues for 1.3.", "Integrated in Nutch-trunk #1530 (See [https://builds.apache.org/job/Nutch-trunk/1530/])\n    "], "tasks": {"summary": "Fine tune Solr schema", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Fine tune Solr schema"}, {"question": "What is the main context?", "answer": "The supplied schema is old and doesn't use more advanced fieldTypes such as Trie based (since Solr 1.4) and perhaps other improvements. We need to fine tune the schema."}]}}
{"issue_id": "NUTCH-995", "project": "NUTCH", "title": "Generate POM file using the Ivy makepom task ", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Chris A. Mattmann", "created": "2011-05-07T05:58:35.479+0000", "updated": "2011-06-29T04:01:00.603+0000", "description": "We currently have a pom.xml file in the SVN repository and use it for publishing our artefacts. The trouble with this is that we need to keep its content in sync with our ivy file. Instead we could use the makepom task (http://ant.apache.org/ivy/history/2.2.0/use/makepom.html) to generate the pom.xml automatically.\n\nThe existing pom.xml for 1.3 needs fixing anyway as it declares dependencies to GORA and has the wrong versions for some dependencies.", "comments": ["Patch for 1.3\nUpgrades to Ivy 2.2 + uses makepom with a template to generate a pom.xml for the task ant deploy", "Assigning to Chris for review", "I'll commit this tomorrow unless someone has an objection", "Hey Julien, I'll test this today and provide feedback shortly.", "Step 1 - woot - patch applies successfully.\n\n{noformat}\n[chipotle:~/tmp/nutch1.3] mattmann% patch -p0 < $HOME/Desktop/Apache/nutch-dev/NUTCH-955-1.3.patch \npatching file default.properties\npatching file ivy/ivy.xml\npatching file ivy/mvn.template\npatching file pom.xml\npatching file build.xml\n{noformat}", "OK, I'm getting an error when running {code}ant deploy{code}, see below:\n\n{noformat}\n  [javadoc] /Users/mattmann/tmp/nutch1.3/src/java/org/apache/nutch/util/domain/TopLevelDomain.java:29: warning - Tag @see: reference not found: http://en.wikipedia.org/wiki/Top-level_domain\n  [javadoc] /Users/mattmann/tmp/nutch1.3/src/plugin/lib-regex-filter/src/java/org/apache/nutch/urlfilter/api/RegexURLFilterBase.java:58: warning - Tag @link: can't find getRulesFile(Configuration) in org.apache.nutch.urlfilter.api.RegexURLFilterBase\n  [javadoc] /Users/mattmann/tmp/nutch1.3/src/plugin/languageidentifier/src/java/org/apache/nutch/analysis/lang/NGramProfile.java:133: warning - @param argument \"t\" is not a parameter name.\n  [javadoc] Building index for all the packages and classes...\n  [javadoc] Building index for all classes...\n  [javadoc] Generating /Users/mattmann/tmp/nutch1.3/build/release/javadoc/stylesheet.css...\n  [javadoc] 77 warnings\n      [jar] Building jar: /Users/mattmann/tmp/nutch1.3/build/release/nutch-1.3-javadoc.jar\n      [jar] Building jar: /Users/mattmann/tmp/nutch1.3/build/release/nutch-1.3-sources.jar\n\ndeploy:\n\nBUILD FAILED\n/Users/mattmann/tmp/nutch1.3/build.xml:210: ivy:makepom doesn't support the \"templatefile\" attribute\n\nTotal time: 1 minute 32 seconds\n[chipotle:~/tmp/nutch1.3] mattmann% \n{noformat}\n\nAny idea what's up?\n", "Strange. It seems to be using Ivy 2.1.0 in resolve \n\n{noformat} \n[ivy:resolve] :: Ivy 2.1.0 \n{noformat}\n\ndespite the fact that default.properties specifies ivy.version=2.2.0\n\n", "Chris - could you please try changing the target: ivy-init-antlib ?\n\n{noformat} \n<!-- target: ivy-init-antlib  ========================================= -->\n  <target name=\"ivy-init-antlib\" depends=\"ivy-download\" unless=\"ivy.found\">\n  \t\n\t<taskdef resource=\"org/apache/ivy/ant/antlib.xml\"\n\t          uri=\"antlib:org.apache.ivy.ant\">\n    <classpath>\n      <pathelement location=\"${ivy.jar}\" />\n    </classpath>\n   </taskdef>\n  \t\n  </target>\n{noformat}\n\nThis should force it to use the version of ivy which has been downloaded and stored in the ivy lib\n\n\nThanks\n\nJulien\n", "Hi Julien, I tried your suggested update and it still fails for me:\n\n{noformat}\n  [javadoc] /Users/mattmann/tmp/nutch1.3/src/plugin/languageidentifier/src/java/org/apache/nutch/analysis/lang/NGramProfile.java:133: warning - @param argument \"t\" is not a parameter name.\n  [javadoc] Building index for all the packages and classes...\n  [javadoc] Building index for all classes...\n  [javadoc] Generating /Users/mattmann/tmp/nutch1.3/build/release/javadoc/stylesheet.css...\n  [javadoc] 77 warnings\n      [jar] Building jar: /Users/mattmann/tmp/nutch1.3/build/release/nutch-1.3-javadoc.jar\n\ndeploy:\n\nBUILD FAILED\n/Users/mattmann/tmp/nutch1.3/build.xml:210: ivy:makepom doesn't support the \"templatefile\" attribute\n\nTotal time: 37 seconds\n[chipotle:~/tmp/nutch1.3] mattmann% \n{noformat}", "{code}\nBUILD FAILED\n/Users/simpatico/NetBeansProjects/Experiments/test/nutch/build.xml:216: Problem: failed to create task or type antlib:org.apache.maven.artifact.ant:mvn\nCause: The name is undefined.\nAction: Check the spelling.\nAction: Check that any custom tasks/types have been declared.\nAction: Check that any <presetdef>/<macrodef> declarations have taken place.\n\nThe definitions in the namespace antlib:org.apache.maven.artifact.ant are:\n    install-provider\n    localRepository\n    remoteRepository\n    dependencies\n    pom\n    authentication\n    install\n    proxy\n    deploy\nTotal time: 1 minute 31 seconds\n\n{code}\n\nThis is what's in build.xml:216:\n{code}\n\t<artifact:mvn>\n\t\t<arg value=\"org.apache.maven.plugins:maven-gpg-plugin:1.1:sign-and-deploy-file\" />\n\t\t<arg value=\"-Durl=${maven-repository-url}\" />\n\t\t<arg value=\"-DrepositoryId=${maven-repository-id}\" />\n\t\t<arg value=\"-DpomFile=pom.xml\" />\n\t\t<arg value=\"-Dfile=${maven-jar}\" />\n                       <arg value=\"-Papache-release\" />\n\t</artifact:mvn>\n{code}\n\nSteps to reproduce:\n{code}\n$ svn co http://svn.apache.org/repos/asf/nutch/branches/branch-1.3 nutch\n$ cd nutch ; patch -p0 -i $PATH_TO_DOWNLOADED_PATCH/NUTCH-955-1.3.patch\n$ ant deploy\n{code}\n\n", "@Gabriele : the error you are getting probably means that the jar file containing the maven tasks is not available on the classpath. use 'ant -lib' and point to the directory containing this jar. Note that this issue is slightly different : we want to generate the pom.xml first (then get the maven task to work). \n\nWe should probably separate the generation of the pom.xml from the 'deploy' task so that people can get it without doing the release on Apache (for which they don't have the creds anyway)\n\nI gather you did not get the issue Chris had with the wrong version of the ivy jar being used? ", "New version of the patch which : \n- removes the Ivy jar in the ivy dir (it is downloaded instead)\n- puts the pom generation in a separate task\n\nStill not found how to get force the script to use the jar in the ivy dir. A workaround is to call \"ant -lib ivy generate-pom\"", "the first patch worked for me.", "@Julien: for the second patch:\n{quote}\n$ ant -lib ivy generate-pom\nBuildfile: debug/nutch/build.xml\n\ngenerate-pom:\n\nBUILD FAILED\ndebug/nutch/build.xml:186: ivy:makepom doesn't support the \"templatefile\" attribute\n{quote}\n\nAlso, note that the patch name should be NUTCH-995 and not 997.\n\n", "This patch (in addition to Julien's) maps the src directory structures to maven.\nThis helps IDE users navigate, debug, and build it as a maven project.\n \nThis is what it adds to the pom.xml:\n+   <build>\n+        <sourceDirectory>src/java</sourceDirectory>\n+        <testSourceDirectory>src/test</testSourceDirectory>\n+        <testResources>\n+      <testResource>\n+        <directory>src/testresources</directory>\n+      </testResource>\n+    </testResources>\n+    </build>", "Latest patch from Julien fails on the latest branch-1.3 for me:\n\n{noformat}\nThe text leading up to this was:\n--------------------------\n|Index: build.xml\n|===================================================================\n|--- build.xml\t(revision 1126948)\n|+++ build.xml\t(working copy)\n--------------------------\nPatching file build.xml using Plan A...\nHunk #1 failed at 17.\n1 out of 1 hunks failed--saving rejects to build.xml.rej\ndone\n{noformat}\n\nWill try to fix real quick.\n", "@Chris : could be due to recent changes on the build file. The issue can be solved by simply specify 2.2.0 in the conf file and use 'ant -lib ivy ....' to force it to load the one we want. I haven't found how to get ANT/IVY to use the version we specify instead of relying on the one in the ANT local install - which in many cases is 2.1.0. \n\nWe could also add the Maven task jar in the same lib to facilitate the second part of the publishing but it is less important as this is typically done only once by the person pushing the release. ", "Thanks Jul, that helps. I'll try and update the build.xml per your instructions and test real quick to see if I can get this committed. Thanks to Gabriele too for the help and nudge!", "Gabriele, your patch doesn't cleanly apply:\n\n{noformat}\n[chipotle:nutch-dev/1.3-release/apache-nutch-1.3-svn] mattmann% patch -p0 < mvn-template-build.patch \npatch unexpectedly ends in middle of line\npatch: **** Only garbage was found in the patch input.\n{noformat}\n", "Added mvn.template in r1131455. Working through the rest of this patch.", "opps..I've issues getting proper diffs, I guess context lines are missing. The patch is to simply add the following under the mvn.template under the root element:\n{code}\nIndex: ivy/mvn.template\n===================================================================\n--- ivy/mvn.template\t(revision 0)\n+++ ivy/mvn.template\t(revision 0)\n+   <build>\n+        <sourceDirectory>src/java</sourceDirectory>\n+        <testSourceDirectory>src/test</testSourceDirectory>\n+        <testResources>\n+      <testResource>\n+        <directory>src/testresources</directory>\n+      </testResource>\n+    </testResources>\n+    </build>\n{code}", "Finishing touches applied in r1131458. I was able to publish a staging repository to repository.apache.org by running:\n\n{noformat}\nant -lib ivy deploy\n{noformat}\n\nI think we're good here. Thanks to [~jnioche] and to Gabreile for the help!\n", "Resolved per comments on the issue. Thanks to Julien and Gabriele!", "Committed in trunk revision 1131472.\n", "I'm actually still trying to build it w/o success. I've [posted on macports mlist|http://lists.macosforge.org/pipermail/macports-users/2011-June/024450.html] to figure out if it's a singlular issue to me =(\n\nWith apache solr though this is how I mavenize(install locally):\n\ncd $SOLR_HOME/..; ant get-maven-poms; mvn -N -Pbootstrap install; mvn -DskipTests install\nTheir get-mave-poms essentially copies a minimal pom.xml and then it's maven under bootstrap profile that does the work (instead of maven ant tasks). This is JFYI, I'm happy with whatever will work, but it should be noted how Solr solution is more newbie friendly.", "Sorry, to stick in the gullet but this doesn't quite work yet.\n$ ant deploy (and release) work for me too, but the next command (the purpose of this from a non-committer prospective) $ mvn install fails.\n{code}\n$ mvn install\n[INFO] Scanning for projects...\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Apache Nutch 1.3\n[INFO] ------------------------------------------------------------------------\nDownloading: http://repo1.maven.org/maven2/com/sun/jdmk/jmxtools/1.2.1/jmxtools-1.2.1.jar\nDownloading: http://repo1.maven.org/maven2/com/sun/jmx/jmxri/1.2.1/jmxri-1.2.1.jar\nDownloading: http://repo1.maven.org/maven2/javax/jms/jms/1.1/jms-1.1.jar\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 8.127s\n[INFO] Finished at: Sun Jun 05 08:44:38 CEST 2011\n[INFO] Final Memory: 4M/81M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal on project nutch: Could not resolve dependencies for project org.apache.nutch:nutch:jar:1.3: The following artifacts could not be resolved: javax.jms:jms:jar:1.1, com.sun.jdmk:jmxtools:jar:1.2.1, com.sun.jmx:jmxri:jar:1.2.1: Could not transfer artifact javax.jms:jms:jar:1.1 from/to java.net (https://maven-repository.dev.java.net/nonav/repository): No connector available to access repository java.net (https://maven-repository.dev.java.net/nonav/repository) of type legacy using the available factories WagonRepositoryConnectorFactory -> [Help 1]\n{code}\n\nI've tried 'mvn dependency:get -DrepoUrl=http://download.java.net/maven/2/ -Dartifact=com.sun.jdmk:jmxtools' and even that doesn't get the dependency. [Looking it up in maven central|http://search.maven.org/#artifactdetails%7Ccom.sun.jdmk%7Cjmxtools%7C1.2.1%7Cjar] no jar is indeed distributed for the artifact.\n\nThe trouble comes from log4j dependency, which depends on those special jars, as shown in the dependency graph below:\n!http://content.screencast.com/users/simpatico/folders/Jing/media/21de9493-fd87-45b6-9732-241583885b2f/00000046.png!\n\nA copy-paste solution is detailed [here|http://stackoverflow.com/questions/2310633/maven-dependency-log4j-error].\n\n", "Hi Gabriele,\n\nHmm...If you have a look at $NUTCH/ivy/ivy.xml, you'll note that hadoop includes exclusions:\n\n{code:xml}\n                <dependency org=\"org.apache.hadoop\" name=\"hadoop-core\" rev=\"0.20.2\"\n                        conf=\"*->default\">\n                        <exclude org=\"hsqldb\" name=\"hsqldb\" />\n                        <exclude org=\"net.sf.kosmosfs\" name=\"kfs\" />\n                        <exclude org=\"net.java.dev.jets3t\" name=\"jets3t\" />\n                        <exclude org=\"org.eclipse.jdt\" name=\"core\" />\n                        <exclude org=\"org.mortbay.jetty\" name=\"jsp-*\" />\n                        <exclude org=\"ant\" name=\"ant\" />\n                </dependency>\n{code}\n\nYet those don't appear in the pom file:\n\n{code:xml}\n                <dependency>\n                        <groupId>org.apache.hadoop</groupId>\n                        <artifactId>hadoop-core</artifactId>\n                        <version>0.20.2</version>\n                        <optional>true</optional>\n                </dependency>\n{code}\n\nIt seems we're suffering from this:\n\nhttp://old.nabble.com/makepom-and-%3Cexclusions%3E-tag-td31661179.html\n\nAny ideas?\n", "Looks like there was a fairly recent IVY issue (IVY-1294) that took care of this. I suppose we could try and include a patched version of Ivy trunk (which I don't want to depend on since it's not a release). Or, we could simply do a mod of the dependency tag for log4j with some ant-fu. Since there is a work-around though, I don't see this as a blocker, just an inconvenience.\n", "I'm !sure the excluded dependencies you mentioned in ivy are the problematic ones in maven (at least telling from their names). If you know they are transitive dependencies, then how about simply using log4j:1.2.14? That doesn't have the problematic dependencies on jmxtools, and I suspect there's no API change or feature nutch depends on in the latest log4j (BTW I'm !sure why people (still) use log4j).\n\nI agree w you on !adding a snapshot depenency.", "BTW as Julien remarked earlier adding a generate-poms (pls pls call it 'get-maven-poms' so that it's the same as Solr) also doesn't require maven ant tasks library (and is much quicker than ant deploy, and hence there are fewer possible points of failure).", "Hi Gabriele, thanks. \n\nNutch is not Solr. It doesn't have to use the same names for Ant targets that Solr does. As for the generate-poms, I think that would be a nice optimization -- later. As far as now, it's not a blocker towards generating the release POMs, and pushing to Central, which is something that the Nutch committers do. As a consumer of Nutch from Central, generating the POMs (and thus running ant deploy) shouldn't be a concern as it's not a target intended for consumers to run. \n", "{quote}Nutch is not Solr. It doesn't have to use the same names for Ant targets that Solr does.{quote}\n\nSure, it's just a wish for standards (cross-apache too). It also helps ctrl+F people looking for 'maven' and 'pom' in the build script.\n\nI as a nutch consumer wasn't really concerned with pushing to Central. I'm concerned about 'consuming' Nutch with a mvn install (@see NUTCH-892)\n", "bq. Sure, it's just a wish for standards (cross-apache too). It also helps ctrl+F people looking for 'maven' and 'pom' in the build script.\n\nThe naming of Ant targets is not something I would imagine would be standardized -- and if so -- you've arrived at Maven :-) \n\nbq. I as a nutch consumer wasn't really concerned with pushing to Central. I'm concerned about 'consuming' Nutch with a mvn install \n\nAnd as noted above, it looks like we have a workaround for that. What's stopping you from doing:\n\n{code:xml}\n<dependency>\n <groupId>org.apache.nutch</groupId>\n <artifactId>nutch</artifactId>\n <version>1.3</version>\n <exclusions>\n   <exclusion>\n      <groupId>com.sun.jdmk</groupId>\n      <artifactId>jmxtools</artifactId>\n   </exclusion>\n   <exclusion>\n      <groupId>com.sun.jmx</groupId>\n      <artifactId>jmxri</artifactId>\n   </exclusion>\n</dependency>\n{code}\n\n", "{quote} What's stopping you from doing {quote}\n\nNothing! But it'd be a pity if every newbie would try ant release/deploy; mvn install see the depressing build fail, persist and search for this issue (or ask (bother?) on the mlist), and finally add this (BTW + jms exclusion).\nif possible would rather downgrade log4j. \n\nI've tried pasting those exclusions to mvn.template but w/o luck.", "Hi Gabriele,\n\n\n\nWell, that's the point they won't have to run ant release/deploy ; mvn install. If the jars are up on Central, you simply include Nutch as a Maven dependency in your project and you're OK, Maven will download them for you. There's no need to run mvn install. \n\n\nWe can simply put it up on the Wiki -- aka create a page: How do I use Nutch in a Maven project?\n\n\nI'd rather not use Maven as a rationale to downgrade a dependency. That seems to be a little brittle to me.\n\n\nThey don't belong in mvn.template -- they belong in whatever project you're trying to include Nutch in as a Maven dependency, along with the Nutch JAR dependency block like I pasted.\n\nCheers,\nChris\n\n\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nChris Mattmann, Ph.D.\nSenior Computer Scientist\nNASA Jet Propulsion Laboratory Pasadena, CA 91109 USA\nOffice: 171-266B, Mailstop: 171-246\nEmail: chris.a.mattmann@nasa.gov\nWWW:   http://sunset.usc.edu/~mattmann/\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nAdjunct Assistant Professor, Computer Science Department\nUniversity of Southern California, Los Angeles, CA 90089 USA\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n", "I've gone ahead and created a wiki page for Nutch Maven support:\n\nhttp://wiki.apache.org/nutch/NutchMavenSupport", "Bulk close of resolved issues for 1.3.", "Integrated in Nutch-trunk #1530 (See [https://builds.apache.org/job/Nutch-trunk/1530/])\n    "], "tasks": {"summary": "Generate POM file using the Ivy makepom task ", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Generate POM file using the Ivy makepom task "}, {"question": "What is the main context?", "answer": "We currently have a pom.xml file in the SVN repository and use it for publishing our artefacts. The trouble with this is that we need to keep its content in sync with our ivy file. Instead we could us"}]}}
{"issue_id": "NUTCH-996", "project": "NUTCH", "title": "Indexer adds solr.commit.size+1 docs", "status": "Closed", "priority": "Trivial", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-05-08T01:42:37.273+0000", "updated": "2011-05-23T16:49:45.961+0000", "description": "SolrIndexer adds one additional document. This issue can be spotted easily with Solr 3.1 which accurately reports the number of added docs in the log.", "comments": ["Committed for trunk in rev. 1101279 and for 1.3 in 1101280.\nCommit.size might be a bit ambiguous here though."], "tasks": {"summary": "Indexer adds solr.commit.size+1 docs", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Indexer adds solr.commit.size+1 docs"}, {"question": "What is the main context?", "answer": "SolrIndexer adds one additional document. This issue can be spotted easily with Solr 3.1 which accurately reports the number of added docs in the log."}]}}
{"issue_id": "NUTCH-997", "project": "NUTCH", "title": "IndexingFitlers to store Date objects instead of Strings", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": "Julien Nioche", "created": "2011-05-17T13:40:07.527+0000", "updated": "2011-06-25T12:53:50.510+0000", "description": "See Nutch-985.\n\nSeveral IndexingFilters generate fields containing Dates with String values. This patch changes this so that Date objects are stored then converted into whatever type and format are required during the indexing.", "comments": ["Patch which : \n- modifies the schema for SOLR\n- BasicIndexingFilter, FeedIndexingFilter and MoreIndexingFilter generate fields with Date objects as value\n- SolrWriter normalises the String representation of Date objects ", "Good work, especially that it supercedes the other ticket. I'll try and test the patch this week.", "Well, it seems this afternoon is this week as well. Anyway, it runs and works like a charm in 1.3. +1 for committing this. We can then resolve NUTCH-985 as won't fix. If desirable i can then also commit NUTCH-994. The issue with the pom NUTCH-995 is then the last.", "Thanks for reviewing it. I had tested this yesterday as well so now that there is 2 of us I will commit it in 1.3 and trunk. We can then resolve 985 as fixed. \nI'll have a look at NUTCH-994 later this week\n", "1.3 : Committed revision 1124202\n\nCan't apply to trunk as NutchDocuments can have only String values there. Will open a separate issue for it ", "Bulk close of resolved issues for 1.3."], "tasks": {"summary": "IndexingFitlers to store Date objects instead of Strings", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "IndexingFitlers to store Date objects instead of Strings"}, {"question": "What is the main context?", "answer": "See Nutch-985.\n\nSeveral IndexingFilters generate fields containing Dates with String values. This patch changes this so that Date objects are stored then converted into whatever type and format are re"}]}}
{"issue_id": "NUTCH-998", "project": "NUTCH", "title": "index-basic should use filename if title is empty", "status": "Closed", "priority": "Minor", "reporter": "Markus Jelsma", "assignee": "Markus Jelsma", "created": "2011-05-18T11:31:34.576+0000", "updated": "2011-08-15T14:19:12.521+0000", "description": "In some cases documents are indexed with empty title fields, this is not very user friendly. Although this can be remedied in Solr using a conditional copyField in a custom update request processor i'd rather see it fixed in Nutch itself.\n\nAny thoughts? ", "comments": ["-1 : I'd rather not do that and leave to the search front end to decide on what do display when a proper title is missing. In terms of relevancy and scoring having a real title is not the same as populating one with the filename + better to know what is what.  ", "I agree. But i think there are use-cases in which it's more convenient users to toggle an optional configuration parameter to enable this behaviour. I also fix it in the front end but in cases of several frontends i have to reimplement it again and again."], "tasks": {"summary": "index-basic should use filename if title is empty", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "index-basic should use filename if title is empty"}, {"question": "What is the main context?", "answer": "In some cases documents are indexed with empty title fields, this is not very user friendly. Although this can be remedied in Solr using a conditional copyField in a custom update request processor i'"}]}}
{"issue_id": "NUTCH-999", "project": "NUTCH", "title": "Normalise String representation for Dates in IndexingFilters", "status": "Closed", "priority": "Major", "reporter": "Julien Nioche", "assignee": null, "created": "2011-05-18T12:00:24.236+0000", "updated": "2013-05-22T03:53:36.919+0000", "description": "NUTCH-997 has been applied to Nutch-1.3 so that various indexing filters store Date objects as value for fields. However in trunk NutchDocuments can have only String values which means that we will have to convert the Dates to Strings in each indexing filter.  ", "comments": ["Patch which changes the schema.xml to use the Date type + use DateUtil.getThreadLocalDateFormat().format() in Feed and More IndexingFilters", "Committed revision 1126421.", "I don't see this committed for 2.0 in solrwriter.java but this is marked as resolved.\n\n{code}\n    for(final Entry<String, List<String>> e : doc) {\n      for (final String val : e.getValue()) {\n        inputDoc.addField(solrMapping.mapKey(e.getKey()), val);\n        String sCopy = solrMapping.mapCopyKey(e.getKey());\n        if (sCopy != e.getKey()) {\n        \tinputDoc.addField(sCopy, val);\n        }\n      }\n    }\n{code}", "Reopened to keep it on the radar for 2.0.", "It was fixed but not in solrwriter. Please ignore my weekly stupidity.", "Integrated in Nutch-trunk #1530 (See [https://builds.apache.org/job/Nutch-trunk/1530/])\n    "], "tasks": {"summary": "Normalise String representation for Dates in IndexingFilters", "classification": "task", "qa_pairs": [{"question": "What is the issue about?", "answer": "Normalise String representation for Dates in IndexingFilters"}, {"question": "What is the main context?", "answer": "NUTCH-997 has been applied to Nutch-1.3 so that various indexing filters store Date objects as value for fields. However in trunk NutchDocuments can have only String values which means that we will ha"}]}}
